# config.yaml

# Default settings, backward compatibility
model:
  type: "ChatUI"    # or "hf"
  #hf_model: "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct" # old version, deprecated by hf
  chatui_model: "llama3:8b"

  # Provider model settings
  providers:
    ChatUI:
      default: "Llama-3-8B"
      options:
        - name: "Llama-3.2-1B"
          id:   "llama3.2:1b"
        - name: "Llama-3-8B"
          id:   "llama3:8b"
    
    Ollama:
      default: "Llama-3.1-8B"
      options:
        - name: "Llama-3.1-8B"
          id:   "llama3.1:8b"

    Huggingface:
      default: "Llama-3.1-8B"
      options:
        - name: "Llama-3.1-8B"
          id:   "meta-llama/Llama-3.1-8B-Instruct"

# Retriever - parameters
retrieval:
  method: hybrid
  semantic_weight: 0.5
  top_k: 100
  semantic_k: 50  
  lexical_k: 50 
  min_docs: 0
  max_docs: 10
  threshold: 0.3


# Model generation - parameters
generation:
  n_samples: 5       # 0 triggers dummy/demo
  temperature: 0.9   
  top_p: 0.95
  max_new_tokens: 300


# Uncertainty estimation - parameters
uncertainty: 
  method: "deg_mat" # default

  scaling:
    type: "quantile"
    
    # for each UE method, point to the fitted‚Äêscaler on disk
    paths:
      lexical_similarity: "data/fitted_scalers/lex_score_quantile_scaler.joblib"
      deg_mat: "data/fitted_scalers/deg_score_quantile_scaler.joblib"
      eccentricity: "data/fitted_scalers/ecc_score_quantile_scaler.joblib"

  lexical_similarity:
    metric: "rougeL"
  deg_mat:
    batch_size: 10
    device: "cpu"
    affinity: "entail"
    verbose: True
  eccentricity: 
    similarity_score: "NLI_score"
    affinity: "entail"
    verbose: True
    thres: 0.7 # paper default of 0.9, doesn't work with only 5 samples

