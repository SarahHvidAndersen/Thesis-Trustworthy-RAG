{
    "document_type": "research_paper",
    "title": "Straight Choices; The Psychology of Decision Making; 3",
    "author": "Newell Ben R.",
    "source": "raw_syllabi\\master_courses\\Decision_making\\pdf_material\\straight_choices.pdf",
    "date_published": "2022-03-31",
    "keywords": "",
    "flag": "",
    "text": "Straight Choices Straight Choices provides a fascinating introduction to the psychology of decision making, enhanced by a discussion of relevant examples of decision problems faced in everyday life. Thoroughly revised and updated throughout, this edition provides an integrative account of the psychology of decision making and shows how psychological research can help us to understand our uncertain world. The book emphasizes the relationship between learning and decision making, arguing that the best way to understand how and why decisions are made is in the context of the learning and knowledge acquisition which precedes them, and the feedback which follows. The mechanisms of learning and the structure of environments in which decisions are made are carefully examined to explore their impact on our choices. The authors then consider whether we are all constrained to fall prey to cognitive biases, or whether, with sufficient exposure, we can find optimal decision strategies and improve our decision making. This edition highlights advances made in judgement and decision making research, with additional coverage of behavioural insights, nudging, artificial intelligence, and explanation-based decision making. Written in a non-technical manner, this book is an essential read for all students and researchers in cognitive psychology, behavioural economics, and the decision sciences, as well as anyone interested in the nature of decision making. Ben R. Newell is Professor of Cognitive Psychology and Deputy Head in the School of Psychology at the University of New South Wales, Australia. David A. Lagnado is Professor in Cognitive and Decision Sciences in the Division of Psychology and Language Sciences at University College London, UK. David R. Shanks is Professor of Psychology and Deputy Dean of the Faculty of Brain Sciences at University College London, UK. \n\nStraight Choices The Psychology of Decision Making Third edition Ben R. Newell, David A. Lagnado, and David R. Shanks \nCover image: © Getty Images Third edition published 2022 by Routledge 4 Park Square, Milton Park, Abingdon, Oxon, OX14 4RN and by Routledge 605 Third Avenue, New York, NY 10158 Routledge is an imprint of the Taylor & Francis Group, an informa business © 2022 Ben R.Newell, David A. Lagnado and David R. Shanks The right of Ben R.Newell, David A. Lagnado and David R. Shanks to be identified as authors of this work has been asserted in accordance with sections 77 and 78 of the Copyright, Designs and Patents Act 1988. All rights reserved. No part of this book may be reprinted or reproduced or utilised in any form or by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying and recording, or in any information storage or retrieval system, without permission in writing from the publishers. Trademark notice : Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without intent to infringe. First edition published by Psychology Press, 2007 Second edition published by Psychology Press 2015 British Library Cataloguing-in-Publication Data A catalogue record for this book is available from the British Library Library of Congress Cataloging-in-Publication Data A catalog record has been requested for this book ISBN: 978-1-032-26781-4 (hbk) ISBN: 978-1-032-26784-5 (pbk) ISBN: 978-1-003-28989-0 (ebk) DOI: 10.4324/9781003289890 Typeset in Bembo by MPS Limited, Dehradun \nSandra, Zoila, Isabella and James Tracy Ray and Ella, Will and Miranda \n\nContents Preface to the Third Edition ix Acknowledgements xi 1 Falling off the Straight and Narrow 1 2 Decision Quality and a Historical Context 15 3 Stages of Judgement I: Discovering, Acquiring, and Combining Information 27 4 Stages of Judgement II: Feedback Effects and Dynamic Environments 49 5 Appraising Probability Judgements 62 6 Judgemental Heuristics and Biases 77 7 Explanation-Based Decision Making 98 8 Analysing Decisions I: A General Framework 112 9 Analysing Decisions II: Prospect Theory and Preference Reversals 125 10 Decisions From Experience 146 11 Decisions Across Time 157 12 Learning to Choose, Choosing to Learn 177 \n13 Optimality and Expertise 195 14 Two Systems of Judgement and Decision Making? 208 15 Emotional Influences on Decision Making 226 16 Group Decision Making 241 17 Applying Psychological Insights to the World Outside the Laboratory 255 18 Learning to Make Good Decisions: When, How, and Why (Not)? 269 References 275 Index 313 viii Contents \nPreface to the Third Edition In Straight Choices, we present a scholarly yet accessible introduction to the psychology of decision making, enhanced by a discussion of relevant examples of decision problems faced in everyday life. We provide an integrative account in which clear connections are made between empirical results and how these results can help us to understand our uncertain world. An innovative feature of Straight Choices is the emphasis on an exploration of the relationship between learning and decision making. Our thesis is that the best way to understand how and why decisions are made is in the context of the learning that precedes them and the feedback that follows them. Decisions do not emerge out of thin air but rather are informed by our prior experience, and each decision yields some information (did it work out well or badly?) that we can add to our stock of experience for future benefit. This novel approach allows us to integrate findings from the decision and learning literatures to provide a unique perspective on the psychology of decision making. A lot has happened in the field of judgement and decision making since we wrote the second edition of Straight Choices seven years ago, not least the surge in popularity of behavioural insights and ‘nudging’, as well as developments in artificial intelligence that are making decision algorithms ubiquitous. This new edition highlights recent developments with expanded coverage of those topics among many others. All of the chapters have been updated with new sections and new references reflecting the advances in understanding. We have also added a new chapter on Explanation-based decision making which discusses the growing impact of that approach in elucidating how we make complex decisions. The book is divided into 18 easily digestible chapters and the material is presented in as non-technical a manner as possible. Each chapter begins with a ‘highlights’ section and concludes with some suggestions for further reading. New to the third edition, each chapter also includes Questions for Discussion which can be used by instructors to generate class discussion, or as questions for assessments, and by students as guides for further thinking and research. The book is highly appropriate and accessible for any student with an interest in decision making – be they students of psychology, economics, social science, marketing, or business. The book should also appeal to more senior scholars of decision making, or indeed any cognitive psychologists who are \nseeking an up-to-date review of current research and are interested in the novel learning-based perspective which we provide. Throughout the book, we have also tried to emphasize the practical applications of much of the research on decision making. We hope that by reading this book you will gain a greater understanding of the psychology of how – and how well – we make decisions and that you will apply that understanding to improve your own decision making. x Preface to the Third Edition \nAcknowledgements It is, of course, impossible to acknowledge all the people who have influenced our thinking about the issues discussed in this book, so we will not attempt to name them for fear of missing some. Very special thanks are due, however to Peter Ayton and John Maule who provided insightful and very helpful criticism of a draft of the first edition, Ken Manktelow and Linden Ball who provided very useful feedback on the proposal for the second edition, and anonymous reviewers who did the same for this third edition. We would also like to thank the Editorial Team at Psychology Press for their excellent assistance throughout the publication process. Ben Newell would like to thank his co-authors for agreeing to, once again, get the band back together and write this new edition. As with the first and second editions, it has been very educational and a pleasure to have shared the experience with two such wonderful colleagues. Thanks are also due to members of the UNSW Cognition lab for many stimulating discussions about the nature of judgement and decision making over the last several years. The continuing support of the Australian Research Council is gratefully acknowledged. David Lagnado would like to thank Adam Harris and the members of the Causal Cognition Lab for continued discussion of key problems in decision making, and Toby Pilditch, Ulrike Hahn, Alice Liefgreen, Stephen Dewitt, Tamara Shengelia, Norman Fenton, and Martin Neil for research collaborations cited in the book. David Shanks would like to thank the UK Economic and Social Research Council, who for several years has provided funding for his research, and members of his research group and in particular Maarten Speekenbrink for many enlightening discussions about aspects of decision making and learning. Ben Newell, David Lagnado, and David Shanks, Sydney and London, October 2021. \n\n1 Falling Off the Straight and Narrow Chapter Highlights • An overview of the book • Insights into decisions about health, wealth, and guilt vs. innocence The cult film Donnie Darko begins with the hero Donnie narrowly surviving (or does he?) a bizarre accident. Donnie is lying in bed in his suburban family home when he is woken by a strange voice. The voice ‘leads’ him down the stairs, out of the house, and into the street. Moments later, a horrendous screeching noise signals the arrival of an aeroplane’s jet engine crashing through the roof of the house. The engine completely destroys Donnie’s bedroom. Most of us would agree that being killed by a falling jet engine is an extremely unlikely, freak occurrence. Indeed, if we were asked the question which is more likely: being killed by falling aeroplane parts or being killed by a shark? – the majority of us would probably think a shark attack more likely (Plous, 1993), but we would be wrong. According to Newsweek (Death Odds, 1990), we are 30 times more likely to be killed by falling aeroplane parts than by sharks. The reason (or reasons) why we tend to err in answering this question is just one of the many intriguing, challenging, and fundamentally important issues that are addressed in this book. Understanding the psychology of how – and how well – we make decisions can have a significant impact on how we live our lives (and how to avoid freak deaths). Even for a decision as simple as buying a book (a decision that you may well be contemplating right now), we can engage in a series of quite complex thought processes: noting the attributes of different alternatives (cost, ap- pearance, recommendations), comparing different alternatives by making ‘trade-offs’ on these attributes (e.g., this one is cheaper but it was not re- commended), and deciding how to allocate our limited resources (e.g., money for books or beer). These processes, and many more besides, can be in- vestigated in systematic ways to discover what leads us to make the decisions DOI: 10.4324/9781003289890-1 \nwe do, how we should make decisions given the preferences we have, and why our decision making sometimes goes awry. Our Approach and the Plan of this Book In this book, we provide a novel perspective on judgement and decision making along with an accessible review and integration of many of the key research findings. Our perspective is novel in that we view judgement and decision making as often exquisitely subtle and well tuned to the world, especially in situations where we have the opportunity to respond repeatedly under similar conditions where we can learn from feedback. We argue that many of the well-documented errors or biases of judgement often occur in one-shot decision situations where we do not have the chance to learn ade- quately about the environment. Focusing on errors in these one-shot situations can be a very fruitful research strategy, as the ‘heuristics and biases’ approach which has dominated the field has demonstrated (Kahneman, Slovic, & Tversky, 1982). However, the downside of this approach is that it can lead to an overly pessimistic view of human judgement and decision making (Gigerenzer, 1996; Lejarraga & Hertwig, 2021). Our perspective aims to reclaim the original reason for emphasizing errors, namely that errors can be thought of as quirks akin to visual illusions, and which reveal something about how the mind works. Like visual illusions, they arise in a system which is in general extremely accurate in its functioning. Take the sharks versus falling aeroplane parts example. In a one-shot de- cision about the likelihood of death, we might choose sharks erroneously. One explanation for such a choice is that we base our decision on the ease with which we can recall instances of people being killed by sharks or by falling aeroplane parts. Shark attacks are likely to be easier to recall – presumably because they receive wider coverage in the media – and so we answer ‘sharks’. In general, using the ease-of-recall or ‘availability’ heuristic will serve us well, but in certain situations, particularly when we are insensitive to the distribu- tion of information in the environment (i.e., insensitive to the fact that shark attacks receive more media coverage than falling aeroplane parts), we make errors (cf., Tversky & Kahneman, 1974). One of the key messages of our approach is that being given the opportunity to learn about information in the environment through repetition and feedback often gives rise to exceptionally accurate judgements and decisions. This message is pursued most directly in Chapters 10, 12, and 13, although the theme of learning runs throughout the book. Some readers might find these chapters a little more challenging than the others but we encourage you to persevere. Chapters 1 and 2 introduce many of the concepts that will be relevant to our exploration of judgement and decision making, through considering some practical decisions (e.g., What medical treatment should I choose?) and by giving a brief historical overview of the field. Chapters 3 and 4 take us on a journey through the stages of judgement from the discovery of 2 Falling Off the Straight and Narrow \ninformation to the role of feedback. Chapter 5 presents some formal ways of appraising our probability judgements and then in Chapter 6 we look at how people actually make judgements. Chapter 7 explores how people tackle more complex problems, with a focus on evidential reasoning. Moving from judge- ments to decisions, Chapter 8 presents formal methods for analysing decisions and then Chapter 9 examines how people actually make decisions and choices under uncertainty. Chapter 11 extends this analysis to examine the influence of time on decisions. Chapter 14 assesses the popular idea that there are two ‘systems’ for decision making, a deliberative one and an intuitive one that operate in rather different ways. The next three chapters provide some insights into the role that emotion plays on our decisions (Chapter 15), the way groups make decisions (Chapter 16), and an investigation of some of the more practical methods for implementing what we have learned about decision making in the laboratory to the world outside (Chapter 17). Chapter 18 revisits the key questions about when, why and how to make good decisions in light of the major findings and theories discussed in the preceding chapters. The book can be read as a whole – cover-to-cover – or if you have particular interests then the chapters are, for the most part, self-contained enough to enable you to dip in and choose the parts that appeal. Our aims are twofold: to introduce you to this exciting field and to help you improve your own decision-making skills. Decisions, Decisions… We are faced with a plethora of decisions, choices, and judgements every day and throughout our lives: what to have for lunch, where to go on holiday, what car to buy, whom to hire for a new faculty position, whom to marry, and so on. Such examples illustrate the abundance of decisions in our lives and thus the importance of understanding the how and why of decision making. Some of these decisions will have little impact on our lives (e.g., what to have for lunch); others will have long-lasting effects (e.g., whom to marry). To in- troduce many of the relevant concepts, in this first chapter we consider three important decisions that we might face in the course of our lives: 1) which medical treatment should I choose, 2) is this person guilty or innocent, and 3) how should I invest my money? For each situation, we examine some of the factors that can influence the decisions we make. We cover quite a bit of ground in these three examples so do not worry if the amount of information is rather overwhelming. The aim here is simply to give a taste of the breadth of issues that can affect our decision making. There will be ample opportunity in later chapters to explore many of these issues in more depth. Which Medical Treatment Should I Choose? Martin and Simon have just received some devastating news: they have both been diagnosed with lung cancer. Fortunately, their cancers are still at relatively Falling Off the Straight and Narrow 3 \nearly stages and should respond to treatment. Martin goes to see his doctor and is given the following information about two alternative therapies – radiation and surgery: Of 100 people having surgery, on average, 10 will die during treatment, 32 will have died by one year, and 66 will have died by five years. Of 100 people having radiation therapy, on average, none will die during treatment, 23 will die by one year and 78 will die by five years. Simon goes to see his doctor, who is different from Martin’s, and is told the following about the same two therapies: Of 100 people having surgery, on average, 90 will survive the treatment, 68 will survive for one year, and 34 will survive for five years. Of 100 people having radiation therapy, on average, all will survive the treatment, 77 will survive for one year, and 22 will survive for five years. Which treatment do you think Martin will opt for and which one will Simon opt for? If they behave in the same way as patients in a study by McNeil et al. (1982), then Martin will opt for the radiation treatment and Simon will opt for surgery. Why? You have probably noticed that the efficacy of the two treatments is equivalent in the information provided to Martin and Simon. In both cases, radiation therapy has lower long-term survival chances but no risk of dying during treatment, whereas surgery has better long-term prospects but there is a risk of dying on the operating table. The key difference between the two is the way in which the in- formation is presented to the patients. Martin’s doctor presented or framed the information in terms of mortality , namely how many people will die from the two treatments, whereas Simon’s doctor framed the information in terms of how many people will survive . It appears that the risk of dying during treatment looms larger when it is presented in terms of mortality (in the framing adopted by Martin’s doctor) than in terms of survival (in the framing chosen by Simon’s doctor) – making surgery less attractive for Martin but more attractive for Simon. This simple change in the framing of information can have a large impact on the decisions we make. McNeil et al. (1982) found that across groups of pa- tients, students and doctors, on average radiation therapy was preferred to surgery 42% of the time when the negative mortality frame was used (prob- ability of dying), but only 25% of the time when the positive survival frame (probability of living) was used (see also Tversky & Kahneman, 1981). Positive versus negative framing is not the only type of framing that can affect decisions about medical treatments. Edwards et al. (2001), in a com- prehensive review, identified nine different types of framing including those comparing verbal, numerical, and graphical presentation of risk information, manipulations of the base rate (absolute risk) of treatments, using lay versus 4 Falling Off the Straight and Narrow \nmedical terminology, and comparing the amount of information (number of factual statements) presented about choices. The largest framing effects were evident when relative, as opposed to absolute risk, information was presented to patients (Edwards et al., 2001). Relative and absolute risks are two ways of conveying information about the efficacy of a treatment, but unlike the previous example, they are not logically equivalent. Consider the following two statements adapted from an article about com- municating the efficacy of cholesterol-reducing drugs (Skolbekken, 1998; see also Gigerenzer, 2002): 1. ‘Simvastatin is proven to reduce the risk of a coronary mortality by 3.5%’. 2. ‘Simvastatin is proven to reduce the risk of a coronary mortality by 42%’. A person suffering from high cholesterol would presumably be far more willing to take the drug Simvastatin when presented with Statement 2 than when presented with Statement 1. Moreover, a doctor is more likely to prescribe the drug if presented by a pharmaceutical company with Statement 2. But is this willingness well placed? Implicit in Statement 1 is that the risk referred to is the absolute risk re- duction – that is the proportion of patients who die without taking the drug (those who take a placebo) minus the proportion who die having taken the drug (Gigerenzer, 2002). In the study discussed by Skolbekken (1998), the proportion of coronary mortalities for people taking the drug was 5.0% compared to 8.5% of those on a placebo (a reduction of 3.5%). In Statement 2, absolute risk has been replaced by relative risk reduction – that is the absolute risk reduction divided by the proportion of patients who die without taking the drug. Recall that the absolute risk reduction was 3.5% and the proportion of deaths for patients on the placebo was 8.5%, thus the 42% reduction in the statement comes from dividing 3.5 by 8.5. Table 1.1 provides some simple examples of how the relative risk reduction can remain constant while the absolute risk reduction varies widely. Not surprisingly, several studies have found much higher percentages of patients assenting to treatment when relative as opposed to absolute risk reductions are presented. For example, Hux and Naylor (1995) reported that 88% of patients assented to lipid-lowering therapy when relative risk reduction information was provided, compared with only 42% when absolute risk reduction in- formation was given. Similarly, Malenka et al. (1993) found that 79% of hy- pothetical patients preferred a treatment presented with relative risk benefits compared to 21% who chose the absolute risk option. As Edwards et al. (2001) conclude, ‘relative risk information appears much more “persuasive” than the corresponding absolute risk…data’ (p. 74), presumably just because the numbers are larger. So what is the best way to convey information about medical treatment? Zikmund-Fisher (2013) proposes a taxonomy of risk communication with seven different levels pertaining to increasing levels of precision about outcomes. Falling Off the Straight and Narrow 5 \nAccording to Zikmund-Fisher, risk formats should be tailored to patients’ needs. If the goal is simply to order risks then statements of possibility suffice (e.g., You are at risk of stroke). However, if assessments of risk magnitude and trade-offs are required, then uncertainties need to be expressed more precisely. As an example, Skolbokken (1998) advocates an approach in which one avoids using value-laden words such as risk or chance, and carefully explains the absolute rather than relative risks. Thus, for a patient suffering from high cholesterol who is considering taking Simvastatin, a doctor should tell him or her something like: ‘If 100 people like you are given no treatment for five years, 92 will live and eight will die. Whether you are one of the 92 or one of the eight, I do not know. Then, if 100 people like you take a certain drug every day for five years, 95 will live and five will die. Again, I do not know whether you are one of the 95 or one of the five’ (Skolbokken, 1998, p. 1958). Zikmund-Fisher’s work suggests that this level of precision is likely to give patients the information they need and reduce errors or biases in decision making. Is This Person Guilty or Innocent? At some point in your life, it is quite likely that you will be called for jury duty. As a member of a jury, you will be required to decide about the guilt or innocence of a defendant. The way in which juries and the individuals that comprise them arrive at their decisions has been the topic of much research (e.g., Hastie, 1993). We will explore this work in much more detail in Chapter 7; here we focus on one aspect: the impact of scientific, especially DNA, evidence on jurors’ decisions about the guilt or innocence of defendants. Faced with DNA evidence in a criminal trial many jurors are inclined to think, ‘science does not lie’; these jurors appear to be susceptible to ‘white coat syndrome’, an unquestioning belief in the power of science which generates misplaced confidence and leads to DNA evidence being regarded as infallible (Goodman-Delahunty & Newell, 2004). Indeed, some research confirms that people often overestimate the accuracy and reliability of scientific evidence (in comparison with other types of evidence, such as eyewitness testimony or confessions), thus assigning it undeserved probative value. For example, mock Table 1.1 Examples of absolute and relative risk reduction Treatment group Placebo group Survivals Mortalities Survivals Mortalities Relative risk reduction (%) Absolute risk reduction (%) 9,000 1,000 8,000 2,000 50 10 9,900 100 9,800 200 50 1 9,990 10 9,880 20 50 0.1 Note: Adapted from Skolbekken, J. A. (1998). Communicating the risk reduction achieved by cho- lesterol reducing drugs. British Medical Journal, 316 , 1956–1958. 6 Falling Off the Straight and Narrow \njurors rated blood tests as significantly more reliable than testimony from an eyewitness (Goodman, 1992). Is it simply because we have so much trust in science that DNA evidence is so compelling, or are there other reasons? Consider the 2001 trial of Wayne Edward Butler in which he was convicted of murdering Celia Douty in Brampton Island, Queensland, Australia in 1983. Police had suspected Butler for a long time but it was not until DNA profiling was used that a case was brought against him. The victim’s body had been found covered by a red towel stained with semen. DNA profiling techniques unavailable in 1983 established the probability that the semen stains were Butler’s, and on the basis of this evidence, he was charged. At trial, a forensic expert told the jury that the probability of someone else having a DNA profile that matched the one obtained from the semen (i.e., the random match probability, RMP) was one in 43 trillion . Extreme probabilities such as this make it appear that there is no margin of error – the defendant must be guilty! It is not only the fact that DNA evidence is grounded in the scientific method that makes it appear more objective and indeed even foolproof, but also the manner in which DNA evidence is presented – the probabilities cited by the DNA experts—that make this evidence so influential and persuasive to jurors (Martire, Kemp, & Newell, 2013). Clearly, these numbers sound compelling, but what does an infinitesimal RMP like 1 in 43 trillion really mean? Assuming that no errors occurred in the laboratory processing and that the probability of a random match can be stated with some legitimacy, what should a conscientious juror conclude? Often people interpret the probability not simply as the likelihood that a randomly chosen other person will have the same DNA as that found on the towel (the correct interpretation), but as the probability that the defendant was not guilty (an incorrect interpretation). The leap from a ‘match probability’ to an in- ference about the guilt of the defendant is dubbed the ‘prosecutor’s fallacy’ (Thompson & Schumann, 1987) and its commission has been observed in many trials (Koehler, 1993). One famous example of the prosecutor’s fallacy is the case of People v. Collins (1968). In this case, the prosecution secured a conviction by erroneously cal- culating a 1 in 12 million probability that a random couple would possess a series of characteristics (a female with a blond ponytail, a man with black hair and a black beard) and then fallaciously equating this probability with the probability that the accused couple did not commit the robbery. Fortunately, the original conviction was overturned in the appeals court and a stern warning was given about the dangers of ‘trial by mathematics’ (Koehler, 1993). More recent work has examined the extent to which jurors understand the match probabilities that are often presented in trials. For example, Koehler, Chia, and Lindsey (1995) gave students written summaries of a murder case that included evidence about a DNA match between the defendant and a blood trace recovered from the victim’s fingernails. One group reviewed two items of information: 1) a random match probability of 1 in 1,000,000,000, and 2) a probability of 1 in 1,000 that a human error had occurred leading to Falling Off the Straight and Narrow 7 \nan incorrect match. A second group was told simply that the combined probability of error from random matches and laboratory mistakes was 1 in 1000. Both groups studied the evidence and then provided verdicts (guilty or not guilty). What is your intuition about the result? If you are like the students in the experiment then you will have found the evidence about the ‘1 in a billion’ random match probability compelling and be more likely to judge the de- fendant ‘guilty’ faced with this number. In fact, Koehler et al. (1995) found that almost three times as many guilty verdicts were recorded in the group given that figure. This pattern of results was replicated with jurors. Figure 1.1 displays the results from the two participant populations. What is wrong with this inference? Why shouldn’t we be more con- vinced by the 1 in a billion figure? The answer lies in how we should correctly combine the random match probability and the human error probability. Koehler et al. (1995) use a baseball analogy to illustrate the problem: consider a baseball infielder who makes throwing errors less than one time in a million but makes fielding errors about two times in a hundred. The chance of the player making an error on the next attempt either because he drops the ball or because he makes a bad throw is at least two out of a hundred. If he makes an error it will almost certainly be a fielding error – but it is still an error. The important point is that even if the player reduces his throwing error rate to one in a hundred million or one in a billion it will have very little effect on his overall error rate. So, as Koehler et al. (1995) point out ‘a baseball talent scout should be no more impressed by the infielder’s throwing ability than a legal factfinder should be upon 0 10 20 30 40 50 60 University Students Jurors Percentage of guilty verdict RMP Absent RMP 1 in a billion Figure 1.1 Percentage of guilty verdicts. RMP, random match probability. Drawn using data reported in Koehler, J.J, Chia, A., & Lindsey, S. (1995). The random match probability in DNA evidence: Irrelevant and prejudicial? Jurimetrics Journal, 35, 147–163. 8 Falling Off the Straight and Narrow \nhearing the vanishingly small random match probabilities’ in DNA evi- dence at trial (p. 211). In both cases, the lower-bound threshold for error estimates is set by the greater probability – fielding errors in the case of the infielder and laboratory errors in the case of DNA evidence (see also Chapter 5 for a more in-depth treatment of this issue). The example illustrates that the human error rate – the DNA laboratory error rate – is the number that really matters. Even if there is only a 1 in 43 trillion probability of a random match, if the lab conducting the analysis makes errors of the order of one in a hundred or a thousand samples, then the random match probability is essentially irrelevant. Forensic experts often know this. Koehler’s experiments show that, unfortunately, jurors may not, and can as a result make flawed judgements about the probative value or weight to accord to DNA evidence. Consistent with the medical studies discussed above, there are ways of portraying information to jurors that can improve the decisions they make. One such modification is the presentation of DNA evidence in natural fre- quency formats (e.g., 1 in a 1,000,000 rather than probability formats (e.g., 0.0001%). In Chapter 6, we will discuss why such changes in format have a facilitative effect on decision making, and in Chapter 15, we will see how these formats can elicit different emotional reactions, but for now, we will briefly review a study relevant to the legal domain. Lindsey, Hertwig, and Gigerenzer (2003) presented jurors and law students with a sexual assault case which included expert testimony on DNA matching linking the suspect and the crime scene. One group received all information in a probability format, while a second group received identical information presented in a frequency format. Figure 1.2 displays the percentage of guilty 0 10 20 30 40 50 60 law students Jurors Percentage of guilty verdict Probabilities Natural Frequencies Figure 1.2 Percentage of guilty verdicts made by the two samples. From Lindsey, S., Hertwig, R., & Gigerenzer, G. (2003). Communicating statistical evidence. Jurimetrics Journal, 43, 147–163. Copyright 2003 by American Bar Association. Adapted with permission. Falling Off the Straight and Narrow 9 \nverdicts by the two groups of participants who received the different formats of expert numerical evidence. The results depicted in Figure 1.2 clearly show that the same statistical in- formation presented in different formats has a strong impact on the decisions made by students and jurors. When frequency formats were used there were significantly fewer guilty verdicts. Once again, it is sobering to think that such a minor format change can have a major influence on both students’ and jurors’ decisions. The results of the studies briefly reviewed here along with many others indicate that jurors’ decisions can be influenced strongly by variations in the presentation of scientific evidence (for additional examples see Martire et al., 2013). In the light of these findings, as Koehler and Macchi (2004) conclude, ‘it might be appropriate to present statistical evidence to jurors in multiple ways to minimize the influence of any particular bias’ (p. 545). This work is not only of academic interest but also has important implications for how legislation is developed regarding the presentation of scientific evidence in court (for an extensive discussion see the President’s Council on Science and Technology’s 2016 report on Forensic Science in Criminal Courts). How Should I Invest My Money? Imagine that you have just won a substantial sum of money on the lottery (if only!) and that you are faced with the enviable problem of deciding how best to invest your newfound wealth. Although you might be tempted to hide the cash under your mattress, you might also consider putting the money in the stock market – but what stocks should you invest in? The problem you face is to work out how to ‘beat’ the notoriously un- predictable stock market. Unfortunately, modern theories of finance claim that players in the financial market are well informed, smart and greedy and that it is therefore impossible to make money for nothing in the long term. This general idea is often described as the Efficient Market Hypothesis (Malkiel, 2003). However, against the background of this rather pessimistic outlook, one extremely simple rule of thumb for investment choice might be able to help you: invest in the stocks of the companies that you recognize. Borges et al. (1999) claim that such ‘recognition-based’ investment decisions can lead to much higher returns than from stocks selected by financial experts. This ‘stock selection heuristic’ states simply that when picking a subset of stocks from all those available for investment one should choose those objects in the larger set that are highly recognized. Given this formulation, it is clear that the heuristic is only useful for people who recognize some but not all of a given set of stocks. If you do not re- cognize any stocks you cannot pick highly recognized ones, and similarly if you are an expert and recognize all stocks the heuristic cannot be used. You need what Ortmann et al. (2008) described as a ‘beneficial degree of ignorance’. 10 Falling Off the Straight and Narrow \nHow well can such a simple rule perform? Borges et al. (1999) put their recognition heuristic to the test in the following way. Germans and Americans were asked to indicate those companies that they recognized from ones listed in the Standard and Poor 500 and from 298 additional stocks trading on German stock exchanges in December 1996. Four categories of participants were interviewed: Munich pedestrians, Chicago pedestrians, University of Munich finance students, and University of Chicago finance students. The former two groups were described as ‘laypersons’, the latter two as ‘experts’. The recognition responses of these four groups were then used to construct stock portfolios of highly recognized companies (those recognized by 90% or more of the participants in a group) for both domestic recognition (companies from the respondents’ own country) and international recognition (foreign companies). This resulted in eight recognition-based portfolios. Over a six-month period (December 1996 to June 1997), these high recognition portfolios were com- pared against portfolios of ‘unrecognized’ companies (those recognized by 10% or less of the participants in a group), market indices, mutual funds, and chance portfolios (constructed by selecting companies at random). Figure 1.3 displays the data from the two German groups (experts and laypeople) on the domestic stocks. It can be seen clearly that the portfolios of highly recognized stocks produced much higher returns over the six-month period than those of the unrecognized stocks. Even more impressively, the high recognition companies outperformed the market index and the managed mutual funds. The data for all the groups showed similar patterns – the re- cognized stocks always outperformed the unrecognized ones – although re- cognition did not outperform the market index or mutual funds for the US domestic recognition markets. 0 10 20 30 40 50 60 Lay people Experts Returns % Highly Recognised Unrecognized Market Index Mutual Fund Chance Portfolio Figure 1.3 Performance of the portfolios by German laypeople and experts in the German domestic market. From data reported in Borges, B., Goldstein, D. G., Ortmann, A., & Gigerenzer, G. (1999). Can ignorance beat the stock market? In G. Gigerenzer, P. M. Todd, & the ABC Research Group (Eds.), Simple heuristics that make us smart (pp. 59–72). New York: Oxford University Press. Copyright 1999 by Oxford University Press. Adapted with permission. Falling Off the Straight and Narrow 11 \nThese results appear to suggest that we can go from ‘recognition to riches’ (Ortmann et al., 2008) and that ignorance can indeed be beneficial (see also Alter & Oppenheimer, 2006, for a related ‘trick’ for beating the stock market using the pronunciation fluency of company names). And, it may not only be in the financial domain that ignorance can be good for you. For example, Goldstein and Gigerenzer (2002) reported that German students made slightly more correct inferences about the relative sizes of American cities than US students – despite the US students knowing much more about and recognizing more of the cities. Goldstein and Gigerenzer suggest that this counter-intuitive ‘less-is-more’ effect occurs because the German students were able to rely more often on the recognition heuristic (simply inferring that a recognized city is larger than an unrecognized one) than the US students. The US students, because of their higher rate of recognition, were forced to rely on other knowledge about the cities, which in some instances appeared to lead them to incorrect inferences. We will return to the recognition heuristic in Chapter 3 and scrutinize the claims about the benefits of ignorance, but for now, let us return to the question of what to do with your money. Even if you are not fortunate enough to win the lottery, a financial decision that you will probably have to make at some point in your life is how to save for your retirement. As Benartzi and Thaler (2001, 2002) have noted, there is a growing worldwide trend towards giving individuals some responsibility in making their own asset allocation decisions in defined contribution saving plans. Such devolvement of responsibility raises the question of people’s ability to make these decisions. For example, if you were asked to allocate your contributions among money markets, insurance contracts, bond funds and stock funds, how would you do it? According to Benartzi and Thaler (2001), many investors simply use a ‘1/n strategy’ in which they divide contributions evenly across the funds offered in the plan. In their first experiment, Benartzi and Thaler offered participants a plan with a bond fund and a stock fund and found that the majority of par- ticipants opted for a 50:50 split between the funds – consistent with the use of a 1/n strategy. In a follow-up study, two multiple plans were compared – one with five funds comprising four stock funds and one bond fund, the other also with five but comprising four bond funds and one stock fund. The question was do these different combinations of stock and bond funds lead to different allocations of contributions? In the plan dominated by bond funds, participants allocated 43% of their contributions to the single stock fund. However, in the plan dominated by stock funds, they allocated 68% of their contributions to the stock funds. As the 1/n rule predicts, the total amount allocated to a particular type of fund (stocks) increases when there are more such funds in the portfolio. This result shows that a simple change in the composition of the two plans gives rise to a 25% shift in the amount allocated to the riskier stock funds. Put simply, when more stocks funds were offered, more of the available re- sources were allocated to them. The result implies that participants’ attitudes to 12 Falling Off the Straight and Narrow \nrisk (i.e., exposure to fluctuations in the stock market) are highly contingent on the way in which options are presented (cf., Hilton, 2003, and Chapter 9). The ‘1/n strategy’ is a special case of a more general choice heuristic described by Read and Loewenstein (1995) as the ‘diversification heuristic’. The idea is that when people are asked to make several choices simulta- neously they tend to diversify rather than selecting the same item several times. Simonson (1990) demonstrated the use of such a heuristic in an ex- periment in which he offered students the opportunity to choose three items from a selection of snack foods (chocolate bars, crisps, etc.) to be eaten during class time each week. One group were told at the start of the first class that they had to select snacks in advance for the following three weeks, while another group was given the opportunity to select a snack at the beginning of each class. Simonson found that 64% of the participants in the advance choice condition chose three different items, whereas only 9% did so when each choice was made shortly before being consumed. This outcome is consistent with the idea that people seek variety when asked to make advance choices (Read & Loewenstein, 1995) but value variety much less when making choices about immediate consumption. This rather naive diversification strategy might be useful in many cir- cumstances but is it appropriate for investment decisions? Benartzi and Thaler (2001) conclude that using a diversification heuristic ‘can produce a reasonable portfolio [but] it does not assure sensible or coherent decision- making’ (p. 96). For example, an employee with little confidence in his or her ability to invest wisely might assume that an employer has compiled a selection of options that is sensible for his or her plan. However, the plan might offer a large number of higher-risk stock options leading the em- ployee to invest too aggressively (i.e., too heavily in stocks) which may be inappropriate for that person (Benartzi & Thaler, 2001). Summary The examples drawn from the medical, legal and financial arenas clearly show that our decisions can be greatly influenced by the way in which information is provided. Subtle differences in the way numbers are represented or options displayed can affect the decisions we make – sometimes in ways of which we are completely unaware. As we noted at the start of the chapter, our aim is to illustrate the breadth of situations in which understanding how we make decisions is relevant. The details of why some of these effects arise will be explored in the coming chapters. By investigating, systematically, these types of framing and representational issues and understanding the reasons behind the effects, you will have a better chance of keeping your decision making on ‘the straight and narrow’. But what is ‘the straight and narrow’ – what makes a decision correct or incorrect, good or bad? We turn to these questions in Chapter 2. Falling Off the Straight and Narrow 13 \nSuggested Further Reading • The rest of this book! • Goldstein, D. G., & Gigerenzer, G. (2002). Models of ecological rationality: The recognition heuristic. Psychological Review, 109 , 75–90. A detailed examination of how and when relying on recognition can lead to good decisions. • Lejarraga, T., & Hertwig, R. (2021). How experimental methods shaped views on human competence and rationality. Psychological Bulletin, 147, 535-564 . An interesting review that emphasises the importance of learning for understanding how and why we make judgements and decisions; a view very much in keeping with the themes of this book. • Tversky, A., & Kahneman, D. (1974). Judgement under uncer- tainty: Heuristics and biases. Science, 185 , 1124–1131. • Tversky, A., & Kahneman, D. (1981). The framing of decisions and the psychology of choice. Science, 211 , 453–458. Two seminal papers outlining how we are influenced by availability, among other biases, and how framing impacts choice. Questions for Discussion • Can you think of a time when a decision you made was affected by the way in which information was presented? Would you have made a different decision if you had known about framing effects? • Are forensic experts aware of the limitations of their testimony and the way in which jurors comprehend information? (see the PCAST Report for suggestions: https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/ PCAST/pcast_forensic_science_report_final.pdf). • How robust is name recognition as a stock-picking tool? How would you test it? 14 Falling Off the Straight and Narrow"
}