{
    "document_type": "research_paper",
    "title": "Bayesian hypothesis testing for psychologists: A tutorial on the Savage–Dickey method",
    "author": "\"Eric-Jan Wagenmakers; Tom Lodewyckx; Himanshu Kuriyal; Raoul Grasman\"",
    "source": "raw_syllabi\\master_courses\\Decision_making\\pdf_material\\bayesian_hypothesis_testing_for_psychologists.pdf",
    "date_published": "2010-03-03",
    "keywords": "Unavailable",
    "flag": "",
    "text": "Bayesian hypothesis testing for psychologists: A tutorial on the Savage–Dickey method Eric-Jan Wagenmakers a, * , Tom Lodewyckx b , Himanshu Kuriyal c , Raoul Grasman a a University of Amsterdam, Department of Psychology, Roetersstraat 15, 1018 WB Amsterdam, The Netherlands b Leuven University, Department of Psychology, Tiensestraat 102, B-3000 Leuven, Belgium c Indian Institute of Technology, Kharagpur, India a r t i c l e i n f o Article history: Accepted 14 December 2009 Available online 12 January 2010 Keywords: Statistical evidence Model selection Bayes factor Hierarchical modeling Random effects Order-restrictions a b s t r a c t In the ﬁeld of cognitive psychology, the p -value hypothesis test has established a stranglehold on statistical reporting. This is unfortu- nate, as the p -value provides at best a rough estimate of the evi- dence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage–Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plau- sible restrictions on the parameter priors. Practical examples dem- onstrate the method’s validity, generality, and ﬂexibility. \u0002 2009 Elsevier Inc. All rights reserved. 1. Introduction Inside every Non-Bayesian, there is a Bayesian struggling to get out – Dennis Lindley, as cited in Jaynes (2003) . How do cognitive psychologists analyze their data? Gert Gigerenzer answered this question by invoking the Freudian concept of unconscious conﬂict between the Superego, the Ego, and the Id ( Gigerenzer, 1993, 2004; Gigerenzer, Krauss, & Vitouch, 2004 ). In Gigerenzer’s analogy, the cognitive 0010-0285/$ - see front matter \u0002 2009 Elsevier Inc. All rights reserved. doi: 10.1016/j.cogpsych.2009.12.001 * Corresponding author. Fax: +31 20 639 0279. E-mail address: EJ.Wagenmakers@gmail.com (E.-J. Wagenmakers). Cognitive Psychology 60 (2010) 158–189 Contents lists available at ScienceDirect Cognitive Psychology journal homepage: www.elsevier.com/locate/cogpsych \npsychologist’s Superego wants to follow the Neyman–Pearson tradition; it seeks to contrast two well- deﬁned hypotheses (i.e., the null hypothesis and an alternative hypothesis), it operates using concepts of a -level and power, and it is generally concerned with procedures that will work well in the long run. In contrast, the cognitive psychologist’s Ego follows the Fisherian tradition; it does not posit a speciﬁc alternative hypothesis, it ignores power, and it computes a p -value that is supposed to indicate the statistical evidence against the null hypothesis. Finally, the cognitive psychologist’s Id is Bayesian , and it desperately wants to attach probabilities to hypotheses. However, this wish is suppressed by the Superego and Ego. In its continual struggle to obtain what it desires, the Id—although unable to change the statistical analysis procedures that are used—wields its inﬂuence to change and distort the interpretations that these analysis procedures afford. 1 The unconscious Freudian conﬂict has arguably resulted in widespread confusion. Researchers of- ten assume that a small p -value means that the null hypothesis is likely to be false, that a large p -value means that the null hypothesis is likely to be true, and that a 95% conﬁdence interval for a parameter l means that there is a 95% chance that l lies in the speciﬁed interval. All of these conclusions are false ( Haller & Krauss, 2002 )—this is because the conclusions are Bayesian, but the methodology that is used is not. To resolve the unconscious Freudian conﬂict and bring the statistical procedures in line with their interpretation, two courses of action present themselves. First, one can try to suppress the Id even more strongly, perhaps by rigorous statistical education and repeated warnings such as ‘‘Never use the unfortunate expression ‘accept the null-hypothesis’.” ( Wilkinson & the Task Force on Statistical Inference., 1999, p. 599 ). Second, one can explore Bayesian statistical procedures that provide exactly what the Id wants—probabilities for hypotheses. Using Bayesian procedures, one can quantify support both in favor of and against the null hypothesis ( Gallistel, 2009; Rouder, Speckman, Sun, Morey, & Iverson, 2009; Wetzels, Raaijmakers, Jakab, & Wagenmakers, 2009 ), and one can state that the prob- ability that a parameter l lies in a 95% ‘‘credible interval” is, indeed, .95. In this article, we promote the second course of action. In order to keep this article self-contained, we ﬁrst provide a brief overview of the Bayesian para- digm, with special emphasis on the difference between parameter estimation and hypothesis testing. We then describe a method, known as the Savage–Dickey density ratio, to carry out a Bayesian hypothesis test with relative ease. Next we illustrate the practical value of the Savage–Dickey method by applying it to three data sets. The ﬁrst data set is used to test the hypothesis that the sexual behav- ior of so-called virginity pledgers differs from that of non-pledgers (i.e., a hypothesis test for the equal- ity of two rates, Brückner & Bearman, 2005 ); the second data set is used to test the hypothesis that prior study of both choice alternatives improves later performance in a two-choice perceptual identi- ﬁcation task (i.e., a hypothesis test in a hierarchical within-subjects design, Zeelenberg, Wagenmakers, & Raaijmakers, 2002 ); and the third data set is used to test the hypothesis that typically developing children outperform children with ADHD on the Wisconsin card sorting test (i.e., a hypothesis test in a hierarchical between-subjects design, Geurts, Verté, Oosterlaan, Roeyers, & Sergeant, 2004 ). In these examples, we show how the Bayesian hypothesis test can be adjusted to deal with random effects and order-restrictions, both for within-subjects and between-subjects designs. WinBUGS code is presented in Appendix B and R code is available online. 2 2. Bayesian background Before outlining the Savage–Dickey method, it is important to introduce some key concepts of Bayesian inference. More detailed information can be found in Bayesian articles and books that discuss philosophical foundations ( Lindley, 2000; O’Hagan & Forster, 2004 ), computational innovations ( Gamerman & Lopes, 2006 ), and practical contributions ( Congdon, 2003; Ntzoufras, 2009 ). An in- depth discussion on the advantages of Bayesian inference, especially when compared to p -value 1 For more information about the difference between the three statistical paradigms, see for instance Christensen (2005), Hubbard and Bayarri (2003) and Royall (1997) . 2 All computer code is available from the ﬁrst author’s website, http://users.fmg.uva.nl/ewagenmakers/papers.html . E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 159 \nhypothesis testing, is beyond the scope of this article, and we instead refer the interested reader to Berger and Berry (1988b), Edwards, Lindman, and Savage (1963), Sellke, Bayarri, and Berger (2001), Wagenmakers (2007) and Wagenmakers et al. (2008) . Those familiar with Bayesian inference can safely skip to the section that introduces the Savage–Dickey method. 2.1. Bayesian parameter estimation As is customary, we introduce Bayesian parameter estimation by means of the binomial example. Assume we prepare for you a series of 10 factual true/false questions of equal difﬁculty. Interest cen- ters on your latent probability h of answering any one question correctly. In Bayesian inference, uncer- tainty with respect to parameters is—at any point in time—quantiﬁed by probability distributions. Thus, in order to get the Bayesian inference machine off the ground, we need to specify our uncer- tainty with respect to h before seeing the data. Suppose you do not know anything about the topic or about the difﬁculty level of the questions. Then, a reasonable ‘‘prior distribution”, denoted by p ð h Þ , is one that assigns equal probability to every value of h . This uniform distribution is shown by the dotted horizontal line in Fig. 1 . Now we proceed with the test, and ﬁnd that you answered 9 out of 10 questions correctly. After having seen these data, our updated knowledge about h is described by a ‘‘posterior distribution”, de- noted p ð h j s ; n Þ , where s ¼ 9 and n ¼ 10 indicate the number of successes and the number of questions, respectively. Assume that the probability of the data is given by the binomial distribution: p ð s j h ; n Þ ¼ n s \u0002 \u0003 h s ð 1 \u0002 h Þ n \u0002 s : ð 1 Þ The transition from prior p ð h Þ to posterior p ð h j s ; n Þ is then given by Bayes’ rule, p ð h j s ; n Þ ¼ p ð s j h ; n Þ p ð h Þ p ð s j n Þ : ð 2 Þ Density Density Fig. 1. Bayesian parameter estimation for binomial rate parameter h , after observing nine correct responses and one incorrect response. The mode of the posterior distribution for h is 0.9, equal to the maximum likelihood estimate, and the 95% conﬁdence interval extends from 0.59 to 0.98. The two black circles positioned at h ¼ 0 : 5 help to illustrate the Savage–Dickey density ratio discussed later. 160 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nThis equation is often presented as posterior ¼ likelihood \u0003 prior marginal likelihood : ð 3 Þ Note that the marginal likelihood (i.e., the probability of the observed data) does not involve the parameter h , and is given by a single number that ensures that the area under the posterior distribu- tion equals 1. Therefore, Eq. (2) is often written as p ð h j s ; n Þ / p ð s j h ; n Þ p ð h Þ ; ð 4 Þ which says that the posterior is proportional to the likelihood times the prior. The solid line in Fig. 1 shows the posterior distribution for h , which is obtained when the uniform prior is updated with data s ¼ 9 and n ¼ 10. The central tendency of a posterior distribution is often summarized by its mean, median, or mode. Note that with a uniform prior, the mode of a posterior distribution coincides with the classical maximum likelihood estimate or MLE, ^ h ¼ s = n ¼ 0 : 9 ( Myung, 2003 ). The spread of a posterior distribution is most easily captured by a Bayesian x % conﬁdence inter- val that extends from the ð x = 2 Þ th to the ð 100 \u0002 x = 2 Þ th percentile of the posterior distribution. For the posterior distribution in Fig. 1 , a 95% Bayesian conﬁdence interval for h extends from 0.59 to 0.98. In contrast to the classical or orthodox conﬁdence interval, the Bayesian conﬁdence interval has a direct and intuitive interpretation: after observing the data, we can be 95% conﬁdent that the true value of h lies in between 0.59 and 0.98. Now suppose we design a new set of ﬁve questions, of equal difﬁculty as before. How can we for- malize our expectations about your performance on this new set? In other words, how can we use the posterior distribution p ð h j n ¼ 10 ; s ¼ 9 Þ —which after all represents everything that we know about h from the old set—to predict the number of correct responses out of the new set of n new ¼ 5 questions? The mathematical solution is to integrate over the posterior, p ð s new j n new ¼ 5 Þ ¼ Z 1 0 p ð s new j h ; n new ¼ 5 Þ p ð h j n ¼ 10 ; s ¼ 9 Þ d h ; ð 5 Þ where s new is the predicted number of correct responses out of the additional set of ﬁve questions. Computationally, one may think of this procedure as repeatedly drawing a random value h i from the posterior, and using that value to every time determine a single s new i by means of Eq. (1) . The end result, p ð s new j n new ¼ 5 Þ , is the predictive density of the possible number of correct responses in the additional set of ﬁve questions. The important point is that by integrating over the posterior, all predictive uncertainty is taken into account. In contrast, much of classical inference relies on the ‘‘plug-in principle” that in this case would lead us to predict p ð s new j n new ¼ 5 Þ solely based on ^ h , the maximum likelihood estimate. Plug-in procedures ignore uncertainty in h , and hence lead to predic- tions that are overconﬁdent, that is, predictions that are less variable than they should be ( Aitchison & Dunsmore, 1975 ). 3 You are now presented with the new set of ﬁve questions. You answer 3 out of 5 correctly. How do we combine this new information with the old? Or, in other words, how do we update our knowledge of h ? Consistent with intuition, Bayes’ rule entails that the prior that should be updated based on your performance for the new set is the posterior that was obtained based on your performance for the old set. Or, as Lindley put it, ‘‘today’s posterior is tomorrow’s prior” ( Lindley, 1972, p. 2 ). When all the data have been collected, however, the precise order in which this was done is irrelevant; the results from the 15 questions could have been analyzed as a single batch, they could have been analyzed sequen- tially, one-by-one, they could have been analyzed by ﬁrst considering the set of 10 questions and next the set of 5, or vice versa. For all these cases, the end result, the ﬁnal posterior distribution for h , is identical. This again contrasts with classical inference, in which inference for sequential designs is dif- ferent from that for non-sequential designs (for a discussion, see e.g., Anscombe, 1963 ). 3 It should be acknowledged that classical statisticians can account for uncertainty in the estimation of h by repeatedly drawing a bootstrap sample from the data, calculating the associated bootstrap MLE, and ﬁnding the corresponding prediction for s new (e.g., Wagenmakers, Ratcliff, Gomez, & Iverson, 2004 ). E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 161 \nThus, a posterior distribution describes our uncertainty with respect to a parameter of interest, and the posterior is useful—or, as a Bayesian would have it, necessary—for probabilistic prediction and for sequential updating. Unfortunately, the posterior distribution or any of its summary measures can only be obtained in closed form for a restricted set of relatively simple models. To illustrate in the case of our binomial example, the uniform prior is a so-called beta distribution with parameters a ¼ 1 and b ¼ 1, and when combined with the binomial likelihood this yields a posterior that is also a beta dis- tribution, be it with parameters a þ s and b þ n \u0002 s . In simple conjugate cases such as these, where the prior and the posterior belong to the same distributional family, it is possible to obtain closed form solutions for the posterior distribution, but in other more interesting cases it is not. For a long time, researchers did not know how to proceed with Bayesian inference when the pos- terior could not be obtained in close form. As a result, practitioners interested in models of realistic complexity did not much use Bayesian inference. This situation changed with the advent of com- puter-driven sampling methodology generally known as Markov chain Monte Carlo (i.e., MCMC; e.g., Gamerman & Lopes, 2006; Gilks, Richardson, & Spiegelhalter, 1996 ). Using MCMC techniques such as Gibbs sampling or the Metropolis–Hastings algorithm, researchers can now directly sample se- quences of values from the posterior distribution of interest, foregoing the need for closed form ana- lytic solutions. At the time of writing, the adage is that Bayesian models are limited only by the user’s imagination. To provide a concrete and simple illustration of Bayesian inference using MCMC, we revisit our binomial example of 9 correct responses out of 10 questions, and the associated inference problem for h , the probability of answering any one question correctly. Throughout this article, we use the general-purpose WinBUGS program ( Lunn, Thomas, Best, & Spiegelhalter, 2000; Lunn, Spiegelhalter, Thomas, & Best, 2009 ; an introduction for psychologists is given by Sheu & O’Curry, 1998 ) that allows the user to specify and ﬁt models without having to hand-code the MCMC algorithms. Although WinBUGS does not work for every application, it will work for most applications in the ﬁeld of psychology. The WinBUGS program is easy to learn and is supported by a large community of active researchers. 4 The WinBUGS program requires the user to construct a ﬁle that contains the model speciﬁcation, a ﬁle that contains initial values for the model parameters, and a ﬁle that contains the data. The model speciﬁcation ﬁle is most important. For our binomial example, we set out to obtain samples from the prior and the posterior of h . The associated WinBUGS model speciﬁcation code is three lines long: model { theta \u0004 dbeta(1,1) # the uniform prior for updating by the data k \u0004 dbin(theta,n) # the data; in our example, k = 9 and n = 10 thetaprior \u0004 dbeta(1,1) # a uniform prior not for updating } In this code, the ‘‘ \u0004 ” or twiddle symbol denotes ‘‘is distributed as”, dbeta(a,b) indicates the beta distribution with parameters a and b , and dbin(theta,n) indicates the binomial distribution with rate theta and n observations. These and many other distributions are build in to the WinBUGS system. The ‘‘#” or hash sign is used for commenting out what should not be compiled. As WinBUGS is a declarative language, the order of the three lines is inconsequential. When this code is executed, the user obtains a sequence of samples (i.e., an MCMC chain) from the posterior p ð h j D Þ and a sequence of samples from the prior p ð h Þ . In more complex models, it may take some time before the chain converges from its starting value to what is called its stationary distribu- tion. To make sure that we only use those samples that come from the stationary distribution (and are hence unaffected by the starting values) it is good practice to discard the ﬁrst samples as ‘‘burn-in”, and to diagnose convergence by running multiple chains. For instance, Fig. 2 shows the ﬁrst 100 iterations for three chains that were set up to draw values from the posterior for h . The three chains are almost indistinguishable, and they do not have slow 4 For more information on WinBUGS see http://www.mrc-bsu.cam.ac.uk/bugs/ . 162 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nupward or downward drift; these are two qualitative signs that the chains have converged and that samples are being drawn from the posterior distribution. Quantitative measures for diagnosing con- vergence are also available (e.g., the Gelman and Rubin (1992) b R statistic, that compares within-chain to between-chain variability; for more recommendations regarding convergence see e.g., Gamerman and Lopes (2006) , Gelman (1996) , and Gelman and Hill (2007) ). After assuring ourselves that the chains have converged, we can use the sampled values to plot a histogram, construct a density estimate, and compute values of interest. To illustrate, the three chains from Fig. 2 were run for 3000 iterations each, for a total of 9000 samples for the prior and the posterior of h . Fig. 3 plots histograms 5 for the prior (i.e., dotted line) and the posterior (i.e., thick solid line). In addition, the thin solid lines represent logspline nonparametric density estimates ( Stone, Hansen, Koop- erberg, & Truong, 1997 ). The mode of the logspline density estimate for the posterior of h is 0.89, whereas the 95% conﬁdence interval is (0.59, 0.98), matching the analytical result shown in Fig. 1 . Of course, this example represents an ideal scenario; in more complicated models, convergence might be obtained only after many MCMC iterations—that is, chains may move very slowly from their starting point to the stationary distribution. This problem is often easy to diagnose by running multiple chains with overdispersed starting values. Another problem is that, even when the chains have arrived at the posterior distribution, consecutive samples might be highly correlated. This is less worrisome than the problem of nonconvergence (after all, the samples are draws from the correct posterior distribution), but it does mean that more samples need to be collected before the entire posterior is adequately covered. This problem is easy to diagnose by computing the auto- correlation of the chains. A relatively high autocorrelation suggests that we need to draw relatively many samples. Thus, for complex models it is important to use MCMC algorithms that are efﬁcient, reliable, and quick. This is currently an active area of research. Nevertheless, the fundamental theoretical obstacles for Bayesian parameter estimation have been overcome. In ﬁelds such as statistics, artiﬁcial intelligence, and machine learning, MCMC algorithms are now used on a routine basis. MCMC Iteration Fig. 2. Three MCMC chains for binomial rate parameter h , after observing nine correct responses and one incorrect response. 5 These histograms were constructed such that the total area under each histogram equals one. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 163 \n2.2. Bayesian hypothesis testing Up to this point we have concerned ourselves with parameter estimation, implicitly taking the appropriateness of the underlying model for granted. In much of social science, however, researchers entertain more than just a single statistical model. In fact, the statistical models often represent com- peting theories or hypotheses, and the focus of interest is on which substantive theory or hypothesis is more correct, more plausible, and better supported by the data. For example, researchers might want to know whether the improvement of performance with practice follows a power function or an expo- nential function. As another example, we might want to know the extent to which your performance in our test (i.e., 9 correct answers out of 10 questions) is consistent with the hypothesis that you were just guessing. This may involve a test of M 1 : h ¼ 0 : 5 versus M 2 : h – 0 : 5. The fundamental and general Bayesian solution to the foregoing model selection of hypothesis test- ing problems is as follows. For simplicity, assume that you contemplate two alternative accounts of the data, M 1 and M 2 , and that you seek to quantify model uncertainty in terms of probability. Consider ﬁrst M 1 . Bayes’ rule dictates how your prior probability of M 1 ; p ð M 1 Þ , is updated through the observed data D to give the posterior probability of M 1 ; p ð M 1 j D Þ : p ð M 1 j D Þ ¼ p ð M 1 Þ p ð D j M 1 Þ p ð M 1 Þ p ð D j M 1 Þ þ p ð M 2 Þ p ð D j M 2 Þ : ð 6 Þ In the same way, one can calculate the posterior probability of M 2 ; p ð M 2 j D Þ . The ratio of these posterior probabilities is given by p ð M 1 j D Þ p ð M 2 j D Þ ¼ p ð M 1 Þ p ð M 2 Þ p ð D j M 1 Þ p ð D j M 2 Þ : ð 7 Þ This equation shows that the change from prior odds p ð M 1 Þ = p ð M 2 Þ to posterior odds p ð M 1 j D Þ = p ð M 2 j D Þ is determined entirely by the ratio of the marginal likelihoods p ð D j M 1 Þ = p ð D j M 2 Þ . This ratio is generally Density Density Fig. 3. MCMC-based Bayesian parameter estimation for binomial rate parameter h , after observing nine correct responses and one incorrect response. The thin solid lines indicate the ﬁt of a nonparametric logspline density estimator. Based on this density estimator, the mode of the posterior distribution for h is approximately 0.89, and the 95% conﬁdence interval extends from 0.59 to 0.98, closely matching the analytical results from Fig. 1 . The two black circles positioned at h ¼ 0 : 5 again help to illustrate the Savage–Dickey density ratio discussed later. 164 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nknown as the Bayes factor ( Jeffreys, 1961 ), and the Bayes factor, or the log of it, is often interpreted as the weight of evidence coming from the data ( Good, 1985 ). A hypothesis test based on the Bayes factor supports the model under which the observed data are most likely (for details see Berger & Pericchi, 1996; Bernardo & Smith, 1994, chap. 6; Klugkist, Laudy, & Hoijtink, 2002, cha 7; Klugkist et al., 2005a; Kass & Raftery, 1995; MacKay, 2003; Myung & Pitt, 1997; O’Hagan, 1995 ). Therefore, the Bayes factor represents ‘‘the standard Bayesian solution to the hypothesis testing and model selection problems” ( Lewis & Raftery, 1997, p. 648 ); in the following, we will use ‘‘Bayesian hypothesis test” as a shortcut for ‘‘a hypothesis test based on the Bayes factor”. Thus, when the Bayes factor for M 1 versus M 2 equals 2 (i.e., BF 12 ¼ 2), this means that the data are twice as likely to have occurred under M 1 than under model M 2 . When the prior odds are equal, such that M 1 and M 2 are equally likely a priori, the Bayes factors can be converted to posterior probabilities: p ð M 1 j D Þ ¼ BF 12 = ð BF 12 þ 1 Þ . This means that BF 12 ¼ 2 translates to p ð M 1 j D Þ ¼ 2 = 3. To illustrate, consider again our binomial example of 9 correct responses out of 10 questions, and the test between two models for performance: guessing (i.e., M 1 : h ¼ 0 : 5) versus not guessing (i.e., M 2 : h – 0 : 5). From Eq. (1) , the marginal likelihood for M 1 ; p ð D j M 1 Þ , is simply 10 9 \u0002 \u0003 1 2 \u0004 \u0005 10 . The marginal likelihood for model M 2 is more difﬁcult to calculate, as h is a free parameter. In general, the marginal likelihood is obtained by integrating out the model parameters in accordance with the law of total probability: p ð D j M 2 Þ ¼ Z p ð D j h ; M 2 Þ p ð h j M 2 Þ d h : ð 8 Þ This means that the marginal likelihood is computed by averaging the likelihood over the prior; conceptually, the likelihood is evaluated for every possible parameter value, weighted with its prior plausibility, and added to a summed total. When we again use the uniform distribution for h as a prior, such that p ð h j M 2 Þ \u0004 Beta ð 1 ; 1 Þ , then Eq. (8) famously simpliﬁes to p ð D j M 2 Þ ¼ 1 = ð n þ 1 Þ . Thus, in our binomial example, BF 12 ¼ 10 9 \u0002 \u0003 1 2 \u0004 \u0005 10 ð n þ 1 Þ \u0005 0 : 107. This means that the data are 1 = 0 : 107 \u0005 9 : 3 times more likely under M 2 than they are under M 1 . With unit prior odds, the posterior probability for M 1 is 0 : 107 = ð 0 : 107 þ 1 Þ \u0005 : 10, which means that the complementary posterior probability for M 2 is approximately .90. These are probabilities assigned to hypotheses, and they are exactly what researchers (or, in Gigerenzer’s Freudian analogy, their Ids) want to know about. Posterior model probabilities are not just necessary to quantify our degree of belief or preference for the candidate models under consideration. They are also necessary for Bayesian model averaging (e.g., Draper, 1995; Hoeting, Madigan, Raftery, & Volinsky, 1999; Madigan & Raftery, 1994 ). For in- stance, in a regression context we might have one model, M 1 , that predicts a certain post-surgery survival rate by gender, age, weight, and history of smoking. A second model, M 2 , includes two addi- tional predictors, namely body-mass index and ﬁtness. We compute posterior model probabilities and ﬁnd that p ð M 1 j D Þ ¼ : 6 and consequently p ð M 2 j D Þ ¼ : 4. For a given patient, M 1 predicts a survival rate of 90%, and M 2 predicts a survival rate of 80%. What is our best prediction for our patient’s sur- vival rate? It is tempting to base our prediction solely on M 1 , which is after all the preferred model. However, this would ignore the uncertainty inherent in the model selection procedure, and it would ignore the very real possibility that the best model is M 2 , according to which the survival rate is 10% lower than it is for M 1 . The Bayesian solution is to weight the two competing predictions with their associated posterior model probabilities, fully taking into account the uncertainty in the model selection procedure. In our example, the model-averaged prediction for survival rate would be : 6 \u0003 90 % þ : 4 \u0003 80 % ¼ 86 % . 2.2.1. Additional advantages of Bayesian hypothesis testing We have seen how Bayes factors and posterior model probabilities describe the relative support or preference for a set of candidate models, and how they can be used for model averaged predictions. Other advantages of Bayesian hypothesis testing include the following ( Wagenmakers, Lee, Lode- wyckx, & Iverson, 2008; see also Berger & Pericchi, 2001; Dennis, Lee, & Kinnell, 2008; Kass & Raftery, 1995 ): E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 165 \n1. Coherence is guaranteed. Suppose we have a set of three candidate models, M 1 ; M 2 , and M 3 . As p ð D j M 1 Þ p ð D j M 3 Þ ¼ p ð D j M 1 Þ p ð D j M 2 Þ p ð D j M 2 Þ p ð D j M 3 Þ ; ð 9 Þ this means that BF 13 ¼ BF 12 \u0003 BF 23 . For instance, when the data are ﬁve times as likely to occur under M 1 than under M 2 , and seven time as likely under M 2 than under M 3 , it follows that the data are 5 \u0003 7 ¼ 35 times as likely under M 1 than under M 3 . No comparable result exists in clas- sical statistics. 2. Parsimony is automatically rewarded. The main challenge of hypothesis testing or model selection is to identify the model with the best predictive performance (e.g., Myung, Forster, & Browne, 2000; Wagenmakers & Waldorp, 2006 ). However, it is not immediately obvious how this should be done; complex models will generally provide a better ﬁt to the observed data than simple models, and therefore one cannot simply prefer the model with the best ‘‘goodness-of-ﬁt”—such a strategy would lead to overﬁtting. Intuition suggest that this tendency for overﬁtting should be counteracted by putting a premium on simplicity. This intuition is consistent with the law of par- simony or ‘‘Ockham’s razor” which states that, when everything else is equal, simple models are to be preferred over complex models ( Jaynes, 2003, chap. 20; Myung & Pitt, 1997 ). Formal model selection methods try to quantify the tradeoff between goodness-of-ﬁt and parsi- mony. Many of these methods measure a model’s overall performance by the sum of two com- ponents, one that measures descriptive accuracy and one that places a premium on parsimony. The latter component is also known as the Ockham factor ( MacKay, 2003, chap. 28 ). For many model selection methods, the crucial issue is how to determine the Ockham factor. One of the attractive features of Bayesian hypothesis testing is that it automatically determines the model with the best predictive performance – Bayesian hypothesis testing therefore incorporates what is known as an automatic Ockham’s razor ( Myung & Pitt, 1997 ). To see why this is the case, consider that every statistical model makes a priori predictions. Com- plex models have a relatively large parameter space, and are therefore able to make many more predictions and cover many more eventualities than simple models. However, the drawback for complex models is that they need to spread out their prior probability across their entire param- eter space. In the limit, a model that predicts almost everything has to spread out its prior prob- ability so thinly that the occurrence of any particular event will not greatly add to that model’s credibility. As shown by Eq. (8) , the marginal likelihood for a model M is calculated by averaging the likelihood p ð D j h ; M Þ over the prior p ð h j M Þ . When the prior is very spread out, it will occupy a relatively large part of the parameter space in which the likelihood is almost zero, and this greatly decreases the average or marginal likelihood. Consider for instance the situation shown in Fig. 1 . The prior on the rate parameter h was assumed to be uniform from 0 to 1, h \u0004 U ½ 0 ; 1 \u0006 . A different, more parsimonious model could take into account the prior knowledge that h is unli- kely to be lower than 0.5, as this would mean that your ability would be lower than chance (recall that the questions were true/false, such that the absence of any knowledge corresponds to h ¼ 0 : 5). This more informed model could be instantiated as h \u0004 U ½ 0 : 5 ; 1 \u0006 , and we could then use the Bayes factor to compute the relative plausibility of model M 1 : h \u0004 U ½ 0 ; 1 \u0006 versus M 2 : h \u0004 U ½ 0 : 5 ; 1 \u0006 . The more complex model M 1 kept its options open by assigning half of its prior mass to values for h that are smaller than 0.5. This could have been advantageous when the data would have turned out differently (e.g., 1 correct answer instead of 9). As it is, the values of h that are smaller than 0.5 are very unlikely; hence, the average likelihood is almost twice as high for the parsimonious model M 2 than it is for the more complex model M 1 . 3. Evidence can be obtained in favor of the null hypothesis. Bayesian hypothesis testing allows one to obtain evidence in favor of the null hypothesis. Because theories and models often predict the absence of a difference, it is vital for scientiﬁc progress to be able to quantify evidence in favor of the null hypothesis (e.g., Gallistel, 2009; Rouder et al., 2009; Wetzels et al., 2009 ). In the ﬁeld of visual word recognition, for instance, the entry-opening theory ( Forster, Mohan, & Hector, 2003 ) predicts that masked priming is absent for items that do not have a lexical representation; 166 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nAnother example from that literature concerns the work by Bowers, Vigliocco, and Haan (1998) , who have argued that priming effects are equally large for words that look the same in lower and upper case (e.g., kiss/KISS) or that look different (e.g., edge/EDGE), a ﬁnding supportive of the hypothesis that priming depends on abstract letter identities. A ﬁnal example comes from the ﬁeld of recognition memory, where Dennis and Humphreys’ bind cue decide model of episodic memory (BCDMEM) predicts the absence of a list-length effect and the absence of a list-strength effect ( Dennis & Humphreys, 2001 ). This radical prediction of a null effect allows researchers to distinguish between context-noise and item-noise theories of infer- ence in memory ( Dennis et al., 2008 ). In Bayesian statistics, the null hypothesis has no special status, and evidence for it is quantiﬁed just as it is for any other hypothesis. In classical statistics, support for informative predictions from null hypothesis can only be indirect. 4. Evidence may be monitored as it accumulates. Bayesian hypothesis testing allows one to monitor the evidence as the data come in ( Berger & Berry, 1988a ). In contrast to frequentist inference, Bayesian inference does not require special corrections for ‘‘optional stopping” ( Wagenmakers, 2007 ). Consider, for instance, a hypothetical experiment on the neural substrate of dissociative identity disorder. In this experiment, the researcher Lisa has decided in advance to use functional mag- netic resonance imaging (fMRI) to test 30 patients and 30 normal controls. Lisa inspects the data after 15 participants in each group have been tested, and ﬁnds that the results convincingly demonstrate the pattern she hoped to ﬁnd. Unfortunately for Lisa, she cannot stop the experi- ment and claim a signiﬁcant result, as she would be changing the sampling plan halfway through and be guilty of ‘‘optional stopping”. She has to continue the experiment, wasting not just her time and money, but also the time and efforts of the people who undergo needless testing. In contrast, for Bayesian hypothesis testing there is nothing wrong with gathering more data, examining these data, and then deciding whether or not to stop collecting new data – no special corrections are needed. As stated by Edwards et al. (1963) , ‘‘( \u0007 \u0007 \u0007 ) the rules governing when data collection stops are irrelevant to data interpretation. It is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience.” ( Edwards et al., 1963, p. 193 ). 2.2.2. Challenges for Bayesian hypothesis testing Bayesian hypothesis testing using Bayes factors (Eq. (7) ) faces two main challenges, one conceptual and one computational. The conceptual challenge is that the Bayesian hypothesis test is acutely sen- sitive to the speciﬁcation of the prior distributions for the model parameters (e.g., Bartlett, 1957; Liu & Aitkin, 2008 ). This distinguishes hypothesis testing from parameter estimation, in which the data quickly overwhelm the prior; the accumulation of data forces prior opinions that are very different to converge to posterior opinions that are very similar. For parameter estimation then, the choice of a prior distribution is not really all that critical unless there are very few data points. In contrast, for Bayesian hypothesis testing the prior distributions are crucial and have a lasting impact. This occurs because the marginal likelihood is an average taken with respect to the prior. Con- sider for instance the prior for the mean l of a Normal distribution with known variance. One might be tempted to use an ‘‘uninformative” prior, one that does not express much preference for one value of l over the other. One such vague prior could be a Normal distribution with mean zero and variance 10,000. But, from a marginal likelihood perspective, this prior is consistent with almost any value of l . When one hedges one’s bets to such an extreme degree, the Bayes factor is likely to show a pref- erence for a simple model (e.g., one in which l ¼ 0), even when the data appear wildly inconsistent with it. The main problem here is not that the Bayesian hypothesis test corrects for model complexity as manifested in the prior distribution. This is the automatic Ockam’s razor that is an asset, not a liability, of the Bayesian hypothesis test. Instead, the problem seems to be that researchers have only a vague idea of the vagueness of their prior knowledge, or that researchers seek to use a prior that is ‘‘objec- tive”, and uses as little prior knowledge as possible. When the vagueness of the prior is arbitrary, so are the results from the Bayesian hypothesis test. When the vagueness of the prior is purposefully E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 167 \nlarge, the results from the Bayesian hypothesis test tend to indicate a preference for the simple model, regardless of the data. In order to increase the robustness of Bayesian hypothesis testing to the vagueness of the prior, several procedures have been proposed, including the local Bayes factor ( Smith & Spiegelhalter, 1980 ), the intrinsic Bayes factor ( Berger & Mortera, 1999; Berger & Pericchi, 1996 ), the fractional Bayes factor ( O’Hagan, 1995 ), and the partial Bayes factor ( O’Hagan, 1995 ; for a summary see Gill, 2002, chap. 7 ). The idea of the partial Bayes factor is to sacriﬁce a small part of the data to obtain a posterior that is robust to the various priors one might entertain. The Bayes factor is then calculated by integrat- ing the likelihood over this posterior instead of over the original prior. Procedures such as these are still undergoing further development and deserve more study. The problem of vague priors is particularly evident for parameters that can take on values across the entire real line, such as the mean l of a Normal distribution. We believe that in such cases, when- ever possible, the construction of a prior should be guided by the substantive knowledge in the do- main of application. As Dennis Lindley has pointed out repeatedly, l is only a Greek letter, an abstraction that may obscure the fact that it refers to something about which we have detailed prior knowledge. When l stands for a person’s weight, few rational people would assign l an ‘‘uninforma- tive” Normal prior distribution with mean zero and variance 10,000. In this paper, we sidestep this conceptual challenge to some extent, as we focus completely on dis- crete data problems (i.e., those that involve a hit or a miss, a success of a failure, a yes or a no). In such cases, a perfectly plausible prior assigns equal mass to every value of the underlying rate parameter h . In some cases, we use order-restrictions and assign equal mass to every value of h greater than .5. We feel that in the absence of detailed prior knowledge, this assumption is reasonable. Note, however, that our approach in this paper is entirely general; when you are willing to defend and use a different prior, you are free to do so. The second challenge for Bayesian hypothesis testing—the one that is the focus of this article—is that the marginal likelihood and the Bayes factor are often quite difﬁcult to compute. Earlier, we saw that with a uniform prior on the binomial rate parameter h (i.e., p ð h j M Þ \u0004 Beta ð 1 ; 1 Þ ), the mar- ginal likelihood R p ð D j h ; M Þ p ð h j M Þ d h simpliﬁes to 1 = ð 1 þ n Þ . However, in all but a few simple mod- els, such simpliﬁcations are impossible. In order to be able to compute the marginal likelihood or the Bayes factor for more complex models, a series of different computational methods has been developed. A recent summary lists as many as 15 different methods ( Gamerman & Lopes, 2006, chap. 7 ). For instance, one method computes the marginal likelihood through what is called the candidates’ formula ( Besag, 1989 ) or the basic marginal likelihood identity ( Chib, 1995; Chib & Jeliazkov, 2001 ). One simply exchanges the roles of posterior and marginal likelihood to obtain p ð D Þ ¼ p ð D j h Þ p ð h Þ p ð h j D Þ ; ð 10 Þ which holds for any value of h . When the posterior is available analytically, one only needs to plug in a single value of h and obtain the marginal likelihood immediately. This method can however also be applied when the posterior is only available through MCMC output, either from the Gibbs sampler ( Chib, 1995 ) or the Metropolis–Hastings algorithm ( Chib & Jeliazkov, 2001 ). Another method to compute the marginal likelihood is to repeatedly sample parameter values from the prior, calculate the associated likelihoods, and then take the likelihood average. When the poster- ior is highly peaked compared to the prior—as will happen with many data or with a medium-sized parameter space—it becomes necessary to employ more efﬁcient sampling methods, with a concom- itant increase in computational complexity. Finally, it is also possible to compute the Bayes factor directly, without ﬁrst calculating the con- stituent marginal likelihoods. The basic idea is to generalize the MCMC sampling routines for param- eter estimation to incorporate a ‘‘model indicator” variable. In the case of two competing models, the model indicator variable k , say, can take on two values—for instance, k ¼ 1 when the sampler is in model M 1 , and k ¼ 2 when the sampler is in model M 2 . The Bayes factor is then estimated by the relative frequency with which k ¼ 1 versus k ¼ 2. This MCMC approach to model selection 168 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nis called transdimensional MCMC (e.g., Sisson, 2005 ), an approach that encompasses both reversible jump MCMC Green, 1995 and the product space technique ( Carlin & Chib, 1995; Lodewyckx et al., 2009 ). Almost all of these computational methods suffer from the fact that they become less efﬁcient and more difﬁcult to implement as the underlying models become more complex. We now turn to an alternative method, whose implementation is extremely straightforward. The methods’ main limita- tion is that it applies only to nested models, a limitation that also holds for p -values. 3. The Savage–Dickey density ratio In the simplest classical hypothesis testing framework, one contemplates two models: the null hypothesis, that ﬁxes one of its parameters to a pre-speciﬁed value of substantive interest, say H 0 : / ¼ / 0 ; and the alternative hypothesis, in which that parameter is free to vary, say H 1 : / – / 0 . Hence, the null hypothesis is nested under the alternative hypothesis, that is, H 0 can be obtained from H 1 by setting / equal to / 0 . Note that in the classical framework, H 0 is generally a sharp null hypoth- esis, or a ‘‘point null”. That is, the null hypothesis states that / is exactly equal to / 0 . For example, in the binomial example from Fig. 1 you answered 9 out of 10 questions correctly. Were you guessing or not? The classical and the Bayesian framework deﬁne H 0 : h ¼ : 5 as the null hypothesis for chance performance. The alternative hypothesis under which H 0 is nested could be de- ﬁned as H 1 : h – : 5, or, more speciﬁcally, as H 1 : h \u0004 Beta ð 1 ; 1 Þ , which states that h is free to vary from 0 to 1, and that it has a uniform prior distribution as shown in Fig. 1 . For the binomial example, the Bayes factor for H 0 versus H 1 could be obtained by analytically integrating out the model parameter h . However, the Bayes factor may likewise be obtained by only considering H 1 , and dividing the height of the posterior for h by the height of the prior for h , at the point of interest. This surprising result was ﬁrst published by Dickey and Lientz (1970) , who attributed it to Leonard J. ‘‘Jimmie” Savage. The result is now generally known as the Sa- vage–Dickey density ratio (e.g., Dickey, 1971; Gamerman & Lopes, 2006, pp. 72–74, pp. 79–80; Kass & Raftery, 1995, p. 780–781; O’Hagan & Forster, 2004, pp. 174–177 ; for extensions and generaliza- tions see Chen, 2005; Verdinelli & Wasserman, 1995 ). Mathematically, the Savage–Dickey density ratio says that BF 01 ¼ p ð D j H 0 Þ p ð D j H 1 Þ ¼ p ð h ¼ : 5 j D ; H 1 Þ p ð h ¼ : 5 j H 1 Þ : ð 11 Þ A straightforward mathematical proof is presented in Appendix A (see also O’Hagan & Forster, 2004, pp. 174–177 ). In Fig. 1 , the two thick dots located at h ¼ : 5 provide the required information. It is evident from the ﬁgure that after observing 9 out of 10 correct responses, the height of the density at h ¼ : 5 has de- creased, so that one would expect these data to cast doubt on the null hypothesis and support the alternative hypothesis. Speciﬁcally, the height of the prior distribution at h ¼ : 5 equals 1, and the height of the posterior distribution at h ¼ : 5 equals 0.107. From Eq. (11) the corresponding Bayes fac- tor is BF 01 ¼ 0 : 107 = 1 ¼ 0 : 107, and this corresponds exactly to the Bayes factor that was calculated by integrating out h . It is clear that the same procedure can be followed when the height of the posterior is not available in closed form, but instead has to be approximated from the histogram of MCMC samples. Fig. 3 shows the logspline estimates ( Stone et al., 1997 ) for the prior and the posterior densities as obtained from MCMC output. The estimated height of the prior and posterior distributions at h ¼ : 5 equal 1.00 and 0.107, respectively. In most nested model comparisons, H 0 and H 1 have several free parameters in common. These parameters are usually not of direct interest, and they are not the focus of the hypothesis test. Hence, the common parameters are known as nuisance parameters . For instance, one might want to test whether or not the mean of a Normal distribution is zero (i.e., H 0 : l ¼ l 0 versus H 1 : l – l 0 ), whereas the variance r 2 is common to both models and not of immediate interest. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 169 \nIn general then, the framework of nested models features a parameter vector h ¼ ð / ; w Þ , where / denotes the parameter of substantive interest that is subject to test, and w denotes the set of nuisance parameters. The null hypothesis H 0 posits that / is constrained to some special value, i.e. / ¼ / 0 . The alternative hypothesis H 1 assumes that / is free to vary. Now consider H 1 , and let / ! / 0 ; this effec- tively means that H 1 reduces to H 0 —it is therefore reasonable to assume that p ð w j / ! / 0 ; H 1 Þ ¼ p ð w j H 0 Þ (but see Consonni & Veronese, 2008 ). In other words, when / ! / 0 the prior for the nuisance parameters under H 1 should equal the prior for the nuisance parameters under H 0 . When this condi- tion holds, Appendix A shows that the nuisance parameters affect the Bayes factor only through the posterior for / , so that again BF 01 ¼ p ð D j H 0 Þ p ð D j H 1 Þ ¼ p ð / ¼ / 0 j D ; H 1 Þ p ð / ¼ / 0 j H 1 Þ ; ð 12 Þ which equals the ratio of the heights for the posterior and the prior distribution for / at / 0 . Thus, the Savage–Dickey density ratio holds under relatively general conditions. Eq. (12) conveys several important messages: 1. Relevance of the prior for the parameter of interest. The denominator of Eq. (12) features the height of the prior for / at / ¼ / 0 . This means that the choice of prior can greatly inﬂuence the Bayes factor, a fact that is also illustrated by Figs. 1 and 3 . The choice of prior will also inﬂuence the shape of the posterior, of course, but this inﬂuence quickly diminishes as the data accumulate. This point under- scores the conceptual challenge for the Bayes factors that was noted earlier (e.g., Bartlett, 1957; Liu & Aitkin, 2008 ). For example, consider again a test for a Normal mean l , with H 0 : l ¼ 0 and H 1 : l – 0. Suppose the prior for l is a uniform distribution that ranges from \u0002 a to a , and suppose that the number of observations is reasonably large. In this situation, the data will have over- whelmed the prior, so that the posterior for l is relatively robust against changes in a . In contrast, the height of the prior at l ¼ 0 varies directly with a : if a is doubled, the height of the prior at l ¼ 0 becomes twice as small, and according to Eq. (12) this would about double the Bayes factor in favor of H 0 . In the limit, as a grows very large, the height of the prior at l ¼ 0 goes to zero, which means that the Bayes factor will go to inﬁnity, indicating decisive support for the null hypothesis. 2. Irrelevance of the prior for nuisance parameters. In contrast to the prior for the parameter of interest / , Eq. (12) indicates that the prior for the nuisance parameters w is not critical. Hence, priors on the nuisance parameters can be vague or even improper (e.g., Hsiao, 1997, p. 659; Kass & Raftery, 1995, p. 783; Kass & Vaidyanathan, 1992 ). Intuitively, the prior vagueness of nuisance parameters is pres- ent in both models and cancels out in the computation of the Bayes factor ( Rouder et al., 2009 ). 3. Relative ease of computing the Bayes factor in nested models. Eq. (12) shows that in nested models, under plausible assumptions on the prior structure for the nuisance parameters, computation of the Bayes factor is relatively straightforward. All that is needed is an estimate of posterior and prior ordinates under the alternative hypothesis H 1 . This computational shortcut is often much less involved than the more generic solution, which involves integrating out nuisance parameters w for H 0 , and parameters w and / for H 1 , as follows: BF 01 ¼ p ð D j H 0 Þ p ð D j H 1 Þ ¼ R p ð D j / ¼ / 0 ; w Þ p ð / ¼ / 0 ; w Þ d w R R p ð D j w ; / Þ p ð w ; / Þ d w d / : ð 13 Þ To the best of our knowledge, the Savage–Dickey method has only been used in psychology once before, by Wetzels et al. (2009) , who used it to develop a WinBUGS implementation of the t -test pro- posed by Rouder et al. (2009) . 4. Summary and prelude to the examples So far, we have introduced Bayesian parameter estimation, MCMC sampling, and the advantages and challenges of Bayesian hypothesis testing. In order to address the computational challenge that comes with Bayesian hypothesis testing, we outlined the Savage–Dickey density ratio method. This 170 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nstraightforward and exact method applies to nested models, and for its computation the user only re- quires the height of the posterior and the height of the prior distribution—for the parameter that is tested, at the point of interest (see Eq. (12) and Figs. 1 and 3 ). Throughout the preceding sections, Bayesian concepts have been discussed by reference to a single, extremely simple binomial example. The next sections discuss three more complicated examples, using real data taken from the psychological literature. This reﬂects our belief that the advantages of Bayesian hypothesis testing and the practical feasibility of the Savage–Dickey method are best illus- trated by concrete examples that are highly relevant to psychological practice. 5. Example 1: equality of proportions In their article ‘‘After the promise: the STD consequences of adolescent virginity pledges”, Brückner and Bearman (2005) analyzed a series of interviews conducted as part of the National Longitudinal Study of Adolescent Health ( Add Health ). The focus of the article was on the sexual behavior of adoles- cents, aged 18–24, who have made a virginity pledge , that is, a public or written pledge to remain a virgin until marriage. Scientiﬁc studies suggest that the sexual behavior of pledgers is not very differ- ent from that of nonpledgers—except for the fact that pledgers are less likely to use condoms when they ﬁrst have sex. The Brückner and Bearman (2005) study presents a wealth of data, but here our focus is on a small subset of the data: 424 out of 777 pledgers ( \u0005 54.6%) indicated that they had used a condom at ﬁrst sex, versus 5416 out of 9072 nonpledgers ( \u0005 59.7%). To what extent does a statistical analysis support the assertion that pledgers are less likely than nonpledgers to use a condom at ﬁrst sex? A frequentist test for equality of proportions indicates that p \u0005 : 006, which tells us that when H 0 is true (i.e., the proportions of condom users are equal in the two groups), then the probability is about .006 that we would encounter a result at least as extreme as the one that was in fact observed. But this is not the kind of information that researchers really care about; researchers want to know the extent to which the data support the claim that pledgers are less likely than nonpledgers to use a condom at ﬁrst sex. Our Bayesian model for these data is simple and general. We assume that the number of condom users ( s 1 ¼ 424 and s 2 ¼ 5416) among the pledgers and the nonpledgers ( n 1 ¼ 777 and n 2 ¼ 9072) is governed by binomial rate parameters h 1 and h 2 , respectively. Denote the difference between the two rate parameters by d , that is, d ¼ h 1 \u0002 h 2 . Fig. 4 shows this model in graphical model notation (for introductions, see Gilks, Thomas, & Spiegelhalter, 1994; Lauritzen, 1996; Lee, 2008; Spiegelhalter, 1998 ). In this notation, nodes represent variables of interest, and the graph structure is used to Fig. 4. Bayesian graphical model for the pledger data. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 171 \nindicate dependencies between the variables, with children depending on their parents. Continuous variables are represented with circular nodes and discrete variables are represented with square nodes; observed variables are shaded and unobserved variables are not shaded. The double borders around the unobserved continuous variable d indicates that it is deterministic (i.e., calculated without noise from other variables) rather than stochastic. In Fig. 4 , for instance, the discrete observed variable s 1 indicates the number of condom users in the group of pledgers. This observed variable depends both on the (discrete, observed) number of pledgers n 1 , and on the continuous, unobserved binomial rate parameter h 1 . In our Bayesian model, we assume that the rate parameters h 1 and h 2 each have a uniform prior distribution (i.e., p ð h ð\u0007Þ Þ \u0004 Beta ð 1 ; 1 Þ ). These uniform prior distributions induce a triangular prior distri- bution for the difference parameter d : p ð d Þ ¼ 1 þ d for d 6 0 ; 1 \u0002 d for d > 0 : \u0006 ð 14 Þ The null hypothesis states that the rates h 1 and h 2 are equal, and hence H 0 : d ¼ 0. The unrestricted alternative hypothesis states that the rates are free to vary, H 1 : d – 0, and the restricted alternative hypothesis states that the rate is lower for the pledgers than for the nonpledgers, H 2 : d < 0. Below we examine these alternative hypothesis in turn. 5.1. Unrestricted analysis The problem of testing H 0 : d ¼ 0 versus H 1 : d – 0 is still relatively simple. The Bayes factor in sup- port for the null hypothesis (i.e., BF 01 ¼ p ð D j H 0 Þ = p ð D j H 1 Þ ) is given for instance by de Braganca Pereira and Stern (1999) : BF 01 ¼ n 1 s 1 \u0002 \u0003 n 2 s 2 \u0002 \u0003 n 1 þ n 2 s 1 þ s 2 \u0002 \u0003 ð n 1 þ 1 Þð n 2 þ 1 Þ n 1 þ n 2 þ 1 : ð 15 Þ For the pledger data, this yields BF 01 \u0005 0 : 45, which means that the data are about 1 = 0 : 45 \u0005 2 : 22 times more likely under the alternative hypothesis than under the null hypothesis. Note that although the Bayesian hypothesis test supports the alternative hypothesis, the result is much less convincing than a p -value of .006 suggests. To apply the Savage–Dickey method, we ﬁrst draw samples from the posterior and the prior distri- butions for d (the WinBUGS code can be found in Appendix B). We ran three chains for 100,000 iter- ations each, and we discarded the ﬁrst 1000 iterations of each chain as burn-in. After conﬁrming by means of visual inspection and the Gelman and Rubin (1992) b R statistic that the chains had converged, we collapsed the samples across the three chains. The left panel of Fig. 5 shows the resulting histograms for the posterior and prior distributions for d plotted on their entire range. In this panel, the thin solid line for the prior indicates the analytical dis- tribution given in Eq. (14) . For the posterior distribution, the thin solid line indicates a logspline non- parametric density estimate ( Stone et al., 1997 ), the procedure that we will use throughout this article to estimate distributions. The right panel of Fig. 5 zooms in on the relevant region around d ¼ 0. The almost ﬂat line is the analytical distribution of the prior, and the sharply decreasing line is the logspline estimate for the posterior. The two dots mark the height of both densities at d ¼ 0. From a visual comparison of the height of the dots, it is clear that the point d ¼ 0 is supported about twice as much under the prior as it is under the posterior. That is, the data have decreased the support for d ¼ 0 by a factor of two. Application of the Savage–Dickey method (i.e., Eq. (12) ) yields BF 01 \u0005 0 : 47, which leads to the conclusion that the data are about 2.17 times more likely under the alternative hypothesis than under the null. Thus, the result from the MCMC-based Savage–Dickey method (i.e., BF 10 ¼ 2 : 17) and the ana- lytical solution (i.e., BF 10 ¼ 2 : 22) are in reasonable agreement. 172 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nFinally, note that the conclusions from the Bayesian hypothesis test (i.e., roughly twice as much evidence for H 1 as for H 0 ) are more conservative than those that follow from Bayesian parameter esti- mation; the Bayesian 95% conﬁdence interval for the posterior of d is ð\u0002 0 : 09 ; \u0002 0 : 01 Þ and does not in- clude 0. The reason for the discrepancy is that the Bayesian hypothesis test punishes H 1 for assigning prior mass to values of d that yield very low likelihoods (i.e., the automatic Ockham’s razor discussed previously, see Berger & Delampady, 1987 for a discussion). 5.2. Order-restricted analysis Many substantive psychological questions can be formulated as order-restrictions (e.g., Hoijtink, Klugkist, & Boelen, 2008; Klugkist et al., 2005a ). Here we focus on a test of H 0 : d ¼ 0 versus H 2 : d < 0, an order-restricted alternative hypothesis that states that the rate of condom use is lower for the pledgers than for the nonpledgers. In the Bayesian framework, order-restrictions can be implemented in several ways (e.g., O’Hagan & Forster, 2004, pp. 70–71 ). For instance, order-restrictions can be enforced before MCMC sampling, by appropriately constraining the prior distributions, or they can be implemented after the MCMC sam- pling, by retaining only those MCMC samples that obey the order-restriction (e.g., Gelfand, Smith, & Lee, 1992, p. 525 ). The left panel of Fig. 6 shows the histograms for the posterior and prior distributions for d under the restricted model H 2 : d < 0. These histograms were obtained by selecting from the previous unre- stricted analysis only those MCMC samples that obey the order-restriction. For the prior, the thin solid line indicates the analytical distribution, and for the posterior it indicates the order-restricted logs- pline estimate. Note that for the prior, the effect of the order-restriction is to double the mass on d ¼ 0, from a va- lue of 1 to a value of 2. In contrast, the order-restriction does not much affect the posterior, as most of its mass was already smaller than 0. The right panel of Fig. 6 zooms in on the relevant area around d ¼ 0 and shows the effect of the order-restriction on the Bayesian hypothesis test. Again, the almost ﬂat line is the analytical distribution of the order-restricted prior, and the associated dot indicates its height at d ¼ 0. The sharply decreasing line is the logspline estimate for the order-restricted posterior, Density Full Scale Density Zoomed in Density Fig. 5. Prior and posterior distributions of the rate difference d for the unrestricted analysis of the pledger data. The left panel shows the distributions across their entire range (prior: histogram and analytical result; posterior: histogram and logspline density estimate). The right panel zooms in on the area that is relevant for the test of H 0 : d ¼ 0 versus H 1 : d – 0 (prior: analytical result; posterior: logspline density estimate). The dots indicate the height of the two distributions at d ¼ 0. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 173 \nand the associated solid dot indicates the logspline estimate of the height of the posterior based on the subset of MCMC samples that obey the order-restriction. The open dot immediately below indicates the height of the posterior estimated from an alternative method, one that is based on renormalizing the order-restricted posterior (i.e., dividing the height of the unrestricted posterior at d ¼ 0 by the area of the unrestricted posterior that lies to the left of d ¼ 0). A visual comparison of the height of the prior and posterior at d ¼ 0 conﬁrms that the order-restric- tion has increased the evidence in favor of the alternative hypothesis. Speciﬁcally, the logspline esti- mate yields BF 02 \u0005 0 : 26 (i.e., BF 20 \u0005 3 : 78), and the estimate based on renormalizing the posterior yields BF 02 \u0005 0 : 23 (i.e., BF 20 \u0005 4 : 34). Thus, both methods lead to the conclusion that there is roughly four times as much evidence for H 2 as for H 0 . The foregoing may lead one to conclude that the effect of order-restrictions are similar in the Bayesian and the frequentist framework; in the Bayesian framework, the order-restriction increased the evidence against H 0 roughly by a factor of two, and in the frequentist framework, a one-sided p -value provides twice as much evidence against H 0 as a two-sided p -value. However, this correspon- dence only holds because the posterior for d is largely consistent with the order-restriction. In general, one may distinguish between the following three situations, which form points on a continuum of possibilities: 1. Posterior largely consistent with the order-restriction. This situation occurred for the pledger data. The order-restriction increases the height of the prior by two, but it hardly increases the height of the posterior. This means that when the order-restriction is almost fully supported by the data, this can only increase the support in favor of the alternative hypothesis by a factor of two. For example, for the pledger data the unrestricted test of H 0 : d ¼ 0 versus H 1 : d – 0 yields BF 10 \u0005 2 : 22. This means that the Bayes factor in favor of H 2 : d < 0 versus H 0 : d ¼ 0 cannot be lar- ger than 2 : 22 \u0003 2 ¼ 4 : 44. 2. Posterior neither consistent nor inconsistent with the order-restriction. This situation occurs when the data are uninformative with respect to the direction of the effect, so that the posterior is symmet- rical around d ¼ 0 (assuming that d is the parameter of interest and 0 is the value at which the Density Full Scale Density Zoomed in Density Fig. 6. Prior and posterior distributions of the rate difference d for the order-restricted analysis of the pledger data. The left panel shows the distributions across their entire range (prior: histogram and analytical result; posterior: histogram and logspline density estimate). The right panel zooms in on the area that is relevant for the test of H 0 : d ¼ 0 versus H 2 : d < 0 (prior: analytical result; posterior: logspline density estimate). The dots indicate the height of the two distributions at d ¼ 0. 174 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nalternative model collapses to the null model). In this case, the order-restriction increases the height of both the prior and the posterior by 2, so that the end result is unaffected. 3. Posterior largely inconsistent with the order-restriction. This situation occurs when the data suggest that the effect is in the direction opposite to that suggested by the order-restriction. In this case, the order-restriction again increases the height of the prior by 2, but it increases the height of the pos- terior much more. Consider, for instance, the right panel of Fig. 5 , and an order-restricted test of H 0 : d ¼ 0 versus H 3 : d > 0. To determine the height of the order-restricted posterior at d ¼ 0 one may divide the height of the unrestricted posterior (i.e., 0.47 according to the logspline method) by its area to the right of zero, which is approximately .003. The Bayes factor in favor of H 0 : d ¼ 0 versus H 3 : d > 0 would then be ð 0 : 47 =: 003 Þ = 2 \u0005 78, which constitutes strong support for the null hypothesis. 6. Example 2: a hierarchical Bayesian one-sample t -test In their article ‘‘Priming in implicit memory tasks: Prior study causes enhanced discriminability, not only bias”, Zeelenberg et al. (2002) reported three experiments in two-alternative forced-choice perceptual identiﬁcation. In the test phase of each experiment, a stimulus (e.g., a picture of a clothes pin) is brieﬂy presented and masked. Immediately after the mask the participant is confronted with two choice options—the target (i.e., the picture of the clothes pin) and a similar foil alternative (e.g., the picture of a stapler; see Fig. 7 for an example); the participant’s goal is to identify the target. Prior to the test phase, the Zeelenberg et al. experiments featured a study phase, in which partic- ipants studied a subset of the choice alternatives that would also be presented in the later test phase. Two conditions were critical: the ‘‘study-neither” condition, in which neither choice alternative was studied, and the ‘‘study-both” condition, in which both choice alternatives were studied. In the ﬁrst two experiments reported by Zeelenberg et al., participants choose the target stimulus more often in the study-both condition than in the study-neither condition. This both-primed beneﬁt suggests that prior study leads to enhanced discriminability, not just a bias to prefer the studied alter- native (e.g., Ratcliff & McKoon, 1997 ; for a discussion see also Bowers, 1999; Wagenmakers, Zeelen- berg, & Raaijmakers, 2003 ). Here we focus on statistical inference for the Experiment 3 from Zeelenberg et al. (2002) . In the study phase of this experiment, all 74 participants were presented with 21 pairs of similar pictures (e.g., the clothes pin/stapler example shown in Fig. 7 ). In the test phase, all participants had to identify brieﬂy presented target pictures among a set of two alternatives. The test phase was composed of 42 pairs of similar pictures, 21 of which had been presented in the study phase. In order to assess the evidence in favor of the both-primed beneﬁt, the authors carried out a stan- dard analysis and computed a one-sample t -test: ‘‘Mean percentage of correctly identiﬁed pictures was calculated for each participant. When neither the target nor the foil had been studied, 71.5% of the pictures were correctly identiﬁed. When both the target and the foil had been studied, 74.7% of the pictures were correctly identiﬁed. The differ- ence between the study-both and study-neither conditions was signiﬁcant, t ð 73 Þ ¼ 2 : 19 ; p < : 05.” This analysis has two main disadvantages. First, the t -test assumes that the data are Normally dis- tributed. For the Zeelenberg experiment, this assumption is certainly incorrect, as the difference be- tween two proportions is constrained to lie between \u0002 1 and 1 (see Example 1). Second, the analysis from Zeelenberg et al. ignores the fact that the experimental design is nested (i.e., trials are Fig. 7. Example pair of similar pictures used in Experiment 3 from Zeelenberg et al. (2002) . E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 175 \nnested within participants), a situation that calls for a hierarchical or multi-level analysis (e.g., Gelman & Hill, 2007; Rouder, Lu, Morey, Sun, & Speckman, 2008 ). In other words, it is unlikely that the both- primed beneﬁt is a ﬁxed effect, in the sense that it is the same for each and every participant—it is more reasonable to assume that the both-primed beneﬁt is a random effect (cf. Rouder et al., 2007 ). Our Bayesian test of the both-primed beneﬁt proceeds as follows. We start by assuming that for participant i the number of correct choices is binomially distributed with parameter h i . Unfortunately, h i is deﬁned on the rate scale, which ranges from 0 to 1. This is an awkward scale for modeling additive effects, as a change from .55 to .65 is not the same as a change from .85 to .95. Hence, we do not model h i , but instead choose to model / i , the deterministic probit transformation of h i . The probit transform is the inverse cumulative distribution function of the standard Normal distri- bution, so that, for instance, a rate of h i ¼ 0 : 5 maps onto a probit rate of / i ¼ 0, and a rate of h i ¼ 0 : 975 maps onto a probit rate of / i ¼ 1 : 96. The probit transform is shown in Fig. 8 . In contrast to the rate scale, the probit scale covers the entire real line, and lends itself easily to hierarchical modeling (Rouder & Lu, 2005). For each participant i , the both-primed beneﬁt a i is given by the difference between performance in the study-both and study-neither condition, a i ¼ / SB ; i \u0002 / SN ; i . Our model incorporates two random ef- fects; ﬁrst, each participant’s baseline level of performance / SN ; i is assumed to be drawn from a group- level Normal distribution with mean l / and standard deviation r / . Second, each participant’s both- primed beneﬁt is assumed to be drawn from a group-level Normal distribution with mean l a and standard deviation r a . Note that such normal distributions are easily deﬁned on the probit scale, but not on the rate scale. Fig. 9 shows the model in graphical form. In order to accommodate the hierarchical structure of the model, we use plate notation, enclosing with square boundaries subsets of the graph that have independent replications. Because each participant contributes to both the study-neither and the study-both conditions, the design is ‘‘within-subjects” and the square boundaries therefore enclose both conditions. For the parameters that are not subject to statistical test (i.e., l / ; r / , and r a ) we speciﬁed uninfor- mative priors. The prior for the group mean of the study-neither condition, l / , is a truncated standard Normal (i.e., greater than zero only on the positive real line), which on the rate scale translates to a uniform distribution from 0.5 to 1 (cf. Rouder & Lu, 2005, p. 588 ). For r / and r a , we chose priors that are uniform from 0 to 10. Fig. 8. The probit transformation. h ¼ U ð / Þ and / ¼ U \u0002 1 ð h Þ , where U denotes the cumulative distribution function of the standard normal distribution. 176 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nFinally, and critically, our model incorporates a parameter d that quantiﬁes effect size , d ¼ l a = r a . Effect size is a dimensionless quantity, and this makes it relatively easy to deﬁne a principled prior. Reasonable default choices for priors on effect size include the Cauchy distribution (i.e., a t distribution with one degree of freedom) and the standard Normal distribution (e.g., Gönen, Johnson, Lu, & Westfall, 2005; Rouder et al., 2009 ). The latter prior is known as the ‘‘unit information prior”, as it carries as much information as a single observation (Kass & Wasserman, 1995). The standard Normal distribution is the prior for effect size that we will use in this example and the next. With the statistical model in place, we can now turn to hypothesis testing. The null hypothesis states that there is no both-primed beneﬁt, and hence the effect size is zero: H 0 : d ¼ 0. The alternative, order-restricted hypothesis states that there is a both-primed beneﬁt, and hence H 1 : d > 0. This test is, in fact, a hierarchical extension of the Bayesian one-sample t -test proposed by Gönen et al. (2005), Rouder et al. (2009) ; the difference is that our hierarchical t -test is deﬁned on the level of individual parameters instead of raw data. Our model can therefore be thought of as a Bayesian hierarchical one- sample t -test. We implemented our Bayesian hierarchical t -test by means of the Savage–Dickey method. First we drew MCMC samples from the posterior distribution for d (the WinBUGS code can be found in Appen- dix B). As in Example 1, we ran three chains for 100,000 iterations each, and we discarded the ﬁrst 1000 iterations of each chain as burn-in. After conﬁrming by means of visual inspection and the Gelman and Rubin (1992) b R statistic that the chains had converged, we collapsed the samples across the three chains. Fig. 9. Bayesian graphical model for the Zeelenberg data. In a within-subjects design, 74 participants performed a two- alternative forced-choice perceptual identiﬁcation task, in both ‘‘study-neither” (SN) and ‘‘study-both” (SB) conditions. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 177 \nFig. 10 visualizes the results—for the prior on effect size d , the thin solid line indicates the Normal distribution that has been truncated and renormalized to take into account the order restriction that d > 0. For the posterior order-restricted distribution on effect size d , the thin solid line indicates the logspline nonparametric density estimate, and the thick solid line indicates the histogram of MCMC samples. As in Example 1, the two dots mark the height of prior and posterior densities at d ¼ 0. From a visual comparison of the height of the dots, it is clear that the point d ¼ 0 is supported about four times as much under the prior as it is under the posterior. That is, the data have decreased the support for d ¼ 0 by a factor of four. Application of the Savage–Dickey method (i.e., Eq. (12) ) yields BF 01 \u0005 0 : 22, which leads to the conclusion that the data are about 4.49 times more likely under the alternative hypothesis than under the null hypothesis. Thus, the data support the assertion that there is a both-primed beneﬁt, but the extent of this sup- port is somewhat weaker than is suggested by the p -value. 7. Example 3: a hierarchical Bayesian two-sample t -test In their article ‘‘How speciﬁc are executive functioning deﬁcits in Attention Deﬁcit Hyperactivity Disorder and autism?”, Geurts et al. (2004) studied the performance of children with ADHD and aut- ism on a range of cognitive tasks. Here we focus on a small subset of the data and consider the ques- tion whether children that develop typically (i.e., ‘‘normal controls”) outperform children with ADHD on the Wisconsin Card Sorting Test (WCST; Grant & Berg, 1948; Heaton, Chelune, Talley, Kay, & Curtiss, 1993 ). The WCST requires that participants learn, by trial and error, to sort cards according to an implicit rule. The complication is that, over the course of the experiment, the sorting rule sometimes changes. This means that in order to avoid too many mistakes, participants have to suppress the tendency to perseverate and quickly discover and adopt the new rule. Because of these task demands, performance on the WCST is thought to quantify cognitive ﬂexibility or set shifting ability. The experiment of interest contains data from 26 normal controls and 52 children with ADHD. Each child performed the WCST, and the measure of interest is the number of correctly sorted cards relative Density Fig. 10. Prior and posterior distribution of the effect size d for the hierarchical, order-restricted analysis of the Zeelenberg data. For the prior, the thin line gives the analytical result; for the posterior, the thick line gives the histogram and the thin line gives the logspline density estimate. The dots indicate the height of the two distributions at d ¼ 0. 178 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nto the total number of sorting opportunities. The WCST provides a maximum of 128 cards to sort, but, depending on a child’s performance, this number could also be lower. Overall, the group of normal controls sorted the cards correctly on 65.4% of the cases, and the group of ADHD children sorted the cards correctly on 66.9% of the cases. A between-subjects (i.e., two-sample) frequentist t -test on the proportion of correctly sorted cards does not allow one to reject the null hypothesis, t ð 40 : 2 Þ ¼ 0 : 37 ; p ¼ : 72. But this statistic does not quantify the evidence in favor of the null hypothesis. Another problem with this frequentist t -test is that it ignores the fact that trials are nested in partic- ipants—a design that, as in Example 2, calls for a hierarchical/multi-level/random effects analysis. Our hierarchical model is speciﬁed as follows. We assume that for child i in the group of normal controls, the number of correctly sorted cards K NC ; i (out of N NC ; i opportunities) is binomially distrib- uted with rate parameter h NC ; i . As in the previous example, this rate parameter is then transformed to the probit scale (cf. Fig. 8 ), which yields the corresponding parameter / NC ; i . The comparable assumptions are made for child j in the group of ADHD children, resulting in the associated parameter / AD ; j . Next, our model incorporates random effects; for both the normal controls and the group of ADHD children, the probitized rates of correct responding (i.e., / NC ; \u0007 and / AD ; \u0007 ) are assumed to be drawn from group-level Normal distributions. Denoting the grand mean by l , and the group difference in means by a , the group-level Normal distribution for the normal controls is deﬁned as N ð l þ a = 2 ; r 2 Þ and that for the ADHD children as N ð l \u0002 a = 2 ; r 2 Þ , where r denotes the standard deviation for the group-level distribution. Fig. 11 shows the model in graphical form. As in Fig. 9 , the hierarchical structure of the model is accommodated by plate notation, enclosing with square boundaries subsets of the graph that have independent replications. Because every child participants in only one of the two conditions, the de- sign is ‘‘between-subjects” and the square boundaries enclose each condition separately. For the parameters that are not subject to statistical test (i.e., l and r ) we speciﬁed uninformative priors. The prior for the grant mean l is a standard Normal, which on the rate scale translates to a uni- form distribution from 0 to 1 (cf. Rouder & Lu, 2005, p. 588 ). For r , we chose a prior that is uniform from 0 to 10. As in the previous example, the key aspect of our model is a parameter d that quantiﬁes effect size , d ¼ a = r . We again use the ‘‘unit information” standard normal prior on d , completing the speciﬁcation of the model. Hypothesis testing now proceeds as before. The null hypothesis states that normal controls and ADHD children perform the same on the WCST, and hence the effect size is zero: H 0 : d ¼ 0. The unre- stricted alternative hypothesis states that there is a difference in performance, and hence H 1 : d – 0. Lastly, the order-restricted hypothesis states that normal controls perform better than ADHD children, such that H 2 : d > 0. These tests are hierarchical extensions of the Bayesian one-sample t -test ( Gönen et al., 2005; Rouder et al., 2009 ); as in Example 2, the difference is that our hierarchical t -tests are de- ﬁned on the level of individual parameters instead of raw data. Below we examine the unrestricted analysis (i.e., H 0 versus H 1 ) and the restricted analysis (i.e., H 0 versus H 2 ) in turn. 7.1. Unrestricted analysis We implemented our Bayesian hierarchical two-sample t -test by means of the Savage–Dickey method. As in the previous two examples, we drew MCMC samples from the posterior distribution for d (the WinBUGS code can be found in Appendix B), we ran three chains for 100,000 iterations each, and we discarded the ﬁrst 1000 iterations of each chain as burn-in. We also conﬁrmed by means of visual inspection and the Gelman and Rubin (1992) b R statistic that the chains had converged, and we then collapsed the samples across the three chains. The left panel of Fig. 12 visualizes the result. The ADHD children performed slightly better than the normal controls, and this is reﬂected in a posterior distribution for d which is slightly asymmetrical around zero, assigning more mass to negative than to positive values of d . The Bayesian 95% conﬁ- dence interval for d is ( \u0002 0.54, 0.42). The left panel of Fig. 12 also shows that the data have made the value d ¼ 0 more likely than it was before (i.e., at d ¼ 0, the posterior is higher than the prior). Speciﬁcally, the ratio of the heights yields BF 01 ¼ 3 : 96, which indicates that the data are about four times more likely under H 0 than they are E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 179 \nunder H 1 . Thus, the data support the claim that normal controls and ADHD children perform equally well on the WCST over the claim that these groups perform differently. 7.2. Order-restricted analysis The order-restricted hypothesis states that normal controls outperform children with ADHD on the WCST (i.e., H 2 : d > 0). This hypothesis may be entertained because it is plausible a priori ; However, the data show that, if anything, the reverse is true: the mean percentage of correct card selections was 1.5% higher for the group of ADHD children than for the normal controls. What can we expect when we test H 0 : d ¼ 0 versus H 2 : d > 0? First, note that the posterior for d is not far from being symmetrical around zero. If it were com- pletely symmetrical, we would have ‘‘case 2” discussed above: ‘‘Posterior neither consistent nor incon- sistent with the order-restriction”. In this case the height of both the prior and the posterior is multiplied by 2, so that their ratio stays the same. Second, the the posterior for d is not quite symmet- rical around zero, and assigns slightly more mass to values that are inconsistent with H 2 . This will slightly increase the support for H 0 over H 2 . These two considerations lead us to expect that the evi- dence in favor of H 0 over H 2 (i.e., BF 02 ) will be slightly larger than that of H 0 over H 1 (i.e., BF 01 ¼ 3 : 96). The right panel of Fig. 12 shows the result of the order-restricted analysis. As before, the relatively ﬂat line is the analytical distribution of the order-restricted prior, and the associated dot indicates its Fig. 11. Bayesian graphical model for the Geurts data. In a between-subjects design, 26 typically developing children (i.e, ‘‘normal controls”, NC) and 52 children with ADHD (AD) performed the Wisconsin Card Sorting Test. 180 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nheight at d ¼ 0. The sharply decreasing line is the logspline estimate for the order-restricted posterior, and the associated solid dot indicates the logspline estimate of the height of the posterior based on the subset of MCMC samples that obey the order-restriction. The alternative method based on renormal- izing the order-restricted posterior yielded a virtually identical result. A quantitative comparison of the height of the prior and posterior at d ¼ 0 conﬁrms our expectation that the order-restriction slightly increases the evidence in favor of H 0 . Speciﬁcally, the logspline esti- mate yields BF 02 ¼ 4 : 94. Thus, under H 0 the data are about ﬁve times more likely than they are under the order-restricted alternative, a result that is slightly more convincing than the one obtained when H 0 is pitted against the unrestricted alternative. In sum, the data support the assertion that normal controls and children with ADHD perform similarly on the WCST, even though the evidence is not overwhelming. 8. Limitations of the Savage–Dickey density ratio So far we have focused on the advantages of the Savage-Dickey density ratio method. However, the method also has its limitations, and these include the following: 1. Markov chain Monte Carlo. The Savage–Dickey method depends directly on the posterior distribu- tion for the parameter that is subject to test. For most interesting models, this posterior is not avail- able in closed-form, but instead has to be approximated by MCMC techniques. Fortunately, these MCMC techniques are implemented in the popular WinBUGS program ( Lunn et al., 2000; Lunn et al., 2009; Ntzoufras, 2009 ); when using WinBUGS, all researchers have to do is to describe their model using an intuitive scripting language, and the details of the sampling process are automat- ically taken care of by WinBUGS (see Appendix B for examples). 2. Convergence. As explained in the section on Bayesian parameter estimation, with MCMC comes an obligation to monitor convergence; if the MCMC chains have not converged, the samples do not come from the posterior distribution, and the Savage–Dickey test will produce the wrong results. For the simple statistical models that are popular in psychology, convergence is generally very fast. Unrestricted Analysis Density Order−Restricted Analysis Density Fig. 12. Prior and posterior distribution of the effect size d for the hierarchical analysis of the Geurts data (left panel: unrestricted analysis; right panel: order-restricted analysis) For the prior, the thin line gives the analytical result; for the posterior, the thick line gives the histogram and the thin line gives the logspline density estimate. The dots indicate the height of the two distributions at d ¼ 0. E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 181 \n3. Density estimation. For its computation, the Savage–Dickey method requires an estimate of the height of a uni-dimensional posterior distribution at a single point. In this article, we have used the logspline nonparametric density estimator proposed by Stone et al. (1997) . This estimator is implemented in in the R package polspline , and concrete examples of its use are provided in the online R code associated with this article. We chose the logspline density algorithm because it generally performs well when the posterior is restricted (e.g., only positive values are allowed), and we chose a nonparametric estimator because we wanted to avoid any assumptions about the form of the posterior distribution. Nevertheless, the nonparametric density estimators will not be reliable when the results are extreme, that is, when the point of interest lies in the extreme tail of the posterior distribution— in the tail, the information about height is based on relatively few samples. This problem can be diagnosed by using the Savage–Dickey method multiple times to see whether the result is stable. Also, one might argue that when the point of interest is far out in the tails of the posterior distri- bution, the qualitative conclusion is evident and reliable (i.e., the data support H 1 over H 0 ), even if the quantitative result is not. 4. Nested models. The Savage–Dickey method can only be applied to nested models. This means that one model—the null hypothesis—needs to be a special case of a more general model. Although this is a clear limitation, scientiﬁc practice has shown that in the ﬁeld of psychology, the overwhelming majority of model comparisons involve nested models (i.e., those models that also allow the com- putation of a p -value). 5. Borel–Kolmogorov paradox. A ﬁnal limitation of the Savage–Dickey method originates from the way in which the priors need to be speciﬁed: p ð w j / ! / 0 ; H 1 Þ ¼ p ð w j H 0 Þ , where w are the nuisance parameters and / is the parameter that is subject to test (i.e., H 0 holds that / takes on the speciﬁc value / 0 ). This prior speciﬁcation is intuitively plausible, but it has two disadvantages. The ﬁrst dis- advantage is that it is implicitly assumed that the common nuisance parameters fulﬁll exactly the same roles, whether they are part of H 0 or H 1 ; some people believe this assumption is too strict (for a discussion see Consonni & Veronese, 2008 ). The second disadvantage is that the priors are con- structed by conditioning on an event that has probability zero, namely / ! / 0 . This way of condi- tioning invokes the Borel–Kolmogorov paradox, a paradox that makes the results of the Bayesian hypothesis test depend on the parametrization used ( Consonni & Veronese, 2008 ; for a summary see Wetzels, Grasman, & Wagenmakers, submitted for publication ). Concretely, this means that a test of l ¼ 0 can yield a result that differs from a conceptually equivalent test of l = r ¼ 0. Several alternative procedures have been proposed to circumvent the Borel–Kolmogorov para- dox. These alternatives deﬁne priors for nested models so that one does not condition on an event of probability zero. Unfortunately, all methods appear to come with drawbacks of their own (reviewed in Consonni & Veronese, 2008 ), and presently their does not appear to be a sin- gle method that is universally accepted. For most models used in psychology (e.g., regression) the choice of parametrization is clear, but this only slightly alleviates the general concern. Despite these limitations, we hope that our examples have shown that the Savage–Dickey method can be a useful as a relatively straightforward implementation of the Bayesian hypothesis test. 9. Concluding comments The goal of this article was to familiarize psychologists with Bayesian hypothesis testing as an alternative to calculating p -values. We have outlined a simple yet general Bayesian hypothesis test, implemented via the Savage–Dickey density ratio, that can be used to quantify the statistical evidence for and against members from a set of nested models. We have illustrated the use of this hypothesis test with concrete examples that are relevant to the analysis of routine psychological experiments. In particular, we have shown how the Bayesian hypothesis can be applied to hierarchical designs that involve order-restrictions, and how the results can quantify statistical support both for and against the null hypothesis. Throughout this article we have illustrated the Savage–Dickey method with applications that re- quired relatively simple statistical models; for instance, the applications had only two conditions, did not contain any covariates, and did not assume any variability across items. It is clearly desirable 182 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nto extend the Bayes factor hypothesis test to more general scenarios such as those that involve gen- eralized linear models ( Dey, Ghosh, & Mallick, 2000 ) and variable selection in regression ( Liang, Paulo, Molina, Clyde, & Berger, 2008 ). The extension of the Bayesian hypothesis test to more general statis- tical models is ongoing, and it is likely that MCMC-based methods will be crucial for their ﬂexible application (e.g., Ntzoufras, 2009, chap. 11 ). Outside of the context of basic statistical models, the Savage–Dickey method could also be used for Bayesian hypothesis testing in a range of relatively complex mathematical process models such as the Expectancy–Valence model for the Iowa Gambling Task ( Busemeyer & Stout, 2002 ; Wetzels, Vandekerckhove, Tuerlinckx, & Wagenmakers, in press ), the Ratcliff diffusion model for response times and accuracy ( Vandekerckhove, Tuerlinckx, & Lee, 2008; Wagenmakers, 2009 ), models of categorization such as ALCOVE ( Kruschke, 1992 ), multinomial processing trees ( Batchelder & Riefer, 1999 ), and the ACT-R model ( Weaver, 2008 ). For instance, a team of researchers might study the effect of alcohol on the parameters of the Rat- cliff diffusion model; at some point, they might wish to test the hypothesis that alcohol has an effect on response caution. The Savage–Dickey method allows them to calculate easily the statistical support for and against this hypothesis without having to integrate out all other parameters in the model, a requirement that necessitates the use of relatively complicated numerical techniques. For decades, cognitive psychologists carried out their statistical analysis within a single paradigm, the paradigm of p -values. This is unfortunate, not only because such a narrow focus restricts one’s sta- tistical horizon, but also because p -values only indirectly answer the kinds of questions that research- ers would like to see answered. This article does not just provide a theoretical analysis of the problem (see also Nickerson, 2000; Wagenmakers, 2007 ), but it also offers a practical and ﬂexible alternative. Cast in Gigerenzer’s Freudian analogy, it is our hope that the Bayesian hypothesis test will help to re- solve the unconscious conﬂict that plagues cognitive psychologists, and resolve it so that the Id can ﬁnally see its wish granted: probabilities assigned to parameters and hypotheses! Acknowledgments This research was supported by a Vidi grant from the Dutch Organization for Scientiﬁc Research (NWO). We thank Rene Zeelenberg for sending us the perceptual identiﬁcation data ( Zeelenberg et al., 2002, Experiment 3 ), and we thank Hilde Geurts for sending us the Wisconsin Card Sorting Test data ( Geurts et al., 2004 ). Correspondence concerning this article may be addressed to Eric–Jan Wagenmakers, University of Amsterdam, Department of Psychology, Roetersstraat 15, 1018 WB Amsterdam, the Netherlands. Appendix A. Derivation of the Savage–Dickey density ratio This appendix provides the derivation of the Savage–Dickey density ratio (e.g., Dickey & Lientz, 1970, Lindley, 1972, pp. 30–32, O’Hagan & Forster, 2004, pp. 174–177 ). Consider a simple model or null hypothesis, H 0 , that consists of parameter vector h ¼ ð / ; w Þ , with / set equal to some special value of substantive interest, i.e., / ¼ / 0 . The complex model or alternative hypothesis, H 1 , augments H 0 and states that / – / 0 . Thus, parameter / is the focus of interest, whereas common parameters w are so-called nuisance parameters. A well-known example is that of the Nor- mal model, in which one might test whether or not the mean is zero (i.e., H 0 : l ¼ l 0 versus H 1 : l – l 0 ), whereas the variance r 2 is not of interest. To simplify notation, let subscripts 0 and 1 de- note the densities under hypothesis H 0 and H 1 , respectively. Now assume that the conditional density for / is continuous at / 0 , such that lim / ! / 0 p 1 ð w j / Þ ¼ p 0 ð w Þ . Then, the prior for the nuisance parameters in the complex model, conditional on / ! / 0 equals the prior for the nuisance parameters in the simple model, where / ¼ / 0 by deﬁnition, so that p 1 ð w j / ¼ / 0 Þ ¼ p 0 ð w Þ . As explained in the main text, the Bayes factor is the ratio of marginal likelihoods, BF 01 ¼ p ð D j H 0 Þ = p ð D j H 1 Þ ¼ p 0 ð D Þ = p 1 ð D Þ . The marginal likelihood under H 0 is given by E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 183 \np 0 ð D Þ ¼ Z p 0 ð D j w Þ p 0 ð w Þ d w : ð 16 Þ Using the continuity condition, this can be rewritten as p 0 ð D Þ ¼ Z p 1 ð D j w ; / ¼ / 0 Þ p 1 ð w j / ¼ / 0 Þ d w ¼ p 1 ð D j / ¼ / 0 Þ : ð 17 Þ We then apply Bayes’ rule to the right-hand side of Eq. (17) to obtain p 0 ð D Þ ¼ p 1 ð / ¼ / 0 j D Þ p 1 ð D Þ p 1 ð / ¼ / 0 Þ : ð 18 Þ We can now obtain the Bayes factor by dividing p 0 ð D Þ —written as in Eq. (18) —by p 1 ð D Þ . The latter fac- tor cancels, and we are left with BF 01 ¼ p 0 ð D Þ p 1 ð D Þ ¼ p 1 ð / ¼ / 0 j D Þ p 1 ð / ¼ / 0 Þ ; ð 19 Þ which is the ratio of the posterior and prior ordinate, a.k.a. the Savage–Dickey density ratio. As shown by Wetzels, Grasman, and Wagenmakers (submitted for publication) , the Savage–Dickey method is a special, ‘‘exact equality” case of the more general encompassing prior approach advocated by Hoijtink, Klugkist, and colleagues (Hoijtink et al., 2008; Klugkist et al., 2005a; Klugkist, Laudy, & Hoijtink, 2005b; Mulder et al., in press). Another generalization of the Savage–Dickey method was presented by Verdinelli and Wasserman (1995) . Appendix B. WinBUGS code This appendix provides the WinBUGS computer code that implements the models discussed in this article. The WinBUGS program (e.g., Lunn et al., 2000 , http://www.mrc-bsu.cam.ac.uk/bugs/ ) requires that the user constructs a ﬁle containing the model speciﬁcation, a ﬁle containing initial values for the model parameters, and a ﬁle containing the data. Below, we provide only the model speciﬁcation ﬁles. The additional computer code is available on the ﬁrst author’s website, http://www.users.fmg.uva.nl/ ewagenmakers/papers.html . B.1. Example 1: pledger data The WinBUGS code below implements the graphical model shown in Fig. 4 . model { # Uniform Prior on Rates: theta1 \u0004 dbeta(1,1) theta2 \u0004 dbeta(1,1) # Binomial Distribution for Observed Counts: s1 \u0004 dbin(theta1,n1) s2 \u0004 dbin(theta2,n2) # Difference between Rates: delta < - theta1-theta2 # Priors # Make \"Dummy\" Variables That Copy The Prior, # But Are Never Updated By Data theta1prior \u0004 dbeta(1,1) theta2prior \u0004 dbeta(1,1) deltaprior < - theta1prior-theta2prior } 184 E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 \nB.2. Example 2: Zeelenberg data The WinBUGS code below implements the graphical model shown in Fig. 9 . model { for(i in 1:74) # 74 Participants { # Binomial Distributions for Observed Counts: K.SN[i] \u0004 dbin(theta.SN[i], N.SN[i]) K.SB[i] \u0004 dbin(theta.SB[i], N.SB[i]) # Transformation to Parameters on the Probit Scale: theta.SN[i] < - phi(phi.SN[i]) theta.SB[i] < - phi(phi.SB[i]) # Individual Parameters that Quantify Performance in # the Study-Neither Condition Come From a Group-Level Distribution: phi.SN[i] \u0004 dnorm(mu.phi,tau.phi) # NB. tau.phi is the precision, defined as 1/variance # On the Probit Scale, Priming Effects Are Additive: phi.SB[i] < - phi.SN[i] + alpha[i] # alpha[i] is the priming effect for participant i # Individual Priming Effects Come From a Group-Level Distribution: alpha[i] \u0004 dnorm(mu.alpha,tau.alpha) # NB. tau.alpha is the precision, defined as 1/variance } # Group-Level Priors for the Study-Neither Condition: mu.phi \u0004 dnorm(0,1)I(0,) # NB1. The I(0,) command ensures that all samples for mu.phi are > 0 # NB2. This prior for mu.phi corresponds to a uniform prior the rate scale, # ranging from 0.5 to 1. # Uninformative Prior on the Group-Level Standard Deviation: sigma.phi \u0004 dunif(0,10) # Transformation from Standard Deviation to Precision: tau.phi < - pow(sigma.phi,-2) # Priors for the Group-Level Priming Effect (cf. Rouder et al., PBR): mu.alpha < - delta * sigma.alpha # Uninformative Prior for sigma.alpha: sigma.alpha \u0004 dunif(0,10) # Transformation from Standard Deviation to Precision: tau.alpha < - pow(sigma.alpha,-2) # The \"Unit Information Prior\" on Effect Size delta (cf. Rouder et al., PBR): delta \u0004 dnorm(0,1)I(0,) # NB. The I(0,) incorporates the order-restriction that allows only # positive values for delta } E.-J. Wagenmakers et al. / Cognitive Psychology 60 (2010) 158–189 185 \nB.3. Example 3: Geurts data The WinBUGS code below implements the graphical model shown in Fig. 11 . model { for(i in 1:26) # 26 Normal Control Participants { # Binomial Distributions for Observed Counts: K.NC[i] \u0004 dbin(theta.NC[i],N.NC[i]) # Transformation to Parameters on the Probit Scale: theta.NC[i] < - phi(phi.NC[i]) # Individual Parameters Come From a Group-Level Distribution: phi.NC[i] \u0004 dnorm(mu.NC,tau) # NB. tau is the precision, defined as 1/variance } for(j in 1:52) # 52 ADHD Participants { # Binomial Distributions for Observed Counts: K.AD[j] \u0004 dbin(theta.AD[j],N.AD[j]) # Transformation to Parameters on the Probit Scale: theta.AD[j] < - phi(phi.AD[j]) # Individual Parameters Come From a Group-Level Distribution: phi.AD[j] \u0004 dnorm(mu.AD,tau) # NB. tau is the precision, defined as 1/variance } mu.NC < - mu + (.5*alpha) mu.AD < - mu \u0002 (.5*alpha) # NB. mu is the grand mean, alpha is the effect (i.e., the group difference) # Group-Level Priors: mu \u0004 dnorm(0,1) # NB. This prior for mu corresponds to a uniform prior the rate scale, # ranging from 0 to 1. # Uninformative Prior on the Group-Level Standard Deviation: sigma \u0004 dunif(0,10) # Transformation from Standard Deviation to Precision: tau < - pow(sigma, \u0002 2) alpha < - delta * sigma # NB. This allows one to put a prior on effect size delta (cf. Rouder et al., PBR) # The \"Unit Information Prior\" on Effect Size delta (cf. Rouder et al., PBR): delta \u0004 dnorm(0,1) }"
}