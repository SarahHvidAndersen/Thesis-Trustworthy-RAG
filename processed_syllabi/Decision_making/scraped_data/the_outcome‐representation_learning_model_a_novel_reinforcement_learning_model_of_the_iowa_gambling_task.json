{
    "document_type": "research_paper",
    "title": "The Outcome‐Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Decision_making\\pdf_material\\Haines-2018-The-outcomerepresentation-learning-.pdf",
    "date_published": "2018-11-20",
    "keywords": "Unavailable",
    "flag": "",
    "text": "Cognitive Science 42 (2018) 2534–2561 © 2018 Cognitive Science Society, Inc. All rights reserved. ISSN: 1551-6709 online DOI: 10.1111/cogs.12688 The Outcome-Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task Nathaniel Haines, a Jasmin Vassileva, b,c Woo-Young Ahn a,d a Department of Psychology, The Ohio State University b Department of Psychiatry, Virginia Commonwealth University c Institute for Drug and Alcohol Studies, Virginia Commonwealth University d Department of Psychology, Seoul National University Received 26 October 2017; received in revised form 23 May 2018; accepted 29 August 2018 Abstract The Iowa Gambling Task (IGT) is widely used to study decision-making within healthy and psychiatric populations. However, the complexity of the IGT makes it difﬁcult to attribute varia- tion in performance to speciﬁc cognitive processes. Several cognitive models have been proposed for the IGT in an effort to address this problem, but currently no single model shows optimal per- formance for both short- and long-term prediction accuracy and parameter recovery. Here, we pro- pose the Outcome-Representation Learning (ORL) model, a novel model that provides the best compromise between competing models. We test the performance of the ORL model on 393 sub- jects’ data collected across multiple research sites, and we show that the ORL reveals distinct pat- terns of decision-making in substance-using populations. Our work highlights the importance of using multiple model comparison metrics to make valid inference with cognitive models and sheds light on learning mechanisms that play a role in underweighting of rare events. Keywords: Computational modeling; Reinforcement learning; Substance use; Iowa Gambling Task; Bayesian data analysis; Amphetamine; Heroin; Cannabis 1. Introduction There is a growing interest among researchers to develop and apply computational (i.e., cognitive) models to classical assessment tools to help guide clinical decision-mak- ing (e.g., Ahn & Busemeyer, 2016; Batchelder, 1998; McFall & Townsend, 1998; Correspondence should be sent to Nathaniel Haines, Department of Psychology, The Ohio State Univer- sity, Columbus, OH 43210. E-mail: haines.175@osu.edu (or) Woo-Young Ahn, Department of Psychology, Seoul National University, Seoul 08826, Korea. E-mail: wahn55@snu.ac.kr \nNeufeld, Vollick, Carter, Boksman, & Jett \u0001 e, 2002; Ratcliff, Spieler, & Mckoon, 2000; Treat, McFall, Viken, & Kruschke, 2001; Wallsten, Pleskac, & Lejuez, 2005). Despite this interest, clinical assessment has yet to be inﬂuenced by the many computational assays available today (see Ahn & Busemeyer, 2016). There are many potential reasons for this, but two important factors are the lack of (a) precise characterizations of neu- rocognitive processes and (b) optimal, externally valid paradigms for assessing psychiatric conditions. The Iowa Gambling Task (IGT) is an example, which was successfully used to classify various clinical populations from healthy populations (e.g., Bechara, Damasio, Damasio, & Anderson, 1994; Bechara et al., 2001). Originally developed to detect damage in ven- tromedial prefrontal brain regions, the IGT has since been used to identify a variety of decision-making deﬁcits across a wide range of clinical populations (e.g., Grant, Con- toreggi, & London, 2000; Shurman, Horan, & Nuechterlein, 2005; Stout, Rodawalt, & Siemers, 2001; Whitlow et al., 2004). While the IGT is highly sensitive to decision-mak- ing deﬁcits, the speciﬁc underlying neurocognitive processes that are responsible for these observed deﬁcits are difﬁcult to identify using only behavioral performance data. To address the lack of speciﬁcity provided by the IGT, multiple computational models have been proposed which aim to break down the decision-making process into its com- ponent parts (d’Acremont, Lu, Li, Van der Linden, & Bechara, 2009; Ahn, Busemeyer, Wagenmakers, & Stout, 2008; Busemeyer & Stout, 2002; Worthy, Pang, & Byrne, 2013b), and the modeling approach has been applied to several clinical populations (for a review, see Ahn, Dai, Vassileva, Busemeyer, & Stout, 2016). In particular, the ﬁrst cogni- tive model proposed for the IGT — termed the Expectancy-Valence Learning (EVL) model (Busemeyer & Stout, 2002) — was used to identify differences in cognitive mechanisms between healthy controls and multiple clinical populations ranging from those with sub- stance use to neuropsychiatric disorders (Yechiam, Busemeyer, Stout, & Bechara, 2005). The EVL led to several new competing models, which capture participants’ decision- making behavior more accurately. Speciﬁcally, two models show excellent performance: (a) the Prospect Valence Learning model with Delta rule (PVL-Delta) shows excellent long-term prediction accuracy and parameter recovery (Ahn et al., 2008, 2014; Stein- groever, Wetzels, & Wagenmakers, 2013, 2014), and (b) the Value-Plus-Perseverance model (VPP) shows excellent short-term prediction accuracy (Ahn et al., 2014; Worthy et al., 2013b). Long-term prediction accuracy (a.k.a., absolute performance; Steingroever, Wetzels, & Wagenmakers, 2014) is deﬁned as how well a model can generate the whole choice patterns when only the ﬁtted parameters are used, and short-term prediction accu- racy is deﬁned as a measure of model prediction accuracy on one-step-ahead trials using ﬁtted parameters and a history of choices while penalizing model complexity. Parameter recovery performance indicates how well “true” model parameters can be estimated (i.e., recovered) after they are used to simulate behavior, which is essential for making valid inference with model parameters (Donkin, Brown, Heathcote, & Wagenmakers, 2010; Wagenmakers, Van Der Maas, & Grasman, 2007). Because all three of these metrics are important in understanding how well model parameters capture the true cognitive pro- cesses underlying decision making (see Heathcote, Brown, & Wagenmakers, 2015) and N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2535 \nthere is no single model that shows good performance in all three metrics, it is unclear which model should be used to make inference on the IGT. Additionally, no studies to our knowledge have explicitly assessed different models’ performance across the multiple versions of the IGT. While many studies to date have employed the original version of the task developed in 1994 (Bechara et al., 1994), the modiﬁed version has a non-stationary payoff structure (see section 2.2) and is widely used in practical applications involving populations with severe decision-making impair- ments (e.g., Ahn et al., 2014; Bechara & Damasio, 2002). Importantly, a model that per- forms well across both versions of the task would be more generalizable to other experience-based cognitive tasks which are used extensively in the decision-making and cognitive science literature. To develop a new and improved computational model for the IGT, it is necessary to ﬁrst identify the cognitive strategies that decision makers may engage in during IGT administration. In the sections that follow, we describe four separable cognitive strate- gies/effects that are consistently observed in IGT behavioral data, including (a) maximiz- ing long-term expected value, (b) maximizing win frequency, (c) choice perseveration, and (d) reversal learning. As mentioned previously, the IGT falls under the umbrella of more general experience-based cognitive tasks, so a model that accurately captures these multiple strategies has broad implications for models of decisions from experience. 1.1. Expected value In experience-based cognitive tasks, people typically learn the long-term expected value of choice alternatives across trials and make choices appropriately. The IGT is a speciﬁc instantiation of an experienced-based task in which people make decisions based on expected value (e.g., Bechara et al., 1994; Beitz, Salthouse, & Davis, 2014). In fact, the most common metric used to summarize IGT behavioral performance is the difference between the number of “good” versus “bad” decks selected, where good and bad decks are those with positive and negative expected values, respectively. For example, in Bechara et al.’s (1994) original work, the net good minus bad deck selections was used to successfully differentiate healthy controls from individuals with ventromedial pre- frontal cortex damage. However, it has since become clear that healthy subjects do not always learn to make optimal selections (see Steingroever, Wetzels, Horstmann, Neu- mann, & Wagenmakers, 2013b), which is consistent with extant literature on experience- based tasks (e.g., Erev & Barron, 2005). In extreme cases, healthy controls make deci- sions similar to that of severely impaired decision makers when evaluated using expected value criterion alone (e.g., Caroselli, Hiscock, Scheibel, & Ingram, 2006). The PVL-Delta and VPP models both assume that decision makers ﬁrst value the out- comes according to the Prospect Theory utility function (Kahneman & Tversky, 1979), and the resulting subjective utilities are then used to update decision makers’ trial-by-trial expectations using the delta rule (i.e., the simpliﬁed Rescorla-Wagner updating rule; see Rescorla & Wagner, 1972). Together, the Prospect Theory utility shape and loss aversion parameters determine which decks decision makers learn to prefer — holding other 2536 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nparameters constant, low loss aversion can lead to a preference for disadvantageous decks (i.e., decks A and B) because large losses become discounted, while a shape parameter closer to 0 (and below 1) makes decks with frequent gains more valuable than those with infrequent gains despite having the same objective expected value (see section 2.3; Ahn et al., 2008). Notably, reduced loss aversion on the IGT, but not a difference in utility shape, has been linked to decision-making deﬁcits in multiple clinical populations (Ahn et al., 2014; Vassileva et al., 2013), suggesting that differential valuation of gains versus losses is an individual difference with potential real-world implications. Therefore, a new IGT model should capture differential valuation of gains versus losses. 1.2. Win frequency In experience-based paradigms like the IGT, it is well known that a majority of the individuals have strong preferences for choices (i.e., decks) that win frequently, irrespec- tive of long-term expected value (e.g., Barron & Erev, 2003; Chiu & Lin, 2007; Chiu et al., 2008; Yechiam et al., 2005). For example, across studies using the IGT, deck B (win frequency = 90%) is often more preferred than deck A (win frequency = 50%) despite the long-term value of the two decks being equivalent (Lin, Chiu, Lee, & Hsieh, 2007; Steingroever et al., 2013b). In fact, this preference is so strong that most healthy subjects fail to make optimal decisions when the IGT task structure is altered so that good and bad decks have low and high win frequency, respectively (Chiu et al., 2008). In principle, decision makers may prefer deck B over more advantageous options because they do not accurately account for rare events (i.e., 1 large loss per 10 trials; see Fig. 1). Barron and Erev (2003) describe this general tendency as an underweighting of rare events that may be attributable to multiple cognitive mechanisms, including recency effects, estimation error, and/or reliance on cognitive heuristics (see Hertwig & Erev, 2009). However, it is clear from the IGT literature that recency effects alone cannot account for the observed preferences for decks with high win frequency. For example, Steingroever, Wetzels, and Wagenmakers (2013a) showed that the EVL model (Buse- meyer & Stout, 2002) — despite capturing recency effects using the delta learning rule — cannot account for the win frequency effect in the IGT. Conversely, the concave down- ward Prospect Theory utility function utilized by the PVL-Delta and VPP allows for both models to implicitly account for win frequency (see section 2.3; Ahn et al., 2008). Fur- thermore, the structure of the IGT is such that the high win frequency decks (i.e., B and D) each have a single loss, so the loss aversion parameter in both the PVL-Delta and VPP models may directly underweight the rare, negative outcomes in these decks. There- fore, the PVL-Delta and VPP implicitly capture win frequency effects and underweight- ing of rare events through the Prospect Theory utility function, but their parameters do not dissociate the effects of loss aversion or valuation (i.e., the utility shape) from that of win frequency. Relatedly, the individual posterior distributions of the utility shape param- eter are sometimes not well estimated (e.g., conﬁned around a boundary value), which is problematic from a modeling perspective. This is a potentially important oversight given the centrality of win frequency to healthy participants’ IGT performance, which may N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2537 \ndifferentiate healthy from clinical samples (see Steingroever et al., 2013b). Moreover, a model that explicitly accounts for win frequency may offer insight into experience-based underweighting of rare events. 1.3. Perseveration A series of studies shows that IGT choice preferences can be explained well by heuris- tic models of choice perseveration — the tendency to continue selecting an option regard- less of the choice value. In particular, Worthy, Hawthorne, and Otto (2013a) showed that win-stay/lose-switch choice strategies exhibit good short-term prediction accuracy relative to typical reinforcement learning models, indicating that many decision makers may Fig. 1. Structure of the original and modiﬁed versions of the IGT. Notes. (a) The original version of the Iowa Gambling Task (IGT) maintains a stationary payoff distribution for all 100 trials. Decks A and B are both “bad” decks, each with an expected value of \u0001 250 points. In con- trast, decks C and D are both “good” decks, each with an expected value of + 250 points. Additionally, decks B and D both have a 90% chance of gaining points when chosen, whereas decks A and C have only a 50% chance. We present net outcomes here, but during the actual task, participants will see a gain and loss after each selection. Actual gains presented are + 100 and + 50 for the bad and good decks, respectively. Actual losses range in value depending on the deck. (b) Net gains (i.e., sum of actual gain and loss) for the ﬁrst ten draws from each deck. (c) The modiﬁed version of the IGT is equivalent to the original version in all respects but one: the losses in the modiﬁed version become more and less severe in the bad and good decks, respectively, resulting in a drifting payoff distribution that makes the good decks easier to identify over time. The loss values change in a stepwise manner, where they are incremented after every ten draws from a given deck. (d) Net gains for the second set of 10 draws (i.e., draws 11 – 20) from the modiﬁed IGT. Note that the ﬁrst 10 draws are identical to the original version, and that the bad decks have decreased in expected value while the good decks have increased. 2538 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nengage in simple stay/switch strategies that obfuscate inferences made on their learning processes. Furthermore, decay learning rules (Erev & Roth, 1998) provide better short- term prediction accuracy than typical updating rules (i.e., the delta rule), which may be because they can mimic choice perseveration heuristics by increasing the probability that recently selected decks are chosen again (Ahn et al., 2008). Finally, despite the IGT being designed to capture the exploration – exploitation trade-off (Bechara et al., 1994), recent studies show that healthy participants fail to show evidence of progressing from a state of exploration to exploitation across trials (Steingroever et al., 2013b). Instead, par- ticipants’ individual tendencies to perseverate on or frequently switch choices remain rel- atively stable over time. Therefore, a new IGT model should capture decision makers’ tendencies to stay versus switch decks. Otherwise, other model parameters of theoretical interest (e.g., learning rates, loss aversion, etc.) may become conﬂated with perseverative tendencies. 1.4. Reversal learning Due to the structure of both the original (Bechara et al., 1994) and modiﬁed (Bechara et al., 2001) versions of the IGT (see section 2.2 for the details of the task structure), reversal learning plays a critical role in some people’s decision-making process. For example, deck B appears optimal after its ﬁrst eight selections ( + 100 point rewards on each selection), but the expected value becomes negative after a large loss ( \u0001 1,150 points) on the ninth selection. Because many decision makers begin the IGT with a pro- nounced preference for deck B, which rapidly declines over the ﬁrst 20 – 30 trials (see Steingroever et al., 2014), it is crucial that models can quickly reverse the preference for deck B after a large loss is encountered. In fact, participants who show performance deﬁ- cits on the original version of the IGT become indistinguishable from healthy controls when the deck structure is altered to make the bad decks less appealing during the ﬁrst few draws, and this increase in performance is strongly predictive of reversal learning abilities (Fellows & Farah, 2005). Neither the PVL-Delta nor the VPP models were developed to account for reversal learning. However, the perseverance heuristic in the VPP can potentially mimic short- term effects of reversal learning by increasing the probability of selecting the same choice after a gain while increasing the probability of switching choices after a loss (see sec- tion 2.3; Worthy et al., 2013b). Both reversal learning and counter-factual (i.e., ﬁctive) updating models can exhibit this behavior by updating the unchosen option utilities in ref- erence to the chosen option outcome (e.g., Gl € ascher, Hampton, & O’Doherty, 2009; Loh- renz, McCabe, Camerer, & Montague, 2007). Unlike the VPP’s perseverance heuristic, counter-factual updating can speed the learning process itself, which can lead to more rapid, long-term preference reversals. Importantly, reversal learning/counter-factual rea- soning is a well-replicated behavioral phenomenon (see Roese & Summerville, 2005) and has strong support in the model-based cognitive neuroscience literature in application to reinforcement learning tasks (i.e., experience-based tasks; Gl € ascher et al., 2009; Hampton, Bossaerts, & O’Doherty, 2006). N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2539 \n1.5. The current study In summary, the current state-of-the-art computational models of the IGT do not (a) explicitly account for the various effects observed in behavioral data or (b) provide a compromise between the multiple different model comparison metrics used for model selection (i.e., short- and long-term prediction accuracy and parameter recovery). Here, we present the ORL, a novel reinforcement learning model which explicitly accounts for the effects of expected value, gain – loss frequency, choice perseveration, and reversal- learning with only ﬁve free parameters. By ﬁtting 393 subjects’ IGT choice data, we show that the ORL model provides good short- and long-term prediction accuracy and parameter recovery in comparison to the PVL-Delta and VPP models. Furthermore, the ORL performs consistently well for both the original and modiﬁed version of the IGT and on data collected across multiple different research sites. Finally, we apply the ORL to IGT data collected from amphetamine, heroin, and cannabis users (Ahn et al., 2014; Fridberg et al., 2010), and we show that the ORL identiﬁes theoretically meaningful dif- ferences in decision-making between substance-using groups which are supported by prior studies. 2. Methods 2.1. Participants We used IGT data collected from multiple studies to validate the ORL model, includ- ing (a) an openly accessible, “many labs” collaboration dataset containing IGT data from 247 healthy participants across eight independent studies (Steingroever et al., 2015); 1 (b) data from Ahn et al. (2014), where 48 healthy controls, and 43 pure heroin and 38 pure amphetamine users in protracted abstinence completed a modiﬁed version of the IGT; and (c) data from Fridberg et al. (2010), where 17 chronic cannabis users completed the original version of the IGT. 2 Table 1 summarizes the multiple datasets used in the current study. In total, our study includes data from 393 participants. See the cited studies for speciﬁc details on the participants included in each dataset. 2.2. Tasks In both versions of the IGT, decks A and B are considered “bad” decks because they have a negative expected value, and decks C and D are “good” decks because they have a positive expected value (Fig. 1a and c). The order of cards within each deck (for both versions) is predetermined so that each subject will experience the same sequence of out- comes when drawing from a given deck (e.g., Fig. 1b and d). The original version of the IGT maintains a stationary payoff distribution throughout the task (Bechara et al., 1994), whereas the payoff distribution of the modiﬁed version changes over trials (Bechara 2540 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \net al., 2001) — the net losses in good and bad decks become less and more extreme, respectively, after every 10 selections made from a given deck (c.f. Fig. 1b to d). 2.3. Reinforcement learning models Prospect Valence Learning model with delta rule (PVL-Delta). The PVL-Delta model (Ahn et al., 2008) uses a prospect theory utility function (Kahneman & Tversky, 1979) to transform realized, objective monetary outcomes into subjective utilities: u ð t Þ ¼ x ð t Þ a ; if x ð t Þ \u0003 0 \u0001 k j x ð t Þj a ; otherwise ð 1 Þ Above, t denotes the trial number, u t ð Þ is the subjective utility of the experienced out- come, x t ð Þ is the experienced net outcome (i.e., the amount won minus amount lost on trial t ), and a ð 0 \\ a \\ 2 Þ and k ð 0 \\ k \\ 10 Þ are free parameters which govern the shape of the utility function and sensitivity to losses relative to gains, respectively. The a parame- ter in the Prospect Theory utility function can account for the win frequency effect (e.g., Chiu et al., 2008). For example, when a \\ 1, the summed subjective utility of receiving $1 ﬁve times is greater than receiving $5 once (i.e., the utility curve is concave for posi- tive outcomes and convex for negative ones), so decision makers with an a below 1 would be expected to prefer decks with high win frequency over objectively equivalent decks which win less often (Ahn et al., 2008). Likewise, if k [ 1, the subjective experi- ence of a given loss is greater in magnitude than an equivalent gain, which captures the idea that “losses loom larger than equivalent gains” (Kahneman & Tversky, 1979) when being subjectively evaluated. Note that when making decisions from experience — as in the IGT — the modal participant does not typically show loss aversion (Erev, Ert, & Yechiam, 2008); instead, participants tend to underweight rare events (e.g., Barron & Erev, 2003; Hertwig, Barron, Weber, & Erev, 2004). Previous modeling analyses with Table 1 Breakdown of datasets used in the current study Dataset N Population IGT Version Study Citation Kjome 19 Healthy Modiﬁed Kjome et al. (2010) Premkumar 25 Healthy Modiﬁed Premkumar et al. (2008) Wood 153 Healthy Modiﬁed Wood et al. (2005) Worthy 35 Healthy Original Worthy et al. (2013b) Ahn 48 Healthy Modiﬁed Ahn et al. (2014) Ahn 38 Amphetamine Modiﬁed Ahn et al. (2014) Ahn 43 Heroin Modiﬁed Ahn et al. (2014) Fridberg 15 Healthy Original Fridberg et al. (2010) Fridberg 17 Cannabis Original Fridberg et al. (2010) N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2541 \nthe IGT have exhibited a similar pattern, where group-level loss aversion parameters are mostly below 1 (e.g., Ahn et al., 2014). The PVL-Delta model assumes that decision makers update their expected values for each deck using a simpliﬁed variant of the Rescorla – Wagner rule (i.e., the delta rule; Rescorla & Wagner, 1972): E j ð t þ 1 Þ ¼ E j ð t Þ þ A \u0004 ð u ð t Þ \u0001 E j ð t ÞÞ ð 2 Þ Here, E j t ð Þ is the expected value of chosen deck j on trial t , and A ð 0 \\ A \\ 1 Þ is a learning rate controlling how quickly decision makers integrate recent outcomes into their expected value for a given deck. Expected values are entered into a softmax function to generate choice probabilities: Pr ½ D ð t þ 1 Þ ¼ j \u0005 ¼ e h \u0004 E j ð t þ 1 Þ P 4 k ¼ 1 e h \u0004 E k ð t þ 1 Þ ð 3 Þ where D t ð Þ is the chosen deck on trail t , and h is determined by: h ¼ 3 c \u0001 1 ð 4 Þ Here, c ð 0 \\ c \\ 5 Þ is a free parameter which represents trial-independent choice consis- tency (Yechiam & Ert, 2007). If c is close to 0 or 5, it indicates that decision makers are responding randomly or (near)deterministically, respectively, with respect to their expected values for each deck. Altogether, the PVL-Delta model contains four free parameters ( A , a , c , k ). Value-Plus-Perseverance model (VPP). The VPP model expands upon the PVL-Delta model by adding an additional term for choice perseverance (Worthy et al., 2013b): P j ð t þ 1 Þ ¼ K \u0004 P j ð t Þ þ \u0002 P ; if x ð t Þ \u0003 0 K \u0004 P j ð t Þ þ \u0002 N ; otherwise ð 5 Þ P j t ð Þ indicates the perseveration value for chosen deck j on trial t , which decays by K ð 0 \\ K \\ 1 Þ on each trial. When chosen, the perseveration value for deck j is updated by ϵ P (-Inf < ϵ P < Inf) or ϵ N (-Inf < ϵ N < Inf) based on the sign of outcome. Positive values for ϵ P and ϵ N indicate tendencies for decision makers to “perseverate” the deck chosen on the previous trial, whereas negative values indicate a switching tendency. The VPP assumes that the expected value (from the PVL-Delta model) and persevera- tion terms are integrated into a single value signal: V j ð t þ 1 Þ ¼ w \u0004 E j ð t þ 1 Þ þ ð 1 \u0001 w Þ \u0004 P j ð t þ 1 Þ ð 6 Þ where x ð 0 \\ x \\ 1 Þ is a parameter that controls the weight given to the expected value and perseveration signals. As x approaches 0 or 1, the VPP reduces to the perseveration 2542 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nmodel or the PVL-Delta model alone, respectively. The VPP uses the same softmax func- tion as the PVL-Delta to generate choice probabilities, except that E j t þ 1 ð Þ is replaced with V j t þ 1 ð Þ . Altogether, the VPP contains eight free parameters ( A , a , c , k , ϵ P , ϵ N , K , Outcome-Representation Learning model (ORL). Here, we propose the ORL as a novel learning model for the IGT. Unlike the PVL-Delta and VPP models, the ORL assumes that the expected value and win frequency for each deck are tracked separately as opposed to implicitly within the Prospect Theory utility function (Pang, Blanco, Maddox, & Worthy, 2016). 3 Note that separate tracking of expected value and win frequency makes the ORL similar to the class of risk-sensitive reinforcement learning models which forgo maximizing expected value to minimize potential risks (e.g., Mihatsch & Neuneier, 2002). The expected value of a deck is updated with separate learning rates for positive and negative outcomes: EV j ð t þ 1 Þ ¼ EV j ð t Þ þ A rew \u0004 ð x ð t Þ \u0001 EV j ð t ÞÞ ; if x ð t Þ \u0003 0 EV j ð t Þ þ A pun \u0004 ð x ð t Þ \u0001 EV j ð t ÞÞ ; otherwise ð 7 Þ where EV j t ð Þ denotes the expected value of chosen deck j on trial t , and A rew ð 0 \\ A rew \\ 1 Þ and A pun ð 0 \\ A pun \\ 1 Þ are learning rates which are used to update expectations after reward (i.e., positive) and punishment (i.e., negative) outcomes, respec- tively. Unlike the PVL-Delta and VPP models, the ORL is updating expected values using the objective outcome x t ð Þ , not the subjective utility u t ð Þ . The use of separate learning rates for positive versus negative outcomes allows for the ORL model to account for over- and undersensitivity to losses and gains, similar to the loss aversion parameter shared by the PVL-Delta and VPP. Speciﬁcally, the larger the difference is between the positive and negative learning rates, the more learning is domi- nated by either positive or negative outcomes. We used separate learning rates, as opposed to a loss-aversion parameterization, because there is strong neurobiological and behavioral evidence for learning models with separate learning rates for positive versus negative outcomes (e.g., Doll, Jacobs, Sanfey, & Frank, 2009; Gershman, 2015). For example, Parkinson’s patients learn more quickly from negative compared to positive out- comes, and dopamine medication reverses this bias (Frank, Seeberger, & O’Reilly, 2004). Additionally, positive and negative learning rates are modulated by genes that are par- tially responsible for striatal dopamine functioning (Frank, Moustafa, Haughey, Curran, & Hutchison, 2007), and more recent evidence implicates striatal D1 and D2 receptor stimu- lation in learning from positive and negative outcomes, respectively (Cox et al., 2015). To account for the win frequency effect, the ORL separately tracks win frequency as follows: EF j ð t þ 1 Þ ¼ EF j ð t Þ þ A rew \u0004 ð sgn ð x ð t ÞÞÞ \u0001 EF j ð t ÞÞ ; if x ð t Þ \u0003 0 EF j ð t Þ þ A pun \u0004 ð sgn ð x ð t ÞÞ \u0001 EF j ð t ÞÞ ; otherwise ð 8 Þ N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2543 \nwhere EF j t ð Þ denotes the “expected outcome frequency,” A rew ð 0 \\ A rew \\ 1 Þ and A pun ð 0 \\ A pun \\ 1 Þ are learning rates shared with the expected value learning rule, and sgn x t ð Þ ð Þ returns 1, 0, or \u0001 1 for positive, 0, or negative outcome values on trial t , respec- tively. The ORL model also includes a reversal-learning component for EF j t ð Þ . EF j 0 t ð Þ refers to the expected outcome frequency of all unchosen decks j 0 on trial t : EF j 0 ð t þ 1 Þ ¼ EF j 0 ð t Þ þ A pun \u0004 ð \u0001 sgn ð x ð t ÞÞ C \u0001 EF j 0 ð t ÞÞ ; if x ð t Þ \u0003 0 EF j 0 ð t Þ þ A rew \u0004 ð \u0001 sgn ð x ð t ÞÞ C \u0001 EF j 0 ð t ÞÞ ; otherwise ð 9 Þ Here, the learning rates are shared from the expected value learning rule, and C is the number of possible alternative choices for the chosen deck j . Note that when updating unchosen decks j 0 , the reward learning rate is used if the chosen outcome was negative and the punishment learning rate is used if the chosen outcome was positive. Because there are four possible choices in both versions of the IGT, there are always three possi- ble alternative choices. Therefore, C is set to three in the current study. Note that if there were only a single alternative choice (e.g., simple two-choice tasks), C would be set to 1 and the frequency heuristic would reduce to a “double-updating” rule often used to model choice behavior in probabilistic reversal learning tasks (e.g., Gl € ascher et al., 2009). 4 The ORL model also employs a simple choice perseverance model to capture decision makers’ tendencies to stay or switch decks, irrespective of the outcome: PS j ð t þ 1 Þ ¼ 1 1 þ K ; if D ð t Þ ¼ j PS j ð t Þ 1 þ K ; otherwise ð 10 Þ where K is determined by: K ¼ 3 K 0 \u0001 1 ð 11 Þ Here, PS j t ð Þ is the perseverance weight of deck j on trial t , and K is a decay parameter controlling how quickly decision makers forget their past deck choices. K 0 is estimated 2 0 ; 5 ½ \u0005 , therefore, K 2 0 ; 242 ½ \u0005 (see Eq. 11). The above model implies that the persever- ance weight of the chosen deck is set to 1 on each trial, and subsequently all persever- ance weights decay exponentially before a choice is made on the next trial. We used this parameterization because it showed the best performance for estimating K compared to other parameterizations (e.g., PS j t þ 1 ð Þ ¼ PS j t ð Þ \u0006 K ). Low or high values for K suggest that decision makers remember long or short histories of their own deck selections, respectively. The ORL model assumes that value, frequency, and perseverance signals are integrated in a linear fashion to generate a single value signal for each deck: 2544 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nV j ð t þ 1 Þ ¼ EV j ð t þ 1 Þ þ EF j ð t þ 1 Þ \u0004 b F þ PS j ð t þ 1 Þ \u0004 b P ð 12 Þ Here, b F ð\u00011 \\ b F \\ 1Þ and b P ð\u00011 \\ b P \\ 1Þ are weights which reﬂect the effect of outcome frequency and perseverance on total value with respect to the expected value of each deck. Therefore, values for b F less than or greater than 0 indicate that decision mak- ers prefer decks with low or high win frequency, respectively. Additionally, values for b P less than or greater than 0 indicate that decision makers prefer to switch or stay with recently chosen decks, respectively. Note that the expected value ( EV ) is a reference point which frequency and perseverance effects are evaluated against, so the ORL assumes that the “weight” of EV is equal to 1. The ORL uses the same softmax function as the VPP to generate choice probabilities, except that the choice consistency/inverse temperature parameter ( h ) is set to 1. We do not estimate choice consistency for the ORL due to parameter identiﬁability problems between h , b F , and b P . Altogether, the ORL contains ﬁve free parameters ( A rew , A pun , K , b F , b P ). 5 The ORL model will be added to hBayesDM , an easy-to-use R toolbox for computa- tional modeling of a variety of different reinforcement learning and decision-making models using hierarchical Bayesian analysis (Ahn, Haines, & Zhang, 2017). Additionally, all R codes used to preprocess, ﬁt, simulate, and plot our results will be uploaded to our GitHub repository upon publication of this manuscript (https://github.com/CCS-Lab). 2.4. Hierarchical Bayesian analysis We used hierarchical Bayesian analysis (HBA) to estimate free parameters for each model (Kruschke, 2015; Lee, 2011; Lee & Wagenmakers, 2011; Rouder & Lu, 2005; Shiffrin, Lee, Kim, & Wagenmakers, 2008). HBA offers many beneﬁts over more con- ventional approaches (i.e., maximum likelihood estimation), including (a) modeling of individual differences with shrinkage (i.e., pooling) across subjects, and (b) computation of posterior distributions as opposed to point estimates. Previous studies show that HBA leads to more accurate individual-level parameter recovery than the individual MLE approach (e.g., Ahn, Krawitz, Kim, Busemeyer, & Brown, 2011). The HBA was conducted using Stan (version 2.15.1), a probabilistic programming lan- guage which uses Hamiltonian Monte Carlo (HMC), a variant of Markov Chain Monte Carlo (MCMC), to efﬁciently sample from high-dimensional probabilistic models as spec- iﬁed by the user (Carpenter, Gelman, Hoffman, & Lee, 2016). For each dataset used in the current study, we assumed that individual-level parameters were drawn from group- level distributions. Group-level distributions were assumed to be normally distributed, where the priors for locations (i.e., means) and scales (i.e., standard deviations) were assigned normal distributions. Additionally, we used non-centered parameterizations to minimize the dependence between group-level location and scale parameters (Betancourt & Girolami, 2013). Bounded parameters (e.g., learning rates 2 0 ; 1 ð Þ ) were estimated in N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2545 \nan unconstrained space and then probit-transformed to the constrained space — and scaled if necessary — to maximize MCMC efﬁciency within the parameter space (Ahn et al., 2014, 2017; Wetzels, Vandekerckhove, Tuerlinckx, & Wagenmakers, 2010). Using the reward learning rate A rew from the ORL model as an example, formal speciﬁcation of the bounded parameters followed the form: l A rew \u0007 Normal ð 0 ; 1 Þ r A rew \u0007 Normal ð 0 ; 0 : 2 Þ A rew 0 \u0007 Normal(0,1) A rew ¼ Probit ð l A rew þ r A rew \u0004 A rew 0 Þ ð 13 Þ where l Arew and r Arew are the location and scale parameters for the group-level distribu- tion, A rew 0 is a vector of individual-level parameters on the unconstrained space, A rew is a vector of individual-level parameters after they have been probit-transformed back to the constrained space, and Probit x ð Þ is the inverse cumulative distribution function of the standard normal distribution. This parameterization ensures that after being probit-trans- formed, the hyper prior distribution over the subject-level parameters is (near)uniform between the parameter bounds. For parameters bounded 2 0 ; upper ð Þ (e.g., K ), we used the same parameterization as above but scaled to the upper bound accordingly: K ¼ Probit ð l K þ r K \u0004 K 0 Þ \u0004 5 ð 14 Þ For unbounded parameters (e.g., b F ), we used the same parameterization outline in Eq. 13 except we set the hyper standard deviation to a half-Cauchy 0 ; 1 ð Þ . All models were sampled for 4,000 iterations, with the ﬁrst 1,500 as warmup (i.e., burn-in), across four sampling chains for a total of 10,000 posterior samples for each parameter. Conver- gence to target distributions was checked visually by observing trace-plots and numeri- cally by computing Gelman – Rubin — also known as ^ R — statistics for each parameter (Gelman & Rubin, 1992). ^ R values for all models were below 1.1, suggesting that the variance between chains did not outweigh variance within chains. 2.5. Model comparison: Leave-one-out information criterion We used the leave-one-out information criterion (LOOIC) to compare one-step-ahead prediction accuracy across models. LOOIC is an approximation to full leave-one-out pre- diction accuracy that can be computed using the log pointwise posterior predictive density (lpd) of observed data (Vehtari, Gelman, & Gabry, 2017). Here, we computed the lpd by taking the log likelihood of each subject’s actual choice on trial t þ 1 conditional on their parameter estimates and choices from trials 2 1 ; 2 ; . . . ; t f g . This procedure is iterated for all trials and for each posterior sample. Log likelihoods are then summed across trials within subjects. This summation results in an N \u0006 S lpd matrix, where N is the number of subjects and S is the number of posterior samples. We used the loo R package (Vehtari 2546 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \net al., 2017) to estimate the LOOIC from the lpd matrix. LOOIC is on the deviance scale, where lower values indicate better model ﬁts. 2.6. Model comparison: Choice simulation We used the simulation method to compare long-term prediction accuracy across mod- els (Ahn et al., 2008; Steingroever et al., 2014). The simulation method involves two steps: (a) models are ﬁt to each group’s data, and (b) ﬁtted model parameters from step 1 are used to simulate subjects’ choice behavior given the task payoff structure. Simulated and true choice patterns are then compared to determine how well the model parameters capture subjects’ choice behavior. In the current study, we employ a fully Bayesian simu- lation method, which takes random draws from each subject’s joint posterior distribution across ﬁtted model parameters to simulate choice data (Steingroever et al., 2013a, 2014). We iterated this procedure 1,000 times for each subject (i.e., 1,000 draws from individ- ual-level, joint posteriors), and choice probabilities for each deck were stored for each iteration. We then averaged the choice probabilities for each deck across iterations and then subjects. Finally, we computed the mean squared deviation (MSD) between the experimental and simulated choice probabilities as follows: MSD ¼ 1 4 \u0004 n n t ¼ 1 4 j ¼ 1 ð \u0003 D exp j ð t Þ \u0001 \u0003 D sim j ð t ÞÞ 2 ð 15 Þ where n is the number of trials, t is the trial number, j is the deck number, is the average across-subject probability of choosing deck j on trial t , and is the average across-subject simulated probability (across 1,000 iterations as described above) of selecting deck j on trial t . Before computing MSD scores, we smoothed the experimental data (i.e., \u0003 D exp ð t Þ ) with a moving average of window size 7 (Ahn et al., 2008). Note that this method is dif- ferent from a posterior predictive check because it does not condition on observed response data (Gelman, Hwang, & Vehtari, 2013). 2.7. Model comparison: Parameter recovery Parameter recovery is a method used to determine how well a model can estimate (i.e., recover) known parameter values, and it typically follows two steps: (a) choice data are simulated using a set of true parameters for a given model and task structure, and (b) the model is ﬁt to the simulated choice data and the recovered parameter estimates are com- pared to the true parameters (e.g., Ahn et al., 2011; Donkin et al., 2010; Wagenmakers et al., 2007). We used the same set of parameters to simulate choices from the modiﬁed and original IGT task structure. We generated the parameter set by taking the means of the individual-level posterior distributions of each model ﬁt to the 48 control subjects’ data from Ahn et al. (2014) to ensure that the true parameter values were reasonably dis- tributed and representative of human decision makers for each model. N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2547 \nWe used two different parameter recovery methods. First, we compared the means of the posterior distributions for each individual-level parameter, and for each model, to the true parameters by plotting all the parameter values in a standardized space. We trans- formed parameters by z-scoring the recovered posterior means of each parameter by the mean and standard deviation of true parameters (i.e., the parameter set used to simulate choices) across individual-level parameters, which allowed us to determine how well the location of true parameters was recovered for each parameter and model. Second, we compared each of the true parameters to the entire posterior distribution of the respective recovered parameter by computing rank-ordered (i.e., Spearman’s ) correlations between the true and recovered parameter values across individual-level parameters. We iterated this procedure over each sample from the joint posterior distribution to estimate how well the rank-order between true parameters could be recovered for each parameter and model. The rank-order is particularly important for making inferences on relative parameter dif- ferences between subjects. Together, the parameter recovery methods we used here allowed us to infer how well each model could recover parameters in an absolute and rel- ative sense. 3. Results 3.1. Model comparison: Leave-one-out information criterion Fig. 2 shows the one-step-ahead leave-one-out information criterion (LOOIC) perfor- mance for each model and datasets used in the current study. As seen in the graphs, while the ORL and VPP outperform the PVL-Delta, they show similar performance to one another. Notably, the ORL outperformed the VPP in all three substance-using groups, albeit by only a negligible amount in heroin users. Altogether, the LOOIC comparisons suggest that the ORL shows similar short-term prediction performance to the VPP (i.e., better than the PVL-Delta) across both versions of the IGT and across multiple popula- tions with different decision-making strategies although the ORL has three fewer parame- ters than the VPP (5 vs. 8). 3.2. Model comparison: Choice simulation The raw choice data and choice simulations for each dataset are depicted in Fig. 3, and the mean squared deviations (MSDs) are shown in Table 2. Similarly to previous analyses (Ahn et al., 2014; Steingroever et al., 2013a), the PVL-Delta showed good simulation per- formance for both modiﬁed and original IGT versions in both healthy control and sub- stance-using groups. Unlike previous analyses (Ahn et al., 2014; but see Worthy et al., 2013b), the VPP showed similar performance to the PVL-Delta across datasets. 6 Altogether, the simulation results are less clear on which of the models performs best for long-term pre- diction accuracy. In fact, the variation in performance between datasets is much greater than the variation in performance between models within each dataset (see Table 2) . 2548 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \n3.3. Model comparison: Parameter recovery Parameter recovery results for both versions of the IGT are shown in Fig. 4. For the modiﬁed IGT, the PVL-Delta and ORL both show good parameter recovery across model parameters while the VPP performs poorly. For the VPP, the recovered posterior means were systematically higher than the true parameters for the learning rate ( A ), and systematically lower for the choice consistency ( c ) and reinforcement weight ( x ). For the PVL-Delta and ORL, recovered posterior means were well-distributed around the true parameter means. Additionally, the full posterior recovery results for the VPP showed much more variable correlations between true parameters and the recovered posteriors compared to the PVL-Delta and ORL, suggesting that the PVL-Delta and ORL provide more precise posterior estimates and better capture the variance between individual-level parameter estimates (i.e., “subjects”) compared to the VPP. For the original IGT, parame- ter recovery results were similar. While the VPP showed slightly better performance in the original IGT, still the posterior means for x and c were systematically lower and Fig. 2. Post hoc model ﬁts across models and datasets. Notes. Results of the leave-one-out information criterion (LOOIC) model comparison on one-step-ahead (i.e., short-term) prediction accuracy for each of the datasets analyzed in the current study. Lower LOOIC values indicate better model performance. LOOIC values were baselined by the best model in each comparison. The dashed line represents the zero point (i.e., best model LOOIC = 0), and any deviations from the zero point represent competing model LOOIC values. Error bars represent two standard errors on the difference between the best model and the respective competing model. N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2549 \nFig. 3. True versus simulated choice proportions across time. Notes. Behavioral and simulation performance for the healthy control data for each of the datasets in the current study. Choice behavior is summarized per block, where blocks were constructed by calculating the proportion of choices made from each deck, across subjects, in 20-trial increments (i.e., block 1 = trials 1 – 20, block 2 = trials 21 – 40, etc.). Choice proportions across subjects are represented by points, and gray ribbons indicate 1 standard error. In general, subjects begin with a preference for deck B, but they learn to prefer deck D as they progress through the task. Additionally, subjects show a clear preference for decks with high win frequency (b and d) over alternatives. Simulation performance is summarized per trial, across subjects within each dataset. The grey ribbons represent 1 standard error across subjects’ averaged simulated choice probabilities. 2550 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nposterior means for A were systematically higher than their true values. Together, the parameter recovery results suggest that both the PVL-Delta and ORL provide more accu- rate and precise parameter estimates than the VPP for both versions of the IGT. 3.4. Applications to substance users Because the ORL consistently performed as well or better than competing models across all groups in the current study, we used the ORL to examine group differences in model parameters. Note that we only compared substance-using groups to the healthy control groups within the same studies to minimize any potential between-study effects. Fig. 5 and Fig. 6 show the posterior estimates and differences in posterior estimates for each group, respectively. Below, we use the term “strong evidence” to refer to group differences where the 95% highest density interval (HDI) excludes 0 (Kruschke, 2015). We do not endorse binary interpretations of signiﬁcant differences using this threshold, and we refer readers to the graphical comparisons (Fig. 6) to judge parameters for meaningful differences. Within the dataset from Ahn et al. (2014), the heroin-using group showed strong evidence of lower punishment learning rates than healthy controls (95% HDI = [0.003, 0.04]). A low punish- ment learning rate indicates less updating of expectations after experiencing a loss, a ﬁnding which is consistent with prior studies showing that heroin users have lower loss-aversion than controls (Ahn et al., 2014). We did not ﬁnd strong evidence of differences between amphetamine and heroin users. However, there was some evidence (see Fig. 6) that amphe- tamine users had more negative perseverance weights than heroin users (95% HDI = [ \u0001 2.67, 0.79]). Within the dataset from Fridberg et al. (2010), chronic cannabis users showed strong evidence of greater reward learning rates (95% HDI = [ \u0001 0.23, \u0001 0.05]) and some evidence of lower punishment learning rates (95% HDI = [ \u0001 0.001, 0.04]) compared to healthy controls, which is consistent with a previous analysis of this dataset using the PVL-Delta model showing that cannabis users were more sensitive to rewards and less sen- sitive to losses compared to healthy controls (Fridberg et al., 2010). Lastly, cannabis users showed strong evidence for more negative perseverance weights than healthy controls (95% HDI = [0.004, 4.09]), indicating a strong preference toward switching, as opposed to perse- verating on, choices irrespective to the expected value of each deck. Table 2 Mean squared deviations of true from simulated choice probabilities Dataset Model 1 2 3 4 5 6 7 8 9 ORL 41.6 20.3 6.9 23.4 7.4 15.4 9.7 81.5 25.1 PVL-Delta 44.9 20.6 4.4 17.3 8.5 12.9 7.7 72.8 18.8 VPP 44.7 20.9 6.0 16.9 8.8 15.0 9.0 85.5 20.6 Notes. 1 = Kjome; 2 = Premkumar; 3 = Wood; 4 = Worthy; 5 = Ahn (Healthy); 6 = Ahn (Ampheta- mine); 7 = Ahn (Heroin); 8 = Fridberg (Healthy); 9 = Fridberg (Cannabis). The lowest mean squared devi- ation (MSD) is bolded within each dataset. N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2551 \n4. Discussion We present a novel cognitive model (the ORL) for the IGT which shows excellent short- and long-term prediction accuracy across both versions of the task and across an array of different clinical populations. The ORL explicitly models the four most consis- tent trends found in IGT behavioral data, including long-term expected value, gain – loss frequency, perseverance, and reversal-learning. Overall, we showed that the ORL outper- formed or showed comparable performance to competing models in all three model Fig. 4. Parameter recovery results across models and versions of the IGT. Notes. Parameter recovery results for the modiﬁed and original IGT tasks. Each task structure was simulated for each model using the same set of 48 individual-level parameter sets across modiﬁed and original task structures. Posterior mean results show comparisons of the true parameters with the means of the posterior distributions of the recovered parameters after being standardized. We standardized parameters by z-scoring the true and recovered posterior means by the mean and standard deviation of each of the 48 true parameter sets. This method allowed us to visualize the bias in recovered posterior means, where any values falling above or below the solid diagonal line indicate higher or lower recovered means in reference to the true parameters, respectively. Dashed and dotted lines reﬂect 1 and 2 standard deviations in the standardized space, respectively. Note that some parameter values fell outside of the graphs (particularly for the VPP), but zooming out further obfuscates the results. Full posterior recovery results were generated by computing a Spearman’s rank-order correlation between each set of individual-level true parameters and the respective set of individual-level recovered parameters for each sample in the recovered posterior distribution. Full posterior recovery results, therefore, represent the uncertainty in recovering the relative positions of the true parameters across all individual-level parameters (i.e., across all “subjects”). Distributions with mass closer to 1 indicate that the order between true parameters is recovered well for a given parameter and model. Dotted lines repre- sent 2.5% and 97.5% quantiles, dashed lines represent 25% and 75% quantiles, and the solid line represents the median. Quantiles were calculated across all parameters. 2552 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \ncomparison indices, including post hoc test (LOOIC), simulation performance, and parameter recovery. The results suggest that future research using the IGT should con- sider the ORL a top choice for cognitive modeling analyses. Consistent with prior studies, our model comparison results suggest that any single measure used to compare models might not be sufﬁcient (Ahn et al., 2008, 2014; Stein- groever et al., 2014; Yechiam & Ert, 2007). For example, we found that the ORL consis- tently outperformed the VPP using parameter recovery metrics yet performed similarly to the VPP in short- and long-term prediction accuracy. Our results underscore the impor- tance of using many model comparison metrics in deciding between competing cognitive models (Heathcote et al., 2015; Palminteri, Wyart, & Koechlin, 2017). Many studies use only information criteria such as LOOIC (e.g., Akaike or Bayesian information criteria) Fig. 5. Group-level ORL parameters across healthy and substance-using groups. Notes. (a) Group-level parameter distributions for the healthy controls, amphetamine users, and heroin users who underwent the modiﬁed IGT. (b) Group-level parameter distributions for the healthy controls and chronic cannabis users who underwent the original IGT. N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2553 \nwhen choosing one among many cognitive models, and our results suggest that this may lead to imprecise inferences. Indeed, despite the VPP performing excellently when assessed using information criteria alone (i.e. LOOIC), the parameter recovery results indicate that multiple VPP model parameters might be imprecise at the subject level and biased at the group level (see Fig. 4). For cognitive models to be useful in identifying individual differences (e.g., for clinical decision making), it is crucial that future studies conduct parameter recovery tests to ensure that parameter interpretations are valid. When applied to IGT performance of pure substance users, the ORL revealed that her- oin users in protracted abstinence were less sensitive to punishments (i.e., lower punish- ment learning rates) compared to healthy controls. The ﬁnding of lower punishment sensitivity in the heroin-using group is consistent with Ahn et al. (2014), where heroin users showed lower loss aversion (i.e., k from the VPP) than healthy controls. We also found some evidence that amphetamine users engaged in more switching behavior than heroin users (see b P in Fig. 5 and 6). Although weak in comparison to other reported dif- ferences, this ﬁnding is consistent with a previous study showing that high levels of Fig. 6. Differences in group-level ORL parameters between healthy and substance-using groups. Notes. Differences in group-level parameter distributions (for the ORL) between healthy controls and sub- stance-using groups. Solid red lines highlight the 95% highest posterior density interval (HDI), and dashed red lines reﬂect the 0 point. Values on the left and right sides of each graph represent the proportion of each distribution falling below and above the 0 point, respectively. Note that groups were compared within studies to minimize any confounding effects of task implementation, study design, and other site-speciﬁc experimen- tal details. 2554 N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) \nexperience-seeking traits are positively and negatively predictive of amphetamine and her- oin users, respectively (Ahn & Vassileva, 2016). Notably, behavioral summaries of the amphetamine and heroin user’s choice preferences were indistinguishable (see Ahn et al., 2014). Additionally, the ORL revealed that chronic cannabis users were more sensitive to rewards (i.e., higher reward learning rates) and more likely to engage in exploratory behavior (i.e., more negative perseveration weight) than healthy controls. These ﬁndings converge with previous modeling results using the PVL-Delta (Fridberg et al., 2010) and with pharmacological studies showing that cannabis administration can increase sensitivity to rewards (and not punishments), which in turn may lead to more risk-taking behaviors (Lane, 2002; Lane, Cherek, Tcheremissine, Lieving, & Pietras, 2005). Importantly, our ﬁnding that chronic cannabis users tend to engage in exploratory behavior — irrespective to the value of each deck — suggests that the high levels of risk-taking induced by acute can- nabis consumption may have long-lasting effects that inﬂuence not only sensitivity to rewards but also the tendency to seek out novel stimuli. Future studies may further clarify the temporal relationship between reward sensitivity and sensation seeking in cannabis users by applying the ORL to cross-sectional or longitudinal samples. Finally, research by our own and other groups consistently reveals that computational model parameters are more sensitive to dissociating substance-speciﬁc and disorder-speciﬁc neurocognitive pro- ﬁles than standard neurobehavioral performance indices (see Ahn et al., 2016 for a review). Such parameters show signiﬁcant potential as novel computational markers for addiction and other forms of psychopathology, which could help reﬁne neurocognitive phenotypes and develop more rigorous mechanistic models of psychiatric disorders (Ahn & Busemeyer, 2016). Our results have implications for a wide range of cognitive tasks that involve learning from experience. In particular, our ﬁnding that differential learning rates for positive and negative outcomes can capture the same behavioral patterns that have previously been attributed to a loss aversion parameter (cf. controls vs. heroin users in Fig. 5 to ﬁndings published in Ahn et al., [2014]) suggests that the underweighting of rare events that is observed in experience-based tasks may arise from learning, rather than valuation mecha- nisms (e.g., Barron & Erev, 2003; Hertwig et al., 2004). While the ORL limits this underweighting to tasks including outcomes in both gain and loss domains, future studies may extend the model to capture decisions in purely gain or loss domains by modifying the function that codes outcomes as gains versus losses (see equations 7 – 9). One poten- tial solution could be to code outcomes as gains versus losses based on the sign of the prediction error rather than the objective outcome; in fact, cognitive models utilizing sep- arate learning rates for positive versus negative prediction errors are gaining popularity in the decision sciences due to their theoretical and empirical support (e.g., Gershman, 2015). Conﬂict of interest The authors declare no competing ﬁnancial interests. N. Haines, J. Vassileva, W.-Y. Ahn / Cognitive Science 42 (2018) 2555 \nAcknowledgments Some of the data in this study were collected with ﬁnancial support from the National Institute on Drug Abuse and Fogarty International Center (award number: R01DA021421). Notes 1. We only included data from Steingroever et al. (2015), where participants under- went either the original or modiﬁed versions of the IGT as described in Fig. 1. This criterion excluded any datasets where the order of cards in each deck was random- ized or where participants were required to complete other tasks (i.e., introspective judgements) throughout IGT administration. 2. Healthy controls from Fridberg et al. (2010) are included in the many labs dataset from Steingroever et al. (2015). 3. Pang, B., Byrne, K., A., Worthy, D., A. (unpublished). When more is less: working memory load reduces reliance on a frequency heuristic during decision-making. 4. We tried various versions of the reversal learning process (e.g., reversal learning on EV j ( t ) or both EV j ( t ) and EF j ( t )) and versions of the model without the rever- sal learning component, but the version we report in this paper showed the best model ﬁt. 5. Note that we tried various other models from the reinforcement learning literature, including: variants with the Pearce-Hall updating rule (Pearce & Hall, 1980), work- ing memory models (Collins, Albrecht, Waltz, Gold, & Frank, 2017), and risk aver- sion models (d’Acremont et al., 2009). However, none of these models provided an improved ﬁt of the data and we do not report them for brevity. 6. Note that an error was discovered in simulation code used for the VPP in Ahn et al. (2014), which may partially account for the previous ﬁnding that the VPP exhibited poor simulation performance."
}