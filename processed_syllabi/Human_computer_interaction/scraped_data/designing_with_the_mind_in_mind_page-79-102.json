{
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "text": "CHAPTER 79 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00006-4 Copyright © 2021 Elsevier Inc. All rights reserved. Most people in industrialized nations grow up in households and school districts that promote education and reading. They learn to read as young children and become good readers by adolescence. As adults, most of our activities during a normal day involve reading. Reading is for most educated adults automatic, leaving our conscious minds free to ponder the meaning and implications of what we are reading. Because of this background, it is common for good readers to consider reading a “natural” human activity, like speaking. WE ARE WIRED FOR LANGUAGE BUT NOT FOR READING Speaking and understanding spoken language is a natural human ability, but read- ing is not . Over hundreds of thousands—perhaps millions—of years, the human brain evolved the neural structures necessary to support spoken language. Nor- mal humans are born with an innate ability to learn, with no systematic training, whatever language they are exposed to. After early childhood, this ability decreases significantly. For example, newborn babies can hear and distinguish all the sounds of all languages, but as they learn the language of their home environment, they lose the ability to distinguish sounds that are not distinguished in that language (Eagleman, 2015). By adolescence, learning a new language is the same as learning any other skill: it requires instruction and practice, and the learning and processing are handled by different brain areas from those that handled it in early childhood (Sousa, 2005). In contrast, writing and reading did not exist until a few thousand years BCE and did not become common until only 4 or 5 centuries ago— long after the human brain had evolved into its modern state. At no time during childhood do our brains show any special innate ability to learn to read. Instead, reading is an artificial skill that we learn by systematic instruction and practice, like playing a violin, juggling, or reading music (Sousa, 2005). Reading is Unnatural 6 \nMany people never learn to read well, or at all Because people are not innately “wired” to learn to read, children who either lack caregivers who read to them or receive inadequate reading instruction in school may never learn to read. There are a great many such people, especially in the developing world. By comparison, very few people never learn a spoken language. Some people who learn to read never become good at it. Perhaps their parents did not value and promote reading. Perhaps they attended substandard schools or did not attend school at all. Perhaps they learned a second language but never learned to read well in that language. People who have cognitive or perceptual impairments such as dyslexia may never read easily. A person’s ability to read is specific to a language and script (a system of writing). To see what text looks like to someone who cannot read, look at a paragraph printed in a language and script that you do not know (see Fig. 6.1 ). Alternatively, you can approximate the feeling of illiteracy by taking a page written in a familiar script and language—such as a page of this book—and turning it upside down. Turn this book upside down and try reading the next few paragraphs. This exercise only approximates the feeling of illiteracy. You will discover that the inverted text appears foreign and illegible at first, but after a minute you will be able to read it, albeit slowly and laboriously. Learning to read = training our visual system Learning to read involves training our visual system to recognize patterns—the pat- terns exhibited by text. These patterns run the gamut from low level to high level: l \u0007Lines, contours, and shapes are basic visual features that our brain recognizes innately. We don’t have to learn to recognize them. l \u0007Basic features combine to form patterns that we learn to identify as characters— letters, numeric digits, and other standard symbols. In ideographic scripts, such as Chinese, symbols represent entire words or concepts. (A) (B) FIGURE 6.1 To see how it feels to be illiterate, look at text printed in a foreign script: (A) Amharic and (B) Tibetan. \nl \u0007In alphabetic scripts, patterns of characters form morphemes, which we learn to recognize as packets of meaning—for example, “farm,” “tax,” “-ed,” and “-ing” are morphemes in English. l \u0007Morphemes combine to form patterns that we recognize as words—for exam- ple, “farm,” “tax,” “-ed,” and “-ing” can be combined to form the words “farm,” “farmed,” “farming,” “tax,” “taxed,” and “taxing.” l \u0007Words combine to form patterns that we learn to recognize as phrases, idiomatic expressions, and sentences. l \u0007Sentences combine to form paragraphs. Actually, only part of our visual system is trained to recognize textual patterns involved in reading: the fovea and a small area immediately surrounding it (known as the perifovea ), and the downstream neural networks running through the optic nerve to the visual cortex and into various parts of our brain. The neural networks starting elsewhere in our retinas do not get trained to read. More about this is explained later in the chapter. Learning to read also involves training the brain’s systems that control eye move- ment to move our eyes in a specific way over text. The main direction of eye move- ment depends on the direction in which the language we are reading is written: European language scripts are read left to right, many middle Eastern language scripts are read right to left, and some language scripts are read top to bottom. Beyond that, the precise eye movements differ depending on whether we are reading, skimming for overall meaning, or scanning for specific words. How we read Assuming our visual system and brain have successfully been trained, reading becomes semiautomatic or fully automatic—both eye movement and processing. As explained earlier, the center of our visual field—the fovea and perifovea—is the only part of our visual field that is trained to read. All text that we read enters our visual system after being scanned by the central area, which means that reading requires a lot of eye movement. As explained in Chapter 5 on the discussion of peripheral vision, our eyes constantly jump around, several times a second. Each of these movements, called saccades , lasts about 0.1 second. Saccades are ballistic, like firing a shell from a cannon. Their end point is determined when they are triggered, and once triggered, they always execute to completion. As described in earlier chapters, the destinations of saccadic eye move- ments are programmed by the brain from a combination of our goals, events in the visual periphery, events detected and localized by other perceptual senses, and past history including training. When we read, we may feel that our eyes scan smoothly across the lines of text, but that feeling is incorrect. In reality, our eyes continue with saccades during reading, but the movements generally follow the line of text. They fix our fovea on a word, pause \nthere for a fraction of a second to allow basic patterns to be captured and transmitted to the brain for further analysis, then jump to the next important word (Larson, 2004). Eye fixations while reading always land on words, usually near the center, never on word boundaries (see Fig. 6.2 ). Very common small connector and function words like “a,” “and,” “the,” “or,” “is,” and “but” are usually skipped over, their presence either detected in perifoveal vision or simply assumed. Most of the saccades during reading are in the text’s normal reading direction, but a few—about 1%—jump backwards to previous words. At the end of each line of text, our eyes jump to where our brain guesses the next line begins. 1 How much can we take in during each eye fixation during reading? For reading European-language scripts at normal reading distances and text font sizes, the fovea clearly sees three to four characters on either side of the fixation point. The perifovea sees out about 15–20 characters from the fixation point, but not very clearly (see Fig. 6.3 ). According to reading researcher Kevin Larson (2004), the reading area in and around the fovea consists of three distinct zones (for European-language scripts): Closest to the fixation point is where word recognition takes place. This zone is usually large enough to capture the word being fixated, and often includes smaller function words directly to the right of the fixated word. The next zone extends a few letters past the word recognition zone, and readers gather preliminary information about the next letters in this zone. The final zone extends out to 15 letters past the fixation point. Information gathered out this far is used to identify the length of upcoming words and to identify the best location for the next fixation point. Due to how our visual system has been trained to read, perception around the fixa- tion point is asymmetrical; it is more sensitive to characters in the reading direction 1 Later we will see that centered text disrupts the brain’s guess about where the next line starts. )RXU\u0003VFRUH\u0003DQG\u0003VHYHQ\u0003\\HDUV\u0003DJR\u000f\u0003RXU\u0003IRUHIDWKHUV\u0003EURXJKW\u0003IRUWK\u0003RQ WKLV\u0003FRQWLQHQW\u0003D\u0003QHZ\u0003QDWLRQ\u000f\u0003FRQFHLYHG\u0003LQ\u0003OLEHUW\\\u0003DQG\u0003GHGLFDWHG FIGURE 6.2 Saccadic eye movements during reading jump between important words. FIGURE 6.3 Visibility of words in a line of text, with fovea fixed on the word “years.” \nthan in the other direction. For European-language scripts, this is toward the right. That makes sense because characters to the left of the fixation point have usually already been read. IS READING FEATURE-DRIVEN OR CONTEXT-DRIVEN? As explained earlier, reading involves recognizing features and patterns. Pattern recog- nition, and therefore reading, can be either a bottom-up, feature-driven process, or a top-down, context-driven process. In feature-driven reading, the visual system starts by identifying simple features— line segments in a certain orientation or curves of a certain radius—on a page or display and then combines them into more complex features, such as angles, multiple curves, shapes, and patterns. Then the brain recognizes certain shapes as characters or symbols representing letters, numbers, or for ideographic scripts, words. In alpha- betic scripts, groups of letters are perceived as morphemes and words. In all types of scripts, sequences of words are parsed into phrases, sentences, and paragraphs that have meaning. Feature-driven reading is sometimes referred to as “bottom-up” or “context-free.” The brain’s ability to recognize basic features—lines, edges, angles, etc.—is built in and therefore automatic from birth. In contrast, recognition of morphemes, words, and phrases has to be learned. It starts out as a nonautomatic, conscious process requiring conscious analysis of letters, morphemes, and words, but with enough practice it becomes automatic (Sousa, 2005). Obviously, the more common a mor- pheme, word, or phrase, the more likely that recognition of it will become automatic. With ideographic or pictographic scripts such as Chinese, which have many times more symbols than alphabetic scripts do, people typically take many years longer to become skilled readers. Context-driven or top-down reading operates in parallel with feature-driven reading but works the opposite way: from whole sentences or the gist of a paragraph down to the words and characters. The visual system starts by recognizing high-level patterns like words, phrases, and sentences or by knowing the text’s meaning in advance. It then uses that knowledge to figure out—or guess—what the lower-level components of the high-level pattern must be (Boulton, 2009). Context-driven reading is less likely to become fully automatic, because most phrase-level and sentence-level patterns and contexts don’t occur frequently enough to allow their recognition to become burned into neural firing patterns. But there are exceptions, such as idiomatic expressions. To experience context-driven reading, glance quickly at Fig. 6.4 , then immediately direct your eyes back here and finish reading this paragraph. Try it now. What did the text say? Now look at the same sentence again more carefully. Do you read it the same way now? Also, based on what we have already read and our knowledge of the world, our brains can sometimes predict text that the fovea has not yet read (or its meaning), \nallowing us to skip reading it. For example, if at the end of a page we read “It was a dark and stormy,” we would expect the first word on the next page to be “night.” We would be surprised if it was some other word (e.g., “cow”). Feature-driven, bottom-up reading dominates; context assists It has been known for decades that reading involves both feature-driven (bottom-up) and context-driven (top-down) processing. In addition to being able to figure out the meaning of a sentence by analyzing the letters and words in it, people can determine the words of a sentence by knowing the sentence’s meaning or the letters in a word by knowing what word it is (see Fig. 6.5 ). The question is: Is skilled reading primarily bottom-up or top-down, or is neither mode dominant? Early scientific studies of reading—from the late 1800s through about 1980— seemed to show that people recognize words first, and from that they determine what letters are present. The theory of reading that emerged from those findings was that our visual system recognizes words primarily from their overall shape . This theory failed to account for certain experimental results and so was controversial among researchers, but it nonetheless gained wide acceptance among nonresearchers, espe- cially in the graphic design field (Larson, 2004; Herrmann, 2011). Similarly, educational researchers in the 1970s applied information theory to reading and assumed that because of redundancies in written language, top-down, context-driven reading would be faster than bottom-up, feature-driven reading. This The rain in Spain falls manly in the the plain FIGURE 6.4 Top-down recognition of the expression can inhibit seeing the actual text. Mray had a ltilte lmab, its feclee was withe as sown.  And ervey wehre taht Mray wnet, the lmab was srue to go. (A) (B) Twinkle, twinkle little star, how I wonder what you are FIGURE 6.5 Top-down reading: most readers, especially those who know the songs from which these text passages are taken, can read them even though the words (A) have all but their first and last letters scrambled and (B) are mostly obscured. \nassumption led them to hypothesize that reading for highly skilled (fast) readers would be dominated by context-driven (top-down) processing. This theory was prob- ably responsible for many speed-reading methods of the 1970s and 1980s, which sup- posedly trained people to read fast by taking in whole phrases and sentences at a time. However, empirical studies of readers conducted since then have demonstrated conclusively that those early theories were wrong. Summing up the research are state- ments from reading researchers Kevin Larson (2004) and Keith Stanovich (Boulton, 2009), respectively: Word shape is no longer a viable model of word recognition. The bulk of scientific evidence says that we recognize a word’s component letters, then use that visual information to recognize a word. Context [is] important, but it’s a more important aid for the poorer reader who doesn’t have automatic context-free recognition instantiated. In other words, reading consists mainly of context-free, bottom-up, feature-driven processes. In skilled readers, these processes are well learned to the point of being automatic. Context-driven reading today is considered mainly a backup method that, although it operates in parallel with feature-based reading, is only relevant when fea- ture-driven reading is difficult or insufficiently automatic. Skilled readers may resort to context-based reading when feature-based reading is disrupted by poor presentation of information (see examples later in this chap- ter). Also, in the race between context-based and feature-based reading to decipher the text we see, contextual cues sometimes win out over features. As an example of context-based reading, Americans visiting England sometimes misread “to let” signs as “toilet,” because in the United States they see the word “toilet” often, but they almost never see the phrase “to let”—Americans use “for rent” instead. In less-skilled readers, feature-based reading is not automatic; it is conscious and laborious. Therefore, more of their reading is context-based. Their involuntary use of context-based reading and nonautomatic feature-based reading consumes short-term cognitive capacity, leaving little for comprehension. 2 They have to focus on decipher- ing the stream of words, leaving no capacity for constructing the meaning of sentences and paragraphs. That is why poor readers can read a passage aloud but afterward have no idea what they just read. Why is context-free (bottom-up) reading not automatic in some adults? Lack of training is a reason: some people don’t get enough experience reading as young 2 Chapter 10 describes the differences between automatic and controlled cognitive processing. Here, we will simply say that controlled processes burden working memory, while automatic processes do not. \nchildren for the feature-driven recognition processes to become automatic. As they grow up, they find reading mentally taxing, so they avoid it, which perpetuates and compounds their deficit (Boulton, 2009). SKILLED AND UNSKILLED READING USE DIFFERENT AREAS OF THE BRAIN Before the 1980s, researchers who wanted to understand which parts of the brain are involved in language and reading were limited mainly to studying people who had suf- fered brain injuries. For example, in the mid-19th century, doctors found that people with brain damage near the left temple—an area now called Broca’s area after the doctor who discovered it—can understand speech but have trouble speaking, and that people with brain damage near the left ear—now called Wernicke’s area —can- not understand speech (Sousa, 2005) (see Fig. 6.6 ). In recent decades, new methods of observing the operation of functioning brains in living people have been developed: electroencephalography, functional magnetic resonance imaging, and functional magnetic resonance spectroscopy. These methods allow researchers to watch the responses in different areas of a person’s brain— including the sequence in which they respond—as the person perceives various stim- uli or performs specific tasks (Minnery and Fine, 2009). Using these methods, researchers have discovered that the neural pathways involved in reading differ for novice versus skilled readers. Of course, the first area to respond during reading is the occipital (or visual) cortex at the back of the brain. That is the same regardless of a person’s reading skill. After that, the pathways diverge (Sousa, 2005): l \u0007 Novice . First an area of the brain just above and behind Wernicke’s area becomes active. Researchers have come to view this as the area where, at least with Broca’s area Wernicke’s area FIGURE 6.6 The human brain, showing Broca’s and Wernicke’s areas. \nalphabetic scripts such as English and German, words are “sounded out” and assembled—that is, letters are analyzed and matched with their corresponding sounds. The word-analysis area then communicates with Broca’s area and the frontal lobe, where morphemes and words—units of meaning—are recognized and overall meaning is extracted. For ideographic languages, where symbols rep- resent whole words and often have a graphical correspondence to their meaning, sounding out of words is not part of reading. l \u0007 Advanced . The word-analysis area is skipped. Instead the occipitotemporal area (behind the ear, not far from the visual cortex) becomes active. The prevailing view is that this area recognizes words without sounding them out, then that activity activates pathways toward the front of the brain that correspond to the word’s meaning and mental image. Broca’s area is only slightly involved. Findings from brain scan methods of course don’t indicate exactly what processes are being used, but they support the theory that advanced readers use different pro- cesses from those used by novice readers. POOR INFORMATION DESIGN CAN DISRUPT READING Careless writing or presentation of text can reduce skilled readers’ automatic, con- text-free reading to conscious, context-based reading, burdening working memory and thereby decreasing speed and comprehension. In unskilled readers, poor text presentation can block reading altogether. Poor text presentation can take several forms, including the following. Uncommon or unfamiliar vocabulary Software often disrupts reading by using unfamiliar vocabulary—words the intended readers don’t know very well or at all. One type of unfamiliar terminology is computer jargon, sometimes known as “geek speak.” For example, an intranet application displayed the following error message if a user tried to use the application after more than 15 minutes of letting it sit idle: Your session has expired. Please reauthenticate. The application was for finding resources—rooms, equipment, etc.—within the company. Its users included receptionists, accountants, and managers as well as engi- neers. Most nontechnical users would not understand the word “reauthenticate,” so they would drop out of automatic reading mode into conscious wondering about the message’s meaning. To avoid disrupting reading, the application’s developers could have used the more familiar instruction, “Login again.” For a discussion of how “geek speak” in computer-based systems affects learning, see Chapter 11 . \nReading can also be disrupted by uncommon terms even when they are not com- puter technology terms. Here are some rare English words, including many that appear mainly in contracts, privacy statements, or other legal documents: l \u0007 Aforementioned : mentioned previously l \u0007 Bailiwick : the region in which a sheriff has legal powers; more generally, domain of control l \u0007 Disclaim : renounce any claim to or connection with; disown; repudiate l \u0007 Heretofore : up to the present time; before now l \u0007 Jurisprudence : the principles and theories on which a legal system is based l \u0007 Obfuscate : make something difficult to perceive or understand l \u0007 Penultimate : next to the last, as in “the next to the last chapter of a book” When readers—even skilled ones—encounter such a word, their automatic read- ing processes probably won’t recognize it. Instead, their brain uses less automatic processes, such as sounding out the word’s parts and using them to figure out its meaning, figuring out the meaning from the context in which the word appears, or looking the word up in a dictionary. Difficult scripts and typefaces Even when the vocabulary is familiar, automatic reading can be disrupted by typefaces with unfamiliar or hard-to-distinguish shapes. Context-free, automatic reading is based on recognizing letters and words bottom-up from their lower-level visual features. Our visual system is quite literally a neural network that must be trained to recognize certain combinations of shapes as characters. Therefore, a typeface with difficult-to- recognize features and shapes will be hard to read. For example, try to read Abraham Lincoln’s Gettysburg Address in an outline typeface in ALL CAPS (see Fig. 6.7 ). Comparison studies show that skilled readers read uppercase text 10%–15% more slowly than lowercase text. Current-day researchers attribute that difference mainly to a lack of practice reading uppercase text, not to an inherent lower recognizability of upper- case text (Larson, 2004). Nonetheless, designers should keep in mind that, due to the lack of practice people have in reading text in ALL CAPS, it is harder to read (Herrmann, 2011). Tiny fonts Another way to make text hard to read in software applications, websites, and elec- tronic appliances is to use fonts too small for their intended readers’ visual system to resolve. For example, try to read the first paragraph of the US Constitution in a seven- point font (see Fig. 6.8 ). Developers sometimes use tiny fonts because they have a lot of text to display in a small amount of space. But if the intended users of the system cannot read the text or can read it only laboriously, the text might as well not be there. \nText on noisy background Visual noise in and around text can disrupt recognition of features, characters, and words and therefore drop reading out of automatic feature-based mode into a more conscious and context-based mode. In software user interfaces and websites, visual noise often results from designers’ placing text over a patterned background or dis- playing text in colors that contrast poorly with the background, as an example from uscpfa-sbay.blogspot.com shows (see Fig. 6.9 ). There are situations in which designers intend to make text hard to read. For exam- ple, a common security measure on the Web is to ask site users to identify distorted words as proof that they are a live human beings and not an Internet “‘bot.” This relies on the fact that most people can read text that Internet ‘bots cannot currently read. Text displayed as a challenge to test a registrant’s humanity is called a captcha 3 (see Fig. 6.10 ). Of course, textual captchas should be clear enough for people to read; otherwise it defeats the purpose of having them. In case a user cannot see what is in a captcha, it should be pos- sible for the user to ask for another one, or even a different—e.g., nonvisual—challenge. 3 The term originally comes from the word “capture,” but it is also said to be an acronym for “Completely Automated Public Turing test to tell Computers and Humans Apart.” FIGURE 6.7 Text in ALL CAPS is harder to read because we are not practiced at doing it. Outline typefaces complicate feature recognition. This example demonstrates both. We the people of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, provide for the common defense, promote the general Welfare, and secure the Blessings of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America. FIGURE 6.8 The opening paragraph of the US Constitution presented in a seven-point font. \nText contrasts poorly with background Even when text is displayed on a nonpatterned background, the text can be hard to read if it contrasts poorly with the background. For example, Apple Computer’s App Store (July 2019) showed the expected duration of an app download in gray text on a gray background, with contrast so low that it was almost impossible to see, much less read (see Fig. 6.11 , below progress bar). Low-contrast text is especially difficult for many adults older than 50 to read due to common age-related changes in the human visual system (Johnson and Finn, 2017). Like text in tiny fonts, text that contrasts poorly with the background is, for practical purposes, not there. For ease of reading by all sighted users, the contrast between text and background should be at least 4.5:1. FIGURE 6.9 The blog uscpfa-sbay.blogspot.com uses text on a noisy background and poor color contrast. FIGURE 6.10 Text that is intentionally displayed with noise so that web-crawling software cannot read it is called a captcha. \nInformation buried in repetition Visual noise can also come from the text itself. If successive lines of text contain a lot of repetition, readers receive poor feedback about which line they are focused on, plus it is hard to pick out the important information. For example, recall the example from the California Department of Motor Vehicles website in Chapter 3 (see Fig. 3.3). Another example of repetition that creates noise is the computer store on Apple.com . The pages for ordering a laptop computer list different keyboard options for a computer in a very repetitive way, making it hard to see that the essential differ- ence between the keyboards is the language they support (see Fig. 6.12 ). FIGURE 6.11 Apple’s App Store displayed the expected download time (bottom center, below progress bar) in gray text on a gray background, making it very hard to read. FIGURE 6.12 Apple.com’s “Buy Computer” page lists options in which the important information (keyboard language compatibility) is buried in repetition. \nCentered text (or text aligned with a ragged start) One aspect of reading that is highly automatic in most skilled readers is eye move- ment. In automatic (fast) reading, our eyes are trained to go back to the same horizon- tal position and down one line. If text is centered or otherwise aligned so that each line starts in a different horizontal position, 4 automatic eye movements take our eyes back to the wrong place, so we must consciously adjust our gaze to the actual start of each line. This drops us out of automatic mode and slows us down. With poetry and wedding invitations that may be okay, but with any other type of text it is undesirable. Try reading the left side of Fig. 6.13 quickly, and compare that with reading the right side. Can you tell that your eyes move less efficiently when reading the left side? Fig. 6.14 shows an example of centered paragraphs of text from the homepage of Valco Tronics Inc. ( valcoelectronics.com ). Not only does the page center paragraphs of text, it also uses a blurry white-on-black typeface (see Fig. 6.14 ), diminishing legibility even more. UX designers should avoid centering multiline paragraphs of text (Nielsen, 2008b; Trevellyan, 2017). However, when a page includes separate elements that are not mul- tiline sentences or paragraphs of text, centering them can be OK (see Fig. 6.15 ). Design implications: don’t disrupt reading; support it! Obviously, a designer’s goal should be to support reading, not disrupt it. Skilled (fast) reading is mostly automatic and based on feature, character, and word recognition. The easier the recognition, the easier and faster the reading. Less-skilled reading, by contrast, is greatly assisted by contextual cues. Designers of interactive systems can support both reading methods by following these guidelines: 4 Right-aligned for left-to-right scripts; left-aligned for right-to-left scripts. FIGURE 6.13 Compare reading the text on the left versus the right. Centered paragraph text thwarts automatic eye movement patterns, resulting in slower reading speed. \n1 \u0007Ensure that text in user interfaces allows feature-based automatic processes to function effectively by avoiding the disruptive flaws described earlier: difficult or tiny fonts, patterned backgrounds, centering, etc. FIGURE 6.14 Centered paragraphs of text on the homepage of valcoelectronics.com . FIGURE 6.15 Centering of separate elements is OK; it does not disrupt reading. \n2 \u0007Use restricted, highly consistent vocabularies—sometimes referred to in the industry as plain language 5 —or simplified language (Redish, 2007). 3 \u0007Format text to create a visual hierarchy (see Chapter 3 ) to facilitate easy scanning: use headings, bulleted lists, tables, and visually emphasized words (see Fig. 6.16 ). Experienced information architects, content editors, and graphic designers can be very useful in ensuring that text is presented to support easy scanning and reading. PEOPLE DON’T READ WHEN USING SOFTWARE AND THE WEB; THEY SCAN As explained in Chapter 1 , when people are using an app or a website, they focus on their goal and mostly ignore everything else. When an app or website presents text, we usually read as little of it as absolutely necessary to achieve our goal. This is often summarized as “people don’t read in apps and the Web; they scan.” (Nielsen, 1999, 2008a; Johnson, 2007; Krug, 2014). For example, look at the medical service Web page in Fig. 6.17 , then look back here. Did you notice anything unusual? Now look at Fig. 6.18 , in which some of the text on the page is marked. Did you miss those the first time? It is actually an oversimplification to say “people don’t read text in apps and on the Web.” When people find the content—online books, articles, blog posts, poems, comments—that they want, they do read. But even then, unless they really enjoy the author’s writing for its own sake, they only read as much as necessary to get the infor- mation they need. 5 For more information on plain language see the U.S. government website, www.plainlanguage.gov . FIGURE 6.16 Microsoft Word’s “Help” home page is easy to scan and read. \nMUCH OF THE TEXT IN APPS AND WEBSITES IS UNNECESSARY In addition to committing design mistakes that disrupt reading, many software user interfaces simply present too much text, requiring users to read more than is neces- sary. Software designers often justify lengthy instructions by arguing, “We need all that text to explain clearly to users what to do.” However, instructions can often be shortened with no loss of clarity. Let’s examine how the Jeep company, between 2002 and 2007, shortened its instructions for finding a local Jeep dealer (see Fig. 6.19 ): l \u0007 2002 : The “Find a Dealer” page displayed a large paragraph of prose text, with numbered instructions buried in it, and a form asking for more information than needed to find a dealer near the user. FIGURE 6.17 Simulated medical company web page. (Image courtesy of trumatter.) \nl \u0007 2003 : The instructions on the “Find a Dealer” page had been boiled down to three bullet points, and the form required less information. l \u0007 2007 : “Find a Dealer” was cut to one field (zip code) and a “Go” button and left that way for years. That’s about as much as it can be cut, right? Wrong! l \u0007 2020 : No need for a text-entry field or a “Go” button. Smartphones and comput- ers can identify the user’s location automatically (assuming Location is enabled), so Jeep cut “Find a Dealer” down to one button. Just click it to see local dealers. What’s next? No need for any button at all. Just speak and ask Jeep for nearby dealers. Even when text describes products rather than explaining instructions, it is coun- terproductive to put all a vendor wants to say about a product into a lengthy prose FIGURE 6.18 Simulated medical company web page with additions marked. Did you notice all of these in Fig. 6.17 ? (Image courtesy of trumatter.) \nFIGURE 6.19 Over the years, Jeep.com reduced the reading and data-entry required to find a dealer. \ndescription that people have to read from start to end. Most potential customers cannot or will not read it. Fig. 6.20 shows the reduction in the amount of text in Costco.com ’s displays of laptop computers between 2007 and 2009, and Fig. 6.21 shows that their 2019 website displayed laptop computers with very little text. Design implications: cut unnecessary text—minimize the need for reading Needless text is bad anytime (Strunk and White, 1999) but is especially bad in soft- ware and websites. First of all, if users encounter lots of text on the way to their goals, they simply will ignore most of it. So if you spend time and money writing a lot of text, at best you are wasting your time and your money. Too much text in a user interface loses poor readers, who unfortunately are a sig- nificant percentage of the population. Too much text even alienates good readers; it turns using an interactive system into an intimidating amount of work . FIGURE 6.20 Between 2007 and 2009, Costco.com reduced the text in product descriptions. \nMinimize the amount of prose text in a user interface; don’t present users with long blocks of prose text to read. Don’t have paragraphs welcoming people to your website. In instructions, use the least amount of text that gets most users to their intended goals. In product descriptions, provide a brief overview of the product and let users request more detail if desired. Before releasing an app or a website, go through every screen and cut the amount of text by at least half. Then go through the screens again and cut another 50%. According to UX design guru Steve Krug (2014), that should leave you with about the right amount of text. Consider how much text Jeep cut over many years. It should not have taken them almost 2 decades to do it. Technical writers and content editors can assist greatly in cutting the amount of text in a user interface. For additional advice on how to eliminate unnecessary text, see Ginny Redish’s book Letting Go of the Words (2007). TEST ON REAL USERS Finally, designers should test their designs on the intended user population to be confident that users can read all essential text quickly and effortlessly. Some testing FIGURE 6.21 In 2019, Costco.com displayed laptop computers with no sentences or paragraphs of text. \ncan be done early using prototypes and partial implementations, but it should also be done just before release. Fortunately, last-minute changes to text are usually easy to make. IMPORTANT TAKEAWAYS l \u0007Humans are “prewired” to learn a spoken language but not to learn to read. Learning to read is like learning any nonlanguage skill, such as how to ride a bicycle, play a guitar, or execute a kung fu move. Almost everyone learns a lan- guage, but many people never learn to read. l \u0007Learning to read requires training the neural networks in our eyes and brains to recognize and understand characters, words, sentences, and paragraphs. Only the neural networks that start in the fovea—a small area in the center of our visual field—are involved in reading. The rest of our visual field cannot read, but it does influence where our eyes jump while reading. l \u0007As a form of perception, reading is both a bottom-up, feature-driven process and a top-down, context-driven process. Bottom-up, feature-driven reading—shapes to letters to words to sentences to paragraphs—is the dominant process in skilled readers, and top-down, context-driven reading assists. In less-skilled readers, top- down reading plays a larger role. l \u0007Skilled and unskilled reading use different areas of the brain. In less-skilled read- ers, processing a word includes areas of the brain that “sound out” words to help recognize them. In skilled readers, the “sounding out” areas of the brain are skipped; processing goes straight from visual perception to extracting meaning. l \u0007Poor information presentation can disrupt reading. It can temporarily reduce skilled readers to the level of unskilled readers, and it can block reading for unskilled readers. Poor information presentation includes: l \u0007use of rare or jargon words unfamiliar to readers l \u0007unusual scripts and typefaces, including ALL CAPS (because people are not trained in reading them) l \u0007tiny fonts l \u0007text on noisy backgrounds l \u0007text that contrasts poorly with background l \u0007information buried in repetition l \u0007centered text \nl \u0007People don’t read when using software and the Web. They scan until they find the content they are looking for—e.g., a news article. Even then, they continue to scan if all they want from the content is specific information. l \u0007Much of the text in apps and websites is unnecessary and should be cut out. For the broadest appeal of your software, minimize the need for users to read. l \u0007Test on real users to see if they can understand all the text in your software, and eliminate, rewrite, radically shorten, or replace with graphics any text that is unclear to most of them."
}