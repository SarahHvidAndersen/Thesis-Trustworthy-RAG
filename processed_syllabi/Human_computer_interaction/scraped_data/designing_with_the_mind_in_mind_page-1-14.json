{
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "text": "CHAPTER 1 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00001-5 Copyright © 2021 Elsevier Inc. All rights reserved. Our perception of the world around us is not a true depiction of what is actually there. Our perceptions are heavily biased by at least three factors: l \u0007 The past: our experience l \u0007 The present: the current context l \u0007 The future: our goals PERCEPTION BIASED BY EXPERIENCE Experience—your past perceptions—can bias your current perception in several dif- ferent ways. Perceptual priming Imagine that you own a large insurance company. You are meeting with a real estate manager, discussing plans for a new campus of company buildings. The campus con- sists of a row of five buildings, the last two with T-shaped courtyards providing light for the cafeteria and fitness center. If the real estate manager showed you the map in Fig. 1.1 , you would see five black shapes representing the buildings. Now imagine that instead of a real estate manager, you are meeting with an adver- tising manager. You are discussing a new billboard ad to be placed in certain markets around the country. The advertising manager shows you the same image, but in this scenario the image is a sketch of the ad, consisting of a single word: LIFE. In this sce- nario, you see a word, clearly and unambiguously. When your perceptual system has been primed to see building shapes, you see building shapes, and the white areas between the buildings barely register in your Our Perception is Biased 1 \nperception. When your perceptual system has been primed to see text, you see text, and the black areas between the letters barely register. A relatively famous example of how priming the mind can affect perception is an image, supposedly by R. C. James, 1 that initially looks to most people like a random splattering of paint (see Fig. 1.2 ) similar to the work of the painter Jackson Pollack. Before reading further, look at the image. Only after you are told that it is a Dalmatian dog sniffing the ground near a tree can your visual system organize the image into a coherent picture. Moreover, once you have seen the dog, it is hard to go back to seeing just a random collection of spots. FIGURE 1.1 Building map or word? What you see depends on what you were told to see. FIGURE 1.2 Image showing the effect of mental priming of the visual system. What do you see? \nThese priming examples are visual, but priming can also bias other types of percep- tion, such as sentence comprehension. For example, the headline “New Vaccine Con- tains Rabies” would probably be understood differently by people who had recently heard stories about contaminated vaccines than by people who had recently heard stories about successful uses of vaccines to fight diseases. Familiar perceptual patterns or frames Much of our lives is spent in familiar situations: the rooms in our homes, our yards, our routes to and from school or work, our offices, neighborhood parks, stores, restau- rants, etc. Repeated exposure to each type of situation builds a pattern in our minds of what to expect to see there. These perceptual patterns , which some researchers call frames , include the objects or events usually encountered in a particular situation. For example, you know most rooms in your home well enough that you need not constantly scrutinize every detail. You know their layout and where most objects are located. You can probably navigate much of your home in total darkness. But your experience with homes is broader than your specific home. In addition to having a pattern for your home, your brain has one for homes in general. It biases your percep- tion of all homes, familiar and new. In a kitchen, you expect to see a stove and a sink. In a bathroom, you expect to see a toilet, sink, and shower or bathtub (or both). Our mental frames for situations bias our perception toward seeing the objects and events expected in each situation. They are a mental shortcut: by eliminating the need for us to constantly scrutinize every detail of our environment, they help us get around in our world. However, mental frames also make us see things that aren’t really there. For example, if you visit a house in which there is no stove in the kitchen, you might nonetheless later recall seeing one, because your mental frame for kitchens has a strong stove component. Similarly, part of the frame for eating at a restaurant is paying the bill, so you might recall paying for your dinner even if you absentmindedly walked out without paying. Your brain also has frames for backyards, schools, city streets, business offices, supermarkets, dentist visits, taxis, air travel, and other familiar situations. Anyone who uses computers, websites, or smartphones has frames for the desktop and files, web browsers, websites, and various applications and online services. For example, when experienced Web users visit a new website, they expect to see a site name and logo, a navigation bar, some other links, and maybe a search box. When they book a flight online, they expect to specify trip details, examine search results, make a choice, and make a purchase. When they shop online, they expect a shopping cart and a checkout stage with a payment step. Because users of computer software and websites have these perceptual frames, they often click buttons or links without looking carefully at them. Their perception of the display is based more on what their frame for the situation leads them to expect than on what is actually on the screen. This sometimes confounds software designers, who expect users to see what is on the screen—but that isn’t how human vision and attention works. For example, if the positions of the “Next” and “Back” buttons on the last page of a multistep screen sequence 2 are switched, many people would not immediately notice the change (see Fig. 1.3 ). Their visual system would have been lulled into inattention \nby the consistent placement of the buttons on the several pages that came before. Even after unintentionally going backward a few times by mistakenly clicking “Back” for “Next,” they might continue to perceive the buttons in their standard locations. This is why consistent placement of controls is a recommended user-interface guide- line, to ensure that reality matches the user’s frame for the situation. Similarly, if we are trying to find something but it is in a different place or looks dif- ferent from usual, we might miss it even though it is in plain view because our mental frames tune us to look for expected features in expected locations. For example, if the “Submit” button on one form in a website is shaped differently or is a different color from those on other forms on the site, users might not find it. This expectation- induced blindness is discussed more later in this chapter in the “Perception Biased by Goals” section. Habituation A third way in which experience biases perception is called habituation . Repeated exposure to the same (or highly similar) perceptions dulls our perceptual system’s sensitivity to them. Habituation is a very low-level phenomenon of our nervous sys- tem: it occurs at a neuronal level. Even primitive animals like flatworms and ameba, with very simple nervous systems, habituate to repeated stimuli (e.g., mild electric shocks or light flashes). People, with our complex nervous systems, habituate to a range of events, from low-level ones like a continually beeping tone, to medium-level ones like a blinking ad on a website, to high-level ones like a person who tells the same jokes at every party or a politician giving a long, repetitious speech. We experience habituation in computer usage when the same error messages or “Are you sure?” confirmation messages appear again and again. People initially notice them and perhaps respond, but eventually they click them closed reflexively without bothering to read them. FIGURE 1.3 Users may always perceive the Next button on the right, even when it isn’t. \nHabituation is also a factor in a recent phenomenon variously labeled “social media burnout” (Nichols, 2013), “social media fatigue,” or “Facebook vacations” (Rainie et al., 2013); newcomers to social media sites and tweeting are initially excited by the novelty of microblogging about their experiences, but sooner or later get tired of wasting time reading tweets about every little thing that their “friends” do or see—for example, “Man! Was that ever a great salmon salad I had for lunch today.” Attentional blink Another low-level biasing of perception by past experience occurs just after we spot or hear something important. For a very brief period following the recognition— between 0.15 and 0.45 second—we are nearly deaf and blind to other visual stimuli, even though our ears and eyes stay functional. Researchers call this the attentional blink (Raymond et al., 1992; Stafford and Webb, 2005). 3 It is thought to be caused by the brain’s perceptual and attention mechanisms being briefly fully occupied with processing the first recognition. A classic example: You are in a subway car as it enters a station, planning to meet two friends at that station. As the train arrives, your car passes one of your friends, and you spot him briefly through your window. In the next split second, your window passes your other friend, but you fail to notice her because her image hit your retina during the attentional blink that resulted from your recognition of your first friend. When people use computer-based systems and online services, attentional blink can cause them to miss information or events if things appear in rapid succession. A popular modern technique for making documentary videos is to present a series of still photographs in rapid succession. 4 This technique is highly prone to atten- tional blink effects—if an image really captures your attention (e.g., it has a strong meaning for you), you will probably miss one or more of the immediately follow- ing images. In contrast, a captivating image in an auto-running slideshow (e.g., on a website or an information kiosk) is unlikely to cause attentional blink (i.e., missing the next image) because each image typically remains displayed for several seconds. PERCEPTION BIASED BY CURRENT CONTEXT When we try to understand how our visual perception works, it is tempting to think of it as a bottom-up process, combining basic features such as edges, lines, angles, curves, and patterns into figures and ultimately meaningful objects. To take reading as an example, you might assume that our visual system first recognizes shapes as letters and then combines letters into words, words into sentences, and so on. 3 Chapter 14 discusses the attentional blink interval along with other perceptual intervals. 4 For an example, search YouTube for “history of the world in 2 minutes.” \nBut visual perception—reading in particular—is not strictly a bottom-up process. It includes top-down influences too. For example, the word in which a character appears may affect how we identify the character (see Fig. 1.4 ). Similarly, our overall comprehension of a sentence or a paragraph can even influence what words we see in it. For example, the same letter sequence can be read as different words depending on the meaning of the surrounding paragraph (see Fig. 1.5 ). Contextual biasing of vision need not involve reading. The Müller-Lyer illusion is a famous example (see Fig. 1.6 ): the two horizontal lines are the same length, but the outward-pointing “fins” cause our visual system to see the top line as longer than the line with inward-pointing “fins.” This and other optical illusions (see Fig. 1.7 ) trick us because our visual system does not use accurate, optimal methods to perceive the world. It developed through evolution, a semirandom process that layers jury- rigged—often incomplete and inaccurate—solutions on top of each other. It works fine most of the time but includes many approximations, kludges, hacks, and outright “bugs” that cause it to fail in certain cases. FIGURE 1.4 The same character is perceived as H or A depending on the surrounding letters. Fold napkins. Polish silverware .  Wash dishes. French napkins. Polish silverware .  German dishes. FIGURE 1.5 The same phrase is perceived differently depending on the list it appears in. FIGURE 1.6 \nThe examples in Figs. 1.6 and 1.7 show vision being biased by visual context. How- ever, biasing of perception by the current context works between different senses too. Perceptions in any of our five senses may affect simultaneous perceptions in any of our other senses. What we feel with our tactile sense can be biased by what we hear, see, or smell. What we see can be biased by what we hear, and what we hear can be biased by what we see. The following two examples of visual perception affect what we hear: l \u0007 McGurk effect. If you watch a video of someone saying “bah, bah, bah,” then “dah, dah, dah,” then “vah, vah, vah,” but the audio is “bah, bah, bah” throughout, you will hear the syllable indicated by the speaker’s lip movement rather than the syllable actually in the audio track. 5 Only by closing or averting your eyes do you hear the syllable as it really is. I will bet you did not know you could read lips, and in fact do so many times a day. (C) (B) (A) FIGURE 1.7 (A) The checkerboard does not bulge in the middle; (B) the triangle sides are not bent; and (C) the horizontal blue bars are horizontal, straight, and parallel. (Copyright © Victoria Skye, victoriaskye.com . Used by permission.) \nl \u0007 Ventriloquism. Ventriloquists don’t throw their voice; they just learn to talk without moving their mouths much. Viewers’ brains perceive the talking as coming from the nearest moving mouth: that of the ventriloquist’s puppet (Eagleman, 2012). An example of the opposite—hearing biasing vision—is the illusory flash effect. When a spot is flashed once briefly on a display but is accompanied by two quick beeps, it appears to flash twice. Similarly, the perceived rate of a blinking light can be adjusted by the frequency of a repeating click (Eagleman, 2012). Later chapters explain how visual perception, reading, and recognition function in the human brain. For now, I will simply say that the pattern of neural activity that cor- responds to recognizing a letter, a word, a face, or any object includes input from neu- ral activity stimulated by the context. This context includes other nearby perceived objects and events and even reactivated memories of previously perceived objects and events. Context biases perception not only in people but also in lower animals. A friend of mine often brought her dog with her in her car when running errands. One day, as she drove into her driveway, a cat was in the front yard. The dog saw it and began barking. My friend opened the car door and the dog jumped out and ran after the cat, which turned and jumped through a bush to escape. The dog dove into the bush but missed the cat. The dog remained agitated for some time afterward. Thereafter, for as long as my friend lived in that house, whenever she arrived at home with her dog in the car, he would get excited, bark, jump out of the car as soon as the door was opened, dash across the yard, and leap into the bush. There was no cat, but that didn’t matter. Returning home in the car was enough to make the dog see one—perhaps even smell one. However, walking home on foot, as the dog did after being taken for his daily walk, did not evoke the “cat mirage.” PERCEPTION BIASED BY GOALS In addition to being biased by our past experience and the present context, our perception is influenced by our goals and plans for the future. Specifically, our goals: l \u0007 Guide our perceptual apparatus, so we sample what we need from the world around us; l \u0007 Filter our perceptions—things unrelated to our goals tend to be filtered out preconsciously, never registering in our conscious minds. For example, when people navigate through software or a website, seeking infor- mation or a specific function, they don’t read carefully. They scan screens quickly and superficially for items that seem related to their goal. They don’t simply ignore items unrelated to their goals; they often don’t even notice them. To see this, glance at Fig. 1.8 and look for scissors, and then immediately flip back to this page. Try it now. \nDid you spot the scissors? Now, without looking back at the toolbox, can you say whether there is a screwdriver in the toolbox too? Our goals filter our perceptions in other perceptual senses as well as in vision. A familiar example is the “cocktail party” effect. If you are conversing with someone at a crowded party, you can focus your attention to hear mainly what he or she is saying even though many other people are talking near you. The more interested you are in the conversation, the more strongly your brain filters out surrounding chatter. If you are bored by what your conversational partner is saying, you will probably hear much more of the conversations around you. The effect was first documented in studies of air traffic controllers, who were able to carry on conversations with the pilots of their assigned aircraft even though many differ- ent conversations were occurring simultaneously on the same radio frequency, coming out of the same speaker in the control room (Arons, 1992). Research suggests that our ability to focus on one conversation among several simultaneous ones depends not only on our interest level in the conversation, but also on objective factors, such as the similarity of voices in the cacophony, the amount of general “noise” (e.g., clattering dishes or loud music), and the predictability of what your conversational partner is saying (Arons, 1992). This filtering of perception by our goals is particularly true for adults, who tend to be more focused on goals than children are. Children are more stimulus-driven; their perception is less filtered by their goals. This characteristic makes them more distract- ible than adults, but it also makes them less biased as observers. A parlor game demonstrates this age difference in perceptual filtering. It is similar to the Fig. 1.8 exercise. Most households have a catch-all drawer for kitchen imple- ments or tools. From your living room, send a visitor to the room where the catch-all drawer is with instructions to fetch you a specific tool, such as measuring spoons or a pipe wrench. When the person returns with the tool, ask whether another specific tool was in the drawer. Most adults will not know what else was in the drawer. Chil- dren—if they can complete the task without being distracted by all the cool stuff in the drawer—will often be able to tell you more about what else was there. Perceptual filtering can also be seen in how people navigate websites. Suppose I put you on the home page of New Zealand’s University of Canterbury (see Fig. 1.9 ) FIGURE 1.8 Toolbox: Are there scissors here? \nand asked you to find information about financial support for postgraduate students in the computer science department. You would quickly scan the page for words that were in the goal I gave you: “departments,” “scholarships,” “computer science,” or “postgraduate.” If you spotted a link containing one or more of those words, you would probably click on it. If you are a “search” person, you might instead go to the search symbol (magnifying glass, top right), click it and enter words related to the goal, and click “Go.” Whether you browse or search, it is likely that you would leave the home page without noticing that you were randomly chosen to win $100 (bottom right). Why? Because that was not related to your goal . What is the mechanism by which our current goals bias our perception? There are two: l \u0007 Influencing where we look. Perception is active, not passive. Think of your per- ceptual senses not as simply filtering what comes to you but rather as reaching out into the world and pulling in what you need to perceive. Your hands, your primary touch sensors, literally do this, but the rest of your senses do it too. You constantly move your eyes, ears, hands, feet, body, and attention to sample exactly the things in your environment that are most relevant to what you are doing or about to do (Ware, 2008). If you are looking on a website for a campus map, your FIGURE 1.9 University of Canterbury website: navigating sites requires perceptual filtering. \neyes and pointer-controlling hand are attracted to anything that might lead you to that goal. You more or less ignore anything unrelated to your goal. l \u0007 Sensitizing our perceptual system to certain features. When you are look- ing for something, your brain can prime your perception to be especially sensi- tive to features of what you are looking for (Ware, 2008). For example, when you are looking for a red car in a large parking lot, red cars will seem to pop out as you scan the lot, and cars of other colors will barely register in your consciousness, even though you do in some sense see them. Similarly, when you are trying to find your spouse in a dark, crowded room, your brain “programs” your auditory system to be especially sensitive to the combination of frequencies that make up his or her voice. TAKING BIASED PERCEPTION INTO ACCOUNT WHEN DESIGNING All these sources of perceptual bias of course have implications for user-interface design. Here are three. Avoid ambiguity Avoid ambiguous information displays, and test your design to verify that all users interpret the display in the same way. Where ambiguity is unavoidable, either rely on standards or conventions to resolve it, or prime users to resolve the ambiguity in the intended way. For example, displays on digital devices often add drop-shadows to user-inter- face components to make them look raised in relation to the background surface (see Fig. 1.10 ). This appearance relies on a convention, familiar to most people who use digital devices, that the light source is at the top of the screen. If a tech- nology user does not know this convention, it may be ambiguous to them whether the object is raised or sunken. Be consistent Place information and controls in consistent locations. Controls and data displays that serve the same function on different pages should be placed in the same position on each page on which they appear. They should also have the same color, text fonts, shading, and so on. This consistency allows users to spot and recognize them quickly. Understand the goals Users come to a system with goals they want to achieve. Designers should understand those goals. Realize that users’ goals may vary and that their goals strongly influence what they perceive. Ensure that at every point in an interaction, the information users \nneed is available, is prominent, and maps clearly to a possible user goal so users will notice and use the information. IMPORTANT TAKEAWAYS l \u0007Human perception is not an accurate reflection of what is “out there” in the world. It is biased by our experience, the current context, and our goals. l \u0007Past experience can bias our perception by “priming” our perceptual systems to detect certain objects and events as well as “priming” them not to detect other objects and events. Repeated perception of an event over a short interval can cause habituation , increasing the chances that we will miss later occurrences of the event. With long-term experience, we develop frames for familiar situations that make us perceive things that aren’t there or miss things that are. l \u0007Because our attention has limited capacity, when it is overloaded we can miss other objects and events. This is called attentional blink. l \u0007Perception operates in two ways simultaneously: l \u0007Our perception of whole objects and events is based on our perception of the parts. l \u0007Our perception of the parts is based on our perception of the whole. FIGURE 1.10 Components on digital device screens are given drop-shadows so they are perceived to float above the background, but that perception depends on a convention that the light source is at the top. \nl \u0007Our perception of objects and events can be biased by our emotional state. l \u0007Our perceptual system—particularly our visual system—includes evolutionary hacks and bugs that cause us to misperceive certain stimuli. l \u0007Our goals and plans strongly influence what we pay attention to and therefore what we perceive. l \u0007By following design guidelines based on how human perception works, design- ers can create applications, websites, and appliances that are a good fit with the people who will use them."
}