{
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "text": "Copyright 2008 Psychonomic Society, Inc. 450 This article describes and demonstrates the simple BayesSDT software package—developed in MATLAB— for performing Bayesian analysis using signal detection theory (SDT). As background, we first briefly describe SDT and the Bayesian approach to its analysis. We then present illustrative examples of the BayesSDT software and its outputs. Signal Detection Theory SDT—developed by Peterson, Birdsall, and Fox (1954) and applied to psychophysics by Tanner and Swets (1954)—is a very general, useful, and widely employed method for drawing inferences from data in psychology (see Green & Swets, 1966; Macmillan & Creelman, 2004, for detailed introductions). SDT analysis is often applied to data that take a 2 \u0013 2 form—as shown in Table 1—in which there are “signal” trials and “noise” trials, and “yes” responses and “no” responses. When a “yes” response is given for a signal trial, it is called a hit . When a “yes” re- sponse is given for a noise trial, it is called a false alarm . When a “no” response is given for a signal trial, it is called a miss . When a “no” response is given for a noise trial, it is called a correct rejection . The data needed for an SDT analysis are just the counts of hits, false alarms, misses, and correct rejections. The key assumptions of SDT are shown in Figure 1 and involve representation and decision-making assumptions. Representationally, the idea is that signal and noise tri- als can be represented as values along a unidimensional “strength” construct. Both types of trials are assumed to produce strengths that vary according to a Gaussian distribution along this dimension. The signal strengths are assumed to be greater—on average—than the noise strengths; thus, the signal strength distribution has a greater mean. We deal with only the most common equal- variance form of SDT, in which both the distributions are assumed to have the same variance. The decision-making assumption of SDT is that “yes” and “no” responses are produced by comparing the strength of the current trial with a fixed criterion. If the strength exceeds the criterion, then a “yes” response is made; otherwise, a “no” response is made. Figure 1 provides a formal version of the equal-variance SDT model. Since the underlying strength scale has arbi- trary units, the variances are fixed to 1, and the mean of the noise distribution is fixed to 0. The mean of the signal distribution is d  and measures the discriminability of the signal and noise trials as being the distance between their two distributions. The strength value 1 ⁄ 2 d  is special because it is the crite- rion value at which both signal and noise distributions are equally likely. In this sense, using a criterion of 1 ⁄ 2 d  cor- responds to balanced or unbiased responding. The actual criterion used for responding is denoted k , and distance between this criterion and the unbiased criterion is denoted c . This makes c a measure of bias, because it corresponds to how different the actual criterion is from the unbiased one. Positive values of c correspond to a bias toward saying “no” and thus to an increase in correct rejections at the ex- pense of an increase in misses. Negative values of c corre- spond to a bias toward saying “yes” and thus to an increase in hits at the expense of an increase in false alarms. Figure 1 also shows an alternative measure of bias, ; , which is the likelihood ratio of the signal-to-noise dis- tribution at the criterion, expressed on a log odds scale. Again, positive values of ; correspond to a bias toward saying “no,” whereas negative values correspond to a bias toward saying “yes.” It has been argued that one advan- tage of the c measure over ; is that c is independent of d  , whereas ; is not (Snodgrass & Corwin, 1988). The SDT model—with its representation and decision- making assumptions—naturally makes predictions about BayesSDT: Software for Bayesian inference with signal detection theory University of California, Irvine, California This article describes and demonstrates the BayesSDT MATLAB-based software package for performing Bayesian analysis with equal-variance Gaussian signal detection theory (SDT). The software uses WinBUGS to draw samples from the posterior distribution of six SDT parameters: discriminability, hit rate, false alarm rate, criterion, and two alternative measures of bias. The software either provides a simple MATLAB graphical user interface or allows a more general MATLAB function call to produce graphs of the posterior distribution for each parameter of interest for each data set, as well as to return the full set of posterior samples. Behavior Research Methods 2008, 40 (2), 450-456 doi: 10.3758/BRM.40.2.450 M. D. Lee, mdlee@uci.edu \ncation in psychology—for instance, as models of human cognition (e.g., Anderson, 1991; Griffiths & Tenenbaum, 2005; Kemp, Bernstein, & Tenenbaum, 2005; Sanborn, Griffiths, & Navarro, 2006; Tenenbaum & Griffiths, 2001) and as methods for relating psychological models to data (e.g., Kuss, Jäkel, & Wichmann, 2005; Lee, 2008; Lee & Webb, 2005; Navarro, Griffiths, Steyvers, & Lee, 2006; Pitt, Myung, & Zhang, 2002; Rouder, Lu, Speckman, Sun, & Jiang, 2005). In the context of SDT, Bayesian advantages include avoiding the need for edge corrections that can be a prob- lem in standard analyses (see Snodgrass & Corwin, 1988), 1 and posterior distributions are not constrained to take par- ticular parametric forms; thus, they better represent param- eter values—like very high hit or false alarm rates—that are near the boundary of the theoretically possible range. Bayesian methods are also automatically exact for any sam- ple size and are sensitive to how many data are available in making inferences. Additional potential advantages of Bayesian methods for SDT, which go beyond the function- ality of the software presented in the present article, include hierarchical extensions to model individual differences in participants and in items (e.g., Lee & Paradowski, 2007; Rouder & Lu, 2005) and the straightforward consideration of alternative unequal variance and other extended SDT models without requiring analytic results for inference. Graphical model . Graphical models provide a convenient formalism for expressing many Bayesian models (see Griffiths, Kemp, & Tenenbaum, in press; Jordan, 2004; Lee, 2008, for two psychological intro- ductions and one statistical introduction). In graphical modeling, a graph is created in which nodes represent parameters and data. The structure of the graph then in- dicates dependencies between the parameters and data, with children depending on their parents. The practical advantage of graphical models is that sophisticated and relatively general-purpose Markov chain Monte Carlo (MCMC) algorithms exist that can sample from the full joint posterior distribution of the parameters conditional on the observed data. Our software relies on WinBUGS (Spiegelhalter, Thomas, & Best, 2004), which uses a range of MCMC computational methods, including adap- tive rejection sampling, slice sampling, and Metropolis– Hastings (see, e.g., Chen, Shao, & Ibrahim, 2000; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 2003) to perform posterior sampling. Figure 2 shows the graphical model we use for mak- ing inferences about the SDT parameters in Figure 1. The convention is used that observed variables (i.e., data) have shaded nodes, whereas unobserved variables (i.e., the pa- rameters to be inferred from data) are not shaded. In ad- dition, continuous variables have circular nodes, whereas discrete variables have square nodes. Finally, stochastic variables in our model are shown with single-bordered nodes, whereas deterministic variables (i.e., those that are defined simply as functions of other variables) are shown as double-bordered nodes. The best way to understand the graphical model is to begin with the discriminability d  and bias c parameters. hit and false alarm rates and thus maps onto the counts in Table 1. In Figure 1, the hit rate, h , is shown as the propor- tion of the signal distribution above the criterion k . Simi- larly, the false alarm rate, f , is the proportion of the noise distribution above the criterion k . The usefulness of SDT is that, through this relationship, it is possible to take the sort of data in Table 1 and convert the counts of hits and false alarms into psychologically meaningful measures of discriminability and bias. Discriminability is a measure of how easily signal and noise trials can be distinguished. Bias is a measure of how the decision-making criterion being used relates to the balanced criterion. A Graphical Model for Bayesian Inference Bayesian inference . The Bayesian approach to statisti- cal inference (see, e.g., Gelman, Carlin, Stern, & Rubin, 2004; Jaynes, 2003; Lee & Wagenmakers, 2005) has a number of desirable properties in inferring the parameters of SDT models from data. These advantages stem from the basic Bayesian philosophy of always representing what is known and unknown about parameters of interest using probability distributions. The distributions provide com- plete representations of uncertainty and can be updated in a coherent and principled way using probability theory. Bayesian methods have recently found widespread appli- Table 1 Signal Detection Theory Terminology Response Signal Trial Noise Trial “Yes” Hit False alarm “No” Miss Correct rejection 0 d \u0001 d \u0001 /2 k y x c \u0001 \u0001 log( x / y ) Signal Noise h f Probability Density Strength Figure 1. Equal-variance Gaussian signal detection theory framework. \nThe final component of the Bayesian graphical model involves prior assumptions. We choose priors on discrim- inability and bias that correspond to the assumption of uniform priors for the hit and false alarm rates, consistent with standard practice. It can be proven 2 that the required distributions are d  Gaussian(0,2) (7) and c Gaussian(0, 1 ⁄ 2 ). (8) WinBUGS implementation . The graphical model shown in Figure 2 is straightforward to implement in WinBUGS, using the script in Table 2. The required in- puts are the hit, false alarm, miss, and correct-rejection counts, and the outputs are samples from the posterior distribution of the d  , h , f , k , c , and ; parameters of the SDT model. 3 The BayesSDT Software Package The BayesSDT software package consists of a num- ber of MATLAB scripts and functions, as well as Win- BUGS scripts. It can be downloaded from www.socsci .uci.edu/~mdlee/. BayesSDT requires that MATLAB and WinBUGS 1.4.2 or later be installed. WinBUGS is available from www.mrc-bsu.cam.ac.uk/bugs/. Bayes- SDT also requires the MatBugs MATLAB function avail- able at www.cs.ubc.ca/~murphyk/Software/MATBUGS/ matbugs.html, which allows MATLAB and WinBUGS to pass information to each other. The BayesSDT software provides two MATLAB in- terfaces with the WinBUGS implementation of the SDT graphical model. The easiest to use is a graphical user in- terface (GUI), but a MATLAB function call is also pro- vided to allow more than four data sets to be analyzed simultaneously. Graphical user interface . The BayesSDT GUI is started by running the BayesSDT_GUI MATLAB script. A demonstration of the interface is displayed in Figure 3, showing the entry of three illustrative data sets for anal- ysis. The first is the “many” data set, involving 70 hit counts and 50 false alarm counts from 100 signal and 100 noise trials. The second is the “few” data set, involving 7 hit counts and 5 false alarm counts from 10 signal and 10 noise trials. The idea is that the many data set has the same rate of hits and false alarms as does the few data set, to illustrate how Bayesian analysis represents the decrease in uncertainty with additional data. The third, “perfect” data set involves 10 hits and 0 false alarms from 10 signal and 10 noise trials. The purpose of this data set is to illus- trate how Bayesian analysis deals with edge effects. As Figure 3 shows, the GUI allows a label to be entered for each data set. The color, width, and style properties of the lines used to draw the posterior distribution for each data set can also be specified using standard MATLAB options. The help plot command in MATLAB pro- vides a full list. Checkboxes allow the selection of the SDT parameters to be analyzed. Finally, the number of poste- rior samples to be generated—as well as the number of bins used to display the final posterior densities—can be In order to relate these parameters to the observed data, d  and c can be reparameterized (under the model given by SDT) into hit and false alarm rates h and f . This means that there is a deterministic relationship between h and f on the one hand, and between d  and c on the other, given by h d c ` \b & 1 2 (1) f d c ` \b & 1 2 (2) where \u0014 (·) is the standard Gaussian cumulative density function. The number of signal trials, S , and the number of noise trials, N , are observed data, as are the hit count H and the false alarm count F . The hit and false alarm counts follow a binomial distribution depending on both the hit and false alarm rates, and the number of signal and noise trials, so that H Binomial( S , h ) (3) and F Binomial( N , f ). (4) In this way, SDT relates discriminability d  and bias c pa- rameters to the observed data so that posterior inferences can be drawn. From the posterior distributions of d  and c , the posterior distributions for the criterion k and alter- native bias measure ; can be calculated according to the standard results k d c ` 2 (5) ; \u0015 cd  . (6) S N H F h f d  c k ; k d c d c d c ` ` ` 1 2 B \u000f \u000f 1 2 1 2 h d c f d c H ` ¤ ¦ ¥ ³ µ ´ ` ¤ ¦ ¥ ³ µ ´ & & \u000f \u000f Binomial( h , S ) Binomial( f , N ) F Gaussian(0, 1/2) Gaussian(0, 2) Figure 2. Graphical model for inferring discriminability ( d  ), criterion ( k ), two measures of bias ( c and ; ), and hit ( h ) and false alarm ( f ) rates from H observed hits out of S signal trials and F observed false alarms out of N noise trials. \nrameters are all well defined without the need for edge corrections. It is also worth noting that these hit and false alarm posteriors do not take the Gaussian form assumed by standard analyses, but instead are sensitive to the theoreti- cal bounds on the values of these rate parameters. MATLAB function . Rather than using the BayesSDT GUI, it is possible to call the BayesSDT function directly. This is especially useful for analyzing more than four data sets simultaneously. The BayesSDT function takes a single argument, D, which is a structured variable with 14 fields. D.ndatasets is the number of data sets to be analyzed. D.sdt. This is a matrix with the rows represent- ing data sets and the columns representing signal detection data counts. The four columns correspond to hits, false alarms, misses, and correct rejections, respectively. D.nsamples is the number of posterior samples to generate. D.nbins is the number of bins to use in drawing histograms of the posterior densities. specified. Additional samples and bins lead to smoother approximations to the posterior density, but come at the expense of additional computing time. As a guide to speed, in the present article, collecting and analyzing 10 6 samples and 100 bins for the three data sets—many, few, and perfect—took 273 sec on a PC with a processor speed of 2.4 GHz and 2 Gb of RAM. Figure 4 shows the graphical output of the BayesSDT software, which is the main result of the analysis, although BayesSDT GUI also automatically generates a text file— BayesSDToutput.txt—listing the means and standard devi- ations for each of the parameters. In Figure 4, the posterior distributions for the many data set show that the parameters are estimated with relatively little uncertainty. For the few data set, however, the posterior distributions now represent the much greater degree of uncertainty. For each param- eter, the mode is the same, but the range of possible values the parameter could take is greater. This is a natural con- sequence of the parameter estimates being based on fewer data, and it is well handled by the Bayesian approach. For the perfect data set, the modal hit and false alarm rates are 1.0 and 0.0, but other possibilities have some density; thus, the remaining discriminability, bias, and criterion pa- Table 2 WinBUGS Script for Bayesian SDT Graphical Model # BAYESIAN SIGNAL DETECTION THEORY # # INPUT VARIABLES # H, F, M, C are the hit, false alarm, # miss and correct rejection signal detection counts # # OUTPUT VARIABLES # d, k are discriminability and criterion # h, f are the hit and false-alarm rates # c, b are measures of bias model { # Relating observed counts to underlying Hit and False Alarm rates # Number of Signal Trials is sum of Hit and Miss counts S <- H+M # Number of Noise Trials is sum of False Alarm and Correct Rejection counts N <- F+C # Hit counts are Binomial H ~ dbin(h,S) # False alarm counts are Binomial F ~ dbin(f,N) # Reparameterization, converting Hit and False Alarm rates #  to Discriminability and Bias indices h <- phi(d/2-c) f <- phi(-d/2-c) k <- d/2+c b <- d*c # These priors over Discriminability and Bias correspond #  to uniform priors over the Hit and False Alarm rates MEAND <- 0 MEANC <- 0 LAMBDAD <- 1/2 LAMBDAC <- 2 c ~ dnorm(MEANC,LAMBDAC) d ~ FSDT) } \n(e.g., Lee, 2008). The fields in stats provide means and standard deviations for the posterior samples of each parameter for each data set. The syntax for the Bayes- SDT function call is simply [ samples stats ] \u0015 BayesSDT(D) . Conclusion SDT is a highly useful and widely used method for ana- lyzing psychological data. Bayesian inference for SDT has a number of attractive properties, using the full poste- rior distributions of parameters to represent what is known about discriminability, bias, criterion, and hit and false alarm rates. The BayesSDT software package provides a simple WinBUGS implementation of a suitable Bayesian graphical model and provides a MATLAB GUI and func- tion call for performing the Bayesian analysis. AUTHOR NOTE I thank Douglas Creelman and two anonymous reviewers for help- ful comments, and Joachim Vandekerckhove, Andrew Heathcote, and Geoff Iverson for useful discussions. Correspondence concerning this article should be addressed to M. D. Lee, Department of Cognitive Sci- ences, University of California, 3151 Social Sciences Plaza, Irvine, CA 92697-5100 (e-mail: mdlee@uci.edu). D.labels is a character array with a string label for each data set. D.linecolor is a character array with a MATLAB color for each data set, using the standard MATLAB plot colors. D.linestyle is a character array with a MATLAB color for each data set, using the standard MATLAB line styles. D.linewidth is an array of positive numbers con- taining line widths for each data set. D.dcheck , D.hcheck , D.fcheck , D.kcheck , D.ccheck , and D.bcheck are vectors of 0 and 1 entries for each data set, indicating whether or not the d  , h , f , k , c , and ; parameters, respectively, should be analyzed. The BayesSDT function generates graphs of posterior distributions for the analyzed parameters. It also returns two structured variables— samples and stats — returned by WinBUGS. The fields in samples give the full list of posterior samples of each parameter for each data set. These samples allow more advanced Bayesian analyses based on the full-joint posterior distribution Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Bins Samples 0 d \u0001 d \u0001 /2 k y x c \u0002\u0001 = log( x / y ) Signal Noise h f Figure 3. The BayesSDT graphical user interface. \nJaynes, E. T. (2003). Probability theory: The logic of science (G. L. Bretthorst, Ed.). New York: Cambridge University Press. Jordan, M. I. (2004). Graphical models. Statistical Science , 19 , 140-155. Kemp, C., Bernstein, A., & Tenenbaum, J. B. (2005). A generative theory of similarity. In B. G. Bara, L. W. Barsalou, & M. Bucciarelli (Eds.), Proceedings of the 27th Annual Conference of the Cognitive Science Society [CD-ROM]. Mahwah, NJ: Erlbaum. Kuss, M., Jäkel, F., & Wichmann, F. A. (2005). Bayesian inference for psychometric functions. Journal of Vision , 5 , 478-492. Lee, M. D. (2008). Three case studies in the Bayesian analysis of cogni- tive models. Psychonomic Bulletin & Review , 15 , 1-15. Lee, M. D., & Paradowski, M. J. (2007). Group decision-making on an optimal stopping problem. Journal of Problem Solving , 1 , 53-73. Lee, M. D., & Wagenmakers, E.-J. (2005). Bayesian statistical infer- ence in psychology: Comment on Trafimow (2003). Psychological Review , 112 , 662-668. Lee, M. D., & Webb, M. R. (2005). Modeling individual differences in cognition. Psychonomic Bulletin & Review , 12 , 605-621. MacKay, D. J. C. (2003). Information theory, inference, and learning algorithms . Cambridge: Cambridge University Press."
}