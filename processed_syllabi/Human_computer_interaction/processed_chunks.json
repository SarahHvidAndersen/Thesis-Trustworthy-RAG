[
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Copyright 2008 Psychonomic Society, Inc. 450 This article describes and demonstrates the simple BayesSDT software package—developed in MATLAB— for performing Bayesian analysis using signal detection theory (SDT). As background, we first briefly describe SDT and the Bayesian approach to its analysis. We then present illustrative examples of the BayesSDT software and its outputs. Signal Detection Theory SDT—developed by Peterson, Birdsall, and Fox (1954) and applied to psychophysics by Tanner and Swets (1954)—is a very general, useful, and widely employed method for drawing inferences from data in psychology (see Green & Swets, 1966; Macmillan & Creelman, 2004, for detailed introductions). SDT analysis is often applied to data that take a 2 2 form—as shown in Table 1—in which there are “signal” trials and “noise” trials, and “yes” responses and “no” responses. When a “yes” response is given for a signal trial, it is called a hit . When a “yes” rsponse is given for a noise trial, it is called a false alarm . When a “no” response is given for a signal trial, it is called a miss . When a “no” response is given for a noise trial, it is called a correct rejection . The data needed for an SDT analysis are just the counts of hits, false alarms, misses, and correct rejections. The key assumptions of SDT are shown in Figure 1 and involve representation and decision-making assumptions. Representationally, the idea is that signal and noise trals can be represented as values along a unidimensional “strength” construct. Both types of trials are assumed to produce strengths that vary according to a Gaussian distribution along this dimension. The signal strengths are assumed to be greater—on average—than the noise strengths; thus, the signal strength distribution has a greater mean. We deal with only the most common equavariance form of SDT, in which both the distributions are assumed to have the same variance",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We deal with only the most common equavariance form of SDT, in which both the distributions are assumed to have the same variance. The decision-making assumption of SDT is that “yes” and “no” responses are produced by comparing the strength of the current trial with a fixed criterion. If the strength exceeds the criterion, then a “yes” response is made; otherwise, a “no” response is made. Figure 1 provides a formal version of the equal-variance SDT model. Since the underlying strength scale has arbtrary units, the variances are fixed to 1, and the mean of the noise distribution is fixed to 0. The mean of the signal distribution is d and measures the discriminability of the signal and noise trials as being the distance between their two distributions. The strength value 1 ⁄ 2 d is special because it is the critrion value at which both signal and noise distributions are equally likely. In this sense, using a criterion of 1 ⁄ 2 d coresponds to balanced or unbiased responding. The actual criterion used for responding is denoted k , and distance between this criterion and the unbiased criterion is denoted c . This makes c a measure of bias, because it corresponds to how different the actual criterion is from the unbiased one. Positive values of c correspond to a bias toward saying “no” and thus to an increase in correct rejections at the epense of an increase in misses. Negative values of c corrspond to a bias toward saying “yes” and thus to an increase in hits at the expense of an increase in false alarms. Figure 1 also shows an alternative measure of bias, ; , which is the likelihood ratio of the signal-to-noise ditribution at the criterion, expressed on a log odds scale. Again, positive values of ; correspond to a bias toward saying “no,” whereas negative values correspond to a bias toward saying “yes.” It has been argued that one advatage of the c measure over ; is that c is independent of d , whereas ; is not (Snodgrass & Corwin, 1988)",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The SDT model—with its representation and decisiomaking assumptions—naturally makes predictions about BayesSDT: Software for Bayesian inference with signal detection theory University of California, Irvine, California This article describes and demonstrates the BayesSDT MATLAB-based software package for performing Bayesian analysis with equal-variance Gaussian signal detection theory (SDT). The software uses WinBUGS to draw samples from the posterior distribution of six SDT parameters: discriminability, hit rate, false alarm rate, criterion, and two alternative measures of bias. The software either provides a simple MATLAB graphical user interface or allows a more general MATLAB function call to produce graphs of the posterior distribution for each parameter of interest for each data set, as well as to return the full set of posterior samples. Behavior Research Methods 2008, 40 (2), 450-456 doi: 10.3758/BRM.40.2.450 M. D. Lee, mdlee@uci.edu cation in psychology—for instance, as models of human cognition (e.g., Anderson, 1991; Griffiths & Tenenbaum, 2005; Kemp, Bernstein, & Tenenbaum, 2005; Sanborn, Griffiths, & Navarro, 2006; Tenenbaum & Griffiths, 2001) and as methods for relating psychological models to data (e.g., Kuss, Jäkel, & Wichmann, 2005; Lee, 2008; Lee & Webb, 2005; Navarro, Griffiths, Steyvers, & Lee, 2006; Pitt, Myung, & Zhang, 2002; Rouder, Lu, Speckman, Sun, & Jiang, 2005). In the context of SDT, Bayesian advantages include avoiding the need for edge corrections that can be a prolem in standard analyses (see Snodgrass & Corwin, 1988), 1 and posterior distributions are not constrained to take paticular parametric forms; thus, they better represent paraeter values—like very high hit or false alarm rates—that are near the boundary of the theoretically possible range. Bayesian methods are also automatically exact for any saple size and are sensitive to how many data are available in making inferences",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Bayesian methods are also automatically exact for any saple size and are sensitive to how many data are available in making inferences. Additional potential advantages of Bayesian methods for SDT, which go beyond the functioality of the software presented in the present article, include hierarchical extensions to model individual differences in participants and in items (e.g., Lee & Paradowski, 2007; Rouder & Lu, 2005) and the straightforward consideration of alternative unequal variance and other extended SDT models without requiring analytic results for inference. Graphical model . Graphical models provide a convenient formalism for expressing many Bayesian models (see Griffiths, Kemp, & Tenenbaum, in press; Jordan, 2004; Lee, 2008, for two psychological intrductions and one statistical introduction). In graphical modeling, a graph is created in which nodes represent parameters and data. The structure of the graph then idicates dependencies between the parameters and data, with children depending on their parents. The practical advantage of graphical models is that sophisticated and relatively general-purpose Markov chain Monte Carlo (MCMC) algorithms exist that can sample from the full joint posterior distribution of the parameters conditional on the observed data. Our software relies on WinBUGS (Spiegelhalter, Thomas, & Best, 2004), which uses a range of MCMC computational methods, including adative rejection sampling, slice sampling, and Metropolis– Hastings (see, e.g., Chen, Shao, & Ibrahim, 2000; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 2003) to perform posterior sampling. Figure 2 shows the graphical model we use for maing inferences about the SDT parameters in Figure 1. The convention is used that observed variables (i.e., data) have shaded nodes, whereas unobserved variables (i.e., the prameters to be inferred from data) are not shaded. In adition, continuous variables have circular nodes, whereas discrete variables have square nodes",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In adition, continuous variables have circular nodes, whereas discrete variables have square nodes. Finally, stochastic variables in our model are shown with single-bordered nodes, whereas deterministic variables (i.e., those that are defined simply as functions of other variables) are shown as double-bordered nodes. The best way to understand the graphical model is to begin with the discriminability d and bias c parameters. hit and false alarm rates and thus maps onto the counts in Table 1. In Figure 1, the hit rate, h , is shown as the propotion of the signal distribution above the criterion k . Simlarly, the false alarm rate, f , is the proportion of the noise distribution above the criterion k . The usefulness of SDT is that, through this relationship, it is possible to take the sort of data in Table 1 and convert the counts of hits and false alarms into psychologically meaningful measures of discriminability and bias. Discriminability is a measure of how easily signal and noise trials can be distinguished. Bias is a measure of how the decision-making criterion being used relates to the balanced criterion. A Graphical Model for Bayesian Inference Bayesian inference . The Bayesian approach to statistcal inference (see, e.g., Gelman, Carlin, Stern, & Rubin, 2004; Jaynes, 2003; Lee & Wagenmakers, 2005) has a number of desirable properties in inferring the parameters of SDT models from data. These advantages stem from the basic Bayesian philosophy of always representing what is known and unknown about parameters of interest using probability distributions. The distributions provide coplete representations of uncertainty and can be updated in a coherent and principled way using probability theory. Bayesian methods have recently found widespread applTable 1 Signal Detection Theory Terminology Response Signal Trial Noise Trial “Yes” Hit False alarm “No” Miss Correct rejection 0 d d /2 k y x c log( x / y ) Signal Noise h f Probability Density Strength Figure 1",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Equal-variance Gaussian signal detection theory framework. The final component of the Bayesian graphical model involves prior assumptions. We choose priors on discriinability and bias that correspond to the assumption of uniform priors for the hit and false alarm rates, consistent with standard practice. It can be proven 2 that the required distributions are d Gaussian(0,2) (7) and c Gaussian(0, 1 ⁄ 2 ). (8) WinBUGS implementation . The graphical model shown in Figure 2 is straightforward to implement in WinBUGS, using the script in Table 2. The required iputs are the hit, false alarm, miss, and correct-rejection counts, and the outputs are samples from the posterior distribution of the d , h , f , k , c , and ; parameters of the SDT model. 3 The BayesSDT Software Package The BayesSDT software package consists of a nuber of MATLAB scripts and functions, as well as WiBUGS scripts. It can be downloaded from www.socsci .uci.edu/~mdlee/. BayesSDT requires that MATLAB and WinBUGS 1.4.2 or later be installed. WinBUGS is available from www.mrc-bsu.cam.ac.uk/bugs/. BayeSDT also requires the MatBugs MATLAB function avaiable at www.cs.ubc.ca/~murphyk/Software/MATBUGS/ matbugs.html, which allows MATLAB and WinBUGS to pass information to each other. The BayesSDT software provides two MATLAB iterfaces with the WinBUGS implementation of the SDT graphical model. The easiest to use is a graphical user iterface (GUI), but a MATLAB function call is also prvided to allow more than four data sets to be analyzed simultaneously. Graphical user interface . The BayesSDT GUI is started by running the BayesSDT_GUI MATLAB script. A demonstration of the interface is displayed in Figure 3, showing the entry of three illustrative data sets for anaysis. The first is the “many” data set, involving 70 hit counts and 50 false alarm counts from 100 signal and 100 noise trials. The second is the “few” data set, involving 7 hit counts and 5 false alarm counts from 10 signal and 10 noise trials",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The second is the “few” data set, involving 7 hit counts and 5 false alarm counts from 10 signal and 10 noise trials. The idea is that the many data set has the same rate of hits and false alarms as does the few data set, to illustrate how Bayesian analysis represents the decrease in uncertainty with additional data. The third, “perfect” data set involves 10 hits and 0 false alarms from 10 signal and 10 noise trials. The purpose of this data set is to illutrate how Bayesian analysis deals with edge effects. As Figure 3 shows, the GUI allows a label to be entered for each data set. The color, width, and style properties of the lines used to draw the posterior distribution for each data set can also be specified using standard MATLAB options. The help plot command in MATLAB prvides a full list. Checkboxes allow the selection of the SDT parameters to be analyzed. Finally, the number of postrior samples to be generated—as well as the number of bins used to display the final posterior densities—can be In order to relate these parameters to the observed data, d and c can be reparameterized (under the model given by SDT) into hit and false alarm rates h and f . This means that there is a deterministic relationship between h and f on the one hand, and between d and c on the other, given by h d c ` & 1 2 (1) f d c ` & 1 2 (2) where (·) is the standard Gaussian cumulative density function. The number of signal trials, S , and the number of noise trials, N , are observed data, as are the hit count H and the false alarm count F . The hit and false alarm counts follow a binomial distribution depending on both the hit and false alarm rates, and the number of signal and noise trials, so that H Binomial( S , h ) (3) and F Binomial( N , f ). (4) In this way, SDT relates discriminability d and bias c prameters to the observed data so that posterior inferences can be drawn",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". (4) In this way, SDT relates discriminability d and bias c prameters to the observed data so that posterior inferences can be drawn. From the posterior distributions of d and c , the posterior distributions for the criterion k and altenative bias measure ; can be calculated according to the standard results k d c ` 2 (5) ; cd . (6) S N H F h f d c k ; k d c d c d c ` ` ` 1 2 B 1 2 1 2 h d c f d c H ` ¤ ¦ ¥ 3 μ ́ ` ¤ ¦ ¥ 3 μ ́ & & Binomial( h , S ) Binomial( f , N ) F Gaussian(0, 1/2) Gaussian(0, 2) Figure 2. Graphical model for inferring discriminability ( d ), criterion ( k ), two measures of bias ( c and ; ), and hit ( h ) and false alarm ( f ) rates from H observed hits out of S signal trials and F observed false alarms out of N noise trials. rameters are all well defined without the need for edge corrections. It is also worth noting that these hit and false alarm posteriors do not take the Gaussian form assumed by standard analyses, but instead are sensitive to the theoretcal bounds on the values of these rate parameters. MATLAB function . Rather than using the BayesSDT GUI, it is possible to call the BayesSDT function directly. This is especially useful for analyzing more than four data sets simultaneously. The BayesSDT function takes a single argument, D, which is a structured variable with 14 fields. D.ndatasets is the number of data sets to be analyzed. D.sdt. This is a matrix with the rows represening data sets and the columns representing signal detection data counts. The four columns correspond to hits, false alarms, misses, and correct rejections, respectively. D.nsamples is the number of posterior samples to generate. D.nbins is the number of bins to use in drawing histograms of the posterior densities. specified. Additional samples and bins lead to smoother approximations to the posterior density, but come at the expense of additional computing time",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". specified. Additional samples and bins lead to smoother approximations to the posterior density, but come at the expense of additional computing time. As a guide to speed, in the present article, collecting and analyzing 10 6 samples and 100 bins for the three data sets—many, few, and perfect—took 273 sec on a PC with a processor speed of 2.4 GHz and 2 Gb of RAM. Figure 4 shows the graphical output of the BayesSDT software, which is the main result of the analysis, although BayesSDT GUI also automatically generates a text file— BayesSDToutput.txt—listing the means and standard devations for each of the parameters. In Figure 4, the posterior distributions for the many data set show that the parameters are estimated with relatively little uncertainty. For the few data set, however, the posterior distributions now represent the much greater degree of uncertainty. For each paraeter, the mode is the same, but the range of possible values the parameter could take is greater. This is a natural cosequence of the parameter estimates being based on fewer data, and it is well handled by the Bayesian approach",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is a natural cosequence of the parameter estimates being based on fewer data, and it is well handled by the Bayesian approach. For the perfect data set, the modal hit and false alarm rates are 1.0 and 0.0, but other possibilities have some density; thus, the remaining discriminability, bias, and criterion pTable 2 WinBUGS Script for Bayesian SDT Graphical Model # BAYESIAN SIGNAL DETECTION THEORY # # INPUT VARIABLES # H, F, M, C are the hit, false alarm, # miss and correct rejection signal detection counts # # OUTPUT VARIABLES # d, k are discriminability and criterion # h, f are the hit and false-alarm rates # c, b are measures of bias model { # Relating observed counts to underlying Hit and False Alarm rates # Number of Signal Trials is sum of Hit and Miss counts S <- H+M # Number of Noise Trials is sum of False Alarm and Correct Rejection counts N <- F+C # Hit counts are Binomial H ~ dbin(h,S) # False alarm counts are Binomial F ~ dbin(f,N) # Reparameterization, converting Hit and False Alarm rates # to Discriminability and Bias indices h <- phi(d/2-c) f <- phi(-d/2-c) k <- d/2+c b <- d*c # These priors over Discriminability and Bias correspond # to uniform priors over the Hit and False Alarm rates MEAND <- 0 MEANC <- 0 LAMBDAD <- 1/2 LAMBDAC <- 2 c ~ dnorm(MEANC,LAMBDAC) d ~ FSDT) } (e.g., Lee, 2008). The fields in stats provide means and standard deviations for the posterior samples of each parameter for each data set. The syntax for the BayeSDT function call is simply [ samples stats ] BayesSDT(D) . Conclusion SDT is a highly useful and widely used method for anlyzing psychological data. Bayesian inference for SDT has a number of attractive properties, using the full postrior distributions of parameters to represent what is known about discriminability, bias, criterion, and hit and false alarm rates. The BayesSDT software package provides a simple WinBUGS implementation of a suitable Bayesian graphical model and provides a MATLAB GUI and funtion call for performing the Bayesian analysis",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The BayesSDT software package provides a simple WinBUGS implementation of a suitable Bayesian graphical model and provides a MATLAB GUI and funtion call for performing the Bayesian analysis. AUTHOR NOTE I thank Douglas Creelman and two anonymous reviewers for helful comments, and Joachim Vandekerckhove, Andrew Heathcote, and Geoff Iverson for useful discussions. Correspondence concerning this article should be addressed to M. D. Lee, Department of Cognitive Scences, University of California, 3151 Social Sciences Plaza, Irvine, CA 92697-5100 (e-mail: mdlee@uci.edu). D.labels is a character array with a string label for each data set. D.linecolor is a character array with a MATLAB color for each data set, using the standard MATLAB plot colors. D.linestyle is a character array with a MATLAB color for each data set, using the standard MATLAB line styles. D.linewidth is an array of positive numbers cotaining line widths for each data set. D.dcheck , D.hcheck , D.fcheck , D.kcheck , D.ccheck , and D.bcheck are vectors of 0 and 1 entries for each data set, indicating whether or not the d , h , f , k , c , and ; parameters, respectively, should be analyzed. The BayesSDT function generates graphs of posterior distributions for the analyzed parameters. It also returns two structured variables— samples and stats — returned by WinBUGS. The fields in samples give the full list of posterior samples of each parameter for each data set. These samples allow more advanced Bayesian analyses based on the full-joint posterior distribution Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Hit FA Miss CR Label Color Style W idth Bins Samples 0 d d /2 k y x c = log( x / y ) Signal Noise h f Figure 3. The BayesSDT graphical user interface. Jaynes, E. T. (2003). Probability theory: The logic of science (G. L. Bretthorst, Ed.). New York: Cambridge University Press. Jordan, M. I. (2004). Graphical models. Statistical Science , 19 , 140-155",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "BayesSDT: Software for Bayesian inference with signal detection theory",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\SDTBayes.pdf",
    "date_published": "2010-11-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". T. (2003). Probability theory: The logic of science (G. L. Bretthorst, Ed.). New York: Cambridge University Press. Jordan, M. I. (2004). Graphical models. Statistical Science , 19 , 140-155. Kemp, C., Bernstein, A., & Tenenbaum, J. B. (2005). A generative theory of similarity. In B. G. Bara, L. W. Barsalou, & M. Bucciarelli (Eds.), Proceedings of the 27th Annual Conference of the Cognitive Science Society [CD-ROM]. Mahwah, NJ: Erlbaum. Kuss, M., Jäkel, F., & Wichmann, F. A. (2005). Bayesian inference for psychometric functions. Journal of Vision , 5 , 478-492. Lee, M. D. (2008). Three case studies in the Bayesian analysis of cogntive models. Psychonomic Bulletin & Review , 15 , 1-15. Lee, M. D., & Paradowski, M. J. (2007). Group decision-making on an optimal stopping problem. Journal of Problem Solving , 1 , 53-73. Lee, M. D., & Wagenmakers, E.-J. (2005). Bayesian statistical infeence in psychology: Comment on Trafimow (2003). Psychological Review , 112 , 662-668. Lee, M. D., & Webb, M. R. (2005). Modeling individual differences in cognition. Psychonomic Bulletin & Review , 12 , 605-621. MacKay, D. J. C. (2003). Information theory, inference, and learning algorithms . Cambridge: Cambridge University Press.",
    "chunk_id": "Human_computer_interaction_bayessdt_software_for_bayesian_inference_with_signal_detection_theory.json_chunk_12"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": "Home Library Stories Stats Udit Maitra Follow -- Listen Share This article includes a complete guide for eye-tracking testing from scratch level, including when it can be used in UX research, how can you set it up, evaluate, collect data, and analyze it? Let me tell you a story: it’s 6 a.m. on Monday morning, and the alarm clock is ringing (brrring!). Two brothers, Bobo and Jojo, wake up and need to get ready for school, then they go to the bathroom and brushed their teeth, but they haven’t opened their eyes since leaving their bed. Wait for a second, they haven’t even opened their eyes yet? But how did they find the brush and toothpaste to clean their teeth? I believe some of you have either been through something similar in your lives or perhaps something else happened to you and later you realized it. We humans frequently respond or behave unconsciously with our everyday activity as a result it became of our daily routine, practice, and the same thing happens with our digital media also. When we use Instagram to check the story, maybe we are buying something from Amazon, or watching a video on YouTube, we unconsciously communicate with them the majority of the time. So we as researcher generally try to understand human behavior and attitudes in three ways: So here we will be focusing onPhysiological Measures. But the question is why it is so important in UX research? As you can see in the diagram above, if we gather three distinct types of test evidence for a person, it would be more concrete or rich information because self-report (what users say) might contain some user bias, and observational data (what users do) may contain some observer bias, but physiological measures are completely natural data since humans are unable to control their unconscious actions and behaviors.So if you are looking for some unconscious behavior of your users for your study and you want to add more solid evidence in your research data then collecting physiological measures is the crucial thing",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": ". In this article, We will just are looking at eye-tracking in the physiological measurements category. Why Eye tracking is important in UX research? Purpose of Eye-tracking: let me give you an example:If you find that people aren’t clicking on the right-hand section of the banner Ad to alter the product images (image thumbnail) as seen below, how can you validate your design with the user that section is being looked at or not? Perhaps I think you have understood the problem statement and we need to do user research in order to understand the root cause of the problem,but what method we should use? Since we are not sure how users perceive this Ad when they looked at it suddenly or at the first time, we can’t get rich information by asking the user (user interview) why they are not clicking on this area to explore the different product images because what human say and do most of the time it will be different and we might get some bias results, hencewe will be choosing Observational study and Physiological Measures (eye-tracking) to understand their unconscious behavior. Please note: Apart from UX researsch we use eye tracking in Medical Research, PC and Gaming Research, Human Factors and Simulation, Market Research like to understand the customer’s shoping behaviour in a shop. However, in the field of user experience research, we don’t just use eye monitoring to understand users’ behavior; we integrate it with usability testing, qualitative studies, and other approaches. Here are the below steps in order to run the eye-tracking test. Example: In order to understand where our consumer is looking and how they are perceiving the content in the Ad, we need to verify the above Ad (Fig-4) with an eye-tracking test. 2. Identify the eye tracker device or software:We will be identifying the technologies for eye-tracking tests in this section. There are a variety of eye-tracking systems and applications on the market, each with its own range of benefits and drawbacks. So choose the technology based on your preference. 3",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": ". There are a variety of eye-tracking systems and applications on the market, each with its own range of benefits and drawbacks. So choose the technology based on your preference. 3. Identify the metrics which you need after the test: Common eye-tracking Metrics: There are many metrics associated with eye-tracking data. The following are the most common eye-tracking metrics used by UX researchers. Later, I will be explaining the detailed information about the above metrics. 4. Calibrate device or software:Here we will calibrate our eye-tracking software or devices before running the test. 5. Run the test with your participants:In this step, we will be giving our participants the task and run the test. 6,7. Collect the data and analysis and synthesis:This is the most important aspect of the eye-tracking test; we will get two kinds of data from it. A. Qualitative data and B. Quantitative data. Let me give you a more detailed description. *A fixations is defined by a pause in the eye’s moment within a well defined area. Example: (Fig -9)is an example of how to define a specific region on the page. These regions are typically referred to as “look zones” or “area of interest (AOIs)”. AOI are essentially those things that you want to measure as defined by the set of x,y coordinates. 8. Highlight the findings: Area of Interest (AOI) visualization:It is an efficient way to analyze the eye movement data by AOIs is through a bringing chart. AOI has can be measure with the following types of metrics: A. Sequence:It is basically the order or sequence of gaze points in another word which section user noticed at first, second, third, and thus in a particular area. Example: If a user notices a button a first on a page then the Sequence of that area is 1. B. Entry time (Time of first fixation):During the total test time when the user’s eyes go in a particular element or area then that’s the entry time of that element or area",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": ". B. Entry time (Time of first fixation):During the total test time when the user’s eyes go in a particular element or area then that’s the entry time of that element or area. Example: If a user notices a button in 2 seconds after loading the page, then the Sequence Entry time is 2000 ms. C. Dwell time:Dwell time is the total amount of time spent looking within the AOI. This includes fixations and saccades within the AOI, Including revisiting. Generally the greater the dwell time, the greater level of interest in AOI. As a general rule of thumb, dwell time less than 100 ms participants processed a limited amount of information. A dwell time greater than 500 ms generally means the participant had an opportunity to process the information. D. Hit ratio:It is a percentage of participants who had at least one fixation with the AOI. In other words, this is the number of participants who saw the AOI. Example: In Fig-11, 13 out of 13 participants (or 100%) were fixed within the particular AOI. E. Revisit:Revisits are the number of times that the eye fixates within an AOI, leaves the AOI, and returns back to fixate within the AOI. Revisits indicate the “stickiness” of the AOI. F. Revisitors:It is the number of people who revisit within an AOI. Example: In Fig-11, 8 out of 13 participantsrevisitswithin the particular AOI. G. Average fixation (Fixation duration):The average fixation duration tells you how long the average fixation lasted and can be determined for either individuals or for groups. Fixation durations typically range from 150 ms to 300 ms, it gives the relative engagement with the object. The greater the average fixation duration, the greater level of engagement H. First fixation duration:The first fixation duration provides data about how long that first fixation lasted. If a participant has a short Entry time or Time of First Fixation, and long first fixation duration, the area is in all likelihood very eye-catching, in other words, that section easily grabs users’ attention and the attention period is more",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": ". I. Fixation count (Number of fixations):It is the total number of fixations with an AOI. This is strongly correlated with the dwell time. It is closely related to the use of eye-tracking in UX research is the use of information about the response of the pupil. The most eye-tracking system must detect the locations of the participant's pupil and calculate the diameter to determine where s/he is looking. Consequently, information about pupils' diameter is included in most eye-tracking systems. The study of pupillary response, or the construction and dilations of the pupil, is calledPupillometry,and construction and dilations happen because of some common following reasons: 1. Ambient Light:The Pupil contract and dilates in the response to the Ambient Light. The normal pupil size in adults varies from 2 to 4 mm in diameter in bright light to 4 to 8 mm in the dark. 2. Photographs:Pupils of both sexes dilated after seeing pictures of people of the opposite sex. In females, the difference in pupil size occurred also after seeing pictures of babies and mothers with babies. 3. Cognitive load:Pupillary responses can reflect activation of the brain allocated to cognitive tasks. Greater pupil dilation is associated with increased processing in the brain. 4.Interest:If the brain found something to interest then the pupil dilates, in other words, the greater the interest level is the larger pupil size. As a result, by measuring the diameters of users’ pupil, we can determine how enjoyable our design is, how they are comfortable or stressed, and how much cognitive load is needed while they are seeing or using this. Well, now you have got enough information about eye-tracking testing and it’s time for your turn. Please comment below if you have any doubts or facing any issues while you are doing your own eye-tracking test. All the best! Please applaud if you like it and don’t forget to follow me for your own benefit. Thank you :) www",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "A complete guide for Eye-Tracking testing in UX Research",
    "author": "Udit Maitra",
    "source": "https://maitraudit.medium.com/a-complete-guide-for-eye-tracking-testing-in-ux-research-a3f95d617590",
    "date_published": "2023-03-24 11:14:45.355000+00:00",
    "flag": "",
    "chunk_text": ". All the best! Please applaud if you like it and don’t forget to follow me for your own benefit. Thank you :) www. amazon.in www.tobii.com https://en.wikipedia.org/wiki/Eye_tracking -- -- UX design researcher @Siemens | ex: Amazon Help Status About Careers Press Blog Terms Text to speech Teams",
    "chunk_id": "Human_computer_interaction_a_complete_guide_for_eye-tracking_testing_in_ux_research.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Why You Only Need to Test with 5 Users",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "March 18, 20002000-03-18 Share Some people think that usability is very costly and complex and that user tests should be reserved for the rare web design project with a huge budget and a lavish time schedule. Not true. Elaborate usability tests are a waste of resources. The best results come from testing no more than 5 users and running as many small tests as you can afford. In earlier research, Tom Landauer and I showed that the number of usability problems found in a usability test withnusers is: whereNis the total number of usability problems in the design andLis the proportion of usability problems discovered while testing a single user. The typical value ofLis 31%, averaged across a large number of projects we studied. Plotting the curve forL=31% gives the following result: The most striking truth of the curve is thatzero users give zero insights. As soon as you collect data from asingle test user, your insights shoot up and you have already learned almost a third of all there is to know about the usability of the design. The difference between zero and even a little bit of data is astounding. When you test thesecond user, you will discover that this person does some of the same things as the first user, so there is some overlap in what you learn. People are definitely different, so there will also be something new that the second user does that you did not observe with the first user. So the second user adds some amount of new insight, but not nearly as much as the first user did. Thethird userwill do many things that you already observed with the first user or with the second user and even some things that you have already seen twice. Plus, of course, the third user will generate a small amount of new data, even if not as much as the first and the second user did. As youadd more and more users, you learn less and lessbecause you will keep seeing the same things again and again",
    "chunk_id": "Human_computer_interaction_why_you_only_need_to_test_with_5_users.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Why You Only Need to Test with 5 Users",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". As youadd more and more users, you learn less and lessbecause you will keep seeing the same things again and again. There is no real need to keep observing the same thing multiple times, and you will be very motivated to go back to the drawing board and redesign the site to eliminate the usability problems. After the fifth user, you are wasting your time by observing the same findings repeatedly but not learning much new. The curve clearly shows that you need totest with at least 15 users to discover all the usability problemsin the design. So why do I recommend testing with a much smaller number of users? The main reason is that it is better to distribute your budget foruser testing across many small testsinstead of blowing everything on a single, elaborate study. Let us say that you do have the funding to recruit 15 representative customers and have them test your design. Great.Spend this budget on 3 studies with 5 users each! You want to run multiple tests because the real goal of usability engineering is to improve the design and not just to document its weaknesses. After the first study with five participants has found 85% of the usability problems, you will want to fix these problems in a redesign. After creating the new design, you need totest again. Even though I said that the redesign should \"fix\" the problems found in the first study, the truth is that youthinkthat the new design overcomes the problems. But since nobody can design the perfect user interface, there is no guarantee that the new design does in fact fix the problems. A second test will discover whether the fixes worked or whether they didn't. Also, in introducing a new design, there is always the risk of introducing a new usability problem, even if the old one did get fixed. Also, the second study with 5 users will discover most of the remaining 15% of the original usability problems that were not found in the first round of testing",
    "chunk_id": "Human_computer_interaction_why_you_only_need_to_test_with_5_users.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Why You Only Need to Test with 5 Users",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Also, the second study with 5 users will discover most of the remaining 15% of the original usability problems that were not found in the first round of testing. (There will still be 2% of the original problems left — they will have to wait until the third study to be identified.) Finally, the second study will be able toprobe deeper into the usability of the fundamental structureof the site, assessing issues like information architecture, task flow, and match with user needs. These important issues are often obscured in initial studies where the users are stumped by stupid surface-level usability problems that prevent them from really digging into the site. So the second study will both serve as quality assurance of the outcome of the first study and help provide deeper insights as well. The second study will always lead to a new (but smaller) list of usability problems to fix in a redesign. And the same insight applies to this redesign: not all the fixes will work; some deeper issues will be uncovered after cleaning up the interface. Thus, a third study is needed as well. The ultimate user experience is improved much more by 3 studies with 5 users each than by a single monster study with 15 users. You might think that 15 studies with a single user would be even better than 3 studies with 5 users. The curve does show that we learn much more from the first user than from any subsequent users, so why keep going? Two reasons: You need to test additional users when a website hasseveral highly distinct groups of users. The formula only holds for comparable users who will be using the site in fairly similar ways. If, for example, you have a site that will be used by both children and parents, then the two groups of users will have sufficiently different behavior that it becomes necessary to test with people from both groups. The same would be true for a system aimed at connecting purchasing agents with sales staff",
    "chunk_id": "Human_computer_interaction_why_you_only_need_to_test_with_5_users.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Why You Only Need to Test with 5 Users",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The same would be true for a system aimed at connecting purchasing agents with sales staff. Even when the groups of users are very different, there will still be great similarities between the observations from the two groups. All the users are human, after all. Also, many of the usability problems are related to the fundamental way people interact with the Web and the influence from other sites on user behavior. In testing multiple groups of disparate users, you don't need to include as many members of each group as you would in a single test of a single group of users. The overlap between observations will ensure a better outcome from testing a smaller number of people in each group. I recommend: Nielsen, Jakob, and Landauer, Thomas K.: \"A mathematical model of the finding of usability problems,\"Proceedings of ACM INTERCHI'93 Conference(Amsterdam, The Netherlands, 24-29 April 1993), pp. 206-213. Plan, conduct, and analyze your own studies, whether in person or remote Research Strategies to stay user-centered in fast-paced Agile environments Management Define, share, and implement design operations Management Enable cookiesto watch NN/g videos Usability Testing with 5 Users: Design Process (video 1 of 3) 5 Reasons to Test Even When You “Know” the Answer Hoa Loranger·4 min A Case for Returning to In-Person Usability Testing Hoa Loranger·6 min What Is User Research? Caleb Sponheim·3 min Qualitative Usability Testing: Study Guide Kate Moran·5 min Usability Test, Even When You Know the Answer Hoa Loranger·4 min Thinking Aloud: The #1 Usability Tool Jakob Nielsen·4 min Time Budgets for Usability Sessions Jakob Nielsen·4 min Avoid Leading Questions to Get Better Insights from Participants Amy Schade·4 min Group Notetaking for User Research Susan Farrell·10 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_why_you_only_need_to_test_with_5_users.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 15 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00002-7 Copyright © 2021 Elsevier Inc. All rights reserved. Early in the twentieth century, a group of German psychologists sought to explain how human visual perception works. They observed and cataloged many important visual phenomena. One of their basic findings was that human vision is holistic—our visual system automatically imposes structure on visual input and is wired to perceive whole shapes, figures, and objects rather than disconnected edges, lines, and areas. The German word for “shape” or “figure” is Gestalt, so these theories became known as the Gestalt principles of visual perception . Today’s perceptual and cognitive psychologists regard the Gestalt theory of percetion more as a descriptive framework than as an explanatory and predictive theory. Current theories of visual perception are based heavily on the neurophysiology of the eyes, optic nerve, and brain (see Chapters 4–7). Not surprisingly, the findings of neurophysiological researchers support the observtions of the Gestalt psychologists. We really are—along with other animals—“wired” to perceive our surroundings in terms of whole objects (Stafford and Webb, 2005; Ware, 2008). Consequently, the Gestalt principles are still valid—if not as a fundamental explnation of visual perception, at least as a framework for describing it. They also provide a useful basis for guidelines for graphic design and user-interface design (Soegaard, 2007). For this book, the most important Gestalt principles are Proximity, Similarity, Cotinuity, Closure, Symmetry, Figure/Ground, and Common Fate. The following sections describe each principle and provide examples from both static graphic design and user-interface design. GESTALT PRINCIPLE: PROXIMITY The Gestalt principle of Proximity is that the relative distance between objects in a display affects our perception of whether and how the objects are organized into Our Vision is Optimized to See Structure 2 groups and subgroups",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Objects near each other (relative to others) appear grouped, while those farther apart do not. In Fig. 2.1A , the stars are closer together horizontally than they are vertically, so we see three rows of stars, while the stars in Fig. 2.1B are closer together vertically than they are horizontally, so we see three columns. Although the stars in Fig. 2.1 are similar looking, objects need not look similar for them to appear grouped when placed near each other. For example, in Fig. 2.2 , the groups are defined by how close together the objects are, not by how they look. The Proximity principle is useful when laying out control panels or data forms in software, websites, and electronic appliances. Designers who don’t know about Proximity sometimes use group boxes and horizontal or vertical lines to separate groups of controls and data displays. For example, Outlook’s Distribution List Mebership dialogue box groups the Add , Remove , and Properties buttons together using Proximity but then associates that group with the listbox using an unnecessary (A) (B) FIGURE 2.1 Proximity: items that are closer appear grouped as rows (A) and columns (B). FIGURE 2.2 Proximity: even dissimilar objects that are close together appear grouped. group-box widget (see Fig. 2.3 ). Even less necessary is the group-box labeled “Distrbution list,” since it contains only a single combo-box. “Group box around one item” is a common UI design blooper (Johnson, 2007). Using Proximity, items on a display can be visually grouped simply by spacing them closer to each other than to other controls without using group-boxes or viible borders. Furthermore, Proximity can be applied hierarchically to define groups of subgroups. For example, in Firefox’s keyboard text preferences dialogue box, the three checkboxes to control spell-checking, autocapitalization, and automatic addtion of periods are grouped (see Fig. 2.4 ), and those are grouped with other controls as well as a table control",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2.4 ), and those are grouped with other controls as well as a table control. Graphic design experts recommend using Proximity to avoid visual clutter (Mullet and Sano, 1994), cut wasted ink or pixels that add no data to the presentation (Tufte, 2001), and reduce the amount of code needed to implement it. Proximity also governs perception of the labels on controls. Too much space between a label and the item it labels, and people won’t connect the label with the item. Conversely, if a label is too close to a different item, people may conect it with that item instead of the intended one. For example, poor spacing of radio button labels in a form at Delta.com (2015) could easily cause people to choose the wrong button (see Fig. 2.5A ), whereas the spacing of radio button labels at United.com (2020) shows clearly which label goes with which button (see Fig. 2.5B ). FIGURE 2.3 In Outlook’s Distribution List Membership dialogue box, group boxes are used unnecessarily. Simple proximity—spacing—would be enough. GESTALT PRINCIPLE: SIMILARITY Another factor that affects our perception of grouping is expressed in the Gestalt principle of Similarity , which states that similar-looking objects appear grouped, all other things being equal. In Fig. 2.6 , the slightly larger, “hollow” stars are perceived as a group. Gmail uses similarity— bold versus nonbold text—to help users perceive unread emails as a distinct group from already-read ones (see Fig. 2.7A ). Lyft’s smartphone app uses similarity—car shapes—to let users see at a glance how many of its drivers are available in a potential rider’s vicinity (see Fig. 2.7B ). FIGURE 2.4 In Firefox’s keyboard text preferences dialogue box, controls are grouped using the Proximity principle without group boxes and borders. (B) (A) FIGURE 2.5 Radio button labels: (A) Poor placement; (B) Good placement. FIGURE 2.6 Similarity: items appear grouped if they look more similar to each other than to other objects",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". (B) (A) FIGURE 2.5 Radio button labels: (A) Poor placement; (B) Good placement. FIGURE 2.6 Similarity: items appear grouped if they look more similar to each other than to other objects. (A) (B) FIGURE 2.7 Similarity is used: (A) by Gmail to make unread emails stand out from already-read ones and (B) by Lyft to provide a quick overview of available rides. The Page Setup dialogue box in Mac OS applications uses both Similarity and Proimity to convey groupings (see Fig. 2.8 ). The two very similar and tightly spaced Orentation settings are intended to appear grouped. The two menus are not so tightly spaced but look similar enough that they appear grouped even though that proably wasn’t intended. The Cancel and OK buttons are positioned together, away from everything else. Even without the separator line, they would appear to be a group. FIGURE 2.8 Mac OS Page Setup dialogue box. Similarity and Proximity are used to group settings and controls. (A) (B) FIGURE 2.9 Continuity: Human vision is biased to see continuous forms, even adding missing data if necessary. GESTALT PRINCIPLE: CONTINUITY Several Gestalt principles describe our visual system’s tendency to resolve ambiguity or fill in missing data so we perceive whole objects. The first such principle, the principle of Cotinuity, states that when visual elements are aligned with each other, our visual perception is biased to perceive them as continuous forms rather than disconnected segments. For example, in Fig. 2.9A , we automatically see two crossing lines—one blue and one orange. We don’t see two separate orange segments and two separate blue ones, and we don’t see a blue-and-orange V on top of an upside-down orange-and-blue V. In Fig. 2.9B , due to the vertical alignment of the pieces and the fact that they are spaced to match the curvature of the visible pieces, we see a sea monster in water, not three pieces of one. If we misaligned the pieces or spaced the pieces further than the cuvature suggests, the illusion of continuity would disappear",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If we misaligned the pieces or spaced the pieces further than the cuvature suggests, the illusion of continuity would disappear. A well-known example of the use of the continuity principle in graphic design is the IBM logo. It consists of disconnected blue patches, and yet it is not at all ambiguous. The blue rectangles are stacked vertically with horizontal space between the stacks, so we see three bold letters, perhaps viewed through something like venetian blinds (see Fig. 2.10 ). Slider controls are a user-interface example of the Continuity principle. We see a slider as depicting a single range controlled by a handle that appears somewhere on the slider, not as two separate ranges separated by the handle (see Fig. 2.11A ). Even displaying different colors on each side of a slider’s handle doesn’t completely “break” our perception of a slider as one continuous object, although ComponetOne’s choice of strongly contrasting colors (gray vs. red) certainly strains that percetion a bit (see Fig. 2.11B ). FIGURE 2.10 The IBM company logo uses the Continuity principle to form letters from disconnected patches. (A) (B) FIGURE 2.11 Continuity: we see a slider as a single slot with a handle somewhere on it, not as two slots separated by a handle: (A) Mac OS and (B) ComponentOne. GESTALT PRINCIPLE: CLOSURE Related to Continuity is the Gestalt principle of Closure : our visual system autmatically tries to close open figures so they are perceived as whole objects rather than separate pieces. Thus, we perceive the disconnected arcs in Fig. 2.12A as a circle. Our visual system is so strongly biased to see objects that it can even interpret a totally blank area as an object. We see the combination of shapes in Fig. 2.12B as a white triangle overlapping another triangle and three black circles, even though the figure really only contains three V shapes and three black Pac-Men. The Closure principle is often applied in graphical user interfaces (GUIs)",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The Closure principle is often applied in graphical user interfaces (GUIs). For example, GUIs often represent collections of objects (e.g., documents or messages) as stacks (see Fig. 2.13 ). Just showing one whole object and the edges of others “behind” it is enough to make users perceive a stack of objects, all whole. (A) (B) FIGURE 2.12 Closure: Human vision is biased to see whole objects, even when they are incomplete. FIGURE 2.13 Desktop icons depicting stacks of objects exhibit the Closure principle: partially visible objects are perceived as whole. GESTALT PRINCIPLE: SYMMETRY A third fact about our tendency to see objects is captured in the Gestalt principle that in German is named “prägnanz,” which literally means “good Figure” but is often translated to Simplicity or Symmetry . It states that we tend to parse complex scenes in a way that reduces the complexity. The data in our visual field usually has more than one possible interpretation, but our vision automatically organizes and interprets the data to simplify it and give it symmetry, making it easier to comprehend. For example, we see the complex shape on the far left of Fig. 2.14 as two overlaping diamonds, not as two touching corner bricks or a pinch-waist octahedron with a square in its center. A pair of overlapping diamonds is simpler than the other two interpretations shown on the right—it has fewer sides and more symmetry than the other two interpretations. The Symmetry principle also predicts that we will see Fig. 2.15 as five overlapping rings and not as a jumble of interconnected arcs. not = or FIGURE 2.14 Symmetry: the human visual system tries to resolve complex scenes into combinations of simple, symmetrical shapes. FIGURE 2.15 Symmetry predicts that people will perceive this figure as five overlapping rings. Our visual system’s reliance on symmetry can be exploited to make complex information easier to scan and understand",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Our visual system’s reliance on symmetry can be exploited to make complex information easier to scan and understand. For example, presenting information in a table—a symmetric way to present data—makes it easier to extract the desired infomation (see Table 2.1 ). GESTALT PRINCIPLE: FIGURE/GROUND The next Gestalt principle that describes how our visual system structures the data it receives is Figure/Ground . This principle states that our mind separates the visual field into the figure (the foreground) and ground (the background). The foreground consists of the elements of a scene that are the object of our primary attention, and the background is everything else. The Figure/Ground principle also specifies that the visual system’s parsing of scenes into figure and ground is influenced by characteristics of the scene. For exaple, when a small object or color patch overlaps a larger one, we tend to perceive the smaller object as the figure and the larger object as the ground (see Fig. 2.16 ). Table 2.1 Tables Use Symmetry to Make Data Easier to Scan Than a Prose Presentation Would. Student Quiz 1 Quiz 2 Quiz 3 Quiz 4 Fred (B) 95 92 98 90 Susan H. 99 98 97 95 Sergei L. 83 91 92 88 Hannah N. 75 87 92 83 FIGURE 2.16 Figure/Ground: when objects overlap, we see the smaller as the figure and the larger as the ground. However, our perception of figure versus ground is not completely determined by scene characteristics. It also depends on the viewer’s focus of attention, as illustrated by Fig. 2.17 . Is it a vase or two faces? In user-interface and Web design, the Figure/Ground principle is often used to place an impression-inducing background “behind” the primary displayed content. The background can convey information, such as where the user is in the system as in the Android desktop in Fig. 2.18 , or it can suggest a theme, brand, or mood for interpretation of the content. FIGURE 2.17 Vase or faces? Perception of figure versus ground depends on a viewer’s focus of attention",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2.18 , or it can suggest a theme, brand, or mood for interpretation of the content. FIGURE 2.17 Vase or faces? Perception of figure versus ground depends on a viewer’s focus of attention. FIGURE 2.18 Figure/Ground is also often used to pop up information over other content. Cotent that was formerly the figure—the focus of the user’s attention—temporarily becomes the background for new information, which appears briefly as the new fiure (see Fig. 2.19 ). This approach is usually better than temporarily replacing the old information with the new information, because it provides context that helps keep people oriented regarding their place in the interaction. GESTALT PRINCIPLE: COMMON FATE The previous six Gestalt principles concerned perception of static (unmoving) figures and objects. One final Gestalt principle—Common Fate—concerns moving objects. The Common Fate principle is related to the Proximity and Similarity principles— like them, it affects whether we perceive objects as grouped. The Common Fate priciple states that objects that move together are perceived as grouped or related. For example, in a display showing dozens of pentagons, if seven of them wiggled in synchrony, people would see them as a related group, even if the wiggling pentagons were separated from each other and looked no different from all the other pentagons (see Fig. 2.20 ). Common motion—implying common fate—is used in some animations to show relationships between entities. For example, Google’s Gapminder graphs animate dots representing nations to show changes in various factors of ecnomic development over time. Countries that move together share development histories (see Fig. 2.21 ). (A) (B) FIGURE 2.19 Figure/Ground can be used to display temporary information “over” the page content: (A) call-taction at PBS.org’s mobile website and (B) Android settings pulldown. GESTALT PRINCIPLES: COMBINED Of course, in real-world visual scenes, the Gestalt principles work in concert, not isolation",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". GESTALT PRINCIPLES: COMBINED Of course, in real-world visual scenes, the Gestalt principles work in concert, not isolation. For example, a typical Mac OS desktop usually exemplifies six of the seven principles described here, excluding Common Fate): Proximity, Similarity, Continuity, Closure, Symmetry, and Figure/Ground (see Fig. 2.22 ). On a typical desktop, Common Fate is used (along with Similarity) when a user selects several files or folders and drags them as a group to a new location (see Fig. 2.23 ). With all these Gestalt principles operating at once, a design may imply unintended visual relationships. A recommended practice after designing a display is to view it with each of the Gestalt principles in mind—Proximity, Similarity, Continuity, Closure, Symmetry, Figure/Ground, and Common Fate—to see if the design suggests any reltionships between elements that you do not intend. FIGURE 2.20 Common Fate: items appear grouped or related if they move together. FIGURE 2.21 Common fate: Gapminder animates dots to show which nations have similar development histories (for details, animations, and videos, visit Gapminder.org ). FIGURE 2.22 All of the Gestalt principles except Common Fate play a role in the Mac OS desktop. FIGURE 2.23 Similarity and Common Fate: when users drag folders that they have selected, common highlighting and motion make the selected folders appear grouped. IMPORTANT TAKEAWAYS For easy reference, here are the Gestalt principles of visual perception covered in this chapter, which are those most relevant to user-interface design: l Proximity : Objects near each other (relative to other objects) appear grouped, while those farther apart do not. l Similarity : Objects that look similar appear grouped. l Continuity : Our visual perception is biased to perceive continuous forms rather than disconnected segments. l Closure : Our visual system automatically tries to close figures that are open so they are perceived as whole objects rather than separate pieces",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Closure : Our visual system automatically tries to close figures that are open so they are perceived as whole objects rather than separate pieces. l Symmetry : Our visual system parses complex scenes in a way that reduces their complexity by recognizing symmetries in the scene. l Figure/Ground : Our mind separates the visual field into the figure (the forground) and ground (the background). l Common Fate : Objects that move together are perceived as grouped or related.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-15-30.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Peak–End Rule People judge an experience largely based on how they felt at its peak and at its end, rather than on the total sum or average of every moment of the experience. Key Takeaways • Pay close attention to the most intense points and the final moments (the “end”) of the user journey. • Identify the moments when your product is most helpful, valuable, or entertaining and design to delight the end user. • Remember that people recall negative experiences more vividly than positive ones. Overview An interesting thing happens when we recollect a past event. Instead of consideing the entire duration of the experience, we tend to focus on an emotional peak and on the end, regardless of whether those moments were positive or negative. In other words, we remember each of our life experiences as a series of represetative snapshots rather than a comprehensive timeline of events. Our feelings during the most emotionally intense moments and at the end are averaged in our minds and heavily influence how we assess the overall experience to determine whether we’d be willing to do it again or recommend it to others. This observtion, known as the peak–end rule, strongly suggests we should pay close 53 1 Daniel Kahneman, Barbara L. Fredrickson, Charles A. Schreiber, and Donald A. Redelmeier, “When More Pain Is Preferred to Less: Adding a Better End,” Psychological Science 4, no. 6 (1993): 401–5. 2 Donald A.Redelmeier and Daniel Kahneman, “Patients’ Memories of Painful Medical Treatments: ReaTime and Retrospective Evaluations of Two Minimally Invasive Procedures,” Pain 66, no. 1 (1996): 3–8. 3 Donald A. Redelmeier, Joel Katz, and Daniel Kahneman, “Memories of Colonoscopy: A Randomized Trial,” Pain 104, no. 1–2 (2003): 187–94. attention to these critical moments to ensure users evaluate an overall experience positively. Origins Evidence for the peak–end rule was first explored in the 1993 paper “When More Pain Is Preferred to Less: Adding a Better End” by Daniel Kahneman et al",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Origins Evidence for the peak–end rule was first explored in the 1993 paper “When More Pain Is Preferred to Less: Adding a Better End” by Daniel Kahneman et al. 1 They conducted an experiment in which participants were subjected to two different versions of a single unpleasant experience. The first trial involved participants submerging a hand in 14°C water (roughly 57°F) for 60 seconds. The second trial involved participants submerging the other hand in 14°C water for 60 seconds and then keeping it submerged for an additional 30 seconds as the water was warmed to 15°C. When given the choice of which experience they would repeat, participants were more willing to repeat the second trial, despite it being a longer exposure to the uncomfortable water temperatures. The conclusion by the authors was that the participants chose the longer trial simply because they prferred the memory of it in comparison to the first trial. Subsequent studies would corroborate this conclusion, beginning with a 1996 study by Kahneman and Redelmeier 2 that found that colonoscopy or lithtripsy patients consistently evaluated the discomfort of their experience based on the intensity of pain at the worst and final moments, regardless of length or vaiation in intensity of pain within the procedure. A later study by the same researchers 3 expanded on this by randomly dividing patients into two groups: one that underwent a typical colonoscopy, and another that underwent the same prcedure in addition to having the tip of the scope left in for three extra minutes without inflation or suction. When asked afterward which they preferred, patients who underwent the longer procedure experienced the final moments as less painful, rated their overall experience as less unpleasant, and ranked the prcedure as less aversive in comparison to the other participants",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Additionally, those that underwent the longer procedure were more likely to return for 4 Daniel Kahneman and Amos Tversky, “Subjective Probability: A Judgment of Representativeness,” Cognitive Psychology 3, no. 3 (1972): 430–54. subsequent procedures—a result of these participants judging the experience positively because of the less painful end. PSYCHOLOGY CONCEPT Cognitive Biases To understand the peak–end rule, it is helpful to have an understanding of cognitive biases. The topic warrants an entire book of its own, but here I’ll just give a brief introduction in the context of the peak–end rule. Cognitive biases are systematic errors of thinking or rationality in judgment that influence our perception of the world and our decisiomaking ability. First introduced by Amos Tversky and Daniel Kahneman in 1972, 4 these mental shortcuts increase our efficiency by enabling us to make quick decisions without the need to thoroughly analyze a situation. Instead of constantly becoming paralyzed by the process of mental examination every time we must make a decision, we can rely on these unconscious automatic responses to help expedite things, engaging in heavier mental processing only when necessary. However, cognitive bises can also distort our thinking and perception, ultimately leading to inaccurate judgment and poor decisions. Perhaps you’ve tried to have a logical discussion about a polarizing hot-button issue with someone else, only to discover it was incredibly dificult. The underlying reason for this can quite often be attributed to the fact that we attempt to preserve our existing beliefs by paying attention to information that confirms those beliefs and discounting information that challenges them. This is known as confirmation bias : a bias of belief in which people tend to seek out, interpret, and recall information in a way that confirms their preconceived notions and ideas. This is but one of many common biases humans are susceptible to on a daily basis",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is but one of many common biases humans are susceptible to on a daily basis. The peak–end rule, also a cognitive bias, is known as a memory bias because it impairs the recall of a memory. We remember intensely emtional events more than less emotional events, and this has an effect on how we perceive an experience: we recall not the sum of how we felt throughout the experience but the average of how we felt during its peak emotional moments and at its end. The peak–end rule is related to another cognitive bias known as the recency effect, which states that items near the end of a sequence are the easiest to recall. Examples One company that demonstrates proficiency in understanding how emotion impacts user experience is Mailchimp. The process of creating an email capaign can be quite stressful, but Mailchimp knows how to guide users while keeping the overall tone light and reassuring. Take, for example, the moment when you’re about to hit Send on an email you’ve crafted for your audience’s inboxes. This peak emotional moment represents the accumulation of all the work that has gone into that email campaign, compounded by the potential fear of failure. Mailchimp understands this is an important moment, especially for first-time users, so it goes beyond presenting a simple confirmation modal ( Figure 6-1 ). Figure 6-1. Mailchimp’s email campaign confirmation modal (source: Mailchimp, 2019) By infusing a touch of brand character through illustration, subtle animtion, and humor, the tool defuses what could potentially be a stressful moment. Freddie, the company’s emblematic chimp mascot, hovers his finger over a large 5 Priya Kamat and Candice Hogan, “How Uber Leverages Applied Behavioral Science at Scale,” Uber Engneering (blog), January 28, 2019, https://eng.uber.com/applied-behavioral-science-at-scale . red button as if to imply he is eagerly awaiting your permission",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". red button as if to imply he is eagerly awaiting your permission. The longer you wait, the more nervous Freddie seems to get, which is evident through the beads of sweat that appear on his hand and subtle shaking. Mailchimp’s artful capitalization on key moments doesn’t end there. Once an email campaign is sent, the user is redirected to a confirmation screen ( Figure 6-2 ) providing details pertaining to the campaign. There’s also an Easter egg on this screen that validates the user’s hard work: Freddie gives them a high five, as if to reassure them of a job well done. These details reinforce the feeling of accomplishment and enhance the experience, creating positive mental snashots for people that use this service. Figure 6-2. Mailchimp’s “email sent” screen (source: Mailchimp, 2019) Positive events aren’t the only things that have an impact on how people feel about a product or service. Negative events also provide emotional peaks and can contribute to a user’s lasting impression of an experience. Take, for example, wait times, which can have a profound effect on how people perceive a product or service. Ride-sharing company Uber realized that waiting was an unavoidable part of its business model and sought to reduce this pain point by focusing on three concepts related to wait time: idleness aversion, operational transparency, and the goal gradient effect. 5 Uber Express POOL customers ( Figure 6-3 ) are prsented with an animation that helps to keep them not only informed but also entertained (idleness aversion). The app provides an estimated time of arrival and information on how arrival times are calculated (operational transparency). It clearly explains each step of the process so customers feel that they are continously making progress toward their goal of getting a ride (goal gradient effect). By focusing on people’s perceptions of time and waiting, Uber was able to reduce its post-request cancellation rate and avoid what could easily become a negative emotional peak while using its service. Figure 6-3",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 6-3. Uber Express POOL (source: Uber, 2019) TECHNIQUE Journey Mapping One handy tool for identifying the emotional peaks of end users througout an experience is journey mapping. This qualitative exercise is invaluble for visualizing how people use a product or service through the narrative of accomplishing a specific task or goal. Journey mapping results in the creation of a design artifact ( Figure 6-4 ) that not only helps designers and project stakeholders align to a common mental model but also creates a deeper shared understanding of the customer experience and aids in identifying the challenges and opportunities present within an experience. Figure 6-4. Example journey map Like all design exercises, journey maps can and should be tailored to the purposes and goals of the project. That being said, they’ll usually contain some key information: Lens The lens of a journey map establishes the perspective of the person the experience represents. It usually will contain the persona of the end user, which should be predefined based on research on the taget audience of the product or service (see Chapter 1 ). The lens should capture the specific scenario that the journey map is focused on. This scenario may be real, or it can be anticipated in the case of a product or service that hasn’t been launched yet. Finally, the lens usually describes the expectations of the persona in that scenario. For example, Jane (persona) is using a ride-share service app to order a ride (scenario) that she expects to arrive at her exact location in 10 minutes or less (expectation). Experience The next part of a journey map is the experience section, which illustrates the actions, mindset, and emotions of the end user maped across a timeline. Starting from the top, the experience is first organized into high-level phases. Next are the actions, which define the steps that the end user must take within each phase to accoplish their task or goal",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Next are the actions, which define the steps that the end user must take within each phase to accoplish their task or goal. Following the actions is information pertaiing to the mindset of the end user during the experience. This can vary based on what insights the journey map is aiming to uncover; it’s essentially a contextual layer of information that provides a deeper view into what the customer is thinking during each phase. Typical information captured within this layer includes general thoughts, pain points, questions, or motivations that originate from research and user interviews. Finally, there’s the emotional layer, which is usually represented as a continuous line mapped across the entire experience and which captures the emotional state of the persona during the experience. This layer is especially significant with regard to the peak–end rule, because it captures the emtional peaks of the customer. Insights The last part of a journey map is the insights section, which identfies the important takeaways that surface within the experience. This section usually contains a list of possible opportunities to improve the overall experience. It also typically contains a list of metrics associated with improving the experience, and details on the internal ownership of these metrics. Going back to our ridshare example, providing real-time information on the location of the vehicle after the ride is ordered can help reduce the pain point of waiting (opportunity). That feature will need to be designed and developed by the product team (internal ownership) and can be monitored according to post-ride ratings (metric). KEY CONSIDERATION Negative Peaks It is inevitable that at some point in the lifespan of a product or service something will go wrong. There might be a server failure that has a ripple effect and leads to service outages, or a bug might open up a security vulnerability, or a design decision might be made that fails to consider all customers and leads to some unintended consequences",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". All of these types of situations can have an emotional effect on the people that use your product and may ultimately inform their overall impression of the experience. Such setbacks can also be opportunities, however, if the right falbacks are in place. Take, for example, the all-too-common 404 error page. When a web page can’t be found, users may become frustrated, creating a negative impression. But some companies use this as an opportunity to create a rapport with their customers and enforce their brand personality by leveraging some good old-fashioned humor ( Figure 6-5 ). Figure 6-5. Various 404 pages that use humor and brand personality (sources [clockwise from top left]: Mailchimp, Ueno, Pixar, and GitHub, 2019) Conclusion Our memories are rarely a perfectly accurate record of events. How users recall an experience will determine how likely they are to use a product or service again or recommend it to others. Since we judge past experiences based not on how we felt throughout the whole duration of the event but on the average of how we felt at the peak emotional moments and at the end, it is vital that these moments make a lasting good impression. By paying close attention to these key moments of an experience, we can ensure users recollect the experience as a whole positively.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-53-63.json_chunk_8"
  },
  {
    "document_type": "online_article",
    "title": "Minimize Cognitive Load to Maximize Usability",
    "author": "Kathryn Whitenton",
    "source": "https://www.nngroup.com/articles/minimize-cognitive-load/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "December 22, 20132013-12-22 Share Most computer users have learned that running too many programs at the same time can slow down or even crash the machine. We work around these limitations by closing programs when we aren't using them. Just like computers,human brains have a limited amount of processing power(as further discussed in our course onThe Human Mind and Usability). When the amount of information coming in exceeds our ability to handle it, our performance suffers. We may take longer to understand information, miss important details, or even get overwhelmed and abandon the task. In the field of user experience, we use the followingdefinition: thecognitive loadimposed by a user interface is the amount of mental resources that is required to operate the system. Informally, you can think of mental resources as \"brain power\" — more formally, we're talking about slots in working memory. The term \"cognitive load\" was originally coined by psychologists to describe the mental effort required to learn new information. Though web browsing is a much more casual activity than formal education, cognitive load is still important: users must learn how to use a site's navigation, layout, and transactional forms. And even when the site is fairly familiar, users must still carry around the information that is relevant to their goal. For instance, when planning a vacation, the users’ cognitive load includes interface-related knowledge and specific vacation-related constraints that they may have (such as price and timeframe). When a computer can't handle our processing demands, we can simply upgrade to a newer, more powerful machine. But to date there's no way to increase the actual processing power of our brains. Instead, designers must understand and accommodate these limits. There's no way to eliminate cognitive load entirely — in fact, even if this was possible, it wouldn't be desirable. After all, people visit websites to get information",
    "chunk_id": "Human_computer_interaction_minimize_cognitive_load_to_maximize_usability.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Minimize Cognitive Load to Maximize Usability",
    "author": "Kathryn Whitenton",
    "source": "https://www.nngroup.com/articles/minimize-cognitive-load/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". There's no way to eliminate cognitive load entirely — in fact, even if this was possible, it wouldn't be desirable. After all, people visit websites to get information. They've come to find out something about your product, organization, or content; most likely it's something they didn't already know.Intrinsic cognitive loadis the effort of absorbing that new information and of keeping track of their own goals. Designers should, however, strive to eliminate, or at least minimize,extraneous cognitive load: processing that takes up mental resources, but doesn't actually help users understand the content (for example, different font styles that don’t convey any unique meaning). User attention is a precious resource, and should be allocated accordingly. Many of ourtop usability guidelines— fromchunking contenttooptimizing response times— are aimed at minimizing cognitive load. In addition to these basics, there are 3 more tips for minimizing cognitive load: Create delightful experiences that touch, convince, or excite your audience Interaction Use psychology to predict and explain how your customers think and act Interaction Enable cookiesto watch NN/g videos What Is Cognitive Load? Why the UX Team Doesn't Get the Credit Tanner Kohler·4 min How to Use the Zeigarnik Effect in UX Feifei Liu·5 min Encouraging Flow State in Products Caleb Sponheim·3 min Psychology for UX: Study Guide Tanner Kohler·10 min Change Blindness Causes People to Ignore What Designers Expect Them to See Kathryn Whitenton·4 min Memory Recognition and Recall in User Interfaces Raluca Budiu·8 min Expandable Menus: Pull-Down, Square, or Pie? Raluca Budiu·9 min User-Experience Quiz: 2015 UX Year in Review Raluca Budiu·3 min Satisficing: Quickly Meet Users’ Main Needs Kathryn Whitenton·4 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_minimize_cognitive_load_to_maximize_usability.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "177 CHAPTER 7 Eye Tracking CONTENTS 7.1 HOW EYE TRACKING WORKS 178 7.2 MOBILE EYE TRACKING 180 7.2.1 Measuring Glanceability 181 7.2.2 Understanding Mobile Users in Context 182 7.2.3 Mobile Eye Tracking Technology 183 7.2.4 Glasses 183 7.2.5 Device Stand 183 7.2.6 Software-Based Eye Tracking 185 7.3 VISUALIZING EYE TRACKING DATA 186 7.4 AREAS OF INTEREST 187 7.5 COMMON EYE TRACKING METRICS 189 7.5.1 Dwell Time 189 7.5.2 Number of Fixations 190 7.5.3 Fixation Duration 190 7.5.4 Sequence 190 7.5.5 Time to First Fixation 190 7.5.6 Revisits 191 7.5.7 Hit Ratio 191 7.6 TIPS FOR ANALYZING EYE TRACKING DATA 191 7.7 PUPILLARY RESPONSE 192 7.8 SUMMARY 193 Eye tracking is a powerful tool in user research to gain insights into how indviduals visually examine different scenes, such as web pages, mobile applictions, grocery store shelves, or even billboards on subway platforms. As a UX researcher, eye tracking is a valuable method to better understand how someone visually interacts with any stimuli, answering fundamental questions such as: • What do they notice? • How long do they look at it? • What do they see first? • What don’t they notice (that they should)? Eye tracking has been around since the early 1900s. Huey (1908) devised a system whereby someone would wear a contact lens with a small hole for the pupil. The contact lens was then physically attached to a pointing device which would allow researchers to observe eye movements while reading text. Thankfully, we have come a long way since then. Eye tracking is now affordable (for most budgets), highly accurate, able to measure eye movements across a wide variety of stimuli and scenes, portable (through glasses), and the analysis and visualization tools are powerful and easy to use. Plus, there is no need to inject ink into anyone's eye! Eye tracking is typically performed in one of two ways in the context of user research. In one way, eye tracking is based on a set of research questions that necessitate the need to analyze eye movements",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In one way, eye tracking is based on a set of research questions that necessitate the need to analyze eye movements. This might involve comparing the visual attention patterns of two different web designs. In order to answer this question, the researcher must collect and analyze eye movement data. In this case, the “hit ratio” would tell them the percentage of participants who notice (or fixate) on an object in one web design compared to another web design. Another way in which eye tracking is often used in user research is simply to generate real-time qualitative insights. A stakeholder might be interested in observing the eye movements in real time or as part of a participant recording, without any intention of analyzing the data. Observing eye movements provides an additional layer of data to gain a more complete picture of the user experence. Sometimes, the only associated deliverable is a heat map, with any assocated metrics. No matter what approach you take with eye tracking, it is critical to determine the goals and the desired output before any work begins. The information provided by an eye tracking system can be remarkably useful as part of user research. Simply enabling observers to see where the participant is looking in real time is extremely valuable. Even if you do no further analyses of the eye tracking data, just this real-time display provides insight that would not be possible otherwise. For example, assume a participant is performing a task on a website and there's a link on the homepage that would take him directly to the page required to complete the task. The participant keeps exploring the website, going down dead ends, returning to the homepage, but never reaching the required page. In a situation like this, you would like to know whether the participant ever saw the appropriate link on the homepage or whether he saw the link but dismissed it as not what he wanted (e.g., because of its wording)",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Although you could subsequently ask participants that question, their meory may not be completely accurate. With an eye tracking system, you can tell whether the participant at least fixated on the link long enough to read it. 7.1 HOW EYE TRACKING WORKS Although a few different technologies are used, many eye tracking systems, such as the one shown in Fig. 7.1 , use some combination of an infrared video camera and infrared light sources to track where the participant is looking. The infrared light sources create reflections on the surface of the participant's eye (called the coneal reflection), and the system compares the location of that reflection to the location of the participant's pupil. The location of the corneal reflection relative to the pupil changes as the partiipant moves his eyes. The first activity in any eye tracking study is to calibrate the system by asking the participants to look at a series of known points; then the system can subsequently interpolate where a participant is looking based on the location of the corneal reflection. Typically, the researcher can check the quality of the calibration, usually expressed as degrees that devate from the X and Y visual planes. Deviations less than one degree are geneally considered to be acceptable, and less than one-half of a degree is very good. Most eye tracking systems tell you something about the quality of the calibrtion and an opportunity to attempt another calibration to improve the accuracy. It is critical that the calibration is satisfactory; otherwise, all the eye movement data should not be recorded or analyzed. Without a good calibration, there will be a disconnect between what the participant is actually looking at and what you assume he/she is looking at. Following calibration, the moderator makes sure the eye movement data are being recorded. The biggest issue tends to be particpants who move around in their seat",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Following calibration, the moderator makes sure the eye movement data are being recorded. The biggest issue tends to be particpants who move around in their seat. Occasionally, the moderator is required to ask the participant to move back/forward, left/right, or raise/lower their seat to recapture the participant’s eyes. Fig. 7.1 An eye tracking system from Tobii. This eye tracking hardware is easily portable and plugs into the computer’s USB port. INSTRUCTIONS FOR CALIBRATING In our experience, there are a few simple instructions that can go a long way toward making the experience easy for both participants and researchers, and provide reliable eye tracking data. 1. Make sure the participants are sitting at the right height and distance from the monitor or whatever device (interface) you are tracking. Chairs should ideally be on wheels, with adjustable heights. 2. Let the participants know that the calibration process is quick, simple, and nothing will be touching them. 7.2 MOBILE EYE TRACKING Contributed by Andrew Schall, Modernizing Medicine. Users interact with mobile devices very differently than those in a desktop environment. Think about the kind of tasks that you perform when using your smartphone versus using a laptop. Also consider where you are performing these activities and how the environment affects your experience. Mobile experiences often occur when people are on the go and need to accomplish tasks quickly, and this can be significantly impacted by their context of use. Eye tracking prvides eye gaze behavior that is ideal for understanding how people view content on their mobile devices, as well as usability metrics such as glanceability. 3. When you display the dynamic calibration point (typically a small circle), tell them to visually follow or trace the circle as it moves around the screen. When the circle momentarily stops at each position, make sure they are looking at the center of the circle. 4",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When the circle momentarily stops at each position, make sure they are looking at the center of the circle. 4. Depending on the quality of the calibration, you may have to ask the participant to go through the process a second time. Simply say something “thanks for doing that—we are going to do it once more so we can make sure we have the most accurate capture of your eye movements.” We typically don’t go through a third time, unless it is necessary for the study. 5. During the course of the study, participants may move so you no longer are tracking them. Simply ask them to readjust their position so their eyes are again being tracked. If they move around a lot, you might consider asking them to keep still as best they can. PARTICIPANTS WHO ARE DIFFICULT TO CALIBRATE It is easy to get a good calibration from most participants. However, there are a few instances that can pose particular challenges. If someone has very narrow framed glasses, the system will have difficulty distinguishing the frames of the glasses from the pupil. Also, if someone is wearing heavy eye make-up, specifically if it is reflective, this will make for a challenging calibration. Lastly, if someone is very fidgety in the chair, such as a child, this will mean that many times you will lose the eye, and you will need them to reposition themselves in a proper position. There is not a lot you can do, other than to specify your requirements during your recruit, as well as give clear instructions during the warm-up. Don’t let this discourage you, though. In our experience, we get a good calibration with well over 90% of our participants, even those with glasses. 7.2.1 Measuring Glanceability Glanceability is defined as being able to quickly view and understand informtion. Mobile experiences often rely on the user noticing subtle visual cues that occur within a mobile app and then promptly acting on them",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Mobile experiences often rely on the user noticing subtle visual cues that occur within a mobile app and then promptly acting on them. Some of the questions that can be addressed when measuring glanceability include: • How long does it take a user to notice and read a notification on their smartwatch while they are out for a run? • How quickly can a user find departure times to determine the next suway train to board to get to their destination? • During a meeting, how quickly can a user identify an incoming call and determine whether to answer it or not? An interface with a high degree of glanceability can be identified by reltively low fixation counts, short fixation durations, and short saccades. These eye tracking metrics should be paired with task performance data to determine how quickly the user was able to successfully complete a task based on the informtion that was observed. Fig. 7.2 shows a participant using a mobile app to compare prices in the store with those found online. This eye gaze video showed that this user quickly skimmed over the product name (indicated by the red circle) to make sure that it matched the in-store item. Fig. 7.2 An example of the use of eye tracking technology with a mobile device. 7.2.2 Understanding Mobile Users in Context Eye tracking can provide insights into how your user’s environment and sitution impact their experience. Some of the questions that eye tracking can help us to answer include: • How do the distractions and disruptions on a subway train impact how users consume social media content on their phone? • How easy is it to set up and use two-factor authentication when checking your bank account balance on your smartwatch while waiting in line at a coffee shop? • While texting with a friend, what information does a user look at to determine the highest-rated pub within walking distance of their current location? Fig. 7.3 shows the variety of contexts in which eye tracking can be used in the real environment",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 7.3 shows the variety of contexts in which eye tracking can be used in the real environment. Eye tracking glasses provided a first-person perspective as this participant attempted to set up and use the Alaska Airlines iPhone and Apple Watch apps while waiting for her flight. All of these situations require researchers to get out of the UX lab and take eye trackers into the field to see how mobile applications are used in a real-world Fig. 7.3 An example of mobile eye tracking across multiple devices and media. environment. Eye tracking can tell us how these situations affect what the user looks at while performing tasks with their mobile devices. 7.2.3 Mobile Eye Tracking Technology Conducting eye tracking research with mobile devices presents a few unique challenges. First, consider that neither the participant nor the device nor the eye tracker are stationary. This can impact the eye tracker’s ability to track participants accurately and consistently, and also potentially make it difficult to capture eye tracking data across multiple participants. In addition, mobile technology has grown to include many other devices besides a smartphone. Researchers need to evaluate the users’ experience on tablet devices, smart watches, and other wearables. There are several technologies that can be used to track mobile devices: • Glasses and wearable eye trackers : Eyewear containing eye tracking hardware worn by a participant that is paired with a portable recording device. • Device stand : A platform and arm that is used to affix a mobile device and eye tracker unit. • Software : A software app that uses the embedded camera within a mobile device. 7.2.4 Glasses Eye tracking glasses ( Fig. 7.4 ) can show us exactly what a person is looking at as they move freely in any real-world setting. The glasses provide a first-person pespective that helps us to understand what a user is looking at in their enviroment and to provide added context to their experience when using their mobile device",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While the glasses provide a high degree of freedom for the participant, it makes it very challenging to compare eye tracking data across participants. It is recommended to use the glasses for only qualitative research insights and to rely on eye gaze recordings to tag key observational findings. 7.2.5 Device Stand A mobile device stand is best used when it is most important to standardize the testing environment where your users will be interacting with the device ( Fig. 7.5 ). The stand is used by attaching the mobile device to a platform or cradle, along with an eye tracking unit. A camera is fixed to the stand using an arm and is directed at the face of the mobile device. By restricting the movement of the device and eye tracker, it is possible to overlay eye tracking data from diffeent participants to produce aggregated visualizations such as heatmaps and eye gaze plots. Fig. 7.4 An example of how eye tracking glasses can be used to understand how users consume news on their phone while sipping a latte at their local coffee shop. (This image may be used for editorial purposes with credits to Tobii AB. https://www.tobiipro.com/imagevault/publishedmedia/e317fzptqw0jk3svfn4t/ TobiiPro-Glasses2-Mobile-Devseice-Usability-Cafe-150.jpg?download = 1 ) Fig. 7.5 The Tobii mobile device stand can be paired with a Tobii × 2 eye tracker and the device platform can be used with any model tablet or smartphone. It is important to note that this configuration creates an artificial situation for using a mobile device. Participants interact with the device while it is sitting on the stand instead of holding it in their hands. 7.2.6 Software-Based Eye Tracking Eye movement behavior can vary widely from person to person. In order to generalize eye gaze patterns, we need tracking data from a lot of eyes. Using a software-based eye tracking solution allows any smartphone to become an eye tracking device",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In order to generalize eye gaze patterns, we need tracking data from a lot of eyes. Using a software-based eye tracking solution allows any smartphone to become an eye tracking device. This allows researchers to collect eye tracking data from hudreds, if not thousands, of participants while they interact with a mobile wesite or app. To use this solution, participants will need to install an app on their smarphone, or the software provider will need to embed the code within their app using an SDK. This solution relies on using the camera built into the smarphone, and tracking accuracy can be dependent on sufficient ambient lighting conditions. Strengths and Limitations of Mobile Eye Tracking Solutions Technology Strength Limitation Glasses • Total freedom of movement • Highly portable • Best for qualitative insights • Expensive compared to other eye tracking solutions • Difficult to compare results across participants • No quantitative metrics Stand • Consistent configuration allows for easier comparison across participants • Can produce eye tracking visualizations • Less natural experience for the participant • Not very portable • Limited quantitative analysis capabilities Software • No additional hardware needed • Potential for large-scale data collection • Eye tracking visualizations that can be aggregated across participants • Less accurate than traditional eye tracking systems • Tracking accuracy can be affected by variability in ambient lighting conditions 7.3 VISUALIZING EYE TRACKING DATA There are many ways to visualize eye tracking data. These visualizations tell the story about where people were looking and when. They might be the only thing that your stakeholders really care about. All eye tracking visualizations are either at an individual level, showing eye movements for one participant, or at an aggregate level, showing eye movements for more than one participant. Fig",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". All eye tracking visualizations are either at an individual level, showing eye movements for one participant, or at an aggregate level, showing eye movements for more than one participant. Fig. 7.6 shows the series or sequence of fixations that an individual particpant made on the Emirates Airlines website, also known as a scan path. This is perhaps the most common way to visually represent the eye movements for a single participant. A fixation is defined by a pause in the eye's movement within a well-defined area. Eye fixations are typically around 200 ms to 250 ms (1/5th or 1/4th of a second) but are highly variable (Galley, Betz, & Biniossek, 2015). The fixations are usually numbered to indicate their sequence. The size of each circle is proportional to the length or duration of the fixation. The saccades , or movements between fixations, are shown by the lines. In Fig.7.6 it is easy to notice that the participant was focused primarily on holiday graphics at the top of the screen and the tabs directly below. However, he did not look at the logo at the top left or the content towards the bottom of the screen. Scan paths are an excellent way to show how a participant looked at the page, and what elements they saw in what order. By far the most common way to visually represent eye movement for multple participants is through a heat map ( Fig. 7.7 ). In this visualization, the brighest areas (red) represent greater density of fixations. It is an excellent way to get a sense of what areas of the page attract more (and less) visual attention. As you can see, visual attention on the REI outdoor website was concentrated on ACCURACY OF WEBCAM–BASED EYE TRACKING Burton, Albert, and Flynn (2014) conducted research comparing the accuracy of traditional infrared eye tracking systems with webcam-based eye tracking systems",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Webcam-based eye tracking systems hold great promise for user researchers because of significantly lower cost, but also the ability to capture eye movement data from a large number of geographically dispersed users, without having to come into a lab. The study was very simple. Participants were presented a set of images (large and small size) on a 3 × 3 grid on the screen using both an infrared and webcam eye tracking system. Participants were instructed to look at each of the images as they were presented in different locations on the screen. The results clearly showed that both the infrared and webcam-based eye tracking systems were adequate for capturing eye movement data when looking at larger images in the center of the screen. However, the webcam-based eye tracking system was not as accurate when capturing eye movements specific to smaller images, or any size images as they moved toward the edges of the screen, regardless of their size. the woman’s face and the 40% offer to the left, with very little visual attention afforded to the top navigation elements. It is important to keep in mind that the analysis software allows the researcher to define the scale of what is considered “red” versus “orange,” etc. So, beware that the researcher can easily exaggerate heat maps to show more or less color. We recommend using the default settings on most software; however, it is impotant to experiment with using different scales. 7.4 AREAS OF INTEREST The most common way to analyze eye tracking data is by measuring visual attetion on specific elements or regions. Most researchers are not just interested in how visual attention is generally distributed across an entire web page or scene, but whether participants noticed certain objects and how much time was spent looking at them. This is particularly the case in marketing, whereby the success of an ad campaign is directly tied to getting customers to notice somthing. Also, it’s a concern when there are certain elements that are critical to task Fig",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Also, it’s a concern when there are certain elements that are critical to task Fig. 7.6 Example of one individual’s scan path of eye movements on the Emirates Airlines website. success or having a positive experience. When the users don’t see them, you can be sure that is a problem. Fig. 7.8 is an example of how to define specific regions on the page. These regions are typically referred to as “look zones” or “areas of interest” (AOIs). AOIs are essentially those objects (or collection of objects) that you want to measure, as defined by a set of x , y coordinates. In Fig. 7.8 there, are four AOIs, with the associated statistics for each AOI: • TTTF: This is “time to first fixation,” or the average amount of time to first notice the object. As you can see from Fig. 7.8 , the large text next to the women was noticed first, after less than 1 second, whereas the button to watch a video took nearly 5 seconds on average to first notice. • Time Spent: This is the average dwell time, or the average “time spent” looking at the AOI. As you can see, nearly 2 seconds on average were spent looking at the large image/text block in the center of the screen (AOI 1), and a 1⁄2 second looking at the four calls to action (AOI 2) on the upper right. • Ratio: The ratio is simply the number of participants who fixated, at least one time, within the AOI. All 9 participants (9/9) fixated within the large image/text block, whereas only 5 out of the 9 participants fixated on the logo in the upper left. Fig. 7.7 Example of a heat map of the REI outdoor website. When analyzing the time spent looking at different regions, keep the folloing in mind: • Carefully define each region. Ideally there will be a small amount of white space in between regions to make sure the eye movements don’t get caught in between two AOIs right next to each other. • Each region should be fairly homogeneous, such as navigation, content, ads, legal information, and so forth",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Each region should be fairly homogeneous, such as navigation, content, ads, legal information, and so forth. If you prefer to subdivide your AOIs into individual elements, you can always aggregate as part of post-hoc analysis. • When presenting data by AOIs, the question about where participants actually looked within the region typically comes up. Therefore, we reommend including a heat map, as in Fig. 7.8 , that shows the continuous distribution of fixations. 7.5 COMMON EYE TRACKING METRICS There are many metrics associated with eye tracking data. The following are some of the most common eye tracking metrics used by UX researchers. It’s important that all of these metrics are associated with specific AOIs. Fig. 7.8 is an example of the type of metrics derived from a single AOI. 7.5.1 Dwell Time The dwell time is the total amount of time spent looking within an AOI. This includes all fixations and saccades within the AOI, including revisits. Dwell time is an excellent metric that conveys the level of interest with a certain AOI. Fig. 7.8 Example of common eye tracking statistics for different areas of interest. Obviously, the greater the dwell time, the greater the level of interest in the AOI. As a general rule of thumb, dwell times less than 100ms generally mean the paticipant processed a limited amount of information. A dwell time greater than 500 ms generally means the participant had an opportunity to process. 7.5.2 Number of Fixations The number of fixations is simply the total count of fixations with an AOI. The number of fixations, as expected, is strongly correlated with dwell time. Because of this, we typically just report dwell time. 7.5.3 Fixation Duration Fixation duration is the average time for the fixations. Fixation duration typcally ranges from 150 to 300 ms. Fixation duration, similar to number of fixtions and dwell time, represents the relative engagement with the object. The greater the average fixation duration, the greater the level of engagement",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Fixation duration, similar to number of fixtions and dwell time, represents the relative engagement with the object. The greater the average fixation duration, the greater the level of engagement. 7.5.4 Sequence The sequence represents the order in which each AOI is first fixated. The sequence tells the researcher the relative prominence of each AOI within the context of a given task. Sometimes it is very helpful to know which AOIs are jumping out to users initially and which AOIs are receiving attention later on. Typically, sequence is calculated as the average order that each AOI was visited. Keep in mind that many participants may not have experienced that exact same order. Sequence is just a best estimate. 7.5.5 Time to First Fixation In some situations, it's helpful to know how long it takes users to first notice a particular element. For example, you may know that users spend only 7 seconds on average on the page, but you want to make sure that a specific element, such as a “continue” or “sign up” button, is noticed within the first 5 seconds. It's helpful that most eye tracking systems timestamp each fixation (i.e., the exact time that each fixation occurred). One way to analyze these data is to take an average of all the times at which the particular element was first fixated. The data should be treated as elapsed time, starting from the initial exposure. The average represents the amount of time taken to first notice the element, for all of those who did notice it. Of course, it's possible that some of the participants may not have noticed it all, let alone within the first 5 seconds. Therefore, you may come up with some mileading data showing an artificially quick time by not taking all the participants into account. 7.5.6 Revisits Revisits are the number of times that the eye fixates within an AOI, leaves the AOI, and returns back to fixate within the AOI. Revisits indicate the “stickiness” of the AOI",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 7.5.6 Revisits Revisits are the number of times that the eye fixates within an AOI, leaves the AOI, and returns back to fixate within the AOI. Revisits indicate the “stickiness” of the AOI. Do the users fixate and leave the AOI, never to return, or do their eyes keep coming back? 7.5.7 Hit Ratio The hit ratio is the percentage of participants who had at least one fixation within the AOI. In other words, this is the number of participants who saw the AOI. CAN YOU TRUST WHAT PEOPLE SAY THEY SAW IN A USABILITY TEST? Albert and Tedesco (2010) ran an experiment in which they used eye tracking to test whether usability test participants accurately report what they see. In this study, participants looked at a series of website home pages. After being shown each home page, the moderators pointed out a specific element. Half of the participants indicated if they had looked at specific elements based on three potential answers (did not look at the element, not sure if they looked at the element, or did look at the element). The other half of the participants used a five-point scale based on how much time was spent looking at that element (from “no time at all” up to “a lot of time”). The results showed that in general, the eye movements were consistent with what the participants reported seeing. However, in about 10% of the cases, the participant claimed to have “definitely seen” an element that the eye-movement data showed they did not fixate. In the second group of participants, about 5% of the cases, the participants said they “spent a long time looking at an element” yet did not have any eye fixations on that element. Together, these results suggest that participants self-reporting what they looked at during a usability test is reasonably reliable but certainly not perfect. 7.6 TIPS FOR ANALYZING EYE TRACKING DATA Over the years we have learned a few things about how to analyze eye tracing data. Above all else, we strongly recommend you plan your study carefully, as well as take time to explore the data",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Above all else, we strongly recommend you plan your study carefully, as well as take time to explore the data. It’s easy to draw the wrong conclusion based on a few heat maps. Here are a few other important tips to keep in mind as you dive into the data: • Control the amount of exposure time for each participant. If they did not see the same image or stimuli for the same time, pre-define the time to only include the first 10 or 15 seconds, or whatever duration makes the most sense given the context. • If you are not able to control for exposure time, analyze the dwell time as a percentage, not as an absolute. If someone spent 10 seconds, and the other person spent 1 minute, their eye movements will be very different, as well as the actual amount of time spent looking at each element. • Only look at time data when the participant is engaged with the task. Do not include any time data when the participant is debriefing about their experience and still being tracked. • During the study, make sure that the participant is being tracked. Monitor their eye movements in real time. As soon as they start to slouch or turn their head, gently remind them to maintain their original position. • Be careful when analyzing the eye movements on dynamic websites. Websites that change considerably due to ads, flash, frames, etc. confuse most eye tracking systems. Every new image is essentially treated as seprate stimuli. We strongly recommend that you consolidate as many web pages together as possible, knowing that not every page is exactly identcal. Otherwise, you will end up with way too many web pages that were only viewed by a single participant. An alternative to this is to simply use static images. They are much easier to analyze but lack an interactive experience. • Consider using a trigger AOI to control where participants are initially looking at the start of the experiment. A trigger might say, “look here to start the experiment.” The text might be in the middle part of the page",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A trigger might say, “look here to start the experiment.” The text might be in the middle part of the page. After the participant has fixated on the text for a certain number of seconds, the experiment begins. This means that all participants start looking from the same location. This might be overkill for the typical usability test but should be considered for more tightly controlled eye tracking studies. 7.7 PUPILLARY RESPONSE Closely related to the use of eye tracking in user research studies is the use of information about the response of the pupil. Most eye tracking systems must detect the location of the participant's pupil and calculate its diameter to detemine where he or she is looking. Consequently, information about pupil diaeter is included in most eye tracking systems. The study of pupillary response, or the contractions and dilations of the pupil, is called pupillometry. Most people know that the pupil contracts and dilates in response to the level of ambient light, but many people don't know that it also responds to cognitive processing, arousal, and increased interest. Typically, the greater the level of arousal or inteest, the larger the pupil size. Because pupil dilation is correlated with so many different mental and emtional states, it's difficult to say whether pupillary changes indicate successes or failures in everyday usability testing. However, measuring pupil diameter may be useful in certain situations where the focus is on the amount of mental concentrtion or emotional arousal. For example, if you are mainly interested in eliciting an emotional response to a new graphic on a website, then measuring changes in pupil diameter (from baseline) may be very useful. To do this, simply measure the percentage deviation away from a baseline for each participant and average those deviations across the participants",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To do this, simply measure the percentage deviation away from a baseline for each participant and average those deviations across the participants. Alternatively, you can measure the percentage of participants who experienced dilated pupils (of a certain amount) while attening to a particular graphic or performing a specific function. 7.8 SUMMARY In this chapter we covered eye tracking as a powerful tool in measuring visual attention and engagement. Eye tracking is becoming much easier to use, more accurate, more versatile and powerful, and even quite affordable. Here’s a ­summary of some of the key points to remember. 1. Eye tracking is the best way to measure visual attention of various aspects of a product, such as a website or mobile application. Eye tracing is used to compare the effectiveness of different designs, as well ­calculate metrics based on areas of interest. 2. Eye tracking typically works with the use of infrared technology. By comparing the position of the corneal reflection to the pupil, we can calculate the gaze direction at any time. 3. Calibration is a key part to any eye tracking study. It is important to obtain satisfactory calibration so you can accurately measure eye movements. 4. Eye tracking mobile applications require the use of glasses that permit tracking eye movements, and possibly a device stand to better control the testing environment. 5. Visualizations in eye tracking tell the story about where people were looking and when. The most common visualizations are scan paths showing the movement of fixations, along with their duration. Another popular visualization is a heatmap, which depicts the distribution of visual attention from, typically, a group of people. 6. Areas of Interest, or AOIs, are one of the most common ways to anlyze eye tracking data. AOIs are objects on the screen, such as a particular block of text, functionality, or image",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "Measuring the User Experience (third edition)",
    "author": "Bill ALbert, Tom Tullis",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Measuring the User Experience.pdf",
    "date_published": "2022-09-11",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6. Areas of Interest, or AOIs, are one of the most common ways to anlyze eye tracking data. AOIs are objects on the screen, such as a particular block of text, functionality, or image. We commonly measure the amount of time spent looking at various AOIs, how long an AOI takes to receive it’s first fixation, or the order in which various AOIs are looked at. 7. There are many metrics associated with eye tracking; however, the most common is dwell time, or the total amount of time spent looking at an object (or AOI). Measuring the sequence tells us about the relative importance of different objects, as well as the TTFF. 8. Measuring changes to pupil diameter is a lesser used, but sometimes valuable, way to measure level of arousal or engagement. An increase in pupil diameter has been shown to correlate to heightened levels of interest; however, it is also influenced by external factors, such as light levels.",
    "chunk_id": "Human_computer_interaction_measuring_the_user_experience_page-177-193.json_chunk_19"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 1 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00001-5 Copyright © 2021 Elsevier Inc. All rights reserved. Our perception of the world around us is not a true depiction of what is actually there. Our perceptions are heavily biased by at least three factors: l The past: our experience l The present: the current context l The future: our goals PERCEPTION BIASED BY EXPERIENCE Experience—your past perceptions—can bias your current perception in several diferent ways. Perceptual priming Imagine that you own a large insurance company. You are meeting with a real estate manager, discussing plans for a new campus of company buildings. The campus cosists of a row of five buildings, the last two with T-shaped courtyards providing light for the cafeteria and fitness center. If the real estate manager showed you the map in Fig. 1.1 , you would see five black shapes representing the buildings. Now imagine that instead of a real estate manager, you are meeting with an advetising manager. You are discussing a new billboard ad to be placed in certain markets around the country. The advertising manager shows you the same image, but in this scenario the image is a sketch of the ad, consisting of a single word: LIFE. In this scnario, you see a word, clearly and unambiguously. When your perceptual system has been primed to see building shapes, you see building shapes, and the white areas between the buildings barely register in your Our Perception is Biased 1 perception. When your perceptual system has been primed to see text, you see text, and the black areas between the letters barely register. A relatively famous example of how priming the mind can affect perception is an image, supposedly by R. C. James, 1 that initially looks to most people like a random splattering of paint (see Fig. 1.2 ) similar to the work of the painter Jackson Pollack. Before reading further, look at the image",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". C. James, 1 that initially looks to most people like a random splattering of paint (see Fig. 1.2 ) similar to the work of the painter Jackson Pollack. Before reading further, look at the image. Only after you are told that it is a Dalmatian dog sniffing the ground near a tree can your visual system organize the image into a coherent picture. Moreover, once you have seen the dog, it is hard to go back to seeing just a random collection of spots. FIGURE 1.1 Building map or word? What you see depends on what you were told to see. FIGURE 1.2 Image showing the effect of mental priming of the visual system. What do you see? These priming examples are visual, but priming can also bias other types of percetion, such as sentence comprehension. For example, the headline “New Vaccine Cotains Rabies” would probably be understood differently by people who had recently heard stories about contaminated vaccines than by people who had recently heard stories about successful uses of vaccines to fight diseases. Familiar perceptual patterns or frames Much of our lives is spent in familiar situations: the rooms in our homes, our yards, our routes to and from school or work, our offices, neighborhood parks, stores, restarants, etc. Repeated exposure to each type of situation builds a pattern in our minds of what to expect to see there. These perceptual patterns , which some researchers call frames , include the objects or events usually encountered in a particular situation. For example, you know most rooms in your home well enough that you need not constantly scrutinize every detail. You know their layout and where most objects are located. You can probably navigate much of your home in total darkness. But your experience with homes is broader than your specific home. In addition to having a pattern for your home, your brain has one for homes in general. It biases your percetion of all homes, familiar and new. In a kitchen, you expect to see a stove and a sink",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In addition to having a pattern for your home, your brain has one for homes in general. It biases your percetion of all homes, familiar and new. In a kitchen, you expect to see a stove and a sink. In a bathroom, you expect to see a toilet, sink, and shower or bathtub (or both). Our mental frames for situations bias our perception toward seeing the objects and events expected in each situation. They are a mental shortcut: by eliminating the need for us to constantly scrutinize every detail of our environment, they help us get around in our world. However, mental frames also make us see things that aren’t really there. For example, if you visit a house in which there is no stove in the kitchen, you might nonetheless later recall seeing one, because your mental frame for kitchens has a strong stove component. Similarly, part of the frame for eating at a restaurant is paying the bill, so you might recall paying for your dinner even if you absentmindedly walked out without paying. Your brain also has frames for backyards, schools, city streets, business offices, supermarkets, dentist visits, taxis, air travel, and other familiar situations. Anyone who uses computers, websites, or smartphones has frames for the desktop and files, web browsers, websites, and various applications and online services. For example, when experienced Web users visit a new website, they expect to see a site name and logo, a navigation bar, some other links, and maybe a search box. When they book a flight online, they expect to specify trip details, examine search results, make a choice, and make a purchase. When they shop online, they expect a shopping cart and a checkout stage with a payment step. Because users of computer software and websites have these perceptual frames, they often click buttons or links without looking carefully at them. Their perception of the display is based more on what their frame for the situation leads them to expect than on what is actually on the screen",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Their perception of the display is based more on what their frame for the situation leads them to expect than on what is actually on the screen. This sometimes confounds software designers, who expect users to see what is on the screen—but that isn’t how human vision and attention works. For example, if the positions of the “Next” and “Back” buttons on the last page of a multistep screen sequence 2 are switched, many people would not immediately notice the change (see Fig. 1.3 ). Their visual system would have been lulled into inattention by the consistent placement of the buttons on the several pages that came before. Even after unintentionally going backward a few times by mistakenly clicking “Back” for “Next,” they might continue to perceive the buttons in their standard locations. This is why consistent placement of controls is a recommended user-interface guidline, to ensure that reality matches the user’s frame for the situation. Similarly, if we are trying to find something but it is in a different place or looks diferent from usual, we might miss it even though it is in plain view because our mental frames tune us to look for expected features in expected locations. For example, if the “Submit” button on one form in a website is shaped differently or is a different color from those on other forms on the site, users might not find it. This expectatioinduced blindness is discussed more later in this chapter in the “Perception Biased by Goals” section. Habituation A third way in which experience biases perception is called habituation . Repeated exposure to the same (or highly similar) perceptions dulls our perceptual system’s sensitivity to them. Habituation is a very low-level phenomenon of our nervous sytem: it occurs at a neuronal level. Even primitive animals like flatworms and ameba, with very simple nervous systems, habituate to repeated stimuli (e.g., mild electric shocks or light flashes)",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Even primitive animals like flatworms and ameba, with very simple nervous systems, habituate to repeated stimuli (e.g., mild electric shocks or light flashes). People, with our complex nervous systems, habituate to a range of events, from low-level ones like a continually beeping tone, to medium-level ones like a blinking ad on a website, to high-level ones like a person who tells the same jokes at every party or a politician giving a long, repetitious speech. We experience habituation in computer usage when the same error messages or “Are you sure?” confirmation messages appear again and again. People initially notice them and perhaps respond, but eventually they click them closed reflexively without bothering to read them. FIGURE 1.3 Users may always perceive the Next button on the right, even when it isn’t. Habituation is also a factor in a recent phenomenon variously labeled “social media burnout” (Nichols, 2013), “social media fatigue,” or “Facebook vacations” (Rainie et al., 2013); newcomers to social media sites and tweeting are initially excited by the novelty of microblogging about their experiences, but sooner or later get tired of wasting time reading tweets about every little thing that their “friends” do or see—for example, “Man! Was that ever a great salmon salad I had for lunch today.” Attentional blink Another low-level biasing of perception by past experience occurs just after we spot or hear something important. For a very brief period following the recognition— between 0.15 and 0.45 second—we are nearly deaf and blind to other visual stimuli, even though our ears and eyes stay functional. Researchers call this the attentional blink (Raymond et al., 1992; Stafford and Webb, 2005). 3 It is thought to be caused by the brain’s perceptual and attention mechanisms being briefly fully occupied with processing the first recognition. A classic example: You are in a subway car as it enters a station, planning to meet two friends at that station",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A classic example: You are in a subway car as it enters a station, planning to meet two friends at that station. As the train arrives, your car passes one of your friends, and you spot him briefly through your window. In the next split second, your window passes your other friend, but you fail to notice her because her image hit your retina during the attentional blink that resulted from your recognition of your first friend. When people use computer-based systems and online services, attentional blink can cause them to miss information or events if things appear in rapid succession. A popular modern technique for making documentary videos is to present a series of still photographs in rapid succession. 4 This technique is highly prone to attetional blink effects—if an image really captures your attention (e.g., it has a strong meaning for you), you will probably miss one or more of the immediately folloing images. In contrast, a captivating image in an auto-running slideshow (e.g., on a website or an information kiosk) is unlikely to cause attentional blink (i.e., missing the next image) because each image typically remains displayed for several seconds. PERCEPTION BIASED BY CURRENT CONTEXT When we try to understand how our visual perception works, it is tempting to think of it as a bottom-up process, combining basic features such as edges, lines, angles, curves, and patterns into figures and ultimately meaningful objects. To take reading as an example, you might assume that our visual system first recognizes shapes as letters and then combines letters into words, words into sentences, and so on. 3 Chapter 14 discusses the attentional blink interval along with other perceptual intervals. 4 For an example, search YouTube for “history of the world in 2 minutes.” But visual perception—reading in particular—is not strictly a bottom-up process. It includes top-down influences too. For example, the word in which a character appears may affect how we identify the character (see Fig. 1.4 )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It includes top-down influences too. For example, the word in which a character appears may affect how we identify the character (see Fig. 1.4 ). Similarly, our overall comprehension of a sentence or a paragraph can even influence what words we see in it. For example, the same letter sequence can be read as different words depending on the meaning of the surrounding paragraph (see Fig. 1.5 ). Contextual biasing of vision need not involve reading. The Müller-Lyer illusion is a famous example (see Fig. 1.6 ): the two horizontal lines are the same length, but the outward-pointing “fins” cause our visual system to see the top line as longer than the line with inward-pointing “fins.” This and other optical illusions (see Fig. 1.7 ) trick us because our visual system does not use accurate, optimal methods to perceive the world. It developed through evolution, a semirandom process that layers jurrigged—often incomplete and inaccurate—solutions on top of each other. It works fine most of the time but includes many approximations, kludges, hacks, and outright “bugs” that cause it to fail in certain cases. FIGURE 1.4 The same character is perceived as H or A depending on the surrounding letters. Fold napkins. Polish silverware . Wash dishes. French napkins. Polish silverware . German dishes. FIGURE 1.5 The same phrase is perceived differently depending on the list it appears in. FIGURE 1.6 The examples in Figs. 1.6 and 1.7 show vision being biased by visual context. Hoever, biasing of perception by the current context works between different senses too. Perceptions in any of our five senses may affect simultaneous perceptions in any of our other senses. What we feel with our tactile sense can be biased by what we hear, see, or smell. What we see can be biased by what we hear, and what we hear can be biased by what we see. The following two examples of visual perception affect what we hear: l McGurk effect",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". What we see can be biased by what we hear, and what we hear can be biased by what we see. The following two examples of visual perception affect what we hear: l McGurk effect. If you watch a video of someone saying “bah, bah, bah,” then “dah, dah, dah,” then “vah, vah, vah,” but the audio is “bah, bah, bah” throughout, you will hear the syllable indicated by the speaker’s lip movement rather than the syllable actually in the audio track. 5 Only by closing or averting your eyes do you hear the syllable as it really is. I will bet you did not know you could read lips, and in fact do so many times a day. (C) (B) (A) FIGURE 1.7 (A) The checkerboard does not bulge in the middle; (B) the triangle sides are not bent; and (C) the horizontal blue bars are horizontal, straight, and parallel. (Copyright © Victoria Skye, victoriaskye.com . Used by permission.) l Ventriloquism. Ventriloquists don’t throw their voice; they just learn to talk without moving their mouths much. Viewers’ brains perceive the talking as coming from the nearest moving mouth: that of the ventriloquist’s puppet (Eagleman, 2012). An example of the opposite—hearing biasing vision—is the illusory flash effect. When a spot is flashed once briefly on a display but is accompanied by two quick beeps, it appears to flash twice. Similarly, the perceived rate of a blinking light can be adjusted by the frequency of a repeating click (Eagleman, 2012). Later chapters explain how visual perception, reading, and recognition function in the human brain. For now, I will simply say that the pattern of neural activity that coresponds to recognizing a letter, a word, a face, or any object includes input from neral activity stimulated by the context. This context includes other nearby perceived objects and events and even reactivated memories of previously perceived objects and events. Context biases perception not only in people but also in lower animals. A friend of mine often brought her dog with her in her car when running errands",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Context biases perception not only in people but also in lower animals. A friend of mine often brought her dog with her in her car when running errands. One day, as she drove into her driveway, a cat was in the front yard. The dog saw it and began barking. My friend opened the car door and the dog jumped out and ran after the cat, which turned and jumped through a bush to escape. The dog dove into the bush but missed the cat. The dog remained agitated for some time afterward. Thereafter, for as long as my friend lived in that house, whenever she arrived at home with her dog in the car, he would get excited, bark, jump out of the car as soon as the door was opened, dash across the yard, and leap into the bush. There was no cat, but that didn’t matter. Returning home in the car was enough to make the dog see one—perhaps even smell one. However, walking home on foot, as the dog did after being taken for his daily walk, did not evoke the “cat mirage.” PERCEPTION BIASED BY GOALS In addition to being biased by our past experience and the present context, our perception is influenced by our goals and plans for the future. Specifically, our goals: l Guide our perceptual apparatus, so we sample what we need from the world around us; l Filter our perceptions—things unrelated to our goals tend to be filtered out preconsciously, never registering in our conscious minds. For example, when people navigate through software or a website, seeking infomation or a specific function, they don’t read carefully. They scan screens quickly and superficially for items that seem related to their goal. They don’t simply ignore items unrelated to their goals; they often don’t even notice them. To see this, glance at Fig. 1.8 and look for scissors, and then immediately flip back to this page. Try it now. Did you spot the scissors? Now, without looking back at the toolbox, can you say whether there is a screwdriver in the toolbox too? Our goals filter our perceptions in other perceptual senses as well as in vision",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A familiar example is the “cocktail party” effect. If you are conversing with someone at a crowded party, you can focus your attention to hear mainly what he or she is saying even though many other people are talking near you. The more interested you are in the conversation, the more strongly your brain filters out surrounding chatter. If you are bored by what your conversational partner is saying, you will probably hear much more of the conversations around you. The effect was first documented in studies of air traffic controllers, who were able to carry on conversations with the pilots of their assigned aircraft even though many diffeent conversations were occurring simultaneously on the same radio frequency, coming out of the same speaker in the control room (Arons, 1992). Research suggests that our ability to focus on one conversation among several simultaneous ones depends not only on our interest level in the conversation, but also on objective factors, such as the similarity of voices in the cacophony, the amount of general “noise” (e.g., clattering dishes or loud music), and the predictability of what your conversational partner is saying (Arons, 1992). This filtering of perception by our goals is particularly true for adults, who tend to be more focused on goals than children are. Children are more stimulus-driven; their perception is less filtered by their goals. This characteristic makes them more distracible than adults, but it also makes them less biased as observers. A parlor game demonstrates this age difference in perceptual filtering. It is similar to the Fig. 1.8 exercise. Most households have a catch-all drawer for kitchen implments or tools. From your living room, send a visitor to the room where the catch-all drawer is with instructions to fetch you a specific tool, such as measuring spoons or a pipe wrench. When the person returns with the tool, ask whether another specific tool was in the drawer. Most adults will not know what else was in the drawer",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When the person returns with the tool, ask whether another specific tool was in the drawer. Most adults will not know what else was in the drawer. Chidren—if they can complete the task without being distracted by all the cool stuff in the drawer—will often be able to tell you more about what else was there. Perceptual filtering can also be seen in how people navigate websites. Suppose I put you on the home page of New Zealand’s University of Canterbury (see Fig. 1.9 ) FIGURE 1.8 Toolbox: Are there scissors here? and asked you to find information about financial support for postgraduate students in the computer science department. You would quickly scan the page for words that were in the goal I gave you: “departments,” “scholarships,” “computer science,” or “postgraduate.” If you spotted a link containing one or more of those words, you would probably click on it. If you are a “search” person, you might instead go to the search symbol (magnifying glass, top right), click it and enter words related to the goal, and click “Go.” Whether you browse or search, it is likely that you would leave the home page without noticing that you were randomly chosen to win $100 (bottom right). Why? Because that was not related to your goal . What is the mechanism by which our current goals bias our perception? There are two: l Influencing where we look. Perception is active, not passive. Think of your peceptual senses not as simply filtering what comes to you but rather as reaching out into the world and pulling in what you need to perceive. Your hands, your primary touch sensors, literally do this, but the rest of your senses do it too. You constantly move your eyes, ears, hands, feet, body, and attention to sample exactly the things in your environment that are most relevant to what you are doing or about to do (Ware, 2008). If you are looking on a website for a campus map, your FIGURE 1.9 University of Canterbury website: navigating sites requires perceptual filtering",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If you are looking on a website for a campus map, your FIGURE 1.9 University of Canterbury website: navigating sites requires perceptual filtering. eyes and pointer-controlling hand are attracted to anything that might lead you to that goal. You more or less ignore anything unrelated to your goal. l Sensitizing our perceptual system to certain features. When you are looing for something, your brain can prime your perception to be especially senstive to features of what you are looking for (Ware, 2008). For example, when you are looking for a red car in a large parking lot, red cars will seem to pop out as you scan the lot, and cars of other colors will barely register in your consciousness, even though you do in some sense see them. Similarly, when you are trying to find your spouse in a dark, crowded room, your brain “programs” your auditory system to be especially sensitive to the combination of frequencies that make up his or her voice. TAKING BIASED PERCEPTION INTO ACCOUNT WHEN DESIGNING All these sources of perceptual bias of course have implications for user-interface design. Here are three. Avoid ambiguity Avoid ambiguous information displays, and test your design to verify that all users interpret the display in the same way. Where ambiguity is unavoidable, either rely on standards or conventions to resolve it, or prime users to resolve the ambiguity in the intended way. For example, displays on digital devices often add drop-shadows to user-inteface components to make them look raised in relation to the background surface (see Fig. 1.10 ). This appearance relies on a convention, familiar to most people who use digital devices, that the light source is at the top of the screen. If a tecnology user does not know this convention, it may be ambiguous to them whether the object is raised or sunken. Be consistent Place information and controls in consistent locations",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If a tecnology user does not know this convention, it may be ambiguous to them whether the object is raised or sunken. Be consistent Place information and controls in consistent locations. Controls and data displays that serve the same function on different pages should be placed in the same position on each page on which they appear. They should also have the same color, text fonts, shading, and so on. This consistency allows users to spot and recognize them quickly. Understand the goals Users come to a system with goals they want to achieve. Designers should understand those goals. Realize that users’ goals may vary and that their goals strongly influence what they perceive. Ensure that at every point in an interaction, the information users need is available, is prominent, and maps clearly to a possible user goal so users will notice and use the information. IMPORTANT TAKEAWAYS l Human perception is not an accurate reflection of what is “out there” in the world. It is biased by our experience, the current context, and our goals. l Past experience can bias our perception by “priming” our perceptual systems to detect certain objects and events as well as “priming” them not to detect other objects and events. Repeated perception of an event over a short interval can cause habituation , increasing the chances that we will miss later occurrences of the event. With long-term experience, we develop frames for familiar situations that make us perceive things that aren’t there or miss things that are. l Because our attention has limited capacity, when it is overloaded we can miss other objects and events. This is called attentional blink. l Perception operates in two ways simultaneously: l Our perception of whole objects and events is based on our perception of the parts. l Our perception of the parts is based on our perception of the whole",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Our perception of the parts is based on our perception of the whole. FIGURE 1.10 Components on digital device screens are given drop-shadows so they are perceived to float above the background, but that perception depends on a convention that the light source is at the top. l Our perception of objects and events can be biased by our emotional state. l Our perceptual system—particularly our visual system—includes evolutionary hacks and bugs that cause us to misperceive certain stimuli. l Our goals and plans strongly influence what we pay attention to and therefore what we perceive. l By following design guidelines based on how human perception works, desigers can create applications, websites, and appliances that are a good fit with the people who will use them.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-1-14.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "The Five Mistakes You’ll Make as a Usability Test Moderator What are the most common mistakes that test moderators make? We’ve observed usability tests moderated by consultants, in-house researchers, junior UX researchers and experienced practitioners and there are some common mistakes we come across time and again. These mistakes are like a rite of passage on the route to becoming a UX researcher, but even experienced practitioners aren’t immune from making them. Moderating a usability test is full of bear traps. The moderator may fail to set expectations (by reviewing the purpose of the test and describing the moderator’s role), forget to reassure the participant (“We’re not tesing you”), or fail to check for understanding (by asking the participant to repeat the task in his or her own words). other common mistakes include asking leading or biased questions, and quizzing participants on how they would design the interface. But there are five mistakes that we see usability test moderators make frquently and that eclipse all of these. They are: • Talking too much. • Explaining the design. • Answering questions. • Interviewing rather than testing. • Soliciting opinions and preferences. Talking Too Much When moderating a usability test, you need to fight against the tendency to talk too much. This can happen in two places: at the beginning of the test; and during the session itself. It’s true that you need to provide an introduction to the session to put the participant at ease; and you also need to explain the kind of feedback that you want from the thinking aloud technique. But you shouldn’t go oveboard in your introduction: Five minutes or so is usually enough. Usability testing is about observing participants while they carry out reaistic tasks. This means the golden rule is to shut up. Although moderators tell us they know this, we still see many of them (even some experienced ones) failing to practice it. In the white heat of the test session, they can’t stop themselves from filling the silence",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In the white heat of the test session, they can’t stop themselves from filling the silence. This happens partly because people are not comfortable with silence and partly because there’s a misconception that if the participant isn’t speaing, then you’re not learning anything. But because you’re interested in participant behavior, it’s fine to have periods of silence. of course you want participants to think aloud—but at the same time, you need to allow paticipants space to read, make judgments and generally think about what they are doing. You can avoid this trap by learning to embrace the silence. Ask particpants to do the task. Then shut up, observe, and listen to what they say. If you feel the urge to speak, use a phrase like, “Tell me more about that.” If you force yourself to use the same stock phrase, and none other, it will help you stay silent (you’ll sound stupid if you use it incessantly to fill the silence)—and you won’t do too much damage because you’ll encourage the participant to talk. Explaining the Design If you ever find yourself saying to a test participant, “What the develoers are trying to do here is,” or “The reason they designed it this way is because,” or “What you don’t understand is,” then you should slap youself. When you explain the design of your product to a test participant, it causes two problems. First, you’re no longer able to find out how someone will really behave when they first encounter the design. This is because you’ve given the particpant some background information that real users probably won’t have. And second, even if you were never involved in the design of the prouct, you affiliate yourself with it. Because what the participant hears isn’t an explanation of the product but a defence of the product. This prevents you being seen as a neutral observer and makes it more likely that participants will self-censor their comments. The point where this problem occurs most frequently is during the test tasks themselves",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The point where this problem occurs most frequently is during the test tasks themselves. The participant may use the product the “wrong” way and the moerator feels the need to explain how to use it “properly.” or the participant may be critical of something in the interface, and the moderator feels the urge to defend the design with a phrase like, “The development team thought about doing it that way, but” or the participant may completely misunderstand something in the interface, at which point the moderator will want to correct the participant’s miunderstanding. In particularly bad situations, this moderating style risks turning the usability test into a coaching session, or even an argument. Believe us when we say that no usability test moderator ever won an argment with a participant. If you ever feel the urge to explain the interface or use a phrase like, “Yes, but,” then instead say, “Tell me what you’re doing right now.” You’ll then get behind the behavior without influencing it too much. If you really, really want to explain how to use the product or correct any misconceptions, then wait until the end of the session, once participants have tried it without your help. Answering Questions Here’s another trap we see moderators walk into. It’s like watching a slomotion replay of a dog chasing a stick over a cliff. The participant sets the trap and the moderator runs towards it. Like most traps, it seems fairly innocuous. The participant simply asks a question. now, participant questions are like gold dust. You want participants to ask questions because this indicates they are experiencing a problem with the product: They’re not sure how to proceed, so they ask you. Gold dust, but not gold. You find the gold by observing how the participant answers their quetion: What do they do to solve the problem? Do they find it easy to fix or do they consistently take the wrong path? It’s their behavior that helps you distinguish a low priority problem from a critical one",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This means the route to the gold is to refuse to answer the question. But to any normal human being, refusing to answer a question is alien. From childhood, we’re conditioned to think that ignoring a question makes us appear either rude or stupid. That’s why so many test moderators walk blindly into the trap of answering participants’ questions. Here’s the way to fix this in your own practice. First, in your preamble, tell paticipants you want them to ask questions but you won’t answer, because you want the session to be realistic. Use a phrase like, “Just do what you would do if I wasn’t here.” This then gives you permission not to answer any questions you’re asked. Then, when the inevitable question comes at you during the session, use the “boomerang” technique: Answer the question with a question. So, if the participant asks, “How do I get back to the beginning?,” you respond: “How do you think you get back to the beginning?” If the participant asks, “Whereabouts is the registration form?,” you reply: “Where would you look for it?” Interviewing Rather than Testing If you’ve invested time in getting participants to attend your session, it makes sense to get as much out of them as possible. So you should certainly run a pre-test interview with participants before they start the test tasks to find out more about them and their relevant goals. But while the participant carries out the test tasks—which should represent the bulk of their time in a usability test—you’re an observer. Here’s a common situation that causes a usability test to degrade into an interview: When the development team don’t know much about users. The team may not have done any field research in the past and want to milk this session for all its worth. This shows itself when the participant is interrupted mid-task and asked questions about the way they do this task at home. or when the marketing lead asks you to shoe-horn in a shopping list of questions during a task",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". or when the marketing lead asks you to shoe-horn in a shopping list of questions during a task. As a consequence, the research falls between two stools: It’s neither an interview nor a usability test. Another situation where this can happen is when you have a particularly loquacious participant who wants to engage the moderator in conversation, rather than do the tasks. The participant will continue to look over to the moderator for reassurance and try to make eye contact. The best approach is to prevent this problem from happening in the first place. Adjust your body language to be more of an observer than an inteviewer. Position yourself so you are behind and to one side of the participant. If you sense the participant looking toward you, pretend to take notes and decline the offer of eye contact. Also make it clear to the development team that you’ll run a post-test interview to get an overall assessment and encourage comments regarding topics not raised during the session, and that’s where you’ll cover their shoping list of questions. Soliciting Opinions and Preferences This final mistake is one we see often in people who are new to moderating a usability test. This is because they have confused usability testing with market research. They think their role is to solicit opinions rather than to observe behavior. The way this manifests itself in a test is the moderator will ask the partiipant to compare different designs to see which one they prefer, or they will continually ask the participant if they like or dislike some design feature. Usability testing isn’t about finding out what users like, but rather what works best for them. How to Continuously Improve as a Test Moderator These mistakes almost always occur in novice test moderators as they earn their spurs. But even experienced test moderators make these kinds of mitake during a usability test. The best way to avoid mistakes is to continuously reflect on your own moderating skills",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". But even experienced test moderators make these kinds of mitake during a usability test. The best way to avoid mistakes is to continuously reflect on your own moderating skills. After each usability test, look back over the recordings, especially sessions that you feel went particularly well or badly. Make it part of your personal development to identify three things you can build on or that you could have done better. THINK LIKE A UX RESEARCHER • It’s difficult to assess one’s own competence as a usability test moderator. We mention listening back to a participant recoring, but it can be hard to be objective. one way around this is to use the five mistakes as a checklist as you reflect on your perfomance. Another approach, but more challenging to listen to, is to ask for critical feedback from usability test observers after each participant session. • Some members of development teams misunderstand the purpose of a usability test and expect you to solicit opinions and preferences. They may want you to ask if the participant likes the design or prefers one version over another. How would you manage that expectation? ( Continued ) • We’ve run tests in the past where it’s clearly the test particpant’s style to continually ask questions. It’s how they think aloud. “Where’s the link for the basket? oh, there it is. But how do I find out the shipping costs? Maybe I need to click the basket icon. Where’s the basket icon?” You get the idea. These can be one of the more challenging participants to moderate because the “boomerang” technique can become wearing after a while (both for you and the participant). When is it acceptable to ignore a participant’s question and treat the question like a statement? How could you practice dealing with a participant like this? • We discourage you from explaining the design to the participant but of course you’ll need to step in if the participant gets totally lost or confused",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". At what point should you step in to bring a participant back on track? How “lost” do participants need to get for you to know this is really a problem and not one that they can solve themselves? • Imagine you have a test participant who seems more interested in turning the session into an interview than in doing the test tasks. He turns away from the screen, faces you and tells you anecdotes tenuously related to what he’s meant to be doing. How would you bring the participant back on track? THINK LIKE A UX RESEARCHER (Continued)",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-110-115.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 259 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00015-5 Copyright © 2021 Elsevier Inc. All rights reserved. People make mistakes and commit errors; it’s a fact of life. Nobody is perfect. Desigers of digital technology have to live with that fact. Actually, good designers do more than live with it; their designs take it into account. They avoid designs that make it likely for users to make errors (Norman, 2014). They create digital products and sevices that help people avoid and recover from errors. MISTAKES VERSUS SLIPS When categorizing the types of errors people make, the first distinction is between mistakes and slips (Norman, 1983a; Reeves, 2010). Mistakes are errors of conscious decision. A choice presents itself and a person cosiders the alternatives, weighs the pros and cons of the alternatives and chooses the wrong one. Don Norman calls mistakes “errors of intention”: the person intentioally decides to do something that turns out to be incorrect or to have an undesirable result. Examples of mistakes: l You shop online for a flash memory card for your camera, but the one you order turns out to be incompatible with your camera. l You take the freeway route to a concert, expecting traffic to be OK, but encouter a huge traffic jam. l You vote for someone because you believe the policies they propose will help you, but after they are elected, you learn that those policies actually harm you. People make mistakes either because they have an incorrect understanding of the choices or because they have inaccurate or incomplete information. Don Noman calls this “having a faulty mental model” (Norman, 1983a). Since mistakes are the result of intentional choices and actions, we detect them only afterward, when We Make Errors 15 their ramifications become apparent. In engineering terms, we say mistakes can be detected only with feedback. Slips, the other main type of error, are unintended. A person does something they did not mean to do",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In engineering terms, we say mistakes can be detected only with feedback. Slips, the other main type of error, are unintended. A person does something they did not mean to do. Think of a person slipping and falling on a wet floor and you will understand where the term “slip” comes from. Examples of slips: l You turn your oven ON to let it heat up before baking something but forget to check first to see whether there is anything already in the oven. l You try to say “she sells seashells by the seashore” quickly, but your mouth won’t say the right words. l You send a text to the wrong person. The same action can be either a mistake or a slip depending on whether it was done on purpose. For example, if you receive a company email that was sent to a group of people, you may decide that everyone on the list should get your reply, so you click “Reply All.” Later you learn that replying to everyone was a mistake—e.g., because your reply contained company proprietary information, and some recipients of the first email don’t work at your company. In a different scenario, you receive a group email and intend to click “Reply,” but don’t look carefully and accidently click “Reply All,” which is right next to “Reply.” That’s a slip. SLIP OR MISTAKE? On January 13, 2018, at 8:07 a.m., an emergency alert went out over television, radio, and cellphones in the US state of Hawaii. According to the alert, a ballistic missile was headed for Hawaii and expected to arrive within minutes. The alert urged residents to “seek immediate shelter” and ended with “This is not a drill” (see Fig. 15.1 ). FIGURE 15.1 False alert sent to mobile phones in Hawaii on January 13, 2018, due to an error. On television, the alert, displayed in a scrolling band across the bottom of the screen, was more detailed: The US Pacific Command has detected a missile threat to Hawaii. A missile may impact on land or sea within minutes. THIS IS NOT A DRILL. If you are indoors, stay indoors. If you are outdoors, seek immediate shelter in a building",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A missile may impact on land or sea within minutes. THIS IS NOT A DRILL. If you are indoors, stay indoors. If you are outdoors, seek immediate shelter in a building. Remain indoors well away from windows. If you are driving, pull safely to the side of the road and seek shelter in a building or lay on the floor. We will announce when the threat has ended. THIS IS NOT A DRILL. Take immediate action measures. Hawaii’s outdoor civil defense sirens remained silent, but the alert nonetheless alarmed many Hawaii residents because North Korea was known to be developing nuclear-armed ballistic missiles, the leader of North Korea had recently boasted that those missiles would soon be able to reach the US, and threats had recently been exchanged between the US president and North Korea’s leader. Some people in Hawaii panicked and tried to find shelter; others tried to confirm the truth of the alert (Wikipedia, 2019). In fact, there was no incoming missile. About a half-hour later, a second alert was issued, describing the first alert as a “false alarm.” Was this incident a mistake or a slip? That is unclear. What is clear is that state emergency officials planned to conduct a drill, so from their perspective, what happened was a slip —an unintended action. However, an investigation found that the employee whose job it was to “push the button” did not realize it was a drill and so intentionally clicked the on-screen button to issue a real missile alert, then intentioally clicked “Yes” on the confirmation screen. The employee was susequently fired. From his perspective, sending the alert was a mistake . It is also noteworthy that the US Federal Communications Commision criticized Hawaii’s alert software for (1) not distinguishing between drills and actual alerts—both had the same user interface and confirmtion sequence and language—and (2) being designed so that a single employee could both issue and confirm the posting of a real alert",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". So even if the January 2018 incident was a mistake, the design of the sofware was such that it could easily lead to slips. The alert software and procedures for issuing alerts have since been revised (Park, 2018). TYPES OF SLIPS Slips are categorized based on what causes them (Norman, 1983a). Capture slip : the sequence being executed is similar to another more frequent or bettelearned sequence; the person absent-mindedly switches to the other sequence. Examples: l The first day going to a new job, you find yourself halfway to your old workplace. l You click the CLOSE button on a document you’ve been editing for an hour. A confirmation dialogue box pops up asking if you want to close without saving. Without thinking, you click “yes.” Description slip : right action on the wrong object. Examples: l You do a two-finger swipe to scroll the screen on your laptop, but you’re in Google Maps, so it zooms in (see Fig. 15.2 ). l You absent-mindedly say “Hey Siri” to your Amazon Echo (“Alexa”) device. l You intend to open the calendar app on your phone but unintentionally open the calculator app because its icon looks similar. Data-driven slip : external data interferes with attention. Examples: l You are trying to read an online news article, but flashing ads next to the article keep distracting you. FIGURE 15.2 Panning and zooming in Google Maps’ map-area uses different gestures than everywhere else, causing users often to zoom when they meant to pan and vice versa. l You try to quickly name the colors of the words in Fig. 15.3 but find yourself reciting the words instead. 1 Loss-of-activation slip : goal lost from short-term memory. Examples: l You go into your kitchen to get something, but when you get there, you can’t remember what you came for. l You open Facebook to post vacation photos but spot an interesting post from a friend and spend several minutes reading and responding to the comments, then close Facebook without remembering to post your vacation photos",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Closure slip : final steps of task prematurely dropped from working memory when goal is achieved. Examples: l You use a public computer to check your bank account and walk away without logging out, unintentionally allowing the next user to access your account. l You copy your resumé using an office copier, take the copy, and leave the orignal on the copier. l You finish filling out a long online form and close the browser without submiting the form. 1 This is called the Stroop effect, after the psychologist who published an article describing it (Stroop, 1935). FIGURE 15.3 Stroop effect. Quickly recite the words in the left column. Quickly state the colors in the center column. Quickly state the colors (don’t recite the words) in the right column. FIGURE 15.4 The macOS Image Capture app’s scan controls retain the filename used for the previous scan; users often forget to change the filename, resulting in frequent errors. Mode slip : right action but wrong system status (Norman, 2014). Examples: l You press the accelerator on your car to go forward, not noticing that the transmission is set to reverse. l You set a copier to magnify a document, then forget to reset it to normal for the next document, so it magnifies a document you didn’t want magnified. l You scan a document using the macOS Image Capture app but discover afteward that it saved the scanned image with the filename used for the previous scan (see Fig. 15.4 ). Attention slip : missing an important feature of the information presented. Examples: l You go online to reserve a hotel room for yourself and your spouse but don’t notice until after you’ve completed the booking that the quoted price is per person, not per room. l Making online flight reservations for a trip, you select May 29 for your oubound flight, but when you choose your return date, you don’t notice that the date-chooser is still set to May, so you accidently set your return date to May 25, and the website scolds you for setting a return date before your departure date (see Fig. 15.5 )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 15.5 ). Motor slip : fingers, mouth, legs, etc. don’t do what was intended. Examples: l You intend to type “the”, but your fingers actually type “hte”. l You drag a document on your computer’s screen to the trash but accidently “drop” it, and it disappears into a nearby folder. l You accidently click a link next to the link you meant to click. FIGURE 15.5 The South African Airlines website allows users to set their return date earlier than their departure date, then scolds them for making the error. Twice! DESIGN TO PREVENT MISTAKES: PROVIDE CLEAR, CORRECT INFORMATION The main way to help people avoid making mistakes—errors of intention—is to prvide accurate information in a form they can easily understand. The decision suport systems mentioned in Chapter 12 are examples of software applications that give people accurate information and help them avoid mistakes, whether buying a house, planning a vacation, or planning the most efficient route for a snowplow through a city. Flight booking websites are another sort of decision support system: you enter your requirements—origin, destination, dates, times, and desired class of service—and they show you flights matching those requirements, with information about how the flights differ to help you choose the one that best meets your needs (see Fig. 15.6 ). How do you ensure that information is presented in a form that people can eaily understand? Follow the design guidelines presented in previous chapters—e.g., provide visual structure and hierarchy, use color carefully and sparingly, support reaing (both legibility and comprehension), respect the limitations of human attention and memory, support learning and the development of habits, respect human time requirements, etc. Data visualization methods, described in Chapter 12 , are especially useful for ensuing that people understand the information presented",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Data visualization methods, described in Chapter 12 , are especially useful for ensuing that people understand the information presented. Such methods harness the automatic processes of the human perceptual system (part of system one) to provide information to support rational decision-making (system two). DESIGN TO HELP PEOPLE AVOID SLIPS Poor user-interface design—that is, design mistakes —can cause users to make slips . An example is text autocorrect . It is intended to help people avoid typing errors, but if designed poorly it can increase the chances that users will send texts or emails containing unintended words. Similarly, many of the examples of slips listed earlier in this chapter are made more likely by poor design decisions. Fortunately, there are design guidelines for preventing or reducing the likelihood of each type of slip mentioned in the previous section. The guidelines presented below are adapted from work by Norman (1983a). When a designer of a digital product or service does not follow these guidelines, they are making a mistake that increases the chance that users will make the corresponding type of slip . Capture slips : sequence being performed is similar to another more frequent or bettelearned sequence; person absent-mindedly switches to the latter sequence. Guidelines for avoiding: l Make steps for different tasks noticeably different so no one is confused about which action sequence they are executing (see earlier sidebar on Hawaii alert emergency). FIGURE 15.6 Kayak.com: flight reservation sites help people avoid mistakes when booking a flight. l Avoid overlapping paths: design so different operations have no steps in common. l Prompt users to confirm actions. Description slips : right action on wrong object. Guidelines for avoiding: l Consistency: same operations apply to all objects. l Clearly distinguish objects or areas having different actions or gestures. Data-driven slips : external data interferes with attention",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Clearly distinguish objects or areas having different actions or gestures. Data-driven slips : external data interferes with attention. Guidelines for avoiding: l Guide users toward their (and your) goal. Provide a “process funnel” (van Duyne et al., 2002): once you know the user’s goal, keep them on a straight and narrow path to that goal; don’t distract them from it. Loss-of-activation slips : goal lost from short-term memory. Guidelines for avoiding: l Provide memory aids and a clear display of system progress and status. Closure slips : final steps of task prematurely dropped from working memory when goal is achieved. Guidelines for avoiding: l Warn users of incompleteness—e.g., engine left running or document left on copier. l Complete tasks automatically—e.g., modern cars turn headlights OFF automatically when user exits vehicle; people don’t have to remember to do it. Mode slips : right action but wrong system status. Guidelines for avoiding (Johnson, 1990): l Indicate system status (mode) clearly and strongly. l Revert to “normal” mode after a timeout of more than 10 seconds, the average “unit task” duration (see Chapter 14 ), or when the user quits and later reenters the app or website. l Make modes “spring-loaded”: users have to physically hold the system in the exceptional mode; letting go reverts to normal mode—e.g., keyboard shift key. l Avoid moded designs—some controls or gestures have different effects depending on the system mode—by having separate controls or gestures for every function, but realize that doing that may increase the likelihood of description slips: using the wrong control or gesture for an intended action (Norman, 1983a). Attention slips : missing an important feature of the information presented. Guidelines for avoiding: l Direct the user’s attention to important details using visual hierarchy, percetual pop, movement, vibration, or sound. l Design interactions to prevent nonsensical commands or requests. Motor slips : fingers, mouth, legs, etc. don’t do what was intended",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Design interactions to prevent nonsensical commands or requests. Motor slips : fingers, mouth, legs, etc. don’t do what was intended. Guidelines for avoiding (Johnson and Finn, 2017): l Make click, tap, and swipe targets large. When buttons or links have graphics as well as labels, make both clickable to maximize the clickable area. l Put space between click and tap targets so users are able to click the intended target. l In user interfaces that make use of complex gestures like click-drag, pincspread, double-tap, tap-and-hold, etc., provide simpler alternative gestures for users who have trouble executing complex gestures reliably (see Fig. 15.7 ). l Avoid multilevel menus (sometimes called pull-right menus). DESIGN TO HELP PEOPLE RECOVER FROM ERRORS Regardless of how well designed a digital product or service is, it won’t be perfect. Even if it were perfect, people are not. They will make errors, both mistakes and slips. To be successful, digital products and services should help their users recover from errors. The following are design guidelines for doing that. They apply to any digital product or service regardless of whether users interact with it via keyboard and pointer, touch screen, voice, or anything else. Make actions reversible by making them two-way rather than one-way An example is apps or devices that provide a trash can or trash folder where deleted items are stored temporarily until either they are moved back out of the trash (revering the original delete operation) or the user explicitly empties the trash (see Fig. 15.8 ), which should require confirmation (see below). Similarly, the Apple Photos app and the Voila screen-capture app provide trash folders where deleted images and vieos go until retrieved or finally deleted",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Similarly, the Apple Photos app and the Voila screen-capture app provide trash folders where deleted images and vieos go until retrieved or finally deleted. Airlines and hotels often allow customers to cancel reservations with no penalty up to a specified number of days before the booked date, and some e-commerce sites (A) (B) FIGURE 15.7 The Google Maps mobile version (A) requires two-finger spread/pinch gestures for zoom, but the desktop version (B) allows zoom by two fingers or ± buttons. (A) (B) FIGURE 15.8 Moving deleted files temporarily to trash cans or folders allows users to recover files if necessary. (A) macOS trash folder. (B) Voila trash folder. allow customers to cancel purchases before the product ships. This allows people to reverse transactions that they did not intend (slips) or after further consideration decide they don’t really want (mistakes). This design guideline—making operations reversible whenever possible—also applies to digital products and services that have voice-operated user interfaces (VUIs). Obviously, if you tell Alexa to play a song, you should be able to tell Alexa to stop playing the song or play a different song. Perhaps less obviously but just as important, if you order movie tickets or a shirt online using a VUI, you should be able to cancel the order within some reasonable time limit. Make actions reversible with UNDO Beyond providing reversible operations, apps and websites can provide UNDO for many or most operations. This allows users to undo actions even in cases when they may not even be sure exactly what they did or how to reverse it. For example, while dragging a file from one folder to another folder in a list of folders, you may experence a motor slip and drop the file. If it disappears into another folder, you may not know where it went. For such situations, macOS provides UNDO Move of a file (see Fig. 15.9A ). Without UNDO, you would have to open nearby folders one by one, scan their contents, find the file, and drag it back out",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For such situations, macOS provides UNDO Move of a file (see Fig. 15.9A ). Without UNDO, you would have to open nearby folders one by one, scan their contents, find the file, and drag it back out. Similarly, most text editors—e.g., Microsoft Word—provide UNDO for reversing recent editing actions (see Fig. 15.9B ). That allows you to edit the text, see if you like the result, and if not, easily revert to what you had before without having to reverse all the separate edits. Finally, some email apps—e.g., Gmail—provide UNDO for restoring just-deleted emails (see Fig. 15.9C ). Voice-operated systems can keep track of recent user commands and allow users to simply say their wake-up word followed by “undo” to undo the last command, proably after confirming that that is what they want to do. Alternatively, they can respond with “What should I undo?” followed by a short list of recent commands from which the user chooses one. Make risky, error-prone operations hard to do The likelihood of errors during potentially dangerous operations can be reduced by requiring confirmation (see Fig. 15.10 ) or even requiring multiple steps. Before excuting a user’s request for a transaction, most e-commerce websites show a summary of the transaction and allow the user to cancel it or confirm it. However, a simple one-keystroke confirmation may not be enough. Most of the time the action will be OK and users will confirm it. The confirmation will soon be “burned” into system one as an automatic process, increasing the probability that (A) (B) (C) FIGURE 15.9 (A) MacOS users can undo file operations. (B) Microsoft Word lets users undo edits. (C) Gmail allows users to undo message deletions. users will confirm without thinking (Norman, 1983a). That is a capture slip, and it defeats the purpose of the confirmation. A common way to avoid that problem is to design the confirmation dialogue box so Cancel is the default choice (the one invoked by clicking Enter) and OK (or Continue) requires an extra keystroke",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A common way to avoid that problem is to design the confirmation dialogue box so Cancel is the default choice (the one invoked by clicking Enter) and OK (or Continue) requires an extra keystroke. On Samsung pocket digital cameras (see Fig. 15.11 ), to delete all photos, users have to (1) click the Menu button, (2) select the Delete item, (3) move the selection to Delete All and press OK , (4) click the pop-up confirmation FIGURE 15.10 Apple’s photos app requires confirmation to delete a photo permanently. FIGURE 15.11 Deleting all photos from a Samsung pocket camera requires confirmation, and the confirmation defaults to “No” (don’t delete), so two more keystrokes are required: change choice to “Yes” (delete), click OK. and press the UP key (DISP) to change the confirmation choice from “No” to “Yes,” and (5) press OK . The extra keystrokes make it unlikely that a user will accidently confirm the deletion of all photos. In mission-critical applications, where errors would be very costly, even multistep confirmation may not provide enough safety. One solution is to design important operations so one person initiates them and a different person approves or cofirms them. Large purchases in many organizations are designed that way, as are grade changes in universities. Another way to guard against errors in mission-critical applications is to require potentially dangerous operations to be executed by two or more people acting in concert—maybe even in separate locations. For example, in the sidebar “Slip or Mitake?” earlier in this chapter, the US Federal Communications Commission suggested that Hawaii’s Emergency Alert System should require that two separate staff members must coordinate to issue a real alert. VOICE-RECOGNITION FAILURE AND MISRECOGNITION ARE NOT USER ERRORS When a voice-controlled system does not understand or misunderstands what a user says, the user has neither slipped nor made a mistake. The user did not make an error",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The user did not make an error. The system made the error, and it is the system’s responsibility to correct or work around the error (Pearl, 2018a,b). What should happen is more or less what happens when people talk and one does not understand what the other says. If you and I are talking and I say something and you don’t understand me, you tell me you don’t understand or give me a quizzical look, and I repeat or rephrase what I said. If I say something and you mis understand me, the conversation may continue until the misunderstanding is exposed, then both of us work to correct the misunderstanding. Most voice-controlled systems nowadays rate the confidence that they understood a user utterance. If the system’s confidence that it understood a user command is below a certain threshold, the system should say something like “I’m sorry; I didn’t get that. Could you repeat it?” Even if the system estimates with high confidence that it understood a user, it can repeat what it understood before continuing. That allows misunderstandings to be caught and corrected quickly. IMPORTANT TAKEAWAYS l Errors can be classified as mistakes or slips. Mistakes are conscious errors—you make an intentional choice that turns out to be wrong. Slips are unconscious errors—you do something you didn’t intend to do. l Slips are categorized into several types depending on what causes them: l Capture: while doing one thing, you absent-mindedly switch to doing another. l Description: your action is appropriate for an object different from the one you are working with. l Data-driven: while doing a task, something else distracts you. l Loss of activation: you forget what you were doing or why. l Closure: you forget the last step of a task. l Mode: your actions have a different effect than intended because the system you are using is in a different mode than you thought it was. l Attention: failing to notice important information and so doing the wrong thing. l Motor: your fingers, mouth, legs, etc. don’t do what you intended",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Attention: failing to notice important information and so doing the wrong thing. l Motor: your fingers, mouth, legs, etc. don’t do what you intended. l Design to prevent mistakes by providing clear, complete, accurate information. l Design guidelines to prevent slips depend on the type of slip. For example: l Capture slips: make paths for different actions noticeably different. l Data-driven slips: guide users toward their goal with a “process funnel.” l Loss-of-activation slips: provide memory aids and progress indicators. l Mode slips: clearly indicate the current system mode. l Attention slips: direct the user’s attention to important details. l Motor slips: make click/tap/swipe targets large with space between them. l Design guidelines to help people recover from errors: l Make actions reversible by providing UNDO or by making all actions two-way. l Make risky, error-prone operations hard to do. l Don’t treat voice-recognition failures as user errors. They are system errors.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-259-274.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": "Theories of embodiment in HCI Paul Marshall * and Eva Hornecker ** * UCL Interaction Centre University College London Gower Street, London, WC1E 6BT, UK ** Department of Computer and Information Science University of Strathclyde 26 Richmond St., Glasgow, G1 1XH Author bios: Paul Marshall is a lecturer in interaction design at University College London. His research interests centre on the concept of embodied interaction and how it can be applied to the design and evaluation of technologies that extend and augment individual human capabilities. This has included work on physical interaction and tangible interfaces; on technologies for face-to-face collaboration; on the design of technologies to fit specific physical contexts; and on extended cognition and perception. Eva Hornecker is a lecturer in the Dept. of Computer and Information Science at the University of Strathclyde. Her research interests focus on the design and user experience of 'beyond the desktop' interaction. This includes multitouch surfaces, tangible interaction, whole-body interaction, mobile devices, physical and physically embedded computing, support of social/collaborative interactions, and the social/societal implications of technology. 1. Introduction The concept of embodiment increasing in prominence in thinking about the design of digital technologies, particularly in the decade since the publication of Paul Dourish’s Where the Action Is (Dourish, 2001). In this chapter, we focus on how it has been applied in the area of Human Computer Interaction (HCI). Embodiment typically refers to our being living, feeling, bodily entities situated in a physical world. This contrasts with a view of human cognition as grounded in abstract information processing. Theories of embodiment focus on how our bodies and active experiences shape how we perceive, feel and think. However, rather than being a single coherent theoretical perspective, there are a number of different traditions and emphases",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, rather than being a single coherent theoretical perspective, there are a number of different traditions and emphases. Here, we provide an overview of the core theoretical underpinnings of recent work on embodied interaction in HCI and show how this work has been developed in primarily two related directions through the description of work drawn from the literature. We introduce these two main branches of theory: phenomenology and embodied cognition. In the first main section, we provide a short historical overview of the cognitivist theories that were once ascendant in HCI, and follow the development of alternative views of thinking and acting, drawing from phenomenology through the work of Winograd and Flores, Lucy Suchman and Paul Dourish. In the next section we provide an overview of work in cognitive science described by the umbrella term embodied cognition, which has been influenced by discussions in phenomenology, but has followed a quite different trajectory. This comprises a number of theoretical perspectives, including that cognition is offloaded onto the environment (e.g., Scaife and Rogers, 1996); that the environment is part of the cognitive system (e.g., Hutchins, 1995); and that abstract thinking is grounded in bodily experience (e.g., Lakoff and Johnson, 1999). In the final section, we attempt to show by discussing (highly selective) examples from the literature, some of the diversity of work that has drawn on theories of embodiment in technology development, analysis and evaluation. In particular, we describe four perspectives. Firstly, we discuss Daniel Fällman’s use of Merleau-Ponty’s work in developing a perspective on mobility and bodily interaction that is shaped by a focus on engagement with the immediate context. Secondly, we provide an overview of Toni Robertson’s analysis of the public availability of socially situated action. Thirdly, we discuss Eva Hornecker’s framework on tangibility and social interaction",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Thirdly, we discuss Eva Hornecker’s framework on tangibility and social interaction. Finally we describe Hurtienne’s application of Lakoff and Johnson’s Image Schema theory to the design and evaluation of both new and traditional interfaces. 2. Background 2.1 Cognitivism Many theoretical models in early HCI adopted a perspective, drawn from the then dominant approach in the Cognitive Sciences (especially in artificial intelligence, philosophy of mind and cognitive psychology), that is now described as cognitivism (e.g., Fodor, 1975). The central claim of the cognitivist approach is that thinking is information-processing – the manipulation of physical symbols, representing facts and things in the world, according to syntactical rules in order to make inferences and to guide action. A central claim of this approach was that an information processing system is both necessary and sufficient for general intelligent action (Newell & Simon, 1976). Furthermore, it is the function of the representations and rules used to process them that are of prime importance rather than the details of their implementation. The power of this functionalist approach (cf. Putnam, 1975) is to treat cognition as a formal system that can be studied and modelled separately from the details of neuronal organisation or manifest behaviour; the implementation doesn’t matter as long as it can perform the same function of processing physical symbols. Cognition can therefore potentially occur in a human brain, a digital computer or another medium. Mental processes and physical interaction are treated as separate domains, a philosophical position known as dualism . The theorist most associated with dualism is René Descartes, and thus the cognitivist perspective is often described as Cartesian . The cognitivist programme of research has been successful in modelling cognitive processes such as planning and abstract problem solving, associated with higher-level and specifically human cognition",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In HCI, it is perhaps best represented in Card, Moran and Newell’s (1983) The Psychology of Human Computer Interaction. A well-known model described by Card et al. is GOMS (Goals, Operators, Methods and Selection rules). This model represents interaction with a computer as abstracting and processing information from the environment through the perceptual system, processing it in a separate cognitive system to select an appropriate action according to a prespecified goal and then sending a message to the appropriate body parts (usually the fingers) to carry out the interface action via a motor control system. Another well-known approach in this tradition is requirements specification through a top-down hierarchical task analysis. 2.2 Critiques of cognitivism The cognitivist model of thinking and interaction has come in for sustained criticism from a variety of positions. The philosopher John Searle (1980) has argued that a model based only on the manipulation of abstract symbols doesn’t have an account of how meaning is attached to the symbols in the first place. Therefore, it is questionable whether this is a good account of human reasoning. Dreyfus (1979) critiqued cognitivism from the perspective of Heideggerian philosophy, arguing that systems which represent knowledge about the world as just a collection of symbolic facts are never going to be able to respond flexibly to changing real world contexts. The system will never be able to work out which of the changes are pertinent to the ongoing task and which to ignore. Instead, Dreyfus argues that intelligent action is grounded in a complex history of skilful bodily experiences in the world – knowing how-to do things rather than just knowing-that: “To say a hammer has the function of being for hammering leaves out the defining relation of hammers to nails and other equipment, to the point of building things, and to our skillsand so attributing functions to brute facts couldn’t capture the meaningful organization of the everyday world” (2007, pg. 248)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 248). Dreyfus’s critique draws support from the perceived failure of classical AI to move beyond reasoning in very constrained artificial environments, with simple semantics that are specified in advance to flexible responses in complex changing environments (e.g., Brooks, 1991). 3. Introduction of embodied theories into HCI Although there are many precursors to the current focus on embodied interaction in the design and evaluation of human interface technology, two in particular have had a profound impact in Human-Computer Interaction, and the broader cognitive sciences, which they served to critique: Winograd and Flores’s Understanding Computers and Cognition and Suchman’s Plans and Situated Actions 3.1 Winograd and Flores’s Understanding Computers and Cognition Winograd and Flores (1986) presented an influential alternative to the view of cognition as a formal system, which has had significant influence in HCI, and introduced a new vocabulary to talk about thinking about and acting with technology. Drawing in particular from phenomenology (Heidegger 1927; cf. Dreyfus, 1991), they argued against the cognitivist position that separates thinking from the context in which it occurs. Instead, they propose an understanding of technology use that is inherently historical, material and social. On this view, completely detached reasoning is impossible, as it always depends on a tradition or prunderstanding that derives from a history of interactions with others who share the tradition. In this characterisation, all understanding derives from the state of being-in-the-world, which Heidegger terms Dasein . Winograd and Flores also adopted Heidegger’s concept of thrownness , which emphasises the experience of being fully engaged in skilfully coping within a particular context, where there is no way to predict exactly what the outcome of your actions will be and no stable objective representation of the situation",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". It should be clear that this kind of intrinsically online reasoning (Wheeler, 2005) is very different from the abstract thinking characteristic of cognitivism. Thrownness is closely linked to the idea of readiness-to-hand or transparency, which refers to the way that things ‘disappear’ in the course of everyday action. This doesn’t mean that they literally vanish of course, rather that they cease to be the focus of attention. The canonical example, taken from Heidegger, is of using a hammer. Here the focus is on the activity of hammering. The hammer ceases to be viewed as an object in its own right, in the same way that the tendons of the arm are used transparently during the activity, disappearing into the web of background understandings of relationships between bodies, hammers, nails, activities of making things and so on. Readiness-to-hand is seen to be the primary mode of being-in-the-world in Heidegger’s phenomenology. Again, this is in contrast to the rationalist, cognitivist approach outlined above, where knowledge is represented as a collection of objective facts, separate from the context in which they are used. Winograd and Flores adopt the perspective that nothing can be viewed separately from interpretation. Presence-at-hand refers to the case where a situation is attended to theoretically, and objects or properties are viewed as things in their own right. Heidegger argues that this has been the typical way of viewing the world in scientific analysis, disregarding readiness-to-hand which is the fundamental way of experiencing the world. Presence-at-hand occurs in the event of a breakdown: an “interrupted moment of our habitual, standard, comfortable ‘being-in-thworld’” (Winograd and Flores, 1986, pg. 77). Returning to the example of hammering, the hammer might become present-at-hand in the event of the handle becoming loose, appearing as an object of attention in its own right and available for theoretical reflection",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, again this is not the same as the rationalist cognitivist view of objective knowledge, as present-at-hand reflection will always be related to the background of ready-to-hand experience. Learning is an engaged practice of present-at-hand reflection in a context of use and application. “We do at times engage in conscious reflection and systematic thought, but these are secondary to the pre-reflective experience of being thrown in a situation in which we are already acting. We are always engaged in acting within a situation, without the opportunity to fully disengage ourselves and function as detached observers. Even what we call ‘disengagement’ occurs within thrownness: we do not escape our thrownness, but shift our domain of concern” (Winograd and Flores, 1986, pg. 71). While their work has been criticised for retaining some aspects of rationalism when applying their perspective in system development (Suchman, 1995), Winograd and Flores have had a very significant influence in introducing ideas from phenomenology into the cognitive sciences and HCI. A second significant perspective that also drew upon aspects of phenomenology was developed at around the same time by Lucy Suchman. 3.2 Suchman’s Plans and Situated Actions Lucy Suchman (1987) presented another influential critique of the cognitivist conception of mind as applied to the design of interactive systems. She studied interactions with a photocopier that had been designed to model human cognition as following plans based upon use a planning-based model of cognition in interactions with users: one that treats cognition as the generation of a blueprint of steps to be taken based on goals, which are then used to guide behaviour. Suchman adopted an ethnomethodological orientation (Garfinkel, 1967), drawing in particular from studies of conversation (Sacks, et al. 1972). Ethnomethodology is concerned with the everyday practices by which mutual intelligibility and social order are achieved",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1972). Ethnomethodology is concerned with the everyday practices by which mutual intelligibility and social order are achieved. It treats the objectivity of social facts as an ongoing achievement of the members of a social group. A central concept here is the accountability of members’ methods, i.e., the ways that they are made observable and reportable to others. Ethnomethodology also draws on ideas from phenomenology that emphasise the everyday practical engagement of social interactions (Schutz, 1970). Suchman argued that plans are a representation, but not a specification of behaviour. Thus, they can act as a projection of what will happen or retrospective account of what is or what did happen. However, behaviour itself is generated as situated action : in interaction with the contingencies of the physical and social environment. In this model, a plan is only one resource used to help to guide action. Language use is also situated, relying on its indexicality – a relationship to the changing circumstances in which it is used rather than any abstract collection of shared meaning. Ethnomethodologically-informed conversation analysis (e.g., Sacks, et al., 1972) has shown how the same utterance can have a multitude of contexdependent meanings. For example, “that’s brilliant!” could be either an enthusiastic response or a sarcastic put-down depending upon the context in which it is used. Suchman presents a case study of people struggling to use a photocopier designed with an ‘intelligent’ interface help system. Using detailed video analysis, she shows how problems emerge relating to disparities between the fixed plan implemented in the system of how to complete an action and the user’s actual situated behaviour, which is far more ad hoc and flexible. A particular problem relates to the lack of accountability of the system behaviour and it’s lack of responsiveness to the behaviour of the users",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A particular problem relates to the lack of accountability of the system behaviour and it’s lack of responsiveness to the behaviour of the users. Suchman’s contribution to HCI has been very significant, introducing ideas from ethnomethodology to many in the community, providing a powerful critique of cognitivism and emphasising the importance of studying practice as it occurs in real situations, thus providing part of the intellectual foundation of Computer-Supported Co-operative Work. Paul Dourish (2001) has drawn significantly from both Winograd and Flores and Suchman in developing a view of embodied interaction as a foundational concept for HCI. We describe this approach in the next section. 4. Embodied interaction as a foundational framework Building on the research outlined in the previous section, Dourish (2001) has suggested that embodied interaction should be seen as a foundational concept for HCI. He draws upon and expands the phenomenological perspective that underpins both Winograd and Flores’s and Suchman’s work, including an overview of Husserl’s introduction of ideas in phenomenology and Heidegger’s analysis of Dasein as the inseparability of being and the world. He also draws significantly on Schutz’s (1970) development of social phenomenology. Dourish highlights three elements that are common to this work. Firstly, embodiment – meaning “grounded in everyday, mundane experience” (pg. 125) – is central to all of them; secondly they focus on practice: “everyday engagement with the world directed towards the accomplishment of practical tasks” (pg. 125); and finally, this embodied practice is the source of meaning: “we find the world meaningful primarily with respect to the ways in which we act within it” (pg",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 125); and finally, this embodied practice is the source of meaning: “we find the world meaningful primarily with respect to the ways in which we act within it” (pg. 125) Dourish structures his argument by showing how recent work in the seemingly disparate fields of tangible and social computing can both be viewed as having a common concern with embodiment, in the sense of a focus on engaged activity in the world rather than abstract theorising: “Embodied interaction is the creation, manipulation and sharing of meaning through engaged interaction with artefacts” (Dourish, 2001, pg. 126). For example, Underkoffler and Ishii’s (1999) urban planning workbench (URP) enables users to flexibly explore interactions between wind, reflection and shadow effects for different configurations of buildings by manipulating tangible models. He cites Bowers, Button and Sharrock (1995), who describe how the skilled practices through which workers in a print shop manage their activities often involve stepping outside formalized procedures. Introducing a computerised system that formalised idealised procedures still further, ignoring the situated processes by which things were actually carried out, had a negative impact on the work done. A key issue in the characterisation of embodied interaction is how meaning is understood and mapped onto things in the world. Dourish focuses on three different senses of meaning: ontology, intersubjectivity and intentionality. Ontology deals with the nature of being: how it is structured into different kinds of things and the relationships between them. The phenomenological underpinning of the embodied interaction approach emphasise how ontology is derived through purposeful interactions in the world rather than being objectively defined. Therefore, it can differ significantly between individuals",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Therefore, it can differ significantly between individuals. Thus, if a designer embeds a particular set ontological commitments into the design of a piece of software or other technology, then this can cause problems for users who may not share them, and may have a set of quite different purposes to put the system to than the designer had in mind: “a design may reflect a particular set of ontological commitments on the part of a designer, but it cannot provide an ontology for the user” (Dourish, 2001, pg. 130). The second sense of meaning that Dourish discusses is intersubjectivity, which refers to the ways that two or more people can come to a shared understanding without having direct access to each other’s mental states. Dourish highlights two ways that intersubjectivity is relevant to the design of technology. Firstly in the ways that the designer is able to communicate to the user how they envisage that the technology will be used (Suchman’s photocopier example is a case of this going wrong). Secondly, the extent to which systems enable different users to communicate through them to develop shared ways of using software systems and appropriate them for shared patterns of practice. The third sense, intentionality , refers to the directedness of meaning; the property (of a thought, action etc) of being about something. Intentionality is a core characteristic of embodied interaction, as people act on and through computational representations to enact effects on the world – we are always already directed ‘to’ or ‘towards the world’ since it is our habitat (into which we are thrown) and primary source of meaning, as described by Merleau-Ponty. The way that intentionality is expressed in embodied interaction is through a process of coupling : “By coupling, I mean the way that we can build up and break down relationships between entities, putting them together or taking them apart for the purpose of incorporating them into our action” (Dourish, 2001, pg. 138)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 138). Technologies that support embodied interaction well are ones that make clear how they are coupled to the world, allowing users to orient to them in a variety of ways. Thus, a tangible interface, for example, might be attended to as an iconic representation of digital information, a controller to manipulate digital information, or as a physical object in its own right that can be picked up and shared with others, left on a desk as a reminder to complete a task the next day or moved out of the reach of others, depending upon the way it is coupled to the ongoing concerns of those who are using it. Dourish’s theoretical account of embodied interaction has been very influential, paving the way for work that has explored new forms of tangible (Hornecker and Buur, 2006), mobile (Oulasvirta, et al., 2005), ubiquitous (Chalmers, 2002) and movement-based (Hummels, et al., 2007) interaction. The primary contribution of Dourish’s work has been to analyse how meaning is fluidly negotiated in interaction with technology, the world and other people. His perspective is primarily a phenomenological one, building on the insights of Winograd and Flores and Suchman to present embodiment as a foundational concept for HCI. It has inspired a range of research that often goes back to its own reading of the original phenomenological authors. In particular, Merleau-Ponty’s work is increasingly being referred to, which has a stronger focus on the body and its felt experience. However, there has been another strand of work that has rejected aspects of the cognitivist view of thinking and interaction, but that has not drawn so explicitly from phenomenology. It is to this embodied cognition approach that we turn in the next section. 5. Embodied cognition Embodied cognition refers to a diverse group of theories and approaches that challenge different aspects of the Cartesian view of cognition (cf. Clark, 1997; Wheeler, 2005)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5. Embodied cognition Embodied cognition refers to a diverse group of theories and approaches that challenge different aspects of the Cartesian view of cognition (cf. Clark, 1997; Wheeler, 2005). Much of this work has drawn on the phenomenological critique of cognitivism, but it has also come about as a response to particular technical challenges or experimental findings. Thus, the influence of phenomenology should be seen as more implicit than in the work described in the previous section (cf. Wheeler, 2005). There have also been several precursors to the recent growth of embodied cognition. In particular, Gibson’s (1979) ecological approach to visual perception, which presents perception and action as inseparable has been very influential. Gibson introduced the concept of affordances as action possibilities picked up from the environment, in a relationship to the physical capacities and ongoing concerns of an organism. The concept of affordance was adapted by Donald Norman in HCI (e.g., 1999) to account for the ways in which physical and graphical artefacts suggest how they should be used (in the latter case the affordances are ‘perceived’, rather than real). There is currently little agreement on the core concepts of the embodied cognition approach, with some approaches rejecting Cartesian cognitive science completely, and others retaining some parts of it, such as symbolic representations or some aspects of functionalism. Rohrer (2007) offers a broad survey of the literature on embodiment, describing (at least) twelve different uses of the term. These include the usage in socio-cultural studies where it refers to the environment in which the body is situated, (e.g., Hutchins, 1995); the usage relating to morphology, which looks at how the physical characteristics of a cognitive agent can influence the types of cognitive processing it can carry out; and the use of the term to refer to grounding, or how abstract concepts are linked to a history of concrete, physical experience",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Lakoff and Johnson (1999) suggest that abstract concepts are related by metaphorical mappings to basic-level image schemas – mental structures that are formed through sensormotor interaction with the world to guide our action. These metaphorical mappings preserve the inferential structure of the original domain. For example, Lakoff and Núñez (2000) suggest that Boolean Logic is an extension of a sensori-motor container schema, with the same inferential structure of IN, OUT and transitivity (e.g., a ring contained within a box that is held in a hand is also inside the hand), originally developed through experiences with real containers. Wilson (2002) focuses more narrowly on the literature identified as embodied cognition, identifying six claims: (i) cognition is situated : it takes place in a real-world environment and inherently involves perception and action; (ii) cognition is time pressured : it functions under the pressures of having to interact in real time with a dynamic environment. This and the previous claim were presented by Brooks (1991) as criticisms of robotic systems designed to first build up a model of the environment, devise a plan and then act. He argued that in reality, an organism would not have this luxury, as the world would have changed by the time it decided what it was going to do. Brooks recommends instead building systems that ‘use the world as its own best model’ and which use simpler more responsive architectures; (iii) we offload cognitive work onto the environment : cognitive workload is alleviated by holding or manipulating information in external structures (cf. Kirsh, 2010, Scaife & Rogers, 1996); (iv) the environment is part of the cognitive system : the links between internal and external representations and processing are so fundamental that they should be considered the same unit for analysis. This claim has been described as distributed cognition (Hollan, Hutchins, & Kirsh, 2000; Hutchins, 1995) and the extended mind hypothesis (Clark & Chalmers, 1998)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This claim has been described as distributed cognition (Hollan, Hutchins, & Kirsh, 2000; Hutchins, 1995) and the extended mind hypothesis (Clark & Chalmers, 1998). It is also related to a number of approaches within cognitive science that resist traditional explanations in terms of internal representations (Thelen & Smith, 1994; van Gelder, 1995); (v) cognition is for action : the function of the mind is to guide action, so cognition should be understood in terms of its contribution to behaviour. Glenberg (1997) claims that cognition evolved to coordinate interaction with a three-dimensional world, enhancing survival and hence reproductive success. In this characterisation, the meaning of a situation to an organism is a coordinated set of possible actions, which are determined by physical form, learning history and goals; (vi) off-line cognition is body-based : even when decoupled from the environment, mechanisms evolved for interaction with it play a role in cognition; sensormotor systems are involved in processing even in the absence of task-relevant perceptual input (cf. Lakoff and Johnson, 1998) Theoretical work that makes up the new approaches of embodied cognition has also been very influential in HCI. Hutchins (1995) introduced the theory of distributed cognition , arguing that what classical cognitive science took to be internal, individual acts of information processing, were in fact outcomes of a socio-cultural system. “Having failed to notice that the central metaphor of the physical-symbol-system hypothesis captured the properties of a socio-cultural system rather than those of an individual mind, AI and information-processing psychology proposed some radical conceptual surgery for the modeled human. The brain was removed and replaced with a computer. The surgery was a success. However, there was an apparently unintended side effect: the hands, the eyes, the ears, the nose, the mouth, and the emotions all fell away when the brain was replaced by a computer” (Hutchins, 1995: 363)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The idea of cognition as information processing is retained in distributed cognition. However, the process is analysed as propagating through a variety of representational media, including other minds, physical artefacts and technologies, and parts of the body. Distributed cognition has been used to analyse the flow of information through a variety of socio-technical systems, typically using an ethnographic approach. For example, in critiquing the idea of organisational memory through showing the details of information flow involved in a telephone hotline call (Ackerman & Halverson, 1998). A second perspective, related to Hutchin’s description of distributed cognition, is the analysis of how interaction with external representations can support cognition, change the nature of a cognitive task, or form an intrinsic part of thinking (Kirsh, 2010; Scaife & Rogers, 1996). Kirsh and Maglio (1994) for example, described how expert users of the video game Tetris solved the problem of fitting irregularly-shaped blocks together by physically rotating them on a screen and using the computationally-cheap mental processes of pattern matching and recognition. They call physical activity to reduce the burden of problem solving epistemic action . A third approach attempts to use Lakoff and Johnson’s (1999) work on image schemas and embodied conceptual metaphor in the design and evaluation of technology. Image schemas are representations, abstracted from recurrent patterns of sensori-motor experience. For example, the front-back schema is derived from movement in the world and the structure of the human body where the eyes are positioned on the front side of the body. Embodied conceptual metaphors are extensions of image schemas to think about other entities, which can be more abstract. For example, good and bad can be thought of in terms of up and down (e.g., “things are looking up”), whereas time can be thought of in terms of front and back (e.g., “your future is ahead of you”)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example, good and bad can be thought of in terms of up and down (e.g., “things are looking up”), whereas time can be thought of in terms of front and back (e.g., “your future is ahead of you”). A recent trend in HCI research has been to try to use insights from this work to design novel interface applications and to improve interface usability (e.g., Hurtienne, et al., 2008; Antle et al., 2009) A final example of embodied cognition theory that has found application in the design and evaluation of technology is work that has shown that physical changes to the body, such as adopting a different posture can in some circumstances induce changes in emotional or attitudinal states and social perceptions (e.g., Niedenthal et al., 2005). Bianchi-Berthouze et al. (2007) have experimented with trying to increase user engagement in gameplay by increasing the level of bodily involvement in interaction. 6. Case studies of work in embodied interaction In the previous sections we described some of the theoretical underpinnings of recent work in embodied interaction. Strong and consistent themes have yet to emerge, but we have shown how recent work can often be described as deriving from work on phenomenology or on embodied cognition. In this section, we describe in more detail four examples of recent projects. No attempt is made here to give a comprehensive overview of work that has attempted to apply concepts drawn from embodied interaction – indeed it is unclear if an exhaustive framework could be proposed at this stage. Rather, four quite different projects were selected to give a flavour of the diversity of work in this area. 6.1 Fallmän: supporting skilful engagement with the world in mobile interaction Fällman used embodiment as a guiding perspective for a series of projects that formed part of his PhD research (Fällman 2003). His work is significantly influenced by Merleau-Ponty, but also by other work in phenomenology",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". His work is significantly influenced by Merleau-Ponty, but also by other work in phenomenology. Merleau-Ponty’s influence is evident in the emphasis that Fällman gives to the subjective experience of technology through first-person accounts. Fällman takes a phenomenological perspective to the analysis of mobile technologies, focusing on how they are experienced. His approach is design-oriented research, where research is driven by design, and designing is the means for producing new knowledge (Fällman 2003). Theories of embodiment were utilized in the conceptual design of a series of practical projects, from the design of a support tool for mobile service technicians, a slide scroller on a small screen, and a wearable ‘alternate’ reality helmet. Fällman (2003) argues that the traditional cognitivist HCI-perspective of a disembodied mind is a poor model for understanding or designing mobile interaction. This is because mobility is “strongly situated and rooted in a world ()”. Analysing his own subjective experience of using mobile devices, he finds that these all have the common characteristic of being embedded in a relatively small physical form which relate to the human body – they are held close, felt, worn, etc. Moreover, when using mobile devices our focus is often with on the world (e.g., taking photos, checking the network connection, taking a call). As mobile device use is strongly related to context, Fällman argues that it and mobility should be seen as a mode of being-in-the-world. People “become mobile in different ways – not only corporeal – to be able to get involved in different physical and social contexts”. The design of mobile technologies should therefore not interfere with this involvement or, better still, it should support engagement with the world. This also means that desktop interface metaphors are inappropriate for mobile devices, since they tend to make it difficult to engage with the real world while interacting with the interface",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This also means that desktop interface metaphors are inappropriate for mobile devices, since they tend to make it difficult to engage with the real world while interacting with the interface. Motivated by this theoretical background, one of Fällman’s projects aims to support ‘skillful coping’ with a mobile support system for service technicians. The resulting device is worn on the arm, allowing for hands-free interaction and always being there, but receding into the background of attention when not in use. It is interacted with primarily by pointing at, for example, a broken component in order to access its data sheet, thereby connecting the physical work of industrial components directly with the virtual world of data on the device. Users then interact by tilting to slide the interface, and tapping the screen. Tilting makes use of the notion of embodied interfaces (Fishkin et al 2000), exploiting the fact that humans have an embodied understanding of gravity (Lakoff and Johnson 1999). 6.2 Robertson: the mutual availability of (embodied) action Robertson (1997, 2002) also applies a phenomenological perspective to Computer-Supported Cooperative Work (CSCW) research, investigating the role of embodied action in supporting awareness and coordination. Specifically, she aims to show that Merleau-Ponty’s phenomenology of perception provides a new perspective on cooperative work, by emphasizing how the public availability of actions and artefacts provides environmental support for participants in cooperation. For designers of novel technologies such as those used in distance communication, one of the most important lessons is that perception is learned – over time we gain skill, adapt our perception and body image, and the research question thus becomes what kinds of cues and feedback mechanisms might support us in this process? A focus on lived cognition and perception furthermore highlights the agency of users, who should be supported with resources for action",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Robertson (1997) analysed how embodied action supports cooperative work, based on a field study of collaborative work within an educational game design company where staff often work from home. The study highlights how much cooperative design and development of software relied on communicative interactions between staff. Sharing physical space within company premises “enabled communication by supporting the mutual perception of their embodied actions”, of talking and making or using artefacts within this space. When staff members were not on-site, workers relied on complex work practices that had evolved to support communication. Video analysis of embodied activities revealed several categories of actions. Individual actions could relate to physical objects, as in moving them (for oneself, or to make them available for somebody else), producing new representations (drawing, writing), highlighting aspects of the object, or personal use. Individual actions could also relate to other human bodies, either through communicative gesture, facial expression, talk etc., or through enacting user behaviour or the design object itself (a frequent activity in design). Further actions related to the physical workplace, such as moving around, pointing at things, or changing direction of gaze. Group activities such as conversation, shared attention on an object, creation of shared resources for conversation and shared representations (clearing the table and then sketching on it), shared use of objects, and so on, in turn are comprised of individual actions. These kinds of embodied actions were not specific to any particular phase of the software development process. Thus, distance communication technology in support of distributed design should enable and mediate the mutual perception of embodied actions and negotiation of meaning rather than supporting specific design processes or phases",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This would be greatly facilitated by the reversibility of perception: the ability to anticipate how actions will be seen by others (which in turn enables the ‘actor’ to shape their action for the observer). 6.3 Hornecker: group facilitation mechanisms to influence social interaction Hornecker’s (2005) notion of ‘Embodied Facilitation’ highlights how physical, spatial, and software-determined configuration of a system affects group interaction patterns and influences the social formations and interaction patterns that emerge with and around the system. Hornecker’s argument is that both physical and software design define a structure, and that this structure may facilitate, prohibit or hinder some actions. Specific behaviours are easily feasible, and may even be invited, while other behaviours are made difficult, or even prevented. Systems can thus embody structure and thereby styles, methods and means of facilitation, very similar to how meeting facilitators, educators and professional group work facilitators steer group processes by imposing structure (with an agenda and rules of discussion), staging the setting (placing tables, chairs, flipchart or projection surfaces), and providing work materials (e.g. number of markers for writing). This perspective encourages the analysis technological systems in terms of the resources they provide for participants to engage in or join an activity and to collaborate, and the ways in which they influence how people will coordinate and collaborate. This concept is useful for understanding the effects of UbiComp technologies, which are often embedded in furniture and everyday objects, on social interaction patterns and collaboration. For example, in the design of interactive multitouch tables, the shape, size, and height of the table influence how well groups tend to engage with each other, share an activity, and how many people may take part. Hornecker refers to tangible interaction systems in particular as ‘embodying facilitation’",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Hornecker refers to tangible interaction systems in particular as ‘embodying facilitation’. These are comprised of physical structures embedded in space, and that users interact with through some form of bodily interaction (Hornecker and Buur 2006). The notion of ‘embodied facilitation’ draws in particular from Merleau-Ponty in that it is through bodily interaction that we use these systems, seeing and pre-reflectively interpreting the resources and constraints they provide for action. 6.4 Hurtienne: Using Image Schemas to Design Intuitive Interfaces Hurtienne, Israel and Weber (2008) conducted an empirical analysis of the utility of image schemas (Lakoff and Johnson, 1999; Johnson, 1980) in interface design. This work has shown how image schemas and embodied conceptual metaphors, can be productively utilized in interface design and provide a basic vocabulary for consistent and intuitive mapping. Hurtienne et al showed that violating embodied metaphorical extensions of image schema results in increased reaction times and error rates when interacting with simple GUI interfaces. User interfaces that were congruent with image schemas (e.g. moving a lever upward to evaluate a hotel as being ‘better quality’ or to indicate that its staff is friendly) were not only judged by participants as being better, but also resulted in faster decisions and greater accuracy (Hurtienne, 2011; Hurtienne and Blessing, 2007). This phenomenon could also be demonstrated with two buttons, where the positive (or ‘more’) rating was on top. An example for how a complex interface metaphor builds on a range of image schemas is ‘putting files into the thrashcan’, which employs the schemas of path (dragging), compulsion, containment, and full-empty. Yet, unfortunately, sometimes there can be competing image schemas, which then interfere with unconscious information-processing. Image schemas thus cannot be implemented in a mechanical way",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Yet, unfortunately, sometimes there can be competing image schemas, which then interfere with unconscious information-processing. Image schemas thus cannot be implemented in a mechanical way. Furthermore, as some image schemas relate and depend on each other (a container can also block content from leaking out), there may be competing image schemas, which equally could describe the users’ mental model, and thus there may be several ways of implementing this in an interface. In related work Antle and colleagues have explored the potential of embodied metaphor to support the design of less traditional interfaces, such as a whole body interaction system to encourage reasoning about social justice (Antle, et al., in press; Antle et al., 2009) and tangible and whole body interactive systems to control different properties of sound (Antle et al., 2009) 7. Discussion Embodied interaction has been taken up and developed enthusiastically in the design, analysis and evaluation of interactions with technology in recent years. However, while theoretical work on embodied interaction has been united in challenging the cognitivist model of thinking and its application in system design, this recent explosion of interest has resulted in a sometimes bewildering range of approaches, techniques and claims. In this chapter, we have summarised two broad strands of theory that are being used in HCI. The first rejects the cognitivist model completely, building instead on insights from phenomenology to emphasise the engaged, direct ways that people normally participate in activities and the flexible ways that meaning can be ascribed and negotiated in ongoing practice. Analysis of the details of situated practice is well known in HCI, particularly through the influence of ethnomethodologically informed ethnography, which traces its lineage, in part, to Schutz’s (1970) analysis of the phenomenology of the social world",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". More recently, authors have drawn from other phenomenologists, such as Merleau-Ponty (1962) and have begun to use their insights to design novel technology interactions as well as new methods of analysis. The second strand of theory derives from cognitive science and has attempted to respond to problems with the cognitivist model by adapting, extending or replacing different aspects of it. This work is very diverse. Some, such as that arguing for a greater role for external representations in cognitive processing, have been influential in HCI and interaction design for at least 15 years (e.g., Scaife and Rogers, 1996). Work that characterises cognition as extending beyond the boundary of the skull to include physical movements of the body, other brains, physical artefacts and technologies has also been used for some time in analysing existing socio-technical systems, as part of the distributed cognition approach. We are also beginning to see more work looking at how these insights can be used in designing novel technologies, (cf. Bird, et al., 2009; Nagel et al., 2005). More radical approaches, such as those which model cognition as a dynamical system spanning brain, body, and environment (e.g., Beer, 2000), dispensing with the idea of internal representations altogether, and the enactive approach (e.g., Thompson, 2005), which focuses on the autonomous agency and lived subjectivity of an agent (and has many links with the phenomenological approach), have so far had little influence in HCI. A focus on different aspects of embodied interaction is increasingly popular in HCI and interaction design. However, as in other areas such as cognitive science, there is still no consensus on the core concepts of embodied interaction, or the best ways to apply these concepts. In this chapter we have highlighted two strands of work – one that focuses primarily on insights from phenomenology, and one that focuses primarily on ideas from embodied cognition, which tends to be influenced only implicitly by phenomenology",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". There is however much work still to be done to develop this theory into the kinds of frameworks, methods and perspectives needed in the design, analysis and evaluation of interaction with technology. Further readings Dourish, P. (2001). Where the Action Is: The Foundations of Embodied Interaction. Cambridge: MIT Press. Suchman, L. (1987) Plans and Situated Actions. The Problem of Human Machine Communication. Cambridge: Cambridge University Press. Winograd, T. and Flores, F. (1986): Understanding Computers and Cognition: A New Foundation for Design . Norwood, New Jersey: Ablex Publishing Corp Glossary of terms Cartesian: relating to the work on René Descartes. In particular, this often refers to the philosophical position of dualism – treating the mind as non-physical and therefore separate from the body Embodied conceptual metaphor: suggests that concepts are typically understood through metaphorical mappings to concrete sensori-motor experiences Cognitivism: an approach in cognitive science that treats cognition as information processing on discrete internal symbols Epistemic action: a physical action that changes the nature of a cognitive operation necessary to carry out a particular task. Functionalism: a philosophical approach that considers mental states in terms of the role they play in a cognitive system rather than in terms of their constitution. Image schema: a cognitive structure that abstracts across a number of sensori-motor experiences: for example, the containment schema abstracts across many concrete experiences of something being contained within something else Phenomenology: the study of the structure of conscious experience as experienced (although not necessarily studied) from a first-person perspective Presence-at-hand: the state of attending to an object, tool or representation itself as the focus of an activity. Readiness-to-hand: this concept refers to how, when working with a tool or representation we treat it almost as if it were invisible, focussing instead upon the task it is used for",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Readiness-to-hand: this concept refers to how, when working with a tool or representation we treat it almost as if it were invisible, focussing instead upon the task it is used for. References Ackerman, M. S. and Halverson, C. (1998). Considering an organization's memory. In Proceedings of the 1998 ACM conference on Computer supported cooperative work (CSCW '98). ACM, New York, NY, USA, 39-48 Antle, A.N., Corness, G., & Droumeva, M., (2009) What the Body Knows: Exploring the Benefits of Embodied Metaphors in Hybrid Physical Digital Environments. Interacting with Computers, 21 (1-1), 66-75. Antle, A. N., Corness, G., & Bevans, A. (in press) Balancing Justice: Comparing Whole Body and Controller-based Interaction for an Abstract Domain. Int J. Arts and Technology. Antle, A. N., Corness, G., Bakker, S., Droumeva, E., Hoven, E. v. d., and Bevans, A.. (2009) Designing to support reasoned imagination through embodied metaphor. In Proceedings of Creativity and cognition (C&C '09). ACM, New York, NY, USA, 275-284. Bianchi-Berthouze, N., Kim, W.W. & Darshak, P. Does body movement engage you more in digital game play? And Why? In Proc. ACII 2007, Springer (2007), 102- 113. Bowers, J., Button, G. and Sharrock, W. (1995). Workflow from within and without: technology and cooperative work on the print industry shopfloor. In Proceedings of the fourth conference on European Conference on Computer-Supported Cooperative Work (ECSCW'95), Hans Marmolin, Yngve Sundblad, and Kjeld Schmidt (Eds.). Kluwer Academic Publishers, Norwell, MA, USA, 51-66. Brooks, R.A., Intelligence without representation, Artificial Intelligence 47 (1991), 139–159 Card, S., Moran, T., and Newell, A. (1983). The Psychology of Human-Computer Interaction. Hillsdale, NJ: Lawrence Erlbaum Associates. Chalmers, M. and Galani, A. (2004) Seamful interweaving: heterogeneity in the theory and design of interactive systems. In Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques (DIS '04)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In Proceedings of the 5th conference on Designing interactive systems: processes, practices, methods, and techniques (DIS '04). ACM, New York, NY, USA, 243-252 Clark, A. (1997). Being there: putting brain, body and world together again. Cambridge, Massachusetts: MIT Press. Clark, A. and and Chalmers, D. (1998) The Extended Mind. Analysis 58 (1998): 7–19. Dreyfus, H. L. (1979) What Computers Can’t Do: The Limits of Artificial Intelligence , Harper & Row: New York Dreyfus, H. L. (1991) Being-in-the-world: A Commentary on Heidegger's Being and Time, Division I. Cambridge, MA: MIT Press. Dreyfus, H.L. (2007), “Why Heideggerian AI failed and how fixing it would require making it more Heideggerian”, Philosophical Psychology , 20 (2), pp. 247-268 Dourish, P. (2001). Where the Action Is: The Foundations of Embodied Interaction. Cambridge: MIT Press. Fallman, D. (2003) Design-oriented Human-Computer Interaction, Proceedings of CHI2003, Conference on Human Factors in Computing Systems, New York, NY: ACM Press, pp. 225-- 232. Fallman, D. (2003) in romance with the materials of mobile interaction: a phenomenological approach to the design of mobile information technology. Doctoral thesis, Umea university, Sweden: Larsson & Co. Fishkin, K.P., Gujar, A., Harrison, B.L., Moran, T.P., Want, R. (2000) Embodied User Interfaces for Really Direct Manipulation. Communications of the ACM, Vol 43, Issue 9, 74- 80 Fodor, J.A. (1975), The Language of Thought , Cambridge, MA: Harvard Uni. Press Garfinkel, Harold (1967) Studies in Ethnomethodology , Prentice-Hall: Englewood-Cliffs, New Jersey. Gibson, J. J. (1979) The Ecological Approach to Visual Perception , Houghton-Mifflin, Boston, Glenberg, A. M. (1997). What memory is for. Behavioral and Brain Sciences , 20(1), 1-55. Heath, C., Luff P. (2000). Technology in Action. Cambridge University Press Heidegger, M. (1927/1990). Being and Time (J. Macquarrie & E. Robinson, Trans.). Oxford: Blackwell. Hollan, J., Hutchins, E., & Kirsh, D. (2000)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Technology in Action. Cambridge University Press Heidegger, M. (1927/1990). Being and Time (J. Macquarrie & E. Robinson, Trans.). Oxford: Blackwell. Hollan, J., Hutchins, E., & Kirsh, D. (2000). Distributed cognition: toward a new foundation for human-computer interaction research. ACM Transactions on Computer-Human Interaction , 7(2), 174-196. Hornecker, E. (2005) A Design Theme for Tangible Interaction: Embodied Facilitation. In Proceedings of the 9th European Conference on Computer Supported Cooperative Work (CSCW'05) Kluwer/Springer. pp 23-43 Hornecker, E., & Buur, J. (2006). Getting a grip on tangible interaction: a framework on physical space and social interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2006), Montréal, Québec, Canada, 22-27 April, 437- 446. Hummels, C., Overbeeke, K. and Klooster, S. (2007) Move to get moved: a search for methods, tools and knowledge to design for expressive and rich movement-based interaction. Personal Ubiquitous Comput., 11 (8). 677-690. Hurtienne (2011). Image Schemas and Design for Intuitive Use – Exploring New Guidance for User Interface Design. PhD thesis TU Berlin. http://opus.kobv.de/tuberlin/volltexte/2011/2970/pdf/hurtienne_joern.pdf Hurtienne, J. and Blessing, L. (2007). Design for intuitive use - Testing image schema theory for user interface design. In: Proc. International Conference on Engineering Design, Paris, Design Society. P_386, 1-12. [CD-ROM]. Hurtienne, J., Israel, J. H. and Weber, K. (2008) Cooking up real world buisiness applications combining physicality, digitality, and image schemas, in Proceedings of TEI’08, pp. 239–246, ACM, 2008. Hutchins, E. (1995 ). Cognition in the Wild. Cambridge, MA: MIT Press. Kirsh, D. Thinking with External Representations. AI and Society. Springer: London, (2010) 25:441–454. Kirsh, D., & Maglio, P. (1994). On distinguishing epistemic from pragmatic action. Cognitive Science , 18(4), 513-549. Lakoff, G., & Johnson, M. (1999)",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Springer: London, (2010) 25:441–454. Kirsh, D., & Maglio, P. (1994). On distinguishing epistemic from pragmatic action. Cognitive Science , 18(4), 513-549. Lakoff, G., & Johnson, M. (1999). Philosophy in the flesh: the embodied mind and its challenge to western thought . New York: Basic Books. Lakoff, G., & Núñez, R. (2000). Where Mathematics Comes From: How the Embodied Mind Brings Mathematics into Being . New York: Basic Books. Merleau-Ponty, M. (1962). Phenomenology of Perception (C. Smith, Trans.). London: Routledge & Kegan Paul. Moran, T.P. and R.J. Anderson (1990): The Workaday World as a Paradigm for CSCW Design. In CSCW ’90, Proceedings of the Conference on Computer-Supported Cooperative Work . ACM Press, New York, pp. 381–393. Nagel, S. K., Carl, C., Kringe, T., Martin, R. and Konig, P. (2005) Beyond sensory substitution - learning the sixth sense. Journal of Neural Engineering , 2, 13–26, Newell, A., & Simon, H. (1972). Human problem solving . Englewood Cliffs, NJ: Prentice Hall. Niedenthal, P. M., Barsalou, L. W., Winkielman, P., Krauth-Gruber, S., & Ric, F. (2005). Embodiment in attitudes, social perception, and emotion. Personality and Social Psychology Review , 9(3), 184-211. Norman, D. A. (1999) Affordances, Conventions and Design. Interactions 6 (3), 38-43, May 1999, ACM Press. Oulasvirta, A. ,Tamminen, S., Roto, R. and Kuorelahti, J. (2005) Interaction in 4-Second Bursts: The Fragmented Nature of Attentional Resources in Mobile HCI. In Proceedings of CHI 2005 , Portland, Oregon, USA, 919 – 928. Putnam, H. (1975). Philosophy and our mental life. In H. Putnam (Ed.), Mind, language and reality (Vol. 2, pp. 291-303). Cambridge: Cambridge University Press. Robertson, T. (1997): Cooperative Work and Lived Cognition: A Taxonomy of Embodied Actions. In Proceedings of the Fifth European Conference on Computer-Supported Cooperative Work. Kluwer Academic Publishers, Dordrecht, The Netherlands, pp. 205–220. Robertson, T. (2002) The Public Availability of Actions and Artefacts",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Theories of embodiment in HCI",
    "author": "Paul Marshall and Eva Hornecker",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\embodimentChapterSage.pdf",
    "date_published": "2016-11-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Kluwer Academic Publishers, Dordrecht, The Netherlands, pp. 205–220. Robertson, T. (2002) The Public Availability of Actions and Artefacts. Computer Supported Cooperative Work 11: 299–316, 2002. Robertson, T. and Loke, L. (2009) Designing Situations. Proceedings of OZCHI 2009, ACM. 1-8 Rohrer, T. (2007). The body in space: Dimensions of embodiment. In T. Ziemke, J. Zlatev & R. Frank (Eds.), Body, Language and Mind (Vol. 1: Embodiment). Berlin: Mouton de Gruyter, 339 – 378. Sacks, H., Schegeloff, E.A. and Jefferson, G. A simplest systematics for the organization of turn-taking conversation. Language, 50 (1974), 696-735. Scaife, M., & Rogers, Y. (1996). External Cognition: how do graphical representations work? International Journal of Human-Computer Studies , 45, 185-213. Schutz, A. (1970): On Phenomenology and Social Relations: Selected Writings. In H. Wagner (ed), The Heritage of Sociology collection, Chicago: University of Chicago Press. Searle, John. R. (1980) Minds, brains, and programs. Behavioral and Brain Sciences 3 (3): 417-457 Suchman, L. (1987) Plans and Situated Actions. The Problem of Human Machine Communication. Cambridge: Cambridge University Press. Thelen, E. & Smith, L. B. (1994). A dynamic systems approach to the development of cognition and action . Cambridge, MA: MIT Press. Underkoffler, J., and Ishii, H., Urp: A Luminous-Tangible Workbench for Urban Planning and Design, in Proceedings of Conference on Human Factors in Computing Systems (CHI '99), (Pittsburgh, Pennsylvania USA, May 15-20, 1999), ACM Press, pp. 386-393. van Gelder, T. (1998), The dynamical hypothesis in cognitive science, Behavioral and Brain Sciences , 21 (5), pp. 615-665 Wheeler, M. (2005). Reconstructing the cognitive world. Cambridge, MA: MIT Press. Wilson, M. (2002). Six views of embodied cognition. Psychonomic Bulletin and Review , 9(4), 625-636. Winograd, T. and Flores, F. (1986): Understanding Computers and Cognition: A New Foundation for Design . Norwood, New Jersey: Ablex Publishing Corp",
    "chunk_id": "Human_computer_interaction_embodimentchapterv8pm.json_chunk_30"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Aesthetic–Usability Effect Users often perceive aesthetically pleasing design as design that’s more usable. Key Takeaways • An aesthetically pleasing design creates a positive response in people’s brains and leads them to believe the design actually works better. • People are more tolerant of minor usability issues when the design of a product or service is aesthetically pleasing. • Visually pleasing design can mask usability problems and prevent issues from being discovered during usability testing. Overview As designers, we understand that our work is about more than just how somthing looks; it’s also about how it works. That’s not to say good design can’t also be attractive design. In fact, an aesthetically pleasing design can influence usabiity. Not only does it create a positive emotional response, but it also enhances our cognitive abilities, increases the perception of usability, and extends credibility. In other words, an aesthetically pleasing design creates a positive response in 65 1 F. Gregory Ashby, Alice M. Isen, and And U. Turken, “A Neuropsychological Theory of Positive Affect and Its Influence on Cognition,” Psychological Review 106, no. 3 (1999): 529–50. 2 Masaaki Kurosu and Kaori Kashimura, “Apparent Usability vs. Inherent Usability: Experimental Analysis on the Determinants of the Apparent Usability,” in CHI ’95: Conference Companion on Human Factors in Computing Systems (New York: Association for Computing Machinery, 1995), 292–93. people’s brains and leads them to believe the design actually works better 1 —a phenomenon known as the aesthetic–usability effect. We use automatic cognitive processing to determine at a visceral level if something is beautiful very quickly upon first seeing it, and this extends to digital interfaces as well. First impresions do matter. In this chapter, we’ll explore the origins of this principle, learn more about how our brains interpret information based on aesthetic attractiveness, and take a look at a few examples that make use of this effect",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Origins The origins of the aesthetic–usability effect can be traced back to a study conduted in 1995 by researchers Masaaki Kurosu and Kaori Kashimura at the Hitachi Design Center. 2 Prior to this, the relationship between aesthetics and digital interfaces had been largely unexplored. The study, which began as an attempt to investigate the relationship between inherent usability and something the researchers called “apparent usability,” demonstrated the correlation between people’s perceptions of ease of use and visual attractiveness. Kurosu and Kashimura tested 26 layout patterns of ATM interfaces ( Figure 7-1 ) with 252 participants and asked each of them to rate each design according to both functionality and aesthetics. The participants used a 10-point rating scale to evaluate the usability and visual attractiveness of each design. The results showed that their perception of usability was strongly influenced by their perception of the attractiveness of the interface ( Figure 7-2 ). In other words, apparent usability is less correlated with inherent ease of use than with apparent beauty. 3 Noam Tractinsky, Arthur Stanley Katz, and Dror Ikar, “What Is Beautiful Is Usable,” Interacting with Coputers 13, no. 2 (2000): 127–45. Subsequent research, such as the 2000 study “What Is Beautiful Is Usable” by Noam Tractinsky et al., corroborates Kurosu and Kashimura’s findings and further confirms that the aesthetics of the interface of a system affect users’ peception of the usability of the system. 3 The correlations between perceived attrativeness and other qualities (including trust and credibility) have also been explored, as have the effects of aesthetics on usability testing (see the Key Consieration sidebar “Effect on Usability Tests” ). Figure 7-1. Sample layout patterns (source: Kurosu and Kashimura, 1995) Figure 7-2",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 7-1. Sample layout patterns (source: Kurosu and Kashimura, 1995) Figure 7-2. Correlation of usability with beauty (source: Kurosu and Kashimura, 1995) PSYCHOLOGY CONCEPT Automatic Cognitive Processing Contrary to what we’ve been taught not to do, people do in fact judge books by their covers. This isn’t actually a bad thing, though—in fact, it’s necessary. Automatic cognitive processing is helpful because it enables us to react quickly. Carefully processing every object around us would be slow, inefficient, and in some circumstances dangerous, so we begin to mentally process information and form an opinion based on past experences before directing our conscious attention toward what we’re peceiving. This automatic and involuntary mode of thinking stands in contrast to the slower and more deliberate mode of thinking that follows, and it’s exactly what psychologist and economist Daniel Kahneman explores in his 2015 book Thinking, Fast and Slow (Farrar, Straus and Giroux). This psychodrama with two characters, System 1 and System 2, details the relationship between the two forms of cognitive processing and how it influences our decision making. System 1 operates impulsively and involves little or no psychological effort. It is quick, and there is no sense of voluntary control. This mode of thinking is among the innate abilities we share with other animals, and it enables us to recognize objects, identify danger, direct our attention, avoid loss, and quickly react based on experience or prolonged practice. System 1 is the system that runs automatically and generates informtion (intuitions, feelings, intentions, or impressions) for System 2. System 2 operates more slowly and requires mental effort. It is the system called upon when System 1 runs into difficulty, and it provides support in the form of more detailed and specific processing, with the goal of solving the problem at hand. This is the system of thinking that we use for complex problem solving that requires attention",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is the system of thinking that we use for complex problem solving that requires attention. Focus, research, searching memory, mathematical operations (beyond simple arithmetic), and situational awareness are all things that involve this mode of thinking. The interaction of these two systems centers around minimizing effort and optimizing performance. System 1 handles most of what we think and do, and System 2 takes over when necessary. The implications of this when it comes to digital products and experiences are monumetal. We rely on System 1 to quickly identify information relevant to our 4 Gitte Lindgaard, Gary Fernandes, Cathy Dudek, and J. Brown, “Attention Web Designers: You Have 50 Millseconds to Make a Good First Impression!,” Behaviour & Information Technology 25, no. 2 (2006): 111–26. tasks and to ignore information that isn’t instantly perceived to be relvant. We rapidly scan the available information in search of what will help us achieve our goal, and anything that isn’t a match is passed over. When it comes to the aesthetic–usability effect, System 1 thinking is incredibly important because this is where we form first impressions. In fact, stuies have shown that people form an opinion about a website within 50 milliseconds of seeing it, and that visual appeal is a primary determining factor. 4 Interestingly, the opinion formed during this brief period—the visceral response—rarely changes as users spend more time on the site. While our first impressions are not always foolproof, they usually are reltively accurate and help us to make quick decisions. Examples We’ll start our examples of the aesthetic–usability effect by looking closely at two companies that have put aesthetics at the center of what they do. First is Braun, the German electronics company, which has made an indelible mark in the world of design and exemplifies how aesthetically pleasing products can create a lasting impression",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". First is Braun, the German electronics company, which has made an indelible mark in the world of design and exemplifies how aesthetically pleasing products can create a lasting impression. Under the design direction of Dieter Rams, the company has influenced generations of designers with its products’ balance of functional minmalism and aesthetic beauty. Rams’s “less but better” approach, which emphaszes form following function, has directly resulted in some of the most weldesigned products ever produced. Take, for example, the Braun SK4 record player ( Figure 7-3 ), nicknamed “Snow White’s Coffin” due to its white metal casing and transparent lid. Costructed of powder-coated sheet metal with elmwood side panels, it stood in dratic contrast to the lavishly ornamented all-wood products more typically available to consumers at the time of its production in 1956. The SK4 was one of Braun’s first products to pioneer the company’s new contemporary industrial design laguage in which every detail had a functional purpose, including the plexiglass cover that resolved the rattling at higher volumes observed with metal covers. Products like this one mark a pivotal point in design history, when electronic devices went from being disguised as furniture to being presented as standalone entities that were beautiful and functional. Figure 7-3. Braun SK4 record player, designed by Hans Gugelot and Dieter Rams (source: Museum of Modern Art) Now let’s take a look at another example of a brand that in many ways cotinues Braun’s legacy of functional minimalism balanced by refined aesthetics: Apple. The influence of Braun’s design philosophy on Apple’s products is quite apparent. Devices such as the iPod, iPhone, and iMac echo the beautifully minmal aesthetic of Braun’s product lines while focusing on ease of use ( Figure 7-4 ). Apple’s attention to aesthetics extends beyond industrial design—the brand is well known for creating interfaces that are both elegant and easy to use ( Figure 7-5 )",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Apple’s attention to aesthetics extends beyond industrial design—the brand is well known for creating interfaces that are both elegant and easy to use ( Figure 7-5 ). In fact, its reputation in this regard has become a competitive advatage and helped to usher in a new era in which good design is fundamental to successful businesses. The attention to detail in everything the company creates has directly contributed to Apple becoming one of the most beloved brands in the world. That’s not to say that its products’ interfaces don’t have any usability issues, but people are much more likely to overlook these issues due to the pleaing aesthetic that’s at the core of the design—the aesthetic–usability effect at work. Figure 7-4. Apple iPod (top left), Apple iPhone (top middle), Apple iMac (top right), Braun T3 pocket radio (bottom left), Braun ET44 calculator (bottom middle), and Braun LE1 loudspeaker (bottom right) Figure 7-5. Screenshot of various Apple interface designs (source: Apple, 2019) 5 Andreas Sonderegger and and Juergen Sauer, “The Influence of Design Aesthetics in Usability Testing: Effects on User Performance and Perceived Usability,” Applied Ergonomics 41, no. 3 (2010): 403–10. KEY CONSIDERATION Effect on Usability Tests The positive benefits of aesthetically pleasing design come with a signifcant caveat. Since people tend to believe that beautiful experiences also work better, they can be more forgiving when it comes to usability issues. Psychologists Andreas Sonderegger and Juergen Sauer observed exactly how aesthetics affect usability tests. 5 Using a computer simulation of a mobile phone, 60 adolescents were asked to complete a number of comon tasks. Two separate simulations were used that were functionally identical but differentiated by their visual attractiveness—one was visally appealing (for the time), and the other notably unattractive ( Figure 7-6 ). Figure 7-6",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 7-6. The two prototypes employed in the experiment (source: Sonderegger and Sauer, 2010) Sonderegger and Sauer found that not only did participants rate usability higher for the more attractive phone (the model on the left), but the visual appearance of the phone “had a positive effect on perfomance, leading to reduced task completion times for the attractive model.” What this study implies is that perceived aesthetic quality has the potential to mask usability issues to an extent. This effect applies even when the device isn’t actually easier to use, and it could be prolematic when it comes to usability tests, where identifying issues is critical. Keeping in mind the potential of aesthetics to influence perceived usability, it is important that we mitigate this influence by listening to what users say when evaluating the usability of an experience and, more importantly, watching what they do. Asking questions that lead particpants to look beyond aesthetics can help to uncover usability issues and counter the effects that visual attractiveness can have on usability test results. Conclusion Aesthetically pleasing design can influence usability by creating a positive emtional response, which in turn enhances people’s cognitive abilities. When this happens, users tend to believe the design actually works better and are more likely to overlook minor usability issues. While this might seem like a good thing, it can actually mask usability problems and prevent issues from being dicovered during usability testing.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-65-75.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Interaction Design: beyond human-computer interaction, Fifth Edition Published by John Wiley & Sons, Inc. 10475 Crosspoint Boulevard Indianapolis, IN 46256 www.wiley.com Copyright © 2019 by John Wiley & Sons, Inc., Indianapolis, Indiana Published simultaneously in Canada ISBN: 978-1-119-54725-9 ISBN: 978-1-119-54735-8 (ebk) ISBN: 978-1-119-54730-3 (ebk) Manufactured in the United States of America 10 9 8 7 6 5 4 3 2 1 No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to the Copyright Clearance Center, 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax (978) 646-8600. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permissions. Limit of Liability/Disclaimer of Warranty: The publisher and the author make no representations or warranties with respect to the accuracy or completeness of the contents of this work and specifically disclaim all warranties, including without limitation warranties of fitness for a particular purpose. No warranty may be created or extended by sales or promotional materials. The advice and strategies contained herein may not be suitable for every situation. This work is sold with the understanding that the publisher is not engaged in rendering legal, accounting, or other professional services. If professional assistance is required, the services of a competent professional person should be sought. Neither the publisher nor the author shall be liable for damages arising herefrom",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If professional assistance is required, the services of a competent professional person should be sought. Neither the publisher nor the author shall be liable for damages arising herefrom. The fact that an organization or Web site is referred to in this work as a citation and/or a potential source of further information does not mean that the author or the publisher endorses the information the organization or website may provide or recommendations it may make. Further, readers should be aware that Internet websites listed in this work may have changed or disappeared between when this work was written and when it is read. For general information on our other products and services please contact our Customer Care Department within the United States at (877) 762-2974, outside the United States at (317) 572-3993 or fax (317) 572-4002. Wiley publishes in a variety of print and electronic formats and by print-on-demand. Some material included with standard print versions of this book may not be included in e-books or in print-odemand. If this book refers to media such as a CD or DVD that is not included in the version you purchased, you may download this material at http://booksupport.wiley.com. For more information about Wiley products, visit www.wiley.com. Library of Congress Control Number: 2019932998 W H A T I S I N T E R A C T I O N D E S I G N ? Objectives The main goals of this chapter are to accomplish the following: • Explain the difference between good and poor interaction design. • Describe what interaction design is and how it relates to human-computer interaction and other fields. • Explain the relationship between the user experience and usability. • Introduce what is meant by accessibility and inclusiveness in relation to humacomputer interaction. • Describe what and who is involved in the process of interaction design. • Outline the different forms of guidance used in interaction design",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Describe what and who is involved in the process of interaction design. • Outline the different forms of guidance used in interaction design. • Enable you to evaluate an interactive product and explain what is good and bad about it in terms of the goals and core principles of interaction design. 1.1 Introduction How many interactive products are there in everyday use? Think for a minute about what you use in a typical day: a smartphone, tablet, computer, laptop, remote control, coffee machine, ticket machine, printer, GPS, smoothie maker, e-reader, smart TV, alarm clock, electric tootbrush, watch, radio, bathroom scales, fitness tracker, game console the list is endless. Now think for a minute about how usable they are. How many are actually easy, effortless, and 1.1 Introduction 1.2 Good and Poor Design 1.3 What Is Interaction Design? 1.4 The User Experience 1.5 Understanding Users 1.6 Accessibility and Inclusiveness 1.7 Usability and User Experience Goals Chapter 1 enjoyable to use? Some, like the iPad, are a joy to use, where tapping an app and flicking through photos is simple, smooth, and enjoyable. Others, like working out how to buy the cheapest train ticket from a ticket machine that does not recognize your credit card after completing a number of steps and then makes you start again from scratch, can be very frutrating. Why is there a difference? Many products that req uire users to interact with them, such as smartphones and finess trackers, have been designed primarily with the user in mind. They are generally easy and enjoyable to use. Others have not necessarily been designed with the users in mind; rather, they have been engineered primarily as software systems to perform set functions. An example is setting the time on a stove that req uires a combination of button presses that are not obvious as to which ones to press together or separately. While they may work effectively, it can be at the expense of how easily they will be learned and therefore used in a real-world context",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While they may work effectively, it can be at the expense of how easily they will be learned and therefore used in a real-world context. Alan Cooper (2018), a well-known user experience (UX) guru, bemoans the fact that much of today’s software suffers from the same interaction errors that were around 20 years ago. Why is this still the case, given that interaction design has been in existence for more than 25 years and that there are far more UX designers now in industry than ever before? He points out how many interfaces of new products do not adhere to the interaction design principles validated in the 1990s. For example, he notes that many apps do not follow even the most basic of UX principles, such as offering an “undo” option. He exclaims that it is “inexplicable and unforgivable that these violations continue to resurface in new proucts today.” How can we rectify this situation so that the norm is that all new products are designed to provide good user experiences? To achieve this, we need to be able to understand how to reduce the negative aspects (such as frustration and annoyance) of the user experience while enhancing the positive ones (for example, enjoyment and efficacy). This entails deveoping interactive products that are easy, effective, and pleasurable to use from the users’ perspective. In this chapter, we begin by examining the basics of interaction design. We look at the difference between good and poor design, highlighting how products can differ radically in how usable and enjoyable they are. We then describe what and who is involved in the process of interaction design. The user experience, which is a central concern of interaction design, is then introduced. Finally, we outline how to characterize the user experience in terms of usability goals, user experience goals, and design principles. An in-depth activity is presented at the end of the chapter in which you have the opportunity to put into practice what you have read by evaluating the design of an interactive product",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". An in-depth activity is presented at the end of the chapter in which you have the opportunity to put into practice what you have read by evaluating the design of an interactive product. 1.2 Good and Poor Design A central concern of interaction design is to develop interactive products that are usable. By this we mean products that are generally easy to learn, effective to use, and provide an enjoable user experience. A good place to start thinking about how to design usable interactive products is to compare examples of well-designed and poorly designed ones. Through identfying the specific weaknesses and strengths of different interactive products, we can begin to understand what it means for something to be usable or not. Here, we describe two examples of poorly designed products that have persisted over the years—a voice-mail system used in hotels and the ubiq uitous remote control—and contrast these with two well-designed exaples of the same products that perform the same function. 1.2.1 Voice-Mail System Imagine the following scenario. You are staying at a hotel for a week while on a business trip. You see a blinking red light on the landline phone beside the bed. You are not sure what this means, so you pick up the handset. You listen to the tone and it goes “beep, beep, beep.” Maybe this means that there is a message for you. To find out how to access the message, you have to read a set of instructions next to the phone. You read and follow the first step: 1. Touch 41. The system responds: “You have reached the Sunny Hotel voice message center. Please enter the room number for which you would like to leave a message.” You wait to hear how to listen to a recorded message. But there are no further instrutions from the phone. You look down at the instruction sheet again and read: 2. Touch*, your room number, and #. You do so and the system replies: “You have reached the mailbox for room 106",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You look down at the instruction sheet again and read: 2. Touch*, your room number, and #. You do so and the system replies: “You have reached the mailbox for room 106. To leave a message, type in your password.” You type in the room number again, and the system replies: “Please enter room number again and then your password.” You don’t know what your password is. You thought it was the same as your room nuber, but clearly it is not. At this point, you give up and call the front desk for help. The person at the desk explains the correct procedure for listening to messages. This involves typing in, at the appropriate times, the room number and the extension number of the phone (the latter is the password, which is different from the room number). Moreover, it takes six steps to access a message. You give up. What is problematic with this voice-mail system? • It is infuriating. • It is confusing. • It is inefficient, req uiring you to carry out a number of steps for basic tasks. • It is difficult to use. • It has no means of letting you know at a glance whether any messages have been left or how many there are. You have to pick up the handset to find out and then go through a series of steps to listen to them. • It is not obvious what to do: The instructions are provided partially by the system and partially by a card beside the phone. Now compare it to the phone answering machine shown in Figure 1.1 The illustration shows a small sketch of a phone answering machine. Incoming messages are represented using marbles. The number of marbles that have moved into the pinball-like chute indicates the number of messages. Placing one of these marbles into a dent on the machine causes the recorded message to play. Dropping the same marble into a different dent on the phone dials the caller who left the message. How does the marble answering machine differ from the voice-mail system? • It uses familiar physical objects that indicate visually at a glance how many messages have been left",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". How does the marble answering machine differ from the voice-mail system? • It uses familiar physical objects that indicate visually at a glance how many messages have been left. • It is aesthetically pleasing and enjoyable to use. • It req uires only one-step actions to perform core tasks. • It is a simple but elegant design. • It offers less functionality and allows anyone to listen to any of the messages. The marble answering machine is considered a design classic. It was created by Durrell Bishop while he was a student at the Royal College of Art in London (described by Crampton Smith, 1995). One of his goals was to design a messaging system that represented its basic functionality in terms of the behavior of everyday objects. To do this, he capitalized on people’s everyday knowledge of how the physical world works. In particular, he made use of the ubiq - uitous everyday action of picking up a physical object and putting it down in another place. This is an example of an interactive product designed with the users in mind. The focus is on providing them with a pleasurable experience but one that also makes efficient the activity of receiving messages. However, it is important to note that although the marble answering machine is an elegant and usable design, it would not be practical in a hotel setting. One of the main reasons is that it is not robust enough to be used in public places; for instance, the marbles could easily get lost or be taken as souvenirs. Also, the need to identify the user before allowing the messages to be played is essential in a hotel setting. Therefore, when considering the design of an interactive product, it is important to cosider where it is going to be used and who is going to use it. The marble answering machine would be more suitable in a home setting—provided that there were no children around who might be tempted to play with the marbles! Video Durrell Bishop’s answering machine: http://vimeo.com/19930744",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 1.1 The marble answering machine Source: Adapted from Crampton Smith (1995) 1.2.2 Remote Control Every home entertainment system, be it the smart TV, set-top box, stereo system, and so forth, comes with its own remote control. Each one is different in terms of how it looks and works. Many have been designed with a dizzying array of small, multicolored, and doubllabeled buttons (one on the button and one above or below it) that often seem arbitrarily positioned in relation to one another. Many viewers, especially when sitting in their living rooms, find it difficult to locate the right ones, even for the simplest of tasks, such as pausing or finding the main menu. It can be especially frustrating for those who need to put on their reading glasses each time to read the buttons. The remote control appears to have been put together very much as an afterthought. In contrast, much effort and thought went into the design of the classic TiVo remote cotrol with the user in mind (see Figure 1.2). TiVo is a digital video recorder that was originally developed to enable the viewer to record TV shows. The remote control was designed with large buttons that were clearly labeled and logically arranged, making them easy to locate and use in conjunction with the menu interface that appeared on the TV screen. In terms of its physical form, the remote device was designed to fit into the palm of a hand, having a peanut shape. It also has a playful look and feel about it: colorful buttons and cartoon icons are used that are distinctive, making it easy to identify them. Figure 1.2 The TiVo remote control Source: https://business.tivo.com/ How was it possible to create such a usable and appealing remote device where so many others have failed? The answer is simple: TiVo invested the time and effort to follow a usecentered design process",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Specifically, TiVo’s director of product design at the time involved potential users in the design process, getting their feedback on everything from the feel of the device in the hand to where best to place the batteries, making them easy to replace but not prone to falling out. He and his design team also resisted the trap of “buttonitis” to which so many other remote controls have fallen victim; that is one where buttons breed like rabbits— a button for every new function. They did this by restricting the number of control buttons embedded in the device to the essential ones. Other functions were then represented as part of the menu options and dialog boxes displayed on the TV screen, which could then be selected via the core set of physical control buttons. The result was a highly usable and pleasing device that has received much praise and numerous design awards. DILEMMA What Is the Best Way to Interact with a Smart TV? A challenge facing smart TV providers is how to enable users to interact with online content. Viewers can select a whole range of content via their TV screens, but it involves scrolling through lots of menus and screens. In many ways, the TV interface has become more like a computer interface. This raises the q uestion of whether the remote control is the best input device to use for someone who sits on a sofa or chair that is some distance from the wide TV screen. Smart TV developers have addressed this challenge in a nuber of ways. An early approach was to provide an on-screen keyboard and numeric keypad that prsented a grid of alphanumeric characters (see Figure 1.3a), which were selected by pressing a button repeatedly on a remote control. However, entering the name of a movie or an email address and password using this method can be painstakingly slow; it is also easy to overshoot and select the wrong letter or number when holding a button down on the remote to reach a target character",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". More recent remote controls, such as those provided by Apple TV, incorporate a touchpad to enable swiping akin to the control commonly found on laptops. While this form of touch control expedites skipping through a set of letters displayed on a TV screen, it does not make it any easier to type in an email address and password. Each letter, number, or special character still has to be selected. Swiping is also prone to overshooing when aiming for a target letter, number, or character. Instead of providing a grid, the Apple TV interface displays two single lines of letters, numbers, and special characters to swipe across (see Figure 1.3b). While this can make it q uicker for someone to reach a character, it is still tedious to select a seq uence of characters in this way. For example, if you select a Y and the next letter is an A, you have to swipe all the way back to the begining of the alphabet. 1.2.1 What to Design Designing interactive products req uires considering who is going to be using them, how they are going to be used, and where they are going to be used. Another key concern is to understand the kind of activities people are doing when interacting with these proucts. The appropriateness of different kinds of interfaces and arrangements of input and output devices depends on what kinds of activities are to be supported. For example, if the activity is to enable people to bank online, then an interface that is secure, trusworthy, and easy to navigate is essential. In addition, an interface that allows the user to find out information about new services offered by the bank without it being intrusive would be useful. Might there be a better way to interact with a smart TV while sitting on the sofa? An alternative is to use voice control",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Might there be a better way to interact with a smart TV while sitting on the sofa? An alternative is to use voice control. Remote controls, like Siri or TiVo, for example, have a speech button that when pressed allows viewers to ask for movies by name or more generally by category, for instance, “What are the best sci-fi movies on Netflix?” Smart speakers, such as Amazon Echo, can also be connected to a smart TV via an HDMI port, and, similarly, the user can ask for something general or more specific, for example, “Alexa, play Big Bang Thory, Season 6, Episode 5, on the TV.” On recognizing the command, it will switch on the TV, switch to the right HDMI channel, open Netflix, and begin streaming the specific episode. Some TV content, however, req uires the viewer to say that they are over a certain age by checking a box on the TV display. If the TV could ask the viewer and check that they are over 18, then that would be really smart! Also, if the TV needs the viewer to provide a password to access on-demand content, they won’t want to say it out aloud, character by character, espcially in front of others who might also be in the room with them. The use of biometrics, then, may be the answer. (a) (b) Figure 1.3 Typing on a TV screen (a) by selecting letters and numbers from a square matrix and (b) by swiping along a single line of letters and numbers Source: (b) https://support.apple.com/en-us/HT200107 The world is becoming suffused with technologies that support increasingly diverse activities. Just think for a minute about what you can currently do using digital technology: send messages, gather information, write essays, control power plants, program, draw, plan, calculate, monitor others, and play games—just to name but a few. Now think about the types of interfaces and interactive devices that are available. They too are eq ually diverse: multitouch displays, speech-based systems, handheld devices, wearables, and large interactive displays—again, to name but a few",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". They too are eq ually diverse: multitouch displays, speech-based systems, handheld devices, wearables, and large interactive displays—again, to name but a few. There are also many ways of designing how users can interact with a system, for instance, via the use of menus, commands, forms, icons, gestures, and so on. Furthermore, ever more innovative everyday artifacts are being created using novel materials, such as e-textiles and wearables (see Figure 1.4). The Internet of Things (IoT) now means that many products and sensors can be conected to each other via the Internet, which enables them to talk to each other. Popular household IoT-enabled products include smart heating and lighting and home security sytems where users can change the controls from an app on their phone or check out who is knocking on their door via a doorbell webcam. Other apps that are being developed are meant to make life easier for people, like finding a car parking space in busy areas. The interfaces for everyday consumer items, such as cameras, microwave ovens, toasters, and washing machines, which used to be physical and the realm of product design, are now predominantly digitally based, req uiring interaction design (called consumer electronics). The move toward transforming human-human transactions into solely interface-based ones has also introduced a new kind of customer interaction. Self-checkouts at grocery stores and libraies are now the norm where it is commonplace for customers to check out their own goods or books themselves, and at airports, where passengers check in their own luggage. While more cost-effective and efficient, it is impersonal and puts the onus on the person to interact with the system. Furthermore, accidentally pressing the wrong button or standing in the wrong place at a self-service checkout can result in a frustrating, and sometimes mortifying, experience",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Furthermore, accidentally pressing the wrong button or standing in the wrong place at a self-service checkout can result in a frustrating, and sometimes mortifying, experience. Figure 1.4 Turn signal biking jacket using e-textiles developed by Leah Beuchley Source: Used courtesy of Leah Buechley What this all amounts to is a multitude of choices and decisions that interaction desigers have to make for an ever-increasing range of products. A key q uestion for interaction design is this: “How do you optimize the users’ interactions with a system, environment, or product so that they support the users’ activities in effective, useful, usable and pleasurable ways?” One could use intuition and hope for the best. Alternatively, one can be more pricipled in deciding which choices to make by basing them on an understanding of the users. This involves the following: • Considering what people are good and bad at • Considering what might help people with the way they currently do things • Thinking through what might provide q uality user experiences • Listening to what people want and getting them involved in the design • Using user-centered techniq ues during the design process The aim of this book is to cover these aspects with the goal of showing you how to carry out interaction design. In particular, it focuses on how to identify users’ needs and the context of their activities. From this understanding, we move on to consider how to design usable, useful, and pleasurable interactive products. 1.3 What Is Interaction Design? By interaction design, we mean the following: Designing interactive products to support the way people communicate and interact in their everyday and working lives Put another way, it is about creating user experiences that enhance and augment the way people work, communicate, and interact. More generally, Terry Winograd originally described it as “designing spaces for human communication and interaction” (1997, p. 160)",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". More generally, Terry Winograd originally described it as “designing spaces for human communication and interaction” (1997, p. 160). John Thackara viewed it as “the why as well as the how of our daily interactions using coputers” (2001, p. 50), while Dan Saffer emphasized its artistic aspects: “the art of facilitating interactions between humans through products and services” (2010, p. 4). A number of terms have been used since to emphasize different aspects of what is being designed, including user interface design (UI), software design, user-centered design, product design, web design, user experience design, and interactive system design. Interaction design is generally used as the overarching term to describe the field, including its methods, theories, and approaches. UX is used more widely in industry to refer to the profession. However, the terms can be used interchangeably. Also, it depends on their ethos and brand. 1.3.1 The Components of Interaction Design We view interaction design as fundamental to many disciplines, fields, and approaches that are concerned with researching and designing computer-based systems for people. Figure 1.5 presents the core ones along with interdisciplinary fields that comprise one or more of these, such as cognitive ergonomics. It can be confusing to try to work out the differences between them as many overlap. The main differences between interaction design and the other approaches referred to in the figure come largely down to which methods, philosophies, and lenses they use to study, analyze, and design products. Another way they vary is in terms of the scope and problems they address. For example, information systems is concerned with the application of computing technology in domains such as business, health, and education, whereas ubiq uitous computing is concerned with the design, development, and deployment of pervasive computing technologies (for example, IoT) and how they facilitate social inteactions and human experiences",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". BOX 1.1 Is Interaction Design Beyond HCI? We see the main difference between interaction design (ID) and human-computer interaction (HCI) as one of scope. Historically, HCI had a narrow focus on the design and usability of computing systems, while ID was seen as being broader, concerned with the theory, research, and practice of designing user experiences for all manner of technologies, systems, and proucts. That is one of the reasons why we chose to call our book Interaction Design: beyond human-computer interaction , to reflect this wider range. However, nowadays, HCI has greatly expanded in its scope (Churchill et al., 2013), so much so that it overlaps much more with ID (see Figure 1.6). Interaction Design Academic Disciplines Ergonomics Psychology/ Cognitive Science Informatics Design Engineering Computer Science/ Software Engineering Social Sciences (e.g., Sociology, Anthropology) Ubiquitous Computing Human Factors (HF) Cognitive Engineering Human-Computer Interaction (HCI) Cognitive Ergonomics Information Systems ComputeSupported Cooperative Work (CSCW) Film Industry Industrial Design Artist-Design Product Design Graphic Design Design Practices Interdisciplinary Overlapping Fields Figure 1.5 Relationship among contributing academic disciplines, design practices, and interdiscplinary fields concerned with interaction design (double-headed arrows mean overlapping) 1.3.2 Who Is Involved in Interaction Design? Figure 1.5 also shows that many people are involved in performing interaction design, raning from social scientists to movie-makers. This is not surprising given that technology has become such a pervasive part of our lives. But it can all seem rather bewildering to the onlooker. How does the mix of players work together? Designers need to know many different things about users, technologies, and the interations among them to create effective user experiences. At the least, they need to understand how people act and react to events and how they communicate and interact with each other",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". At the least, they need to understand how people act and react to events and how they communicate and interact with each other. To be able to create engaging user experiences, they also need to understand how emotions work, what is meant by aesthetics, desirability, and the role of narrative in human experence. They also need to understand the business side, technical side, manufacturing side, and marketing side. Clearly, it is difficult for one person to be well versed in all of these diverse areas and also know how to apply the different forms of knowledge to the process of interation design. Interaction design is ideally carried out by multidisciplinary teams, where the skill sets of engineers, designers, programmers, psychologists, anthropologists, sociologists, marketing people, artists, toy makers, product managers, and others are drawn upon. It is rarely the case, Figure 1.6 HCI out of the box: broadening its reach to cover more areas however, that a design team would have all of these professionals working together. Who to include in a team will depend on a number of factors, including a company’s design philosphy, size, purpose, and product line. One of the benefits of bringing together people with different backgrounds and training is the potential of many more ideas being generated, new methods developed, and more cretive and original designs being produced. However, the downside is the costs involved. The more people there are with different backgrounds in a design team, the more difficult it can be to communicate and make progress with the designs being generated. Why? People with different backgrounds have different perspectives and ways of seeing and talking about the world. What one person values as important others may not even see (Kim, 1990). Similarly, a computer scientist’s understanding of the term representation is often very different from that of a graphic designer or psychologist",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Similarly, a computer scientist’s understanding of the term representation is often very different from that of a graphic designer or psychologist. What this means in practice is that confusion, misunderstanding, and communication breakdowns can surface in a team. The various team members may have different ways of talking about design and may use the same terms to mean q uite different things. Other problems can arise when a group of people who have not previously worked as a team are thrown together. For example, Aruna Balakrishnan et al. (2011) found that integration across different disciplines and expertise is difficult in many projects, especially when it comes to agreeing on and sharing tasks. The more disparate the team members—in terms of culture, background, and organizational structures—the more complex this is likely to be. ACTIVITY 1.1 In practice, the makeup of a given design team depends on the kind of interactive product being built. Who do you think should be involved in developing • A public kiosk providing information about the exhibits available in a science museum? • An interactive educational website to accompany a TV series? Comment Ideally, each team will have a number of different people with different skill sets. For example, the first interactive product would include the following individuals: • Graphic and interaction designers, museum curators, educational advisers, software engneers, software designers, and ergonomists The second project would include these types of individuals: • TV producers, graphic and interaction designers, teachers, video experts, software engneers, and software designers In addition, as both systems are being developed for use by the general public, representtive users, such as school children and parents, should be involved. In practice, design teams often end up being q uite large, especially if they are working on a big project to meet a fixed deadline",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In practice, design teams often end up being q uite large, especially if they are working on a big project to meet a fixed deadline. For example, it is common to find teams of 15 or more people working on a new product like a health app. This means that a number of people from each area of expertise are likely to be working as part of the project team. 1.3.3 Interaction Design Consultancies Interaction design is now widespread in product and services development. In particular, website consultants and the computing industries have realized its pivotal role in successful interactive products. But it is not just IT companies that are realizing the benefits of having UXers on board. Financial services, retail, governments, and the public sector have realized too the value of interaction design. The presence or absence of good interaction design can make or break a company. Getting noticed in the highly competitive field of web products req uires standing out. Being able to demonstrate that your product is easy, effective, and engaging to use is seen as central to this. Marketing departments are also realizing how branding, the number of hits, the customer return rate, and customer satisfaction are greatly affected by the usability of a website. There are many interaction design consultancies now. These include established compnies, such as Cooper, NielsenNorman Group, and IDEO, and more recent ones that specialize in a particular area, such as job board software (for example, Madgex), digital media (think of Cogapp), or mobile design (such as CXpartners). Smaller consultancies, such as Bunnyfoot and Dovetailed, promote diversity, interdisciplinarity, and scientific user research, having psycholgists, researchers, interaction designers, usability, and customer experience specialists on board. Many UX consultancies have impressive websites, providing case studies, tools, and blogs",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Many UX consultancies have impressive websites, providing case studies, tools, and blogs. For example, Holition publishes an annual glossy booklet as part of its UX Series (Javornik et al., 2017) to disseminate the outcomes of their in-house research to the wider community, with a focus on the implications for commercial and cultural aspects. This shaing of UX knowledge enables them to contribute to the discussion about the role of technoogy in the user experience. 1.4 The User Experience The user experience refers to how a product behaves and is used by people in the real world. Jakob Nielsen and Don Norman (2014) define it as encompassing “all aspects of the enuser’s interaction with the company, its services, and its products.” As stressed by Jesse Garett (2010, p. 10), “Every product that is used by someone has a user experience: newspapers, ketchup bottles, reclining armchairs, cardigan sweaters.” More specifically, it is about how people feel about a product and their pleasure and satisfaction when using it, looking at it, holding it, and opening or closing it. It includes their overall impression of how good it is to use, right down to the sensual effect small details have on them, such as how smoothly a switch rotates or the sound of a click and the touch of a button when pressing it. An impotant aspect is the q uality of the experience someone has, be it a q uick one, such as taking a photo; a leisurely one, such as playing with an interactive toy; or an integrated one, such as visiting a museum (Law et al., 2009). It is important to point out that one cannot design a user experience, only design for a user experience. In particular, one cannot design a sensual experience, but only create the design features that can evoke it. For example, the outside case of a smartphone can be designed to be smooth, silky, and fit in the palm of a hand; when held, touched, looked at, and interacted with, that can provoke a sensual and satisfying user experience",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Conversely, if it is designed to be heavy and awkward to hold, it is much more likely to end up providing a poor user experience—one that is uncomfortable and unpleasant. Designers sometimes refer to UX as UXD. The addition of the D to UX is meant to encourage design thinking that focuses on the q uality of the user experience rather than on the set of design methods to use (Allanwood and Beare, 2014). As Don Norman (2004) has stressed for many years, “It is not enough that we build products that function, that are understandable and usable, we also need to build joy and excitement, pleasure and fun, and yes, beauty to people’s lives.” ACTIVITY 1.2 The iPod Phenomenon Apple’s classic (and subseq uent) generations of portable music players, called iPods, including the iPod Touch, Nano, and Shuffle, released during the early 2000s were a phenomenal success. Why do you think this occurred? Has there been any other product that has matched this q uality of experience? With the exception of the iPod Touch, Apple stopped production of them in 2017. Playing music via a smartphone became the norm, superseding the need for a separate device. Comment Apple realized early on that successful interaction design involves creating interactive proucts that have a q uality user experience. The sleek appearance of the iPod music player (see Figure 1.7), its simplicity of use, its elegance in style, its distinct family of rainbow colors, a novel interaction style that many people discovered was a sheer pleasure to learn and use, and the catchy naming of its product and content (iTunes, iPod), among many other design features, led to it becoming one of the greatest products of its kind and a must-have fashion item for teenagers, students, and adults alike",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While there were many competing players on the market at the time—some with more powerful functionality, others that were cheaper and easier to use, or still others with bigger screens, more memory, and so forth—the q uality of the overall user experience paled in comparison to that provided by the iPod. Figure 1.7 The iPod Nano Source: David Paul Morris / Getty Images There are many aspects of the user experience that can be considered and many ways of taking them into account when designing interactive products. Of central importance are the usability, functionality, aesthetics, content, look and feel, and emotional appeal. In addition, Jack Carroll (2004) stresses other wide-reaching aspects, including fun, health, social capital (the social resources that develop and are maintained through social networks, shared values, goals, and norms), and cultural identity, such as age, ethnicity, race, disability, family status, occupation, and education. Several researchers have attempted to describe the experiential aspect of a user experence. Kasper Hornbæk and Morten Hertzum (2017) note how it is often described in terms of the way that users perceive a product, such as whether a smartwatch is seen as sleek or chunky, and their emotional reaction to it, such as whether people have a positive experence when using it. Marc Hassenzahl’s (2010) model of the user experience is the most welknown, where he conceptualizes it in terms of pragmatic and hedonic aspects. By pragmatic, it is meant how simple, practical, and obvious it is for the user to achieve their goals. By hedonic, it is meant how evocative and stimulating the interaction is to them. In addition to a person’s perceptions of a product, John McCarthy and Peter Wright (2004) discuss the importance of their expectations and the way they make sense of their experiences when using technology. Their Technology as Experience framework accounts for the user experence largely in terms of how it is felt by the user",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Their Technology as Experience framework accounts for the user experence largely in terms of how it is felt by the user. They recognize that defining experience is incredibly difficult because it is so nebulous and ever-present to us, just as swimming in water is to a fish. Nevertheless, they have tried to capture the essence of human experience by describing it in both holistic and metaphorical terms. These comprise a balance of sensual, cerebral, and emotional threads. How does one go about producing q uality user experiences? There is no secret sauce or magical formula that can be readily applied by interaction designers. However, there are numerous conceptual frameworks, tried and tested design methods, guidelines, and relevant research findings, which are described throughout the book. 1.5 Understanding Users A main reason for having a better understanding of people in the contexts in which they live, work, and learn is that it can help designers understand how to design interactive products that provide good user experiences or match a user’s needs. A collaborative planning tool for a space mission, intended to be used by teams of scientists working in different parts of the world, will have q uite different needs from one targeted at customer and sales agents, to be used in a furniture store to draw up kitchen layout plans. Understanding individual The nearest overall user experience that has all of the above is not so much for a product but for a physical store. The design of the Apple Store as a completely new customer experence for buying technology has been very successful in how it draws people in and what they do when browsing, discovering, and purchasing goods in the store. The products are laid out in a way to encourage interaction. differences can also help designers appreciate that one size does not fit all; what works for one user group may be totally inappropriate for another. For example, children have different expectations than adults about how they want to learn or play",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, children have different expectations than adults about how they want to learn or play. They may find having inteactive q uizzes and cartoon characters helping them along to be highly motivating, whereas most adults find them annoying. Conversely, adults often like talking-head discussions about topics, but children find them boring. Just as everyday objects like clothes, food, and games are designed differently for children, teenagers, and adults, so too should interactive products be designed for different kinds of users. Learning more about people and what they do can also reveal incorrect assumptions that designers may have about particular user groups and what they need. For example, it is often assumed that because of deteriorating vision and dexterity, old people want things to be big—be it text or graphical elements appearing on a screen or the physical controls, like dials and switches, used to control devices. This may be true for some elderly people, but studies have shown that many people in their 70s, 80s, and older are perfectly capble of interacting with standard-size information and even small interfaces, for example, smartphones, just as well as those in their teens and 20s, even though, initially, some might think they will find it difficult (Siek et al., 2005). It is increasingly the case that as people get older, they do not like to consider themselves as lacking in cognitive and manual skills. Being aware of people’s sensitivities, such as aging, is as important as knowing how to design for their capabilities (Johnson and Finn, 2017). In particular, while many older adults now feel comfortable with and use a range of technologies (for instance, email, online shoping, online games, or social media), they may resist adopting new technologies",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is not because they don’t perceive them as being useful to their lives but because they don’t want to waste their time getting caught up by the distractions that digital life brings (Knowles and Hanson, 2018), for example, not wanting to be “glued to one’s mobile phone” like younger generations. Being aware of cultural differences is also an important concern for interaction design, particularly for products intended for a diverse range of user groups from different countries. An example of a cultural difference is the dates and times used in different countries. In the United States, for example, the date is written as month, day, year (05/21/20), whereas in other countries, it is written in the seq uence of day, month, year (21/05/20). This can cause problems for designers when deciding on the format of online forms, especially if intended for global use. It is also a concern for products that have time as a function, such as operating systems, digital clocks, or car dashboards. To which cultural group do they give preference? How do they alert users to the format that is set as default? This raises the q uestion of how easily an interface designed for one user group can be used and accepted by another. Why is it that certain products, like a fitness tracker, are universally accepted by people from all parts of the world, whereas websites are designed differently and reacted to differently by people from different cultures? To understand more about users, we have included three chapters (Chapters 4–6) that explain in detail how people act and interact with one another, with information, and with various technologies, together with describing their abilities, emotions, needs, desires, and what causes them to get annoyed, frustrated, lose patience, and get bored. We draw upon relevant psychological theory and social science research. Such knowledge enables designers to determine which solutions to choose from the many design alternatives available and how to develop and test these further",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Such knowledge enables designers to determine which solutions to choose from the many design alternatives available and how to develop and test these further. 1.6 Accessibility and Inclusiveness Accessibility refers to the extent to which an interactive product is accessible by as many people as possible. Companies like Google and Apple provide tools for their developers to promote this. The focus is on people with disabilities. For example, Android OS provides a range of tools for those with disabilities, such as hearing aid compatibility to a built-in screen reader, while Apple VoiceOver lets the user know what’s happening on its devices, so they can easily navigate and even know who is in a selfie just taken, by listening to the phone. Inclusiveness means being fair, open, and eq ual to everyone. Inclusive design is an ovearching approach where designers strive to make their products and services accommodate the widest possible number of people. An example is ensuring that smartphones are being designed for all and made available to everyone—regardless of their disability, education, age, or income. Whether or not a person is considered to be disabled changes over time with age, or as recovery from an accident progresses throughout their life. In addition, the severity and impact of an impairment can vary over the course of a day or in different environmental conditions. Disability can result because technologies are often designed in such a way as to necessitate a certain type of interaction that is impossible for someone with an impairment. Disability in this context is viewed as the result of poor interaction design between a user and the technology, not the impairment alone. Accessibility, on the other hand, opens up experences so that they are accessible to all. Technologies that are now mainstream once started out as solutions to accessibility challenges. For example, SMS was designed for hearing-impaired people before it became a mainstream technology",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Technologies that are now mainstream once started out as solutions to accessibility challenges. For example, SMS was designed for hearing-impaired people before it became a mainstream technology. Furthermore, designing for accessibility inherently results in inclusive design for all. Accessibility can be achieved in two ways: first, through the inclusive design of technology, and second, through the design of assistive technology. When designing for accessibility, it is essential to understand the types of impairments that can lead to diability as they come in many forms. They are often classified by the type of impairment, for example: • Sensory impairment (such as loss of vision or hearing) • Physical impairment (having loss of functions to one or more parts of the body, for exaple, after a stroke or spinal cord injury) • Cognitive (for instance, learning impairment or loss of memory/cognitive function due to old age or a condition such as Alzheimer’s disease) Within each type is a complex mix of people and capabilities. For example, a person might have only peripheral vision, be color blind, or have no light perception (and be regitered blind). All are forms of visual impairment, and all req uire different design approaches. Color blindness can be overcome by an inclusive design approach. Designers can choose colors that will appear as separate colors to everyone. However, peripheral vision loss or complete blindness will often need an assistive technology to be designed. Impairment can also be categorized as follows: • Permanent (for example, long-term wheelchair user) • Temporary (such as after an accident or illness) • Situational (for instance, a noisy environment means a person can’t hear) The number of people living with permanent disability increases with age. Fewer than 20 percent of people are born with a disability, whereas 80 percent of people will have a disability once they reach 85. As people age, their functional abilities diminish",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Fewer than 20 percent of people are born with a disability, whereas 80 percent of people will have a disability once they reach 85. As people age, their functional abilities diminish. For exaple, people older than 50 often find it difficult to hear conversations in rooms with hard surfaces and lots of background noise. This is a disability that will come to most of us at some point. People with permanent disabilities often use assistive technology in their everyday life, which they consider to be life-essential and an extension of their self (Holloway and Dawes, 2016). Examples include wheelchairs (people now refer to “wearing their wheels,” rather than “using a wheelchair”) and augmented and alternative communication aids. Much curent HCI research into disability explores how new technologies, such as IoT, wearables, and virtual reality, can be used to improve upon existing assistive technologies. Aimee Mullens is an athlete, actor, and fashion model who has shown how prosthetics can be designed to move beyond being purely functional (and often ugly) to being desirable and highly fashionable. She became a bilateral amputee when her legs were amputated below the knee as a one-year-old. She has done much to blur the boundary between disabled and nondisabled people, and she uses fashion as a tool to achieve this. Several prosthetic compnies now incorporate fashion design into their products, including striking leg covers that are affordable by all (see Figure 1.8). Figure 1.8 Fashionable leg cover designed by Alleles Design Studio Source: https://alleles.ca/. Used courtesy of Alison Andersen 1.7 Usability and User Experience Goals Part of the process of understanding users is to be clear about the primary objective of deveoping an interactive product for them",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Is it to design an efficient system that will allow them to be highly productive in their work? Is it to design a learning tool that will be challenging and motivating? Or, is it something else? To help identify the objectives, we suggest classifing them in terms of usability and user experience goals. Traditionally, usability goals are concerned with meeting specific usability criteria, such as efficiency, whereas user experience goals are concerned with explicating the nature of the user experience, for instance, to be aesthetically pleasing. It is important to note, however, that the distinction between the two types of goals is not clear-cut since usability is often fundamental to the q uality of the user experience and, conversely, aspects of the user experience, such as how it feels and looks, are inextricably linked with how usable the product is. We distinguish between them here to help clarify their roles but stress the importance of considering them together when designing for a user experience. Also, historically HCI was concerned primarily with usability, but it has since become concerned with understanding, designing for, and evaluating a wider range of user experience aspects. 1.7.1 Usability Goals Usability refers to ensuring that interactive products are easy to learn, effective to use, and enjoyable from the user’s perspective. It involves optimizing the interactions people have with interactive products to enable them to carry out their activities at work, at school, and in their everyday lives. More specifically, usability is broken down into the following six goals: • Effective to use (effectiveness) • Efficient to use (efficiency) • Safe to use (safety) • Having good utility (utility) • Easy to learn (learnability) • Easy to remember how to use (memorability) Usability goals are typically operationalized as q uestions. The purpose is to provide the interaction designer with a concrete means of assessing various aspects of an interactive product and the user experience",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The purpose is to provide the interaction designer with a concrete means of assessing various aspects of an interactive product and the user experience. Through answering the q uestions, designers can be alerted very early on in the design process to potential design problems and conflicts that they might not have considered. However, simply asking “Is the system easy to learn?” is not going to be very helpful. Asking about the usability of a product in a more detailed way—for example, “How long will it take a user to figure out how to use the most basic functions for a new smartwatch; how much can they capitalize on from their prior experience; and how long would it take the user to learn the whole set of functions?”—will elicit far more information. The following are descriptions of the usability goals and a q uestion for each one: (i) Effectiveness is a general goal, and it refers to how good a product is at doing what it is supposed to do. Question: Is the product capable of allowing people to learn, carry out their work effciently, access the information that they need, or buy the goods that they want? (ii) Efficiency refers to the way a product supports users in carrying out their tasks. The marble answering machine described earlier in this chapter was considered efficient in that it let the user carry out common tasks, for example, listening to messages, through a minimal number of steps. In contrast, the voice-mail system was considered inefficient because it req uired the user to carry out many steps and learn an arbitrary set of seq uences for the same common task. This implies that an efficient way of supporting common tasks is to let the user use single button or key presses. An example of where this kind of efficiency mechanism has been employed effectively is in online shopping. Once users have entered all of the necessary personal details in an online form to make a purchase, they can let the website save all of their personal details",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Once users have entered all of the necessary personal details in an online form to make a purchase, they can let the website save all of their personal details. Then, if they want to make another purchase at that site, they don’t have to re-enter all of their personal details. A highly successful mechanism patented by Amazon.com is the one-click option, which req uires users to click only a single button when they want to make another purchase. Question: Once users have learned how to use a product to carry out their tasks, can they sustain a high level of productivity? (iii) Safety involves protecting the user from dangerous conditions and undesirable situtions. In relation to the first ergonomic aspect, it refers to the external conditions where people work. For example, where there are hazardous conditions—such as X-ray machines or toxic chemicals—operators should be able to interact with and control computer-based systems remotely. The second aspect refers to helping any kind of user in any kind of situation to avoid the dangers of carrying out unwanted actions accidetally. It also refers to the perceived fears that users might have of the conseq uences of making errors and how this affects their behavior. Making interactive products safer in this sense involves (1) preventing the user from making serious errors by reducing the risk of wrong keys/buttons being mistakenly activated (an example is not placing the q uit or delete-file command right next to the save command on a menu) and (2) proviing users with various means of recovery should they make errors, such as an undo funtion. Safe interactive systems should engender confidence and allow the user the opportunity to explore the interface to try new operations (see Figure 1.9a)",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Safe interactive systems should engender confidence and allow the user the opportunity to explore the interface to try new operations (see Figure 1.9a). Another safety mechanism is confirming dialog boxes that give users another chance to consider their intentions (a well-known example is the appearance of a dialog box after issuing the command to delete everything in the trash, saying: “Are you sure you want to remove the items in the Trash permanently?”) (see Figure 1.9b). Question: What is the range of errors that are possible using the product, and what measures are there to permit users to recover easily from them? (iv) Utility refers to the extent to which the product provides the right kind of functionality so that users can do what they need or want to do. An example of a product with high utility is an accounting software package that provides a powerful computational tool that accountants can use to work out tax returns. An example of a product with low utility is a software drawing tool that does not allow users to draw freehand but forces them to use a mouse to create their drawings, using only polygon shapes. Question: Does the product provide an appropriate set of functions that will enable users to carry out all of their tasks in the way they want to do them? (v) Learnability refers to how easy a system is to learn to use. It is well known that people don’t like spending a long time learning how to use a system. They want to get started right away and become competent at carrying out tasks without too much effort. This is especially true for interactive products intended for everyday use (for example social media, email, or a GPS) and those used only infreq uently (for instance, online tax forms). To a certain extent, people are prepared to spend a longer time learning more complex systems that provide a wider range of functionality, such as web authoring tools. In these situations, pop-up tutorials can help by providing contextualized step-by-step material with hands-on exercises",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In these situations, pop-up tutorials can help by providing contextualized step-by-step material with hands-on exercises. A key concern is determining how much time users are prepared to spend learing a product. It seems like a waste if a product provides a range of functionality that the majority of users are unable or unprepared to spend the time learning how to use. Question: Is it possible for the user to work out how to use the product by exploring the interface and trying certain actions? How hard will it be to learn the whole set of funtions in this way? (vi) Memorability refers to how easy a product is to remember how to use, once learned. This is especially important for interactive products that are used infreq uently. If users haven’t used an operation for a few months or longer, they should be able to remember or at least rapidly be reminded how to use it. Users shouldn’t have to keep relearning how to carry (a) (b) Figure 1.9 (a) A safe and unsafe menu. Which is which and why? (b) A warning dialog box for Mac OS X out tasks. Unfortunately, this tends to happen when the operations req uired to be learned are obscure, illogical, or poorly seq uenced. Users need to be helped to remember how to do tasks. There are many ways of designing the interaction to support this. For example, users can be helped to remember the seq uence of operations at different stages of a task through contextualized icons, meaningful command names, and menu options. Also, structuring options and icons so that they are placed in relevant categories of options, for example, placing all of the drawing tools in the same place on the screen, can help the user remember where to look to find a particular tool at a given stage of a task. Question: What types of interface support have been provided to help users remember how to carry out tasks, especially for products and operations they use infreq uently? In addition to couching usability goals in terms of specific q uestions, they are turned into usability criteria",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These are specific objectives that enable the usability of a product to be assessed in terms of how it can improve (or not improve) a user’s performance. Examples of commonly used usability criteria are time to complete a task (efficiency), time to learn a task (learnability), and the number of errors made when carrying out a given task over time (memorability). These can provide q uantitative indicators of the extent to which productivity has increased, or how work, training, or learning have been improved. They are also useful for measuring the extent to which personal, public, and home-based products support leisure and information gathering activities. However, they do not address the overall q uality of the user experience, which is where user experience goals come into play. 1.7.2 User Experience Goals A diversity of user experience goals has been articulated in interaction design, which covers a range of emotions and felt experiences. These include desirable and undesirable ones, as shown in Table 1.1. Desirable aspects Satisfying Helpful Fun Enjoyable Motivating Provocative Engaging Challenging Surprising Pleasurable Enhancing sociability Rewarding Exciting Supporting creativity Emotionally fulfilling Entertaining Cognitively stimulating Experiencing flow Undesirable aspects Boring Unpleasant Frustrating Patronizing Making one feel guilty Making one feel stupid Annoying Cutesy Childish Gimmicky Table 1.1 Desirable and undesirable aspects of the user experience Many of these are subjective q ualities and are concerned with how a system feels to a user. They differ from the more objective usability goals in that they are concerned with how users experience an interactive product from their perspective, rather than assessing how useful or productive a system is from its own perspective. Whereas the terms used to describe usability goals comprise a small distinct set, many more terms are used to describe the mutifaceted nature of the user experience. They also overlap with what they are referring to",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". They also overlap with what they are referring to. In so doing, they offer subtly different options for expressing the way an experience varies for the same activity over time, technology, and place. For example, we may describe listening to music in the shower as highly pleasurable, but consider it more apt to describe listening to music in the car as enjoyable. Similarly, listening to music on a high-end powerful music system may invoke exciting and emotionally fulfilling feelings, while listening to it on a smartphone that has a shuffle mode may be serendipitously enjoyable, especially not knoing what tune is next. The process of selecting terms that best convey a user’s feelings, state of being, emotions, sensations, and so forth when using or interacting with a product at a given time and place can help designers understand the multifaceted and changing nature of the user experience. The concepts can be further defined in terms of elements that contribute to making a user experience pleasurable, fun, exciting, and so on. They include attention, pace, play, interactivity, conscious and unconscious control, style of narrative, and flow. The concept of flow (Csikszentmihalyi, 1997) is popular in interaction design for informing the design of user experiences for websites, video games, and other interactive products. It refers to a state of intense emotional involvement that comes from being completely involved in an activity, like playing music, and where time flies. Instead of designing web interfaces to cater to vistors who know what they want, they can be designed to induce a state of flow, leading the visitor to some unexpected place, where they become completely absorbed",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In an interview with Wired magazine, Mihaly Csikszentmihalyi (1996) uses the analogy of a gourmet meal to describe how a user experience can be designed to be engrossing, “starting off with the appetizers, moving on to the salads and entrées, and building toward dessert and not knoing what will follow.” The q uality of the user experience may also be affected by single actions performed at an interface. For example, people can get much pleasure from turning a knob that has the perfect level of gliding resistance; they may enjoy flicking their finger from the bottom of a smartphone screen to reveal a new menu, with the effect that it appears by magic, or enjoy the sound of trash being emptied from the trashcan on a screen. These one-off actions can be performed infreq uently or several times a day—which the user never tires of doing. Dan Saffer (2014) has described these as micro-interactions and argues that designing these moments of interaction at the interface—despite being small—can have a big impact on the user experience. ACTIVITY 1.3 There are more desirable than undesirable aspects of the user experience listed in Table 1.1. Why do you think this is so? Should you consider all of these when designing a product? ( Continued ) Comment The two lists we have come up with are not meant to be exhaustive. There are likely to be more—both desirable and undesirable—as new products surface. The reason for there being more of the former is that a primary goal of interaction design is to create positive experences. There are many ways of achieving this. Not all usability and user experience goals will be relevant to the design and evaluation of an interactive product being developed. Some combinations will also be incompatible. For example, it may not be possible or desirable to design a process control system that is both safe and fun. Recognizing and understanding the nature of the relationship between usability and user experience goals is central to interaction design",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Recognizing and understanding the nature of the relationship between usability and user experience goals is central to interaction design. It enables designers to become aware of the conseq uences of pursuing different combinations when designing products and higlighting potential trade-offs and conflicts. As suggested by Jack Carroll (2004), articulating the interactions of the various components of the user’s experience can lead to a deeper and more significant interpretation of the role of each component. BOX 1.3 Beyond Usability: Designing to Persuade Eric Schaffer (2009) argues that we should be focusing more on the user experience and less on usability. He points out how many websites are designed to persuade or influence rather than enable users to perform their tasks in an efficient manner. For example, many online shopping sites are in the business of selling services and products, where a core strategy is to entice people to buy what they might not have thought they needed. Online shopping experences are increasingly about persuading people to buy rather than being designed to make shopping easy. This involves designing for persuasion, emotion, and trust, which may or may not be compatible with usability goals. This entails determining what customers will do, whether it is to buy a product or renew a membership, and it involves encouraging, suggesting, or reminding the user of things that they might like or need. Many online travel sites try to lure visitors to purchase additional items (such as hotels, insurance, car rental, car parking, or day trips) besides the flight they originally wanted to book, and they will add a list full of tempting graphics to the visitor’s booking form, which then has to be scrolled through before being able to complete the tranaction",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These opportunities need to be designed to be eye-catching and enjoyable, in the same way that an array of products are attractively laid out in the aisles of a grocery store that one is req uired to walk past before reaching one’s desired product. Some online sites, however, have gone too far, for example, adding items to the cutomer’s shopping basket (for example, insurance, special delivery, and care and handling) that the shopper has to deselect if not desired or start all over again. This sneaky add-on approach can often result in a negative experience. More generally, this deceptive approach to UX has been described by Harry Brignull as dark patterns (see http://darkpatterns.org/). Shoppers often become annoyed if they notice decisions that add cost to their purchase have been made on their behalf without even being asked. For example, on clicking the unsubscribe button on the website of a car rental company, as indicated in Figure 1.10, the user is taken to another page where they have to uncheck additional boxes and then Update. They are then taken to yet another page where they are asked for their reason. The next screen says “Your email preferences have been updated. Do you need to hire a vehicle?” without letting the user know whether they have been unsubscribed from their mailing list. ( Continued ) Email preferences y.rogers@ucl.ac.uk Uncheck the emails you do not want to receive Newsletters UK * required fields NiftyCars Partners offers About your rental Update Email preferences We’d love to get some feedback on why you’re unsubscribing. Emails were too frequent Update Emails were not relevant I am no longer interested in this content I never signed up for newsletters from NiftyCars Figure 1.10 Dark pattern for a car rental company 1.7.3 Design Principles Design principles are used by interaction designers to aid their thinking when designing for the user experience. These are generalizable abstractions intended to orient designers toward thinking about different aspects of their designs",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These are generalizable abstractions intended to orient designers toward thinking about different aspects of their designs. A well-known example is feedback: Products should be designed to provide adeq uate feedback to the users that informs them about what has already been done so that they know what to do next in the interface. Another one that is important is findability (Morville, 2005). This refers to the degree to which a particular object is easy to discover or locate—be it navigating a website, moving through a building, or finding the delete image option on a digital camera. Related to this is the principle of navigability: Is it obvious what to do and where to go in an interface; are the menus structured in a way that allows the user to move smoothly through them to reach the option they want? Design principles are derived from a mix of theory-based knowledge, experience, and comon sense. They tend to be written in a prescriptive manner, suggesting to designers what to provide and what to avoid at the interface—if you like, the dos and don’ts of interaction design. More specifically, they are intended to help designers explain and improve their designs (Thibleby, 1990). However, they are not intended to specify how to design an actual interface, for instance, telling the designer how to design a particular icon or how to structure a web portal, but to act more like triggers for designers, ensuring that they provide certain features in an interface. A number of design principles have been promoted. The best known are concerned with how to determine what users should see and do when carrying out their tasks using an interactive product. Here we briefly describe the most common ones: visibility, feedback, constraints, consistency, and affordance. Visibility The importance of visibility is exemplified by our contrasting examples at the beginning of the chapter. The voice-mail system made the presence and number of waiting messages inviible, while the answering machine made both aspects highly visible",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The voice-mail system made the presence and number of waiting messages inviible, while the answering machine made both aspects highly visible. The more visible functions are, the more likely it is that users will be able to know what to do next. Don Norman (1988) describes the controls of a car to emphasize this point. The controls for different operations are clearly visible, such as indicators, headlights, horn, and hazard warning lights, indicating what The key is to nudge people in subtle and pleasant ways with which they can trust and feel comfortable. Natasha Loma (2018) points out how dark pattern design is “deception and dishonesty by design.” She describes in a TechCrunch article the many kinds of dark patterns that are now used to deceive users. A well-known example that most of us have experienced is unsubscribing from a marketing mailing list. Many sites go to great lengths to make it dificult for you to leave; you think you have unsubscribed, but then you discover that you need to type in your email address and click several more buttons to reaffirm that you really want to q uit. Then, just when you think you are safe, they post a survey asking you to answer a few q uestions about why you want to leave. Like Harry Brignull, she argues that companies should adopt fair and ethical design where users have to opt in to any actions that benefit the company at the expense of the users’ interests. can be done. The relationship between the way the controls have been positioned in the car and what they do makes it easy for the driver to find the appropriate control for the task at hand. In contrast, when functions are out of sight, it makes them more difficult to find and to know how to use. For example, devices and environments that have become automated through the use of sensor technology (usually for hygiene and energy-saving reasons)—like faucets, elevators, and lights—can sometimes be more difficult for people to know how to control, especially how to activate or deactivate them",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This can result in people getting caught short and frustrated. Figure 1.11 shows a sign that explains how to use the automatically controlled faucet for what is normally an everyday and well-learned activity. It also states that the faucets cannot be operated if wearing black clothing. It does not explain, however, what to do if you are wearing black clothing! Increasingly, highly visible controlling devices, like knobs, buttons, and switches, which are intuitive to use, have been replaced by invisible and ambiguous activating zones where people have to guess where to move their hands, boies, or feet—on, into, or in front of—to make them work. Feedback Related to the concept of visibility is feedback. This is best illustrated by an analogy to what everyday life would be like without it. Imagine trying to play a guitar, slice bread using a knife, or write using a pen if none of the actions produced any effect for several seconds. Figure 1.11 A sign in the restrooms at the Cincinnati airport Source: http://www.baddesigns.com There would be an unbearable delay before the music was produced, the bread was cut, or the words appeared on the paper, making it almost impossible for the person to continue with the next strum, cut, or stroke. Feedback involves sending back information about what action has been done and what has been accomplished, allowing the person to continue with the activity. Various kinds of feedback are available for interaction design—audio, tactile, verbal, visual, and combinations of these. Deciding which combinations are appropriate for different types of activities and interactivities is central. Using feedback in the right way can also provide the necessary viibility for user interaction. Constraints The design concept of constraining refers to determining ways of restricting the kinds of user inteaction that can take place at a given moment. There are various ways that this can be achieved",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". There are various ways that this can be achieved. A common design practice in graphical user interfaces is to deactivate certain menu options by shading them gray, thereby restricting the user only to actions permissible at that stage of the activity (see Figure 1.12). One of the advantages of this form of constraining is that it prevents the user from selecting incorrect options and thereby reduces the chance of making a mistake. The use of different kinds of graphical representations can also constrain a person’s interpretation of a problem or information space. For example, flow chart diagrams show which objects are related to which, thereby constraining the way that the information can be perceived. The physical design of a device can also constrain how it is used; for example, the Figure 1.12 A menu showing restricted availability of options as an example of logical constraining. Gray text indicates deactivated options. Source: https://www.ucl.ac.uk external slots in a computer have been designed to allow a cable or card to be inserted in a certain way only. Sometimes, however, the physical constraint is ambiguous, as shown in Fiure 1.13. The figure shows part of the back of a computer. There are two sets of connectors; the two on the right are for a mouse and a keyboard. They look identical and are physically constrained in the same way. How do you know which is which? Do the labels help? Consistency This refers to designing interfaces to have similar operations and use similar elements for achieing similar tasks. In particular, a consistent interface is one that follows rules, such as using the same operation to select all objects. For example, a consistent operation is using the same input action to highlight any graphical object on the interface, such as always clicking the left mouse button. Inconsistent interfaces, on the other hand, allow exceptions to a rule",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Inconsistent interfaces, on the other hand, allow exceptions to a rule. An example is where certain graphical objects (for example, email messages presented in a table) can be higlighted only by using the right mouse button, while all other operations are highlighted using the left mouse button. The problem with this kind of inconsistency is that it is q uite arbitrary, making it difficult for users to remember and making its use more prone to mistakes. One of the benefits of consistent interfaces, therefore, is that they are easier to learn and use. Users have to learn only a single mode of operation that is applicable to all objects. This principle works well for simple interfaces with limited operations, such as a portable radio with a small number of operations mapped onto separate buttons. Here, all the user has to do is to learn what each button represents and select accordingly. However, it can be more problematic to apply the concept of consistency to more complex interfaces, especially when many different operations need to be designed. For example, consider how to design an inteface for an application that offers hundreds of operations, such as a word-processing applcation. There is simply not enough space for a thousand buttons, each of which maps to an individual operation. Even if there were, it would be extremely difficult and time-consuming for the user to search through all of them to find the desired operation. A much more effetive design solution is to create categories of commands that can be mapped into subsets of operations that can be displayed at the interface, for instance, via menus. Figure 1.13 Ambiguous constraints on the back of a computer Source: http://www.baddesigns.com Affordance This is a term used to refer to an attribute of an object that allows people to know how to use it. For example, a mouse button invites pushing (in so doing, activating clicking) by the way it is physically constrained in its plastic shell",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, a mouse button invites pushing (in so doing, activating clicking) by the way it is physically constrained in its plastic shell. At a simple level, to afford means “to give a clue”’ (Norman, 1988). When the affordances of a physical object are perceptually obvious, it is easy to know how to interact with it. For example, a door handle affords pulling, a cup handle affords grasping, and a mouse button affords pushing. The term has since been much popularized in interaction design, being used to describe how interfaces should make it obvous as to what can be done when using them. For example, graphical elements like buttons, icons, links, and scrollbars are discussed with respect to how to make it appear obvious how they should be used: icons should be designed to afford clicking, scrollbars to afford moving up and down, and buttons to afford pushing. Don Norman (1999) suggests that there are two kinds of affordance: perceived and real. Physical objects are said to have real affordances, like grasping, that are perceptually obvious and do not have to be learned. In contrast, user interfaces that are screen-based are virtual and do not have these kinds of real affordances. Using this distinction, he argues that it does not make sense to try to design for real affordances at the interface, except when designing physical devices, like control consoles, where affordances like pulling and pressing are helful in guiding the user to know what to do. Alternatively, screen-based interfaces are better conceptualized as perceived affordances, which are essentially learned conventions. However, watching a one-year-old swiping smartphone screens, zooming in and out on images with their finger and thumb, and touching menu options suggests that kind of learning comes naturally. Applying Design Principles in Practice One of the challenges of applying more than one of the design principles in interaction design is that trade-offs can arise among them",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Applying Design Principles in Practice One of the challenges of applying more than one of the design principles in interaction design is that trade-offs can arise among them. For example, the more you try to constrain an interface, the less visible information becomes. The same can also happen when trying to apply a single design principle. For example, the more an interface is designed to afford through trying to resemble the way physical objects look, the more it can become clutered and difficult to use. It can also be the case that the more an interface is designed to be aesthetic, the less usable it becomes. Consistency can be a problematic design principle; trying to design an interface to be consistent with something can make it inconsistent with something else. Furthermore, sometimes inconsistent interfaces are actually easier to use than consistent interfaces. This is illustrated by Jonathan Grudin’s classic (1989) use of the analogy of where knives are stored in a house. Knives come in a variety of forms, including butter knives, steak knives, table knives, and fish knives. An easy place to put them all and subseq uently locate them is in the top drawer by the sink. This makes it easy for everyone to find them and follows a simple consistent rule. But what about the knives that don’t fit or are too sharp to put in the drawer, like carving knives and bread knives? They are placed in a wooden block. And what about the best knives kept only for special occasions? They are placed in the cabinet in another room for safekeeping. And what about other knives like putty knives and paint-scraping knives used in home improvement projects (kept in the garage) and jack-knives (kept in one’s pockets or backpack)? Very q uickly, the consistency rule begins to break down. Jonathan Grudin notes how, in extending the number of places where knives are kept, inconsistency is introduced, which in turn increases the time needed to learn where they are all stored",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Jonathan Grudin notes how, in extending the number of places where knives are kept, inconsistency is introduced, which in turn increases the time needed to learn where they are all stored. However, the placement of the knives in different places often makes it easier to find them because they are at hand for the context in which they are used and are also next to the other objects used for a specific task; for instance, all of the home improvement project tools are stored together in a box in the garage. The same is true when designing interfaces: introducing inconsistency can make it more difficult to learn an interface, but in the long run it can make it easier to use. ACTIVITY 1.4 One of the main design principles for website design is simplicity. Jakob Nielsen (1999) prposed that designers go through all of their design elements and remove them one by one. If a design works just as well without an element, then remove it. Do you think this is a good design principle? If you have your own website, try doing this and seeing what happens. At what point does the interaction break down? Comment Simplicity is certainly an important design principle. Many designers try to cram too much into a screenful of space, making it unwieldy for people to find the element in which they are interested. Removing design elements to see what can be discarded without affecting the oveall function of the website can be a salutary lesson. Unnecessary icons, buttons, boxes, lines, graphics, shading, and text can be stripped, leaving a cleaner, crisper, and easier-to-navigate website. However, graphics, shading, coloring, and formatting can make a site aesthetically pleasing and enjoyable to use. Plain vanilla sites consisting solely of lists of text and a few links may not be as appealing and may put certain visitors off, never to return. Good interaction design involves getting the right balance between aesthetic appeal and the optimal amount and kind of information per page",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Good interaction design involves getting the right balance between aesthetic appeal and the optimal amount and kind of information per page. In-Depth Activity This activity is intended for you to put into practice what you have studied in this chapter. Specifically, the objective is to enable you to define usability and user experience goals and to transform these and other design principles into specific questions to help evaluate an inteactive product. Find an everyday handheld device, for example, a remote control, digital camera, or smartphone and examine how it has been designed, paying particular attention to how the user is meant to interact with it. ( Continued ) (a) From your first impressions, write down what is good and bad about the way the device works. (b) Give a description of the user experience resulting from interacting with it. (c) Outline some of the core micro-interactions that are supported by it. Are they pleasurable, easy, and obvious? (d) Based on your reading of this chapter and any other material you have come across about interaction design, compile a set of usability and user experience goals that you think will be most relevant in evaluating the device. Decide which are the most important ones and explain why. (e) Translate each of your sets of usability and user experience goals into two or three specific q uestions. Then use them to assess how well your device fares. (f) Repeat steps (c) and (d), but this time use the design principles outlined in the chapter. (g) Finally, discuss possible improvements to the interface based on the answers obtained in steps (d) and (e). Summary In this chapter, we have looked at what interaction design is and its importance when developing apps, products, services, and systems. To begin, a number of good and bad designs were prsented to illustrate how interaction design can make a difference. We described who and what is involved in interaction design and the need to understand accessibility and inclusiveness",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We described who and what is involved in interaction design and the need to understand accessibility and inclusiveness. We explained in detail what usability and user experience are, how they have been characterized, and how to operationalize them to assess the q uality of a user experience resulting from interacing with an interactive product. The increasing emphasis on designing for the user experience and not just products that are usable was stressed. A number of core design principles were also introduced that provide guidance for helping to inform the interaction design process. Key Points • Interaction design is concerned with designing interactive products to support the way people communicate and interact in their everyday and working lives. • Interaction design is multidisciplinary, involving many inputs from wide-ranging disciplines and fields. • The notion of the user experience is central to interaction design. • Optimizing the interaction between users and interactive products req uires consideration of a number of interdependent factors, including context of use, types of activity, UX goals, accessibility, cultural differences, and user groups. • Identifying and specifying relevant usability and user experience goals can help lead to the design of good interactive products. • Design principles, such as feedback and simplicity, are useful heuristics for informing, anlyzing, and evaluating aspects of an interactive product. Further Reading Here we recommend a few seminal readings on interaction design and the user experience (in alphabetical order). COOPER, A., REIMANN, R., CRONIN, D. AND NOESSEL, C. (2014) About Face: The Essentials of Interaction Design (4th ed.). John Wiley & Sons Inc. This fourth edition of About Face provides an updated overview of what is involved in interaction design, and it is written in a personable style that appeals to practitioners and students alike. GARRETT, J. J. (2010) The Elements of User Experience: User-Centered Design for the Web and Beyond (2nd ed.)",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". GARRETT, J. J. (2010) The Elements of User Experience: User-Centered Design for the Web and Beyond (2nd ed.). New Riders Press. This is the second edition of the popular coffetable introductory book to interaction design. It focuses on how to ask the right q uestions when designing for a user experience. It emphasizes the importance of understanding how products work on the outside, that is, when a person comes into contact with those products and tries to work with them. It also considers a business perspective. LIDWELL, W., HOLDEN, K. AND BUTLER, J. (2010) Revised and Updated: 125 Ways to Enhance Usability, Influence Perception, Increase Appeal, Make Better Design Decisions and Teach Through Design . Rockport Publishers, Inc. This book presents classic design principles such as consistency, accessibility, and visibility in addition to some lesser-known ones, such as constancy, chunking, and symmetry. They are alphabetically ordered (for easy reference) with a diversity of examples to illustrate how they work and can be used. NORMAN, D.A. (2013) The Design of Everyday Things: Revised and Expanded Edition . MIT Press. This book was first published in 1988 and became an international best seller, introducing the world of technology to the importance of design and psychology. It covers the design of everyday things, such as refrigerators and thermostats, providing much food for thought in relation to how to design interfaces. This latest edition is comprehensively revised showing how principles from psychology apply to a diversity of old and new technologies. The book is highly accessible with many illustrative examples. SAFFER, D. (2014) Microinteractions: Designing with Details. O’Reilly. This highly accesible book provides many examples of the small things in interaction design that make a big difference between a pleasant experience and a nightmare one. Dan Saffer describes how to design them to be efficient, understandable, and enjoyable user actions",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Dan Saffer describes how to design them to be efficient, understandable, and enjoyable user actions. He goes into detail about their structure and the different kinds, including many examples with lots of illustrtions. The book is a joy to dip into and enables you to understand right away why and how it is important to get the micro-interactions right. INTERVIEW with Harry Brignull Harry Brignull is a user experience cosultant based in the United Kingdom. He has a PhD in cognitive science, and his work involves building better experiences by blending user research and interaction design. In his work, Harry has consulted for companies including Spotify, Smart Pension, The Telegraph, British Airways, Vodafone, and many others. In his spare time, Harry also runs a blog on interaction design that has attracted a lot of eyeballs. It is called 90percentofeverything.com, and it is well worth checking out. What are the characteristics of a good interaction designer? I think of interaction design, user exprience design, service design, and user research as a combined group of discplines that are tricky to tease apart. Every company has slightly different terminoogy, processes, and approaches. I’ll let you into a secret, though. They’re all making it up as they go along. When you see any organization portraying its design and research publicly, they’re showing you a fictionalized view of it for recruitment and marketing purposes. The reality of the work is usually very different. Research and design is naturally messy. There’s a lot of waste, false assumptions, and blind alleys you have to go down before you can define and understand a problem well enough to solve it. If an employer doesn’t understand this and they don’t give you the space and time you need, then you won’t be able to do a good job, regardless of your skills and training. A good interaction designer has skills that work like expanding foam. You expand to fill the skill gaps in your team",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A good interaction designer has skills that work like expanding foam. You expand to fill the skill gaps in your team. If you don’t have a writer present, you need to be able to step up and do it yourself, at least to the level of a credible draft. If you don’t have a researcher, you’ll need to step up and do it yourself. The same goes for deveoping code-based prototypes, planning the user journeys, and so on. You’ll soon learn to become used to working outside of your comfort zone and relish the new challenges that each project brings. How has interaction design changed in the past few years? In-housing of design teams is a big trend at the moment. When I started my cosultancy career in the mid-2000s, the main route to getting a career in industry was to get a role at an agency, like a UX consultancy, a research agency, or a fulservice agency. Big organizations didn’t even know where to start with hiring and building their own teams, so they paid enormous sums to agencies to design and build their products. This turned out to be a pretty ineffective model—when the agencies finish a project, they take all the acq uired expertise away with them to their next clients. These days, digital organizations have wised up, and they’ve started building their own in-house teams. This means that a big theme in design these days is organizational change. You can’t do good design in an organization that isn’t set up for it. In fact, in old, large organizations, the political structure often seems to be set up to saotage good design and development pratices. It sounds crazy, but it’s very common to walk into an organization to find a proect manager brandishing a waterfall Gantt chart while ranting obsessively about Agile (which is a contradiction in terms) or to find a product owner saying in one breath they value user research yet in the next breath getting angry with researchers for bringing them bad news. As well as “leacy technology,” organizations naturally end up with “legacy thinking.” It’s really tricky to change it",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As well as “leacy technology,” organizations naturally end up with “legacy thinking.” It’s really tricky to change it. Design used to be just a department. Nowadays it’s understood that good design req uires the entire organzation to work together in a cohesive way. What projects are you working on now? I’m currently head of UX at a FinTech starup called Smart Pension in London. Pesions pose a really fascinating user-centered design challenge. Consumers hate thinking about pensions, but they desperately need them. In a recent research session, one of the participants said something that really stuck with me: “Planning your pension is like planning for your own funeral.” Humans are pretty terrible at long-term planning over multiple decades. Nobody likes to think about their own mortality. But this is exactly what you need to do if you want to have a happy retirement. The pension industry is full of jagon and off-putting technical complexity. Even fundamental financial concepts like risk aren’t well understood by many cosumers. In some recent research, one of our participants got really tongue-tied tring to understand the idea that since they were young, it would be “high risk” (in the loose nontechnical definition of the word) to put their money into a “low-risk” fund (in the technical definition of the word) since they’d probably end up with lower returns when they got older. Investment is confusing unless you’ve had training. Then, there’s the problem that “a little knowledge can hurt.” Some consumers who think they know what they’re doing can end up suffering when they think they can beat the market by moving their money around between funds every week. Self-service online pension (retirement plans) platforms don’t do anything to help people make the right decisions because that would count as advice, which they’re not able to give because of the way it’s reulated",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Giving an average person a selservice platform and telling them to go sort out their pension is like giving them a Unix terminal and telling them to sort out their own web server. A few PDF fact sheets just aren’t going to help. If consumers want advice, they have to go to a financial advsor, which can be expensive and doesn’t make financial sense unless you have a lot of money in the first place. There’s a gap in the market, and we’re working these sorts of challenges in my team at Smart Pension. What would you say are the biggest chalenges facing you and other consultants doing interaction design these days? A career in interaction design is one of cotinual education and training. The biggest challenge is to keep this going. Even if you feel that you’re at the peak of your skills, the technology landscape will be shifting under your feet, and you need to keep an eye on what’s coming next so you don’t get left behind. In fact, things move so q uickly in interaction design that by the time you read this interview, it will already be dated. If you ever find yourself in a “cofortable” role doing the same thing every ( Continued ) day, then beware—you’re doing yourself a disservice. Get out there, stretch yourself, and make sure you spend some time every week outside your comfort zone. If you’re asked to evaluate a prototype sevice or product and you discover it is really bad, how do you break the news? It depends what your goal is. If you want to just deliver the bad news and leave, then by all means be totally brutal and don’t pull any punches. But if you want to build a relationship with the client, you’re going to need to help them work out how to move forward. Remember, when you deliver bad news to a client, you’re basically explaining to them that they’re in a dark place and it’s their fault. It can be q uite embarrassing and depressing. It can drive stakeholders apart when really you need to bring them together and give them a shared vision to work toward",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "Interaction Design",
    "author": "Helen Sharp;Jennifer Preece;Yvonne Rogers;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Interaction Design Chapter 1.pdf",
    "date_published": "2020-01-22",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It can be q uite embarrassing and depressing. It can drive stakeholders apart when really you need to bring them together and give them a shared vision to work toward. Discovering bad design is an opportunity for improvement. Always pair the bad news with a recommendation of what to do next. NOTE We use the term interactive products generically to refer to all classes of interactive systems, technologies, environments, tools, applications, services, and devices.",
    "chunk_id": "Human_computer_interaction_interaction_design.json_chunk_53"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 79 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00006-4 Copyright © 2021 Elsevier Inc. All rights reserved. Most people in industrialized nations grow up in households and school districts that promote education and reading. They learn to read as young children and become good readers by adolescence. As adults, most of our activities during a normal day involve reading. Reading is for most educated adults automatic, leaving our conscious minds free to ponder the meaning and implications of what we are reading. Because of this background, it is common for good readers to consider reading a “natural” human activity, like speaking. WE ARE WIRED FOR LANGUAGE BUT NOT FOR READING Speaking and understanding spoken language is a natural human ability, but reaing is not . Over hundreds of thousands—perhaps millions—of years, the human brain evolved the neural structures necessary to support spoken language. Nomal humans are born with an innate ability to learn, with no systematic training, whatever language they are exposed to. After early childhood, this ability decreases significantly. For example, newborn babies can hear and distinguish all the sounds of all languages, but as they learn the language of their home environment, they lose the ability to distinguish sounds that are not distinguished in that language (Eagleman, 2015). By adolescence, learning a new language is the same as learning any other skill: it requires instruction and practice, and the learning and processing are handled by different brain areas from those that handled it in early childhood (Sousa, 2005). In contrast, writing and reading did not exist until a few thousand years BCE and did not become common until only 4 or 5 centuries ago— long after the human brain had evolved into its modern state. At no time during childhood do our brains show any special innate ability to learn to read",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". At no time during childhood do our brains show any special innate ability to learn to read. Instead, reading is an artificial skill that we learn by systematic instruction and practice, like playing a violin, juggling, or reading music (Sousa, 2005). Reading is Unnatural 6 Many people never learn to read well, or at all Because people are not innately “wired” to learn to read, children who either lack caregivers who read to them or receive inadequate reading instruction in school may never learn to read. There are a great many such people, especially in the developing world. By comparison, very few people never learn a spoken language. Some people who learn to read never become good at it. Perhaps their parents did not value and promote reading. Perhaps they attended substandard schools or did not attend school at all. Perhaps they learned a second language but never learned to read well in that language. People who have cognitive or perceptual impairments such as dyslexia may never read easily. A person’s ability to read is specific to a language and script (a system of writing). To see what text looks like to someone who cannot read, look at a paragraph printed in a language and script that you do not know (see Fig. 6.1 ). Alternatively, you can approximate the feeling of illiteracy by taking a page written in a familiar script and language—such as a page of this book—and turning it upside down. Turn this book upside down and try reading the next few paragraphs. This exercise only approximates the feeling of illiteracy. You will discover that the inverted text appears foreign and illegible at first, but after a minute you will be able to read it, albeit slowly and laboriously. Learning to read = training our visual system Learning to read involves training our visual system to recognize patterns—the paterns exhibited by text. These patterns run the gamut from low level to high level: l Lines, contours, and shapes are basic visual features that our brain recognizes innately",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These patterns run the gamut from low level to high level: l Lines, contours, and shapes are basic visual features that our brain recognizes innately. We don’t have to learn to recognize them. l Basic features combine to form patterns that we learn to identify as characters— letters, numeric digits, and other standard symbols. In ideographic scripts, such as Chinese, symbols represent entire words or concepts. (A) (B) FIGURE 6.1 To see how it feels to be illiterate, look at text printed in a foreign script: (A) Amharic and (B) Tibetan. l In alphabetic scripts, patterns of characters form morphemes, which we learn to recognize as packets of meaning—for example, “farm,” “tax,” “-ed,” and “-ing” are morphemes in English. l Morphemes combine to form patterns that we recognize as words—for exaple, “farm,” “tax,” “-ed,” and “-ing” can be combined to form the words “farm,” “farmed,” “farming,” “tax,” “taxed,” and “taxing.” l Words combine to form patterns that we learn to recognize as phrases, idiomatic expressions, and sentences. l Sentences combine to form paragraphs. Actually, only part of our visual system is trained to recognize textual patterns involved in reading: the fovea and a small area immediately surrounding it (known as the perifovea ), and the downstream neural networks running through the optic nerve to the visual cortex and into various parts of our brain. The neural networks starting elsewhere in our retinas do not get trained to read. More about this is explained later in the chapter. Learning to read also involves training the brain’s systems that control eye movment to move our eyes in a specific way over text. The main direction of eye movment depends on the direction in which the language we are reading is written: European language scripts are read left to right, many middle Eastern language scripts are read right to left, and some language scripts are read top to bottom",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Beyond that, the precise eye movements differ depending on whether we are reading, skimming for overall meaning, or scanning for specific words. How we read Assuming our visual system and brain have successfully been trained, reading becomes semiautomatic or fully automatic—both eye movement and processing. As explained earlier, the center of our visual field—the fovea and perifovea—is the only part of our visual field that is trained to read. All text that we read enters our visual system after being scanned by the central area, which means that reading requires a lot of eye movement. As explained in Chapter 5 on the discussion of peripheral vision, our eyes constantly jump around, several times a second. Each of these movements, called saccades , lasts about 0.1 second. Saccades are ballistic, like firing a shell from a cannon. Their end point is determined when they are triggered, and once triggered, they always execute to completion. As described in earlier chapters, the destinations of saccadic eye movments are programmed by the brain from a combination of our goals, events in the visual periphery, events detected and localized by other perceptual senses, and past history including training. When we read, we may feel that our eyes scan smoothly across the lines of text, but that feeling is incorrect. In reality, our eyes continue with saccades during reading, but the movements generally follow the line of text. They fix our fovea on a word, pause there for a fraction of a second to allow basic patterns to be captured and transmitted to the brain for further analysis, then jump to the next important word (Larson, 2004). Eye fixations while reading always land on words, usually near the center, never on word boundaries (see Fig. 6.2 ). Very common small connector and function words like “a,” “and,” “the,” “or,” “is,” and “but” are usually skipped over, their presence either detected in perifoveal vision or simply assumed",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6.2 ). Very common small connector and function words like “a,” “and,” “the,” “or,” “is,” and “but” are usually skipped over, their presence either detected in perifoveal vision or simply assumed. Most of the saccades during reading are in the text’s normal reading direction, but a few—about 1%—jump backwards to previous words. At the end of each line of text, our eyes jump to where our brain guesses the next line begins. 1 How much can we take in during each eye fixation during reading? For reading European-language scripts at normal reading distances and text font sizes, the fovea clearly sees three to four characters on either side of the fixation point. The perifovea sees out about 15–20 characters from the fixation point, but not very clearly (see Fig. 6.3 ). According to reading researcher Kevin Larson (2004), the reading area in and around the fovea consists of three distinct zones (for European-language scripts): Closest to the fixation point is where word recognition takes place. This zone is usually large enough to capture the word being fixated, and often includes smaller function words directly to the right of the fixated word. The next zone extends a few letters past the word recognition zone, and readers gather preliminary information about the next letters in this zone. The final zone extends out to 15 letters past the fixation point. Information gathered out this far is used to identify the length of upcoming words and to identify the best location for the next fixation point. Due to how our visual system has been trained to read, perception around the fixtion point is asymmetrical; it is more sensitive to characters in the reading direction 1 Later we will see that centered text disrupts the brain’s guess about where the next line starts. )RXUVFRUHDQGVHYHQ\\HDUVDJRRXUIRUHIDWKHUVEURXJKWIRUWKRQ WKLVFRQWLQHQWDQHZQDWLRQFRQFHLYHGLQOLEHUW\\DQGGHGLFDWHG FIGURE 6.2 Saccadic eye movements during reading jump between important words",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". )RXUVFRUHDQGVHYHQ\\HDUVDJRRXUIRUHIDWKHUVEURXJKWIRUWKRQ WKLVFRQWLQHQWDQHZQDWLRQFRQFHLYHGLQOLEHUW\\DQGGHGLFDWHG FIGURE 6.2 Saccadic eye movements during reading jump between important words. FIGURE 6.3 Visibility of words in a line of text, with fovea fixed on the word “years.” than in the other direction. For European-language scripts, this is toward the right. That makes sense because characters to the left of the fixation point have usually already been read. IS READING FEATURE-DRIVEN OR CONTEXT-DRIVEN? As explained earlier, reading involves recognizing features and patterns. Pattern reconition, and therefore reading, can be either a bottom-up, feature-driven process, or a top-down, context-driven process. In feature-driven reading, the visual system starts by identifying simple features— line segments in a certain orientation or curves of a certain radius—on a page or display and then combines them into more complex features, such as angles, multiple curves, shapes, and patterns. Then the brain recognizes certain shapes as characters or symbols representing letters, numbers, or for ideographic scripts, words. In alphbetic scripts, groups of letters are perceived as morphemes and words. In all types of scripts, sequences of words are parsed into phrases, sentences, and paragraphs that have meaning. Feature-driven reading is sometimes referred to as “bottom-up” or “context-free.” The brain’s ability to recognize basic features—lines, edges, angles, etc.—is built in and therefore automatic from birth. In contrast, recognition of morphemes, words, and phrases has to be learned. It starts out as a nonautomatic, conscious process requiring conscious analysis of letters, morphemes, and words, but with enough practice it becomes automatic (Sousa, 2005). Obviously, the more common a mopheme, word, or phrase, the more likely that recognition of it will become automatic",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Obviously, the more common a mopheme, word, or phrase, the more likely that recognition of it will become automatic. With ideographic or pictographic scripts such as Chinese, which have many times more symbols than alphabetic scripts do, people typically take many years longer to become skilled readers. Context-driven or top-down reading operates in parallel with feature-driven reading but works the opposite way: from whole sentences or the gist of a paragraph down to the words and characters. The visual system starts by recognizing high-level patterns like words, phrases, and sentences or by knowing the text’s meaning in advance. It then uses that knowledge to figure out—or guess—what the lower-level components of the high-level pattern must be (Boulton, 2009). Context-driven reading is less likely to become fully automatic, because most phrase-level and sentence-level patterns and contexts don’t occur frequently enough to allow their recognition to become burned into neural firing patterns. But there are exceptions, such as idiomatic expressions. To experience context-driven reading, glance quickly at Fig. 6.4 , then immediately direct your eyes back here and finish reading this paragraph. Try it now. What did the text say? Now look at the same sentence again more carefully. Do you read it the same way now? Also, based on what we have already read and our knowledge of the world, our brains can sometimes predict text that the fovea has not yet read (or its meaning), allowing us to skip reading it. For example, if at the end of a page we read “It was a dark and stormy,” we would expect the first word on the next page to be “night.” We would be surprised if it was some other word (e.g., “cow”). Feature-driven, bottom-up reading dominates; context assists It has been known for decades that reading involves both feature-driven (bottom-up) and context-driven (top-down) processing",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Feature-driven, bottom-up reading dominates; context assists It has been known for decades that reading involves both feature-driven (bottom-up) and context-driven (top-down) processing. In addition to being able to figure out the meaning of a sentence by analyzing the letters and words in it, people can determine the words of a sentence by knowing the sentence’s meaning or the letters in a word by knowing what word it is (see Fig. 6.5 ). The question is: Is skilled reading primarily bottom-up or top-down, or is neither mode dominant? Early scientific studies of reading—from the late 1800s through about 1980— seemed to show that people recognize words first, and from that they determine what letters are present. The theory of reading that emerged from those findings was that our visual system recognizes words primarily from their overall shape . This theory failed to account for certain experimental results and so was controversial among researchers, but it nonetheless gained wide acceptance among nonresearchers, espcially in the graphic design field (Larson, 2004; Herrmann, 2011). Similarly, educational researchers in the 1970s applied information theory to reading and assumed that because of redundancies in written language, top-down, context-driven reading would be faster than bottom-up, feature-driven reading. This The rain in Spain falls manly in the the plain FIGURE 6.4 Top-down recognition of the expression can inhibit seeing the actual text. Mray had a ltilte lmab, its feclee was withe as sown. And ervey wehre taht Mray wnet, the lmab was srue to go. (A) (B) Twinkle, twinkle little star, how I wonder what you are FIGURE 6.5 Top-down reading: most readers, especially those who know the songs from which these text passages are taken, can read them even though the words (A) have all but their first and last letters scrambled and (B) are mostly obscured. assumption led them to hypothesize that reading for highly skilled (fast) readers would be dominated by context-driven (top-down) processing",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". assumption led them to hypothesize that reading for highly skilled (fast) readers would be dominated by context-driven (top-down) processing. This theory was proably responsible for many speed-reading methods of the 1970s and 1980s, which suposedly trained people to read fast by taking in whole phrases and sentences at a time. However, empirical studies of readers conducted since then have demonstrated conclusively that those early theories were wrong. Summing up the research are statments from reading researchers Kevin Larson (2004) and Keith Stanovich (Boulton, 2009), respectively: Word shape is no longer a viable model of word recognition. The bulk of scientific evidence says that we recognize a word’s component letters, then use that visual information to recognize a word. Context [is] important, but it’s a more important aid for the poorer reader who doesn’t have automatic context-free recognition instantiated. In other words, reading consists mainly of context-free, bottom-up, feature-driven processes. In skilled readers, these processes are well learned to the point of being automatic. Context-driven reading today is considered mainly a backup method that, although it operates in parallel with feature-based reading, is only relevant when feture-driven reading is difficult or insufficiently automatic. Skilled readers may resort to context-based reading when feature-based reading is disrupted by poor presentation of information (see examples later in this chater). Also, in the race between context-based and feature-based reading to decipher the text we see, contextual cues sometimes win out over features. As an example of context-based reading, Americans visiting England sometimes misread “to let” signs as “toilet,” because in the United States they see the word “toilet” often, but they almost never see the phrase “to let”—Americans use “for rent” instead. In less-skilled readers, feature-based reading is not automatic; it is conscious and laborious. Therefore, more of their reading is context-based",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In less-skilled readers, feature-based reading is not automatic; it is conscious and laborious. Therefore, more of their reading is context-based. Their involuntary use of context-based reading and nonautomatic feature-based reading consumes short-term cognitive capacity, leaving little for comprehension. 2 They have to focus on decipheing the stream of words, leaving no capacity for constructing the meaning of sentences and paragraphs. That is why poor readers can read a passage aloud but afterward have no idea what they just read. Why is context-free (bottom-up) reading not automatic in some adults? Lack of training is a reason: some people don’t get enough experience reading as young 2 Chapter 10 describes the differences between automatic and controlled cognitive processing. Here, we will simply say that controlled processes burden working memory, while automatic processes do not. children for the feature-driven recognition processes to become automatic. As they grow up, they find reading mentally taxing, so they avoid it, which perpetuates and compounds their deficit (Boulton, 2009). SKILLED AND UNSKILLED READING USE DIFFERENT AREAS OF THE BRAIN Before the 1980s, researchers who wanted to understand which parts of the brain are involved in language and reading were limited mainly to studying people who had sufered brain injuries. For example, in the mid-19th century, doctors found that people with brain damage near the left temple—an area now called Broca’s area after the doctor who discovered it—can understand speech but have trouble speaking, and that people with brain damage near the left ear—now called Wernicke’s area —canot understand speech (Sousa, 2005) (see Fig. 6.6 ). In recent decades, new methods of observing the operation of functioning brains in living people have been developed: electroencephalography, functional magnetic resonance imaging, and functional magnetic resonance spectroscopy",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These methods allow researchers to watch the responses in different areas of a person’s brain— including the sequence in which they respond—as the person perceives various stiuli or performs specific tasks (Minnery and Fine, 2009). Using these methods, researchers have discovered that the neural pathways involved in reading differ for novice versus skilled readers. Of course, the first area to respond during reading is the occipital (or visual) cortex at the back of the brain. That is the same regardless of a person’s reading skill. After that, the pathways diverge (Sousa, 2005): l Novice . First an area of the brain just above and behind Wernicke’s area becomes active. Researchers have come to view this as the area where, at least with Broca’s area Wernicke’s area FIGURE 6.6 The human brain, showing Broca’s and Wernicke’s areas. alphabetic scripts such as English and German, words are “sounded out” and assembled—that is, letters are analyzed and matched with their corresponding sounds. The word-analysis area then communicates with Broca’s area and the frontal lobe, where morphemes and words—units of meaning—are recognized and overall meaning is extracted. For ideographic languages, where symbols reresent whole words and often have a graphical correspondence to their meaning, sounding out of words is not part of reading. l Advanced . The word-analysis area is skipped. Instead the occipitotemporal area (behind the ear, not far from the visual cortex) becomes active. The prevailing view is that this area recognizes words without sounding them out, then that activity activates pathways toward the front of the brain that correspond to the word’s meaning and mental image. Broca’s area is only slightly involved. Findings from brain scan methods of course don’t indicate exactly what processes are being used, but they support the theory that advanced readers use different prcesses from those used by novice readers",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". POOR INFORMATION DESIGN CAN DISRUPT READING Careless writing or presentation of text can reduce skilled readers’ automatic, cotext-free reading to conscious, context-based reading, burdening working memory and thereby decreasing speed and comprehension. In unskilled readers, poor text presentation can block reading altogether. Poor text presentation can take several forms, including the following. Uncommon or unfamiliar vocabulary Software often disrupts reading by using unfamiliar vocabulary—words the intended readers don’t know very well or at all. One type of unfamiliar terminology is computer jargon, sometimes known as “geek speak.” For example, an intranet application displayed the following error message if a user tried to use the application after more than 15 minutes of letting it sit idle: Your session has expired. Please reauthenticate. The application was for finding resources—rooms, equipment, etc.—within the company. Its users included receptionists, accountants, and managers as well as engneers. Most nontechnical users would not understand the word “reauthenticate,” so they would drop out of automatic reading mode into conscious wondering about the message’s meaning. To avoid disrupting reading, the application’s developers could have used the more familiar instruction, “Login again.” For a discussion of how “geek speak” in computer-based systems affects learning, see Chapter 11 . Reading can also be disrupted by uncommon terms even when they are not coputer technology terms",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Reading can also be disrupted by uncommon terms even when they are not coputer technology terms. Here are some rare English words, including many that appear mainly in contracts, privacy statements, or other legal documents: l Aforementioned : mentioned previously l Bailiwick : the region in which a sheriff has legal powers; more generally, domain of control l Disclaim : renounce any claim to or connection with; disown; repudiate l Heretofore : up to the present time; before now l Jurisprudence : the principles and theories on which a legal system is based l Obfuscate : make something difficult to perceive or understand l Penultimate : next to the last, as in “the next to the last chapter of a book” When readers—even skilled ones—encounter such a word, their automatic reaing processes probably won’t recognize it. Instead, their brain uses less automatic processes, such as sounding out the word’s parts and using them to figure out its meaning, figuring out the meaning from the context in which the word appears, or looking the word up in a dictionary. Difficult scripts and typefaces Even when the vocabulary is familiar, automatic reading can be disrupted by typefaces with unfamiliar or hard-to-distinguish shapes. Context-free, automatic reading is based on recognizing letters and words bottom-up from their lower-level visual features. Our visual system is quite literally a neural network that must be trained to recognize certain combinations of shapes as characters. Therefore, a typeface with difficult-trecognize features and shapes will be hard to read. For example, try to read Abraham Lincoln’s Gettysburg Address in an outline typeface in ALL CAPS (see Fig. 6.7 ). Comparison studies show that skilled readers read uppercase text 10%–15% more slowly than lowercase text. Current-day researchers attribute that difference mainly to a lack of practice reading uppercase text, not to an inherent lower recognizability of uppecase text (Larson, 2004)",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Current-day researchers attribute that difference mainly to a lack of practice reading uppercase text, not to an inherent lower recognizability of uppecase text (Larson, 2004). Nonetheless, designers should keep in mind that, due to the lack of practice people have in reading text in ALL CAPS, it is harder to read (Herrmann, 2011). Tiny fonts Another way to make text hard to read in software applications, websites, and eletronic appliances is to use fonts too small for their intended readers’ visual system to resolve. For example, try to read the first paragraph of the US Constitution in a sevepoint font (see Fig. 6.8 ). Developers sometimes use tiny fonts because they have a lot of text to display in a small amount of space. But if the intended users of the system cannot read the text or can read it only laboriously, the text might as well not be there. Text on noisy background Visual noise in and around text can disrupt recognition of features, characters, and words and therefore drop reading out of automatic feature-based mode into a more conscious and context-based mode. In software user interfaces and websites, visual noise often results from designers’ placing text over a patterned background or diplaying text in colors that contrast poorly with the background, as an example from uscpfa-sbay.blogspot.com shows (see Fig. 6.9 ). There are situations in which designers intend to make text hard to read. For exaple, a common security measure on the Web is to ask site users to identify distorted words as proof that they are a live human beings and not an Internet “‘bot.” This relies on the fact that most people can read text that Internet ‘bots cannot currently read. Text displayed as a challenge to test a registrant’s humanity is called a captcha 3 (see Fig. 6.10 ). Of course, textual captchas should be clear enough for people to read; otherwise it defeats the purpose of having them",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6.10 ). Of course, textual captchas should be clear enough for people to read; otherwise it defeats the purpose of having them. In case a user cannot see what is in a captcha, it should be posible for the user to ask for another one, or even a different—e.g., nonvisual—challenge. 3 The term originally comes from the word “capture,” but it is also said to be an acronym for “Completely Automated Public Turing test to tell Computers and Humans Apart.” FIGURE 6.7 Text in ALL CAPS is harder to read because we are not practiced at doing it. Outline typefaces complicate feature recognition. This example demonstrates both. We the people of the United States, in Order to form a more perfect Union, establish Justice, insure domestic Tranquility, provide for the common defense, promote the general Welfare, and secure the Blessings of Liberty to ourselves and our Posterity, do ordain and establish this Constitution for the United States of America. FIGURE 6.8 The opening paragraph of the US Constitution presented in a seven-point font. Text contrasts poorly with background Even when text is displayed on a nonpatterned background, the text can be hard to read if it contrasts poorly with the background. For example, Apple Computer’s App Store (July 2019) showed the expected duration of an app download in gray text on a gray background, with contrast so low that it was almost impossible to see, much less read (see Fig. 6.11 , below progress bar). Low-contrast text is especially difficult for many adults older than 50 to read due to common age-related changes in the human visual system (Johnson and Finn, 2017). Like text in tiny fonts, text that contrasts poorly with the background is, for practical purposes, not there. For ease of reading by all sighted users, the contrast between text and background should be at least 4.5:1. FIGURE 6.9 The blog uscpfa-sbay.blogspot.com uses text on a noisy background and poor color contrast",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". FIGURE 6.9 The blog uscpfa-sbay.blogspot.com uses text on a noisy background and poor color contrast. FIGURE 6.10 Text that is intentionally displayed with noise so that web-crawling software cannot read it is called a captcha. Information buried in repetition Visual noise can also come from the text itself. If successive lines of text contain a lot of repetition, readers receive poor feedback about which line they are focused on, plus it is hard to pick out the important information. For example, recall the example from the California Department of Motor Vehicles website in Chapter 3 (see Fig. 3.3). Another example of repetition that creates noise is the computer store on Apple.com . The pages for ordering a laptop computer list different keyboard options for a computer in a very repetitive way, making it hard to see that the essential diffeence between the keyboards is the language they support (see Fig. 6.12 ). FIGURE 6.11 Apple’s App Store displayed the expected download time (bottom center, below progress bar) in gray text on a gray background, making it very hard to read. FIGURE 6.12 Apple.com’s “Buy Computer” page lists options in which the important information (keyboard language compatibility) is buried in repetition. Centered text (or text aligned with a ragged start) One aspect of reading that is highly automatic in most skilled readers is eye movment. In automatic (fast) reading, our eyes are trained to go back to the same horizotal position and down one line. If text is centered or otherwise aligned so that each line starts in a different horizontal position, 4 automatic eye movements take our eyes back to the wrong place, so we must consciously adjust our gaze to the actual start of each line. This drops us out of automatic mode and slows us down. With poetry and wedding invitations that may be okay, but with any other type of text it is undesirable. Try reading the left side of Fig. 6.13 quickly, and compare that with reading the right side",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". With poetry and wedding invitations that may be okay, but with any other type of text it is undesirable. Try reading the left side of Fig. 6.13 quickly, and compare that with reading the right side. Can you tell that your eyes move less efficiently when reading the left side? Fig. 6.14 shows an example of centered paragraphs of text from the homepage of Valco Tronics Inc. ( valcoelectronics.com ). Not only does the page center paragraphs of text, it also uses a blurry white-on-black typeface (see Fig. 6.14 ), diminishing legibility even more. UX designers should avoid centering multiline paragraphs of text (Nielsen, 2008b; Trevellyan, 2017). However, when a page includes separate elements that are not mutiline sentences or paragraphs of text, centering them can be OK (see Fig. 6.15 ). Design implications: don’t disrupt reading; support it! Obviously, a designer’s goal should be to support reading, not disrupt it. Skilled (fast) reading is mostly automatic and based on feature, character, and word recognition. The easier the recognition, the easier and faster the reading. Less-skilled reading, by contrast, is greatly assisted by contextual cues. Designers of interactive systems can support both reading methods by following these guidelines: 4 Right-aligned for left-to-right scripts; left-aligned for right-to-left scripts. FIGURE 6.13 Compare reading the text on the left versus the right. Centered paragraph text thwarts automatic eye movement patterns, resulting in slower reading speed. 1 Ensure that text in user interfaces allows feature-based automatic processes to function effectively by avoiding the disruptive flaws described earlier: difficult or tiny fonts, patterned backgrounds, centering, etc. FIGURE 6.14 Centered paragraphs of text on the homepage of valcoelectronics.com . FIGURE 6.15 Centering of separate elements is OK; it does not disrupt reading. 2 Use restricted, highly consistent vocabularies—sometimes referred to in the industry as plain language 5 —or simplified language (Redish, 2007)",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2 Use restricted, highly consistent vocabularies—sometimes referred to in the industry as plain language 5 —or simplified language (Redish, 2007). 3 Format text to create a visual hierarchy (see Chapter 3 ) to facilitate easy scanning: use headings, bulleted lists, tables, and visually emphasized words (see Fig. 6.16 ). Experienced information architects, content editors, and graphic designers can be very useful in ensuring that text is presented to support easy scanning and reading. PEOPLE DON’T READ WHEN USING SOFTWARE AND THE WEB; THEY SCAN As explained in Chapter 1 , when people are using an app or a website, they focus on their goal and mostly ignore everything else. When an app or website presents text, we usually read as little of it as absolutely necessary to achieve our goal. This is often summarized as “people don’t read in apps and the Web; they scan.” (Nielsen, 1999, 2008a; Johnson, 2007; Krug, 2014). For example, look at the medical service Web page in Fig. 6.17 , then look back here. Did you notice anything unusual? Now look at Fig. 6.18 , in which some of the text on the page is marked. Did you miss those the first time? It is actually an oversimplification to say “people don’t read text in apps and on the Web.” When people find the content—online books, articles, blog posts, poems, comments—that they want, they do read. But even then, unless they really enjoy the author’s writing for its own sake, they only read as much as necessary to get the infomation they need. 5 For more information on plain language see the U.S. government website, www.plainlanguage.gov . FIGURE 6.16 Microsoft Word’s “Help” home page is easy to scan and read. MUCH OF THE TEXT IN APPS AND WEBSITES IS UNNECESSARY In addition to committing design mistakes that disrupt reading, many software user interfaces simply present too much text, requiring users to read more than is necesary",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Software designers often justify lengthy instructions by arguing, “We need all that text to explain clearly to users what to do.” However, instructions can often be shortened with no loss of clarity. Let’s examine how the Jeep company, between 2002 and 2007, shortened its instructions for finding a local Jeep dealer (see Fig. 6.19 ): l 2002 : The “Find a Dealer” page displayed a large paragraph of prose text, with numbered instructions buried in it, and a form asking for more information than needed to find a dealer near the user. FIGURE 6.17 Simulated medical company web page. (Image courtesy of trumatter.) l 2003 : The instructions on the “Find a Dealer” page had been boiled down to three bullet points, and the form required less information. l 2007 : “Find a Dealer” was cut to one field (zip code) and a “Go” button and left that way for years. That’s about as much as it can be cut, right? Wrong! l 2020 : No need for a text-entry field or a “Go” button. Smartphones and compuers can identify the user’s location automatically (assuming Location is enabled), so Jeep cut “Find a Dealer” down to one button. Just click it to see local dealers. What’s next? No need for any button at all. Just speak and ask Jeep for nearby dealers. Even when text describes products rather than explaining instructions, it is couterproductive to put all a vendor wants to say about a product into a lengthy prose FIGURE 6.18 Simulated medical company web page with additions marked. Did you notice all of these in Fig. 6.17 ? (Image courtesy of trumatter.) FIGURE 6.19 Over the years, Jeep.com reduced the reading and data-entry required to find a dealer. description that people have to read from start to end. Most potential customers cannot or will not read it. Fig. 6.20 shows the reduction in the amount of text in Costco.com ’s displays of laptop computers between 2007 and 2009, and Fig. 6.21 shows that their 2019 website displayed laptop computers with very little text",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_19"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6.21 shows that their 2019 website displayed laptop computers with very little text. Design implications: cut unnecessary text—minimize the need for reading Needless text is bad anytime (Strunk and White, 1999) but is especially bad in sofware and websites. First of all, if users encounter lots of text on the way to their goals, they simply will ignore most of it. So if you spend time and money writing a lot of text, at best you are wasting your time and your money. Too much text in a user interface loses poor readers, who unfortunately are a sinificant percentage of the population. Too much text even alienates good readers; it turns using an interactive system into an intimidating amount of work . FIGURE 6.20 Between 2007 and 2009, Costco.com reduced the text in product descriptions. Minimize the amount of prose text in a user interface; don’t present users with long blocks of prose text to read. Don’t have paragraphs welcoming people to your website. In instructions, use the least amount of text that gets most users to their intended goals. In product descriptions, provide a brief overview of the product and let users request more detail if desired. Before releasing an app or a website, go through every screen and cut the amount of text by at least half. Then go through the screens again and cut another 50%. According to UX design guru Steve Krug (2014), that should leave you with about the right amount of text. Consider how much text Jeep cut over many years. It should not have taken them almost 2 decades to do it. Technical writers and content editors can assist greatly in cutting the amount of text in a user interface. For additional advice on how to eliminate unnecessary text, see Ginny Redish’s book Letting Go of the Words (2007). TEST ON REAL USERS Finally, designers should test their designs on the intended user population to be confident that users can read all essential text quickly and effortlessly",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_20"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". TEST ON REAL USERS Finally, designers should test their designs on the intended user population to be confident that users can read all essential text quickly and effortlessly. Some testing FIGURE 6.21 In 2019, Costco.com displayed laptop computers with no sentences or paragraphs of text. can be done early using prototypes and partial implementations, but it should also be done just before release. Fortunately, last-minute changes to text are usually easy to make. IMPORTANT TAKEAWAYS l Humans are “prewired” to learn a spoken language but not to learn to read. Learning to read is like learning any nonlanguage skill, such as how to ride a bicycle, play a guitar, or execute a kung fu move. Almost everyone learns a laguage, but many people never learn to read. l Learning to read requires training the neural networks in our eyes and brains to recognize and understand characters, words, sentences, and paragraphs. Only the neural networks that start in the fovea—a small area in the center of our visual field—are involved in reading. The rest of our visual field cannot read, but it does influence where our eyes jump while reading. l As a form of perception, reading is both a bottom-up, feature-driven process and a top-down, context-driven process. Bottom-up, feature-driven reading—shapes to letters to words to sentences to paragraphs—is the dominant process in skilled readers, and top-down, context-driven reading assists. In less-skilled readers, todown reading plays a larger role. l Skilled and unskilled reading use different areas of the brain. In less-skilled reaers, processing a word includes areas of the brain that “sound out” words to help recognize them. In skilled readers, the “sounding out” areas of the brain are skipped; processing goes straight from visual perception to extracting meaning. l Poor information presentation can disrupt reading. It can temporarily reduce skilled readers to the level of unskilled readers, and it can block reading for unskilled readers",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_21"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Poor information presentation can disrupt reading. It can temporarily reduce skilled readers to the level of unskilled readers, and it can block reading for unskilled readers. Poor information presentation includes: l use of rare or jargon words unfamiliar to readers l unusual scripts and typefaces, including ALL CAPS (because people are not trained in reading them) l tiny fonts l text on noisy backgrounds l text that contrasts poorly with background l information buried in repetition l centered text l People don’t read when using software and the Web. They scan until they find the content they are looking for—e.g., a news article. Even then, they continue to scan if all they want from the content is specific information. l Much of the text in apps and websites is unnecessary and should be cut out. For the broadest appeal of your software, minimize the need for users to read. l Test on real users to see if they can understand all the text in your software, and eliminate, rewrite, radically shorten, or replace with graphics any text that is unclear to most of them.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-79-102.json_chunk_22"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Doherty Threshold Productivity soars when a computer and its users interact at a pace (<400 ms) that ensures that neither has to wait on the other. Key Takeaways • Provide system feedback within 400 ms in order to keep users’ attention and increase productivity. • Use perceived performance to improve response time and reduce the perception of waiting. • Animation is one way to visually engage people while loading or processing is happening in the background. • Progress bars help make wait times tolerable, regardless of their accuracy. • Purposefully adding a delay to a process can actually increase its perceived value and instill a sense of trust, even when the process itself actually takes much less time. Overview One of the features that is critical to good user experiences is performance. Emtions can quickly turn to frustration and leave a negative lasting impact when users who are trying to achieve a task are met with slow processing, lack of feeback, or excessive load times. Often overlooked as more a technical best practice, speed should be considered an essential design feature that is core to good user 97 experiences. Whether it’s the amount of time the product or service takes to intially load, how fast it is to respond to interactions and provide feedback, or how quickly subsequent pages load, the speed at which a system responds is key to the overall user experience. There are several factors that can impact the performance of websites and apps, but the most significant is overall page weight. Unfortunately, when it comes to page weight on the web, the average has increased exponentially over the years. According to the HTTP Archive, the average desktop page weight in 2019 was approaching 2 MB (1,940 KB), with mobile not too far behind at 1.7 MB (1,745 KB). This is a vast increase over the average page weights in 2010– 2011: 609 KB on desktop and 262 KB on mobile ( Figure 10-1 ). Figure 10-1",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is a vast increase over the average page weights in 2010– 2011: 609 KB on desktop and 262 KB on mobile ( Figure 10-1 ). Figure 10-1. Average page weight is increasing each year (source: HTTP Archive, 2019) This trend means longer wait times, and waiting is not something people like to do when trying to complete a task. Countless studies reinforce the fact that the longer the wait times that people are subjected to, the more likely it is they will grow frustrated and even abandon the task altogether. Additionally, slow response times from a system lead to a decrease in the productivity of the people using an interface. While a 100 ms response feels instantaneous, a delay of between 100 and 300 ms begins to be perceptible to the human eye, and people begin to feel less in control. Once the delay extends past 1,000 ms (1 second), people begin thinking about other things; their attention wanders, and information important to performing their task begins to get lost, leading to an inevitable reduction in performance. The cognitive load required to continue with the task increases as a result, and the overall user experience suffers. 1 Walter J. Doherty and Ahrvind J. Thadani, “The Economic Value of Rapid Response Time,” IBM technical report GE20-0752-0, November 1982, http://www.utsa.edu/mind/von_restorff_translation.htm . Origins In the early days of desktop computing, 2 seconds was considered an acceptable threshold for response time from a computer when performing a task. The reson for this widely accepted standard was that it provided time for the user to think about their next task. Then, in 1982, a paper published by two IBM emploees challenged this previous standard by stating that “productivity increases in more than direct proportion to a decrease in response time” when the threshold is under 400 ms",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 1 The authors of the study claimed that “when a computer and its users interact at a pace that ensures that neither has to wait on the other, prductivity soars, the cost of the work done on the computer tumbles, employees get more satisfaction from their work, and its quality tends to improve.” It set a new standard that would come to be known as the Doherty threshold, based on Doherty’s observation that computer response times have a disproportionate impact on productivity. Examples In some cases the amount of time required for processing is longer than what is prescribed by the Doherty threshold (>400 ms), and there simply isn’t much that can be done about it. But that doesn’t mean we can’t provide feedback to users in a timely fashion while the necessary processing is happening in the background. This technique helps to create the perception that a website or an app is perforing faster than it actually is. One common example used by platforms such as Facebook ( Figure 10-2 ) is the presentation of a skeleton screen when content is loading. This technique makes the site appear to load faster by instantly displaying placeholder blocks in the areas where content will eventually appear. The blocks are progressively replaced with actual text and images once they are loaded. This reduces the impression of waiting, which increases the perception of speed and responsivness even if the content is loading slowly. Additionally, skeleton screens prevent the jarring and disorienting experience of content jumping around as adjacent material loads by reserving space for each item up front. Figure 10-2. Facebook’s skeleton screen helps the site appear to load faster (source: Facebook.com, 2019) Another way to optimize load times is known as the “blur up” technique. This approach focuses specifically on images, which are often the main contribtor to excessively long load times in both web and native applications",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This approach focuses specifically on images, which are often the main contribtor to excessively long load times in both web and native applications. It works by first loading an extremely small version of an image and scaling it up in the space where the larger image will eventually be loaded. A Gaussian blur is applied to eliminate any obvious pixelation and noise as a result of scaling up the low-resolution image ( Figure 10-3 ). Once the larger version of the image is loaded in the background, it is placed behind the low-resolution version and revealed by fading out the top image. Not only does this technique ensure faster load times by prioritizing performance over content, but it also allocates room for full-sized images up front to prevent page jumping once the high-resolution version of the image is fully loaded. 2 Brad A. Myers, “The Importance of Percent-Done Progress Indicators for Computer–Human Interfaces,” in CHI ’85: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (New York: Association for Computing Machinery, 1985), 11–17. Figure 10-3. Medium uses the “blur up” technique to enable faster page loading (source: Medium.com, 2019) Animation is yet another way to visually engage people while loading or prcessing is happening in the background. A common example is “percent-done progress indicators,” also known as progress bars. Research has shown that siply seeing a progress bar can make wait times seem more tolerable, regardless of its accuracy. 2 This simple UI pattern is effective for several reasons: • It reassures people that their action is being processed. • It provides visual interest while they wait. • It reduces the perception of waiting by shifting focus to the animation of the progress bar as opposed to the actual process of waiting. While we can’t always circumvent the need for processing and the subsquent waiting, we can increase the user’s willingness to wait by providing visual feedback. 3 Robert B",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While we can’t always circumvent the need for processing and the subsquent waiting, we can increase the user’s willingness to wait by providing visual feedback. 3 Robert B. Miller, “Response Time in Man-Computer Conversational Transactions” in Proceedings of the December 9-11, 1968, Fall Joint Computer Conference, Part I , vol. 33 (New York: Association for Compuing Machinery, 1968), 267–77. An example of animation being used to reduce the uncertainty and frustrtion associated with wait times can be found in Google’s famous email client, Gmail ( Figure 10-4 ). The loading screen uses an animated version of its logo in combination with a simple progress bar while the app loads. The effect of this simple yet distinctive animation creates the perception of a shorter wait time and improves the overall user experience by reassuring people that the app is loading. Figure 10-4. Gmail uses a simple yet distinctive animation to shorten the perceived wait time (source: Gmail, 2020) Ten seconds is the commonly recognized limit for keeping the user’s attetion focused on the task at hand—anything exceeding this limit, and they’ll want to perform other tasks while waiting. 3 When wait times must extend beyond the maximum of 10 seconds, progress bars are still helpful but should be augmented with an estimation of the time remaining until completion and a description of the task that is currently being performed. This additional information helps give users an idea of how much time they have to wait until the task is finished and frees them up to do other tasks in the meantime. Take, for example, Apple’s installation screen ( Figure 10-5 ), which is displayed when an update is underway. Figure 10-5. Apple provides an estimated time to completion along with a progress bar during updates (source: Apple macOS, 2019) Another clever technique for improving perceived performance is the optmistic UI",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Apple provides an estimated time to completion along with a progress bar during updates (source: Apple macOS, 2019) Another clever technique for improving perceived performance is the optmistic UI . It works by optimistically providing feedback that an action was sucessful while it is being processed, as opposed to providing feedback only once the action has been completed. For example, Instagram displays comments on photos before they are actually posted ( Figure 10-6 ). This makes the app’s response time seem faster than it actually is: it immediately provides visual feeback that assumes the comment will be successfully posted, and only displays an error afterward in the event that the action isn’t successful. The required procesing still happens in the background, but the user’s perception of the app’s perfomance is improved. Figure 10-6. Instagram optimistically displays comments on photos before they are actually posted to improve perceived performance (source: Instagram, 2019) KEY CONSIDERATION When Response Times Are Too Fast Most issues around response times boil down to them being too slow. It might seem counterintuitive, but it’s important to also consider when response times might be too fast . When the system responds more quickly than the user expects it to, a few problems can occur. First, a change that happens a little too fast may be completely missed—this is especially true when the change is not the result of an action taken by the user but something that happens automatically. Another issue that 4 Mark Wilson, “The UX Secret That Will Ruin Apps for You,” Fast Company , July 6, 2016, https://www.fascompany.com/3061519/the-ux-secret-that-will-ruin-apps-for-you . can occur when a response time is too fast is that it can be difficult for the user to comprehend what happened, since the speed of the change does not allow sufficient time for mental processing. Finally, a too-fast response time can result in mistrust if it doesn’t align with the user’s expectations about the task being performed",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Finally, a too-fast response time can result in mistrust if it doesn’t align with the user’s expectations about the task being performed. Purposefully adding a delay to a process can actually increase its perceived value and instill a sense of trust, even when the process actually takes much less time. 4 Take, for example, Facebook’s Security Checkup process ( Figure 10-7 ), which scans your account for potential security vulnerabilities. Facebook uses it as an opportunity to educate people about what is being scanned and adds additional time to the process to instill trust that the scan is thorough. Figure 10-7. Facebook’s Security Checkup process scans your account for potential security vulnerabilities, extending the time the process actually requires and taking the opportunity to educate you (source: Facebook, 2019) Conclusion Performance is not just a technical consideration for our development colleagues —it is an essential design feature. As designers, it is our responsibility to help ensure the people who use our products and services can achieve their tasks as quickly and efficiently as possible. To this end, it’s important that we provide the appropriate feedback, leverage perceived performance, and use progress bars to reduce the overall sense of waiting.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-97-106.json_chunk_7"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "January 26, 20242024-01-26 Share This is an updated version of an article written by Jakob Nielsen in 2010.Mental models are one of the most important concepts in human-computer interaction (HCI). This article reports a few examples of mental models in user-interface design. Not coincidentally, using concrete examples often helps people understand abstract concepts, such as mental models. Amental modelis what the user believes about the system (web, application, or other kind of product) at hand. Mental models help the user predict how a system will work and, therefore, influence how they interact with an interface. A mental model is a model of what users know (or think they know) about a system such as your website.Users form their predictions about the system based on their mental models and plan their future actions accordingly. A prime goal for designers is to ensure that the user interface clearly communicates the nature of the system so that users form accurate (and thus useful) mental models. How do users create mental models? Often, they just use the knowledge they have about the world to create them. RememberJakob's law of the internet user experience: Users spend most of their time on websites other than yours. Thus, a big part of customers' mental models of your site will be influenced by information gleaned from other sites. Peopleexpect websites to act alike. A mental model is based on each user’s individual background knowledge and past experiences, so different users might construct different mental models of the same system. One of the biggest dilemmas in usability is the gap between designers' and users' mental models. Designers form wonderfully detailed mental models of their own creations, leading them to believe that each feature is easy to understand. On the other hand, most users’ mental models of the system are usually less developed; as a result, they are more likely to make mistakes and find the design harder to use",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". On the other hand, most users’ mental models of the system are usually less developed; as a result, they are more likely to make mistakes and find the design harder to use. Mentalmodels can change over time— due to additional experience with your system or with other systems. As users navigate the web, they construct mental models based on the patterns they encounter. This section discusses a few mental models common in web design. These mental models serve as the foundation for designstandards, enabling designers to create interfaces that align with users’ expectations. When shopping online, users browse through products, add items to their shopping carts, and then check out. This experience is like shopping in a brick-and-mortar store, and people’s expectations are influenced by their experience with physical shopping. Online shopping experiences that depart from the physical shopping model create confusion. In real life, the shopper controls what goes into the shopping cart, so this expectation is carried to online shopping. When an ecommerce website violates this expectation, problems arise. For example, Nomad Lane automatically adds shipping insurance to the total cost when users check out. Users must, first, notice this additional cost and second, actively opt out of shipping insurance. Automatic shipping insurance is not part of users’ shopping cart mental model, so this leads to frustration for anyone who accidentally pays for insurance. Adding unexpected items to a shopping cart is a deceptive pattern in UX design known assneaking or preselection. TheBackbutton is a long-standing essential in browser navigation. Over time, users have built a strong mental model for it: namely,Backis supposed to lead back to the previous screen, essentially acting as an undo. When mobile smartphones became popular, users found themselves presented with another use of theBackbutton in mobile apps",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". When mobile smartphones became popular, users found themselves presented with another use of theBackbutton in mobile apps. Unlike on the web, whereBackwas used to take users to the previous screen, in some mobile apps,Backtook users up in the hierarchy of the site. For example, if, while the user was reading a news story A, they tapped on a link to another news story B, theBackbutton would take them to the list of all news articles instead of taking them back to the news story A. This behavior violated users’ expectations. Over the years, designers have luckily understood that, and now it is rare to find that implementation forBackin mobile apps. Some remnants, however, are still in place. For example, in Android, if you happen to be on the homepage of any app, doing theBackgesture (or tapping theBackbutton) will take you “up” — that is, outside of the app, regardless of whether you arrived to the homepage from another page inside the app or you just tapped on the app icon. Using theBackbutton with an open overlaycan also lead to confusion. When a user opens an overlay, it feels like they are opening a new page entirely, especially when the overlay takes up the whole screen. So, when the user clicks theBackbutton, they expect to exit the overlay. However, in many cases, the user is instead taken two steps back to the page they were on before visiting the page that triggered the overlay. Users expect to be able to input keywords and phrases into a search bar to find any information on a website. A single, well-placed search bar aligns with their expectations, offering a clear point for queries. When a website hasseveral search features on the same page, users often get confused. They’ll enter their query in the wrong box and assume that the site doesn’t have the answer if nothing comes back. (In reality, they might have used a specialized search that doesn’t cover everything.) In the example below, the University of Michigan Library has two search boxes on the same page, each attached to its own database",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The search box on the top right of the page is dedicated to logistical library information, while the search bar in the middle of the page is for academic research. The help text andplaceholder textassociated with each search bar allude to the purpose of each; however, users often perceive search bars as universal. The presence of multiple search bars introduces complexity and a potential for confusion. There's great inertia in users' mental models: stuff that people know well tends to stick, even when it's not helpful. This alone is an argument forbeing conservative and not coming up with new interaction styles. Sometimes, you do need to innovate, but it's best to do so only when the new approach is clearly superior to the old, well-known ways. When you come up with a new design pattern, you face an immense challenge: How do you explain the new concept so users will construct a valid mental model of your system? It's amazing how one misconception can thwart users throughout an entire session and cause them to systematically misinterpret everything that happens. Through failure after failure, they never question their basic assumptions. This is yet another argument for complying with preexisting user expectations whenever possible. If you don't, then make certain that you're clearly explaining what you're doing — while also realizing that you face the added challenge of users'reluctance to read instructions and help messages. Designers are better offleveraging this mental-model inertia instead of fighting it. One example is skeuomorphism, a design trend that involves incorporating elements from the physical world into digital interfaces to guide users in acclimating to new interaction styles. Drawing on familiar, real-world mental models makes new, unfamiliar designs easier to understand. In the realm ofchildren's apps,skeuomorphismplays a particularly beneficial role",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Drawing on familiar, real-world mental models makes new, unfamiliar designs easier to understand. In the realm ofchildren's apps,skeuomorphismplays a particularly beneficial role. Since young users may not yet have well-established mental models for digital navigation, apps designed for kids often use physical-space metaphors, such as maps or doors, as seen in Duo ABC, an app that teaches children reading fundamentals. This approach mimics the tangible experiences that children encounter in physical spaces, like exploring a map or opening doors to discover new areas. Understanding the concept of mental models can help you make sense of usability problems in your design. If people make mistakes on your site, it is often because they've formed erroneous mental models. Even if you cannot change the UI at that point, you can still teach users a more accurate mental model earlier in the user experience. Or, you might have to acknowledge that users won't understand certain distinctions and then stop making those distinctions. In case of a mental model mismatch, you basically have two different options: Mental models are a key concept in the development of instructions, documentation,tutorials, demos, and otherforms of user assistance. All such information must be short while teaching the key concepts that people need to know to make sense of the overall system. One of the main reasons we recommend thethink-aloud method of user testingis that it gives us insights into a user's mental model. When users verbalize what they think, believe, and predict while they use your design, you can piece together much of their mental models. There are more advancedknowledge-elicitation methodsfor gaining deeper insights into mental models, but for most design teams, a few quick think-aloud sessions will suffice. In any case, simpleuser testingis certainly the first step to take if you suspect that erroneous mental models are costing you business",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Mental Models and User Experience Design",
    "author": "Megan Chan",
    "source": "https://www.nngroup.com/articles/mental-models/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". In any case, simpleuser testingis certainly the first step to take if you suspect that erroneous mental models are costing you business. Mental models are an essential concept in UX design and significantly influence the decisions users make while navigating an interface. Understanding users' mental models helps designers create interfaces that resonate with users’ expectations. Foundational concepts that everyone should know Interaction Use psychology to predict and explain how your customers think and act Interaction Enable cookiesto watch NN/g videos What Is a Mental Model? The Hawthorne Effect: 5 Guidelines to Avoid it Rachel Banawa·5 min 3 Ways to Test Your Survey Maddie Brown·3 min Digital Wayfinding Kathryn Whitenton·3 min Card Sorting: Uncover Users' Mental Models for Better Information Architecture Samhita Tankala and Katie Sherwin·11 min Card Sorting: Pushing Users Beyond Terminology Matches Samhita Tankala·5 min Minimize Cognitive Load to Maximize Usability Kathryn Whitenton·2 min Product-Led Growth and UX Sara Paul·6 min 3 Design Processes for High Usability: Iterative Design, Parallel Design, and Competitive Testing Therese Fessenden·7 min The Aesthetic-Usability Effect Kate Moran·6 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_mental_models_and_user_experience_design.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 225 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00013-1 Copyright © 2021 Elsevier Inc. All rights reserved. Have you ever had trouble hitting a tiny button or link on a computer screen or smarphone, or trouble keeping a screen-pointer inside a narrow path required to reach a menu item or link? Perhaps your hand was jittery because you had recently consumed too much coffee or taken a medication. Maybe it was shaking due to high anxiety, anger, or fear. Maybe you have Parkinson’s disease, which makes your hands shake, or arthritis in your hands and arms that restricts your hand movement. Maybe your arm was restricted because it was temporarily in a cast or sling. Maybe you were trying to text someone while riding on a bumpy bus, train, or horse. Maybe you were using an unfamiliar pointing device. Or maybe the target was simply too small or the allowed path too narrow. It turns out that pointing at objects on a display and moving pointers along costrained paths follow consistent, quantitative laws. FITTS’ LAW: POINTING AT DISPLAYED TARGETS The law for pointing at targets is called Fitts’ law, named after the man who discovered it, Paul Fitts 1 (Fitts, 1954; Card et al., 1983). This law says that the larger your target on a screen and the nearer it is to your starting point, the faster you can point to it, the less mental effort required, and the more likely you are to hit the target. The formula for Fitts’ law allows you to predict the time for a person to move a pointer—including a finger—to a target a specified size and distance from the pointer’s starting point. T =a+ b log 2 (1 + D / W) T is the time to move to the target, D is the distance to target, and W is the width of the target along the direction of movement of the pointer. From the formula, you can see that as the distance ( D ) increases, the time ( T ) to reach the target increases, and as the target width ( W ) increases, the time to reach it decreases (see Fig. 13.1 ). Fitts’ law is quite general",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 13.1 ). Fitts’ law is quite general. It applies to any type of pointing: mouse, trackball, track pad, joystick, even fingers on touch screens. It also applies to everyone regardless of 1 The “s” is in his name. Some researchers prefer “Fitts’s”, but I prefer “Fitts’”. Our Hand–Eye Coordination Follows Laws 13 age, physical abilities, or mental state. But people vary in how fast they can move, and devices vary in how fast they can be moved, so the formula includes the parameters a and b to adjust it for those sorts of differences: a is the ease of starting and stopping the movement, and b is a measure of the average difficulty of moving the hand and pointing the device (if any). The dependency of pointing time on target size and distance codified in Fitts’ law can be understood by considering how screen-pointers move. A person sees a target on a screen and decides to hit it. Hands and pointing devices have inertia, so movement toward the target starts slowly but rapidly accelerates until reaching some maximum speed. This initial movement is fairly gross; it is essentially a shot in the general direction of the target without much control. We call the initial shot balistic , like a shell fired from a cannon. As the pointer nears the target, the movement speed slows as the person’s hand–eye feedback loops take control. The movement ends slowly, with finer and finer corrections, until the pointer is on the target (see Fig. 13.2 ). Although the basic prediction of Fitts’ law—people hit on-screen targets faster the closer and larger they are—seems pretty intuitive, the law also predicts something less intuitive: the more the distance decreases or the target grows, the less the decrease in pointing time. If a target is tiny and you double its size, the time people take to hit it decreases, but if you double the size again , the pointing time doesn’t improve quite as much. So beyond a certain size, making a target even larger provides little added beefit (see Fig",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". So beyond a certain size, making a target even larger provides little added beefit (see Fig. 13.3 ), and below a certain distance, moving a target even closer doesn’t help much. A final noteworthy prediction of Fitts’ law is that if the pointer or finger is blocked from moving past the edge of the screen, targets at the edge will be very easy to hit. People can just yank the pointer toward the target until the edge stops it with no need for slow, fine adjustments at the end of the movement. Thus, from the point of view of Fitts’ law, targets at the screen’s edge behave as if they were much larger than they actually are. However, this edge-pointing detail of the law applies mainly to destop and laptop computers, because modern smartphones and tablet computers don’t have raised edges that physically stop fingers. pointer target distance ( D ) width ( W ) FIGURE 13.1 Fitts’ law: pointing time depends on distance ( D ) and the width ( W ) of the target. Design implications of Fitts’ law Fitts’ law is the basis of several common user-interface design guidelines: l Make click-targets—graphical buttons, menu items, links—big enough that they are quick and easy for people to hit. Don’t make people tap or click on tiny targets. That will slow everyone down and cause some to miss their target. An example of click-targets that are too small is provided by United.com’s boarding pass delivery option page (see Fig. 13.4 ). To choose an option, users must click directly on the tiny checkboxes in United.com’s boarding pass delivery option FIGURE 13.2 Graph of velocity over time as pointer moves to target. Courtesy of Andy Cockburn. time to hit target target size FIGURE 13.3 Diminishing marginal benefit of increasing click-target size (if D is constant). page. The circled symbols and text labels near the checkboxes do not accept clicks. Making them accept clicks would not be difficult and would greatly increase the effective size of the click-targets",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The circled symbols and text labels near the checkboxes do not accept clicks. Making them accept clicks would not be difficult and would greatly increase the effective size of the click-targets. See Johnson and Finn (2017) for guidelines on how large a click-target or tap-target must be to be reliably clicable by people of all ages and in all situations. l Make the actual click-target at least as large as the visible click-target. Above all, don’t frustrate users by presenting big buttons that only accept clicks on a small area (e.g., the text label), as the Federal Reserve Bank did in the navigtion buttons on its 2016 website (see Fig. 13.5A ). Accept clicks over at least the entire area of the visible click-target, as the improved 2019 Federal Reserve Bank website did (see Fig. 13.5B ). If the visible target must be small (e.g., a small word embedded in text), design the user interface to treat clicks near the link as if they were on it. l Checkboxes, radio buttons, and toggle switches should accept clicks on their labels as well as on the buttons, thereby increasing the clickable area. l Leave plenty of space between buttons and links so people don’t have trouble hitting the intended one. l Place important targets near the edge of the screen to make them very easy to hit. l Display choices in pop-up and pie menus if possible (see Fig. 13.6 ). They are faster to use than pull-down menus, on average, because they open “around” the screen pointer rather than below it, so users need to move the pointer less to reach most of the items. However, even pull-down menus are faster than pulright (“walking”) menus. l For smartphone apps, consider using menus that people can reach easily with a thumb when holding the phone normally (see Fig. 13.7 ). FIGURE 13.4 Users must click on tiny checkboxes on United.com ’s boarding pass delivery option page; the symbols and text labels near the checkboxes do not accept clicks",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 13.7 ). FIGURE 13.4 Users must click on tiny checkboxes on United.com ’s boarding pass delivery option page; the symbols and text labels near the checkboxes do not accept clicks. (A) (B) FIGURE 13.5 (A) In 2016, the FederalReserve.gov website’s navigation-bar “buttons” accepted clicks only on the label. (B) In 2019, its updated website accepted clicks anywhere in each link’s rectangle. (A) (B) (C) FIGURE 13.6 Types of menus for desktop/laptop computers: (A) pop-up menu, (B) pie menu, and (C) puldown menu. FIGURE 13.7 For mobile phones, menus can be designed so the options are easily reachable with the thumb. (Source: Bob Burrough; used with permission.) STEERING LAW: MOVING POINTERS ALONG CONSTRAINED PATHS A law derived from Fitts’ law governs the time it takes to steer a screen-pointer along a constrained path to a target. Appropriately, it is named the steering law (Accot and Zhai, 1997). It says that if you must keep a pointer within a certain confined path while moving it to a target, then the wider the path, the faster you can move the pointer to the target (see Fig. 13.8 ). Its formula is simpler than that of Fitts’ law: T = a + b (D/W) Like Fitts’ law, the steering law seems like common sense: a wider path means you need not move the pointer carefully; you can move it ballistically—that is, fast. Design implications of the steering law Anyone who has used devices that have point-and-click or touch-screen user intefaces can probably think of situations where they had to steer the screen-pointer or their finger along a constrained path. Those are situations in which the steering law applies. The following are two examples: l Pull-right menus (also called “walking” menus) where you must keep the pointer within a menu item as you move it sideways to a submenu; otherwise, the menu switches to the item above or below. The narrower each menu item is, the slower the menu will be to use",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The narrower each menu item is, the slower the menu will be to use. l Page rulers (e.g., for setting tabs) where you must keep the pointer within the ruler as you drag a tab to a new location; otherwise, the tab doesn’t move (as in recent versions of Microsoft Word). The narrower the ruler, the slower your movement. Pull-right menus are fairly common in software applications. For example, Apple’s Safari browser uses them (see Fig. 13.9 ). Some software applications, such as DataTaer’s DataLogger product (see Fig. 13.10 ), have pull-right menus going out to several levels. To see how widening pointer-movement paths can speed the use of pull-right menus, compare the former and current websites of RoadScholar.org , a travel website aimed at older adult travelers. In mid-2012, a usability test of the website showed that people in the site’s target age group had trouble choosing travel destinations using the site’s pull-right menus (Finn and Johnson, 2013). By early 2013, the site’s desigers had widened the menu items significantly, making it easier and faster for users to choose travel destinations that interested them (see Fig. 13.11 ). That’s the steering law at work. Once upon a time, scrollbars in graphical user interfaces (GUIs) were constrained paths—you had to keep the pointer within the narrow vertical or horizontal scrolbar as you dragged the “elevator” along; otherwise, you lost control of the scrollbar. GUI designers soon realized that this made scrollbars slow, tedious, and error-prone target pointer distance ( D ) width ( W ) width ( W ) difficult, slow easy, fast FIGURE 13.8 Steering law: pointing time depends on distance (D) and width (W) of path. to use, so they eliminated the constraint. Modern scrollbars allow you to move the pointer outside of the bar while you drag the “elevator.” They track motion in the direction of the scrollbar only and ignore any perpendicular motion",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Modern scrollbars allow you to move the pointer outside of the bar while you drag the “elevator.” They track motion in the direction of the scrollbar only and ignore any perpendicular motion. This change effectively made the constrained path as wide as the entire screen, greatly speeding scrollbar operation and eliminating a source of errors. Another case of the steering law at work. FIGURE 13.10 Pull-right menus in DataTaker’s DataLogger application. FIGURE 13.9 Pull-right menus in Apple’s Safari browser. IMPORTANT TAKEAWAYS l The time it takes for a person to move any type of pointer to a specified target can be predicted by a formula called Fitts’ law: T = a + b log 2 ( 1 + D/W ) . The law includes several components that influence hit-time: l D = distance to target: the greater, the longer the time. l W = width of target (in the direction of motion): the wider, the shorter the time. In plain language, bigger targets are faster to hit. l a and b are parameters for the start-lag and inertia of the pointer, respectively. l Design implications of Fitts’ law: l Make click-targets big. l Make the actual click-target at least as large as the visible click-target. l UI controls should accept clicks on their labels as well as on the visible controls. l Leave plenty of space between controls so people don’t hit the wrong one. l Place important targets near the edge of the screen. l On smartphones, design menus so people can easily choose items with their thumb. l The time it takes to move a pointer along a constrained path can be predicted by a formula called the steering law: T = a + b (D/W) . The main thing this law adds to Fitts’ law is that the time to hit a target also depends on the width of the path in which a pointer has to stay while moving: the wider the path, the faster a user can move it to the target. l Design implications of the steering law: l UI controls that require the user to keep the pointer in a narrow path while moving it will be slow and error-prone to use",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Design implications of the steering law: l UI controls that require the user to keep the pointer in a narrow path while moving it will be slow and error-prone to use. Don’t require users to move a pointer within a narrow area toward a target. (A) (B) FIGURE 13.11 RoadScholar.org travel website: (A) in 2012, narrow menu items, and (B) in 2013, wider items.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-225-234.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Jakob’s Law Users spend most of their time on other sites, and they prefer your site to work the same way as all the other sites they already know. Key Takeaways • Users will transfer expectations they have built around one familiar product to another that appears similar. • By leveraging existing mental models, we can create superior user experiences in which the users can focus on their tasks rather than on learning new models. • When making changes, minimize discord by empowering users to continue using a familiar version for a limited time. Overview There is something incredibly valuable to be found in familiarity. Familiarity helps the people interacting with a digital product or service know immediately how to use it, from interacting with the navigation to finding the content they need to processing the layout and visual cues on the page in order to understand the choices available to them. The cumulative effect of mental effort saved ensures a lower cognitive load. In other words, the less mental energy users have to spend learning an interface, the more they can dedicate to achieving their objectives. The easier we make it for people to achieve their goals, the more likely they are to do so successfully. 1 1 Jakob Nielsen, “End of Web Design,” Nielsen Norman Group, July 22, 2000, https://www.nngroup.com/ articles/end-of-web-design . As designers, it is our objective to ensure people successfully achieve their goals when using the interfaces we’ve built by eliminating as much friction as possible. Not all friction is bad—in fact, sometimes it is even necessary. But when there is an opportunity to remove or avoid extraneous friction, or friction that doesn’t provide value or service a purpose, then we should do so. One of the primary ways designers can remove friction is by leveraging common design paterns and conventions in strategic areas such as page structure, workflows, navgation, and placement of expected elements such as search",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When we do this, we ensure people can immediately be productive instead of first having to learn how a website or app works. In this chapter, we’ll take a look at some examples of how this design principle can be achieved—but first, let’s look at its origins. Origins Jakob’s law (also known as “Jakob’s law of the internet user experience”) was put forth in 2000 by usability expert Jakob Nielsen, who described the tendency for users to develop an expectation of design conventions based on their cumulative experience from other websites. 1 This observation, which Nielsen describes as a law of human nature, encourages designers to follow common design convetions, enabling users to focus more on the site’s content, message, or product. In contrast, uncommon conventions can lead to people becoming frustrated, cofused, and more likely to abandon their tasks and leave because the interface does not match up with their understanding of how things should work. The cumulative experience that Nielsen refers to is helpful for people when visiting a new website or using a new product because it informs their undestanding of how things work and what’s possible. This underlying factor is pehaps one of the most important in user experience, and it is directly related to a psychological concept known as mental models . PSYCHOLOGY CONCEPT Mental Models A mental model is what we think we know about a system, especially about how it works. Whether it’s a digital system such as a website or a physical system such as a checkout line in a retail store, we form a model of how a system works, and then we apply that model to new situations where the system is similar. In other words, we use the knowledge we already have from past experiences when interacting with something new",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In other words, we use the knowledge we already have from past experiences when interacting with something new. Mental models are valuable for designers because we can match our designs to our users’ mental models to improve their experience by enbling them to easily transfer their knowledge from one product or experence to another, without the need to first take the time to understand how the new system works. Good user experiences are made possible when the design of a product or service is in alignment with the user’s mental model. The task of shrinking the gap between our own mental models and those of the users is one of the biggest challenges we face, and to achieve this goal we use a variety of methods: user interviews, personas, journey maps, empathy maps, and more. The point of these various methods is to gain a deeper insight into not only the goals and objectives of our users but also users’ preexisting mental models and how all of these factors apply to the product or experience we are designing. Examples Have you ever wondered why form controls look the way they do ( Figure 1-1 )? It’s because the humans designing them had a mental model of what these elements should look like, which they based on control panels they were familiar with in the physical world. The design of web elements like form toggles, radio inputs, and even buttons originated from the design of their tactile counterparts. Figure 1-1. Comparison between control panel elements and typical form elements (source: Jonathan H. Ward [left], Google’s Material Design [right]) When our designs do not align with the user’s mental model, there will be problems. A misalignment can affect not only how users perceive the products and services we’ve helped build, but also the speed at which they understand them. This is called mental model discordance , and it occurs when a familiar prouct is suddenly changed. One notorious example of mental model discordance is the 2018 redesign of Snapchat",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is called mental model discordance , and it occurs when a familiar prouct is suddenly changed. One notorious example of mental model discordance is the 2018 redesign of Snapchat. Instead of gradually introducing changes through slow iteration and extensive beta testing, the company launched a major overhaul that dramatically changed the familiar format of the app by combining watching stories and comunicating with friends in the same place. Unhappy users immediately took to Twitter and expressed their disapproval en masse. Even worse was the subsquent migration of users to Snapchat’s competitor, Instagram. Snap CEO Evan Spiegel had hoped that the redesign would reinvigorate advertisers and allow for ads to be customized to users, but instead it caused ad views and revenue to drop and led to the app’s user count dramatically shrinking. Snapchat failed to ensure the mental model of its users would be aligned with the redesigned version of the app, and the resulting discordance caused a major backlash. But major redesigns don’t always drive users away—just ask Google. Google has a history of allowing users to opt in to redesigned versions of its products, like Google Calendar, YouTube, and Gmail. When the company launched the new version of YouTube in 2017 ( Figure 1-2 ) after years of essentially the same design, it allowed desktop users to ease in to the new Material Design UI without having to commit. Users could preview the new design, gain some familiarity, submit feedback, and even revert to the old version if they preferred it. The inevtable mental model discordance was mitigated by simply empowering users to switch when they were ready. Figure 1-2. Before (left) and after (right) comparison of YouTube redesign in 2017 (source: YouTube) Most ecommerce websites also leverage preexisting mental models. By maing use of familiar patterns and conventions, shopping sites such as Etsy ( Figure 1-3 ) can effectively keep customers focused on the important stuff—fining and purchasing products",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". By maing use of familiar patterns and conventions, shopping sites such as Etsy ( Figure 1-3 ) can effectively keep customers focused on the important stuff—fining and purchasing products. By conforming to users’ expectations about the process of selecting products, adding them to the virtual cart, and checking out, designers can ensure users are able to apply their accumulated knowledge from previous ecommerce experiences; the whole process feels comfortable and familiar. Figure 1-3. Ecommerce sites like Etsy leverage preexisting mental models to keep customers focused on purchasing products rather than on learning new interaction patterns (source: Etsy, 2019) The use of mental models to inform design isn’t isolated to the digital space. Some of my favorite examples can be found in the automotive industry, specifcally in regard to controls. Take, for instance, the 2020 Mercedes-Benz EQC 400 Prototype ( Figure 1-4 ). The seat controls found on the door panel next to each seat are mapped to the shape of the seat. The resulting design makes it easy for users to understand which part of their seat they can adjust by identifying the corresponding button. It’s an effective design because it builds on our preexisting mental model of a car seat and then matches the controls to that mental model. Figure 1-4. Seat controls in the 2020 Mercedes-Benz EQC 400 Prototype, informed by the mental model of a car seat (source: MotorTrend, 2018) These examples demonstrate how we can leverage users’ existing mental models to enable them to become immediately productive. In contrast, failure to consider the mental model a user has formed can result in confusion and frustrtion",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In contrast, failure to consider the mental model a user has formed can result in confusion and frustrtion. The conclusion here also begs an important question: does Jakob’s law argue that all websites or apps should behave identically? Additionally, does it suggest that we should use only preexisting UX patterns, even when there’s a more appropriate solution that’s new? TECHNIQUE User Personas Have you ever heard another designer within your company or organiztion refer to “the user,” but it wasn’t quite clear whom exactly this elusive person was? The process of design becomes more difficult when a design team lacks a clear definition of its target audience, leaving each designer to interpret it in their own way. User personas are a tool that helps solve this problem by framing design decisions based on real needs, not the generic needs of the undefined “user.” These fictional reresentations of a specific subset of the target audience are based on aggregated data from real users of a product or service ( Figure 1-5 ). Figure 1-5. User persona example Personas are intended to foster empathy and serve as memory aids, as well as to create a common mental model of the traits, needs, motivtions, and behaviors of a specific kind of user. The frame of reference that personas help to define is incredibly valuable for teams: it helps team members move away from self-referential thinking and focus on the needs and goals of the user, which is useful for prioritizing new features. Any details about the user that are relevant to the feature or product you’re building will be useful. The items common to most personas include: Info Items such as a photo, memorable tagline, name, age, and occuption are all relevant for the information section of a persona. The idea here is to create a realistic representation of the members of a specific group within your target audience, so this data should be reflective of the similarities they share",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The idea here is to create a realistic representation of the members of a specific group within your target audience, so this data should be reflective of the similarities they share. Details The information within the details section of a user persona helps to build empathy and align focus on the characteristics that impact what is being designed. Common information here includes a bio to create a deeper narrative around the persona, behavioral qualities that are relevant, and frustrations this particular group might have. Additional details could include things like goals and motivations, or tasks the user might perform while using the product or feature. Insights The insights section of a user persona helps to frame the attitude of the user. The intention here is to add an additional layer of context that provides further definition of the specific persona and their mindset. This section often includes direct quotes from user research. KEY CONSIDERATION Sameness I know what you’re thinking: if all websites or apps followed the same design conventions, that would make everything quite boring. This is a completely valid concern, especially given the ubiquity of specific coventions that can be observed today. This pervasive sameness can be attributed to a few factors: the popularity of frameworks to speed up development, the maturity of digital platforms and resulting standards, clients’ desire to emulate their competition, and just plain lack of creatiity. While much of this sameness is purely based on design trends, there is a good reason we see patterns with some conventions, such as the placement of search, navigation in the footer, and multistep checkout flows. Let’s take a moment to consider the alternative: imagine that each and every website or app that you used was completely different in every regard, from the layout and navigation down to the styling and common conventions like the location of the search feature",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Considering what we’ve learned about mental models, this would mean that users could no longer rely on their previous experiences to guide them. Their ability to be instantly productive in achieving the goal they wanted to accomplish would be immediately thwarted because they would first have to learn how to use the website or app. It is no stretch of the imagination to see that this would not be an ideal situation, and conventions would eventally emerge out of pure necessity. That’s not to say that creating something entirely new is never appropriate—there’s certainly a time and a place for innovation. But designers must determine the best approach by taking into considertion user needs and context, in addition to any technical constraints, before reaching for something unique, and they must take care not to sacrifice usability. Conclusion Jakob’s law isn’t advocating for sameness in the sense that every product and experience should be identical. Instead, it is a guiding principle that reminds designers that people leverage previous experience to help them in understaning new experiences. It is a not-so-subtle suggestion that (when appropriate) designers should consider common conventions that are built around existing mental models to ensure users can immediately be productive instead of first needing to learn how a website or app works. Designing in a way that conforms to expectations allows users to apply their knowledge from previous experiences, and the resulting familiarity ensures they can stay focused on the important stuff—finding the information they need, purchasing a product, etc. The best piece of advice I can give in regard to Jakob’s law is to always begin with common patterns and conventions, and only depart from them when it makes sense to. If you can make a compelling argument for making something different to improve the core user experience, that’s a good sign that it’s worth exploring",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If you can make a compelling argument for making something different to improve the core user experience, that’s a good sign that it’s worth exploring. If you go the unconventional route, be sure to test your design with users to ensure they understand how it works.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-1-12.json_chunk_9"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": "We carry a range of biosensors from the top hardware producers. All compatible with iMotions Academia Morten Pedersen Consumer Insights Serena Pang Bryn Farnsworth Discover the essentials of eye tracking technology, from how it works to its applications in research and marketing. Learn best practices for setup, data collection, and analysis to gain deeper insights into human behavior and decision-making. Welcome to the world of eye tracking, a fascinating journey into understanding how our eyes reveal much more than what we just see. Often referred to as the windows to the soul, our eyes offer a unique glimpse into human behavior, attention, and cognition. Whether you’re a student stepping into this intriguing field, a professional exploring new research methodologies, or simply curious about how eye tracking technology can unveil the unseen aspects of human perception, this guide is your starting point. Eye tracking technology, once a tool reserved for high-end research labs, has now found its way into various aspects of our daily lives. From enhancing user experience in technology to advancing medical research, the applications of eye tracking are as diverse as they are groundbreaking. But what exactly is eye tracking? How does it work, and why is it so important in understanding human behavior? In this comprehensive pocket guide, we’ll demystify eye tracking technology, breaking it down into simple, understandable concepts. You’ll discover the basic principles behind how we track eye movements, the different types of eye tracking devices, and the myriad ways this technology is being applied – from improving marketing strategies to developing life-changing assistive devices. So, whether you’re writing a thesis, designing a new video game interface, or just satisfying your curiosity, join us on this eye-opening journey into the world of eye tracking. Let’s explore together how this remarkable technology is not just watching where we look, but also helping to shape the future",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Let’s explore together how this remarkable technology is not just watching where we look, but also helping to shape the future. Also, make sure not to miss ourwebinar covering all about the different types of eye tracking setups. – How exactly does eye tracking work? Eye tracking use is on the rise.While early devices were highly intrusive and involved particularly cumbersome procedures to set up, modern eye trackers have undergone quite a technological evolution in recent years. Long gone are the rigid experimental setups and seating arrangements you might think of. Modern eye trackers are hardly any larger than smart phones and provide an extremely natural experience for respondents. Remote, non-intrusive methods have made eye tracking both an easy-to-use andaccessible tool in human behavior researchthat allows objective measurements of eye movements in real-time. Most modern eye trackers utilize near-infrared technology along with a high-resolution camera (or other optical sensor) to track gaze direction. The underlying concept, commonly referred to asPupil Center Corneal Reflection (PCCR), is actually rather simple. It essentially involves the camera tracking the pupil center, and where light reflects from the cornea. An image of how this looks like is on the right. The math behind it is well, a bit more complex. We won‘t bore you with the nature of algorithms at this point. Image above: Pupil Center Corneal Reflection (PCCR). The light reflecting from the cornea and the center of the pupil are used to inform the eye tracker about the movement and direction of the eye. The accuracy of eye movement measurement heavily relies on a clear demarcation of the pupil and detection of corneal reflection. The visible spectrum is likely to generate uncontrolledreflections, while illuminating the eye with infrared light – which is not perceivable by the human eye – renders the demarcation of the pupil and the iris an easy task – while the light directly enters the pupil, it just reflects from the iris",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". This means that a clear contrast is generated (with little noise) and can, therefore be followed by algorithms (running inside the eye tracker) with ease. Near-infrared light is directed toward the center of the eyes (the pupils) causing visible reflections in the cornea (the outer-most optical element of the eye), and this high-contrast image is tracked by a camera. There are three main types of eye tracker: Screen-based (also called remote or desktop) glasses, (also called mobile) and eye tracking within VR headsets. Webcam-based eye tracking has been seen as an option, but this technology is inherently inferior to infrared-based eye trackers (something we cover in this blog post). Screen-based Glasses The Complete Pocket Guide – How do the trackers compare? Measurement precision certainly is crucial in eye movement research. The quality of the collected data depends primarily on the tracking accuracy of the device you use. Going for a low quality system will prevent you from being able to extract high precision data. A common misconception is that researchers face an inevitable trade-off between measurement accuracy and the amount of movement the respondent can make with their head. The truth is a bit more complex than that Screen-based eye trackers: Require respondents to sit in front of a screen or close to the stimulus being used in the experiment. Although screen-based systems track the eyes only within certain limits, they are able to move a limited amount, as long as it is within the limits of the eye tracker’s range. This range is called theheadbox. The freedom of movement is (usually) sufficiently large for respondents to feel unrestricted. Eye tracking glasses: Are fitted near the eyes and therefore allow respondents to move around as freely as they would like – certainly a plus if your study design requires respondents to be present in various areas (e.g. a large lab setting, or a supermarket). Does that imply that eye tracking glasses are more susceptible to measurement inaccuracies? Not at all",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". a large lab setting, or a supermarket). Does that imply that eye tracking glasses are more susceptible to measurement inaccuracies? Not at all. As long as the device is calibrated properly, head-mounted eye trackers are unaffected by head movements and deliver high precision gaze data just like screen-based devices. Also, as the eye tracking camera is locked to the head‘s coordinate system, the overlaying of eye movements onto the scene camera does not suffer from inaccuracies due to head movement. – Use cases in research You may be surprised to discover that eye tracking is not exactly a novelty – it has in fact been around for many years in psychological research. Given the well-established relationship between eye movements and human cognition, it makes intuitive sense to utilize eye tracking as anexperimental method to gain insightinto the workings of the mind. If eye tracking is old news, how come it is the latest buzz in human behavior research? First let‘s rewind a bit. Studies of eye movements based on simple observation stretch back more than 100 years ago. In 1901, the first eye tracker was built, but could only record horizontal eye movements and required a head-mount. In 1905, eye movements could be recorded using “a small white speck of material inserted into the participant‘s eyes“. Not exactly the most enjoyable experimental setup. It‘s safe to say that eye tracking has come a long way. With technological advancements, modern eye trackers have became less intrusive, more affordable, accessible, and experimental sessions have became increasingly comfortable and easier to set up (long gone are the scary “white specks“ and head-mounts). Currently, eye tracking is being employed by psychologists, neuroscientists, human factor engineers, marketers, designers, architects – you name it, it’s happening. In the following pages, we’ll go through some of the most common application areas for eye tracking, and see how it helps guide new discoveries and insight in each",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". In the following pages, we’ll go through some of the most common application areas for eye tracking, and see how it helps guide new discoveries and insight in each. Neuroscience and psychology utilize eye tracking to analyze the sequence of gaze patterns to gain deeper insights into cognitive processes underlying attention, learning, and memory. How do expectations shape the way we see the world? For example, if you see a picture of a living room, you will have an idea of how the furniture should be arranged. If the scene doesn’t match your expectations, you might be baffled and gaze around the scene as your “scene semantics” (your “rules” of how a living room should look) are violated. Another research area addresses how we encode and recall faces – where do we look to extract the emotional state of others? Eyes and mouth are the most important cues, but there’s definitely a lot more to it. Another research area addresses how we encode and recall faces – where do we look to extract the emotional state of others? Eyes and mouth are the most important cues, but there’s definitely a lot more to it. Eye tracking can also provide insights into processing of text, particularly how eye movements during reading are affected by the emotional content of the texts. Eye tracking can provide crucial information about how we attend to the world – what we see and how we see it. Why is it that some products make an impression on customers while others just don‘t get it right? Eye tracking has become a popular, increasingly vital tool in market research. Many leading brands actively utilize eye tracking to assess customer attention to key messages and advertising as well as to evaluate product performance, product and package design, and overall customer experience. When applied to in-store testing, eye tracking provides information about the ease and difficulty of in-store navigation, search behavior, and purchase choices. There are various different ways in which to investigate human behavior in simulations",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". There are various different ways in which to investigate human behavior in simulations. One of the most common approaches is to use a driving simulator. Such research often use eye tracking glasses combined with a several other sensors to gain a better understanding of human behavior in hazardous situations. Where do drivers look when they face obstacles on the street? How does talking on the phone affect driving behavior? How exactly does speeding compromise visual attention? Insights of that kind can help improve hazard awareness and be applied to increase future driver safety. Automotive research has embraced eye tracking glasses for a long time to asses the drivers‘ visual attention – both with respect to navigation and dashboard layout. In the near future automobiles might even be able to respond to the drivers’ eye gaze, eye movements, or pupil dilation. So what is Human Computer Interaction (HCI)? Essentially, HCI research is concerned with how computers are used and designed, and how this relates to their use by people. From laptops, tablets, smart phones, and beyond, he use of technology can be evaluated by measuring our visual attention to the devices we use. A rapidly growing field that utilizes eye tracking as a methodology for assessment is usability and user experience testing. Eye tracking for website testing is an often utilized approach, giving insights into how websites are viewed and experienced. How do people attend to adverts, communication, and calls to action (CTAs)? If you‘re losing out on revenue, eye tracking data can deliver valuable insights into the gaze patterns of your website visitors – how long does it take them to find a specific product on your site, what kind of visual information do they ignore (but are supposed to see)? Cut to the chase and see exactly what goes wrong. The very same investigations can even be applied to mobile apps on tablets and smartphones",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". The very same investigations can even be applied to mobile apps on tablets and smartphones. What if learning could be an equally satisfying experience for all of us? What exactly does it take to make learning a success? In recent years, eye tracking technology has impressively made its way into educational science to help gain insights into learning behavior in diverse settings ranging from traditional “chalk and talk“ teaching approaches to digital learning. Analyzing visual attention of students during classroom education, for example, delivers valuable information in regard to which elements catch and hold interest, and which are distracting or go unseen. Do students read or do they scan slides? Do they focus on the teacher or concentrate on their notes? Does their gaze move around in the classroom? Eye tracking findings like these can be effectively used to enhance instructional design and materials for an improved learning experience in the classroom and beyond. Eye tracking in combination with conventional research methods or other biosensors can help assess and potentially diagnose conditions such asAttention Deficit Hyperactivity Disorder (ADHD), Autism Spectrum Disorder (ASD), Obsessive Compulsive Disorder (OCD), schizophrenia, Parkinson‘s disease, and Alzheimer‘s disease. Additionally, eye tracking technology can be used to detect states of drowsiness or support multiple other fields of medical use, quality assurance or monitoring. Eye tracking has recently been introduced into the gaming industry and has since become an increasingly prominent tool as Designers are now able to assess and quantify measures such as visual attention and reactions to key moments during game play to improve the overall gaming experience. When combined with other biometric sensors, designers can utilize the data to measure emotional and cognitive responses to gaming. New trends and developments may soon render it possible to control the game based on pupil dilation and eye movements",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_7"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". New trends and developments may soon render it possible to control the game based on pupil dilation and eye movements. – Understanding the results Eye tracking makes it possible to quantify visual attention like no other metric, as it objectively monitors where, when, and what people look at. Fixation and Gaze points Without a doubt, the terms fixation and gaze points are the most prominent metrics in eye tracking literature. Gaze pointsconstitute the basic unit of measure – one gaze point equals one raw sample captured by the eye tracker. The math is easy: If the eye tracker measures 60 times a second, then each gaze point represents a sixtieth of a second (or 16.67 milliseconds). If a series of gaze points happens to be close in time and range, the resulting gaze cluster denotes afixation, a period in which our eyes are locked toward a specific object. Typically, the fixation duration is 100 – 300 milliseconds. The eye movements between fixations are known assaccades. What are they exactly? Take reading a book, for example. While reading, your eyes don’t move smoothly across the line, even if you experience it like that. Instead, your eyes jump and pause, thereby generating a vast number of discrete sequences. These sequences are called saccades. Perceptual span and smooth pursuit Reading involves both saccades and fixations, with each fixation involving aperceptual span. This refers to the number of characters we can recognize on each fixation, between each saccade. This is usually 17-19 letters, dependent on the text. Experienced readers have a higher perceptual span compared to early readers, and can therefore read faster. Imagine watching clouds in the sky as you pass your time waiting at the bus stop. As you now know about saccades, you might expect your eye movements to in this scenario to behave in the same way – but the rules are a bit different for moving objects. Unlike reading, locking your eyes toward a moving object won’t generate any saccades, but asmooth pursuittrajectory",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_8"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Unlike reading, locking your eyes toward a moving object won’t generate any saccades, but asmooth pursuittrajectory. This way of seeing operates as you might expect – the eyes smoothly track the object. This occurs up to 30°/s – at speeds beyond this, saccades are used to catch up to the object. As fixations and saccades are excellent measures of visual attention and interest, they are most commonly used to fuel discoveries with eye tracking data. Now let‘s get practical and have a look at themost common metrics used in eye tracking research(that are based on fixations and gaze points) and what you can make of them. Heat maps are static or dynamicaggregations of gaze points and fixationsrevealing the distribution of visual attention. Following an easy-to-read color-coded scheme, heat maps serve as anexcellent method to visualize which elements of the stimulus were able to draw attention– with red areas suggesting a high number of gaze points (and therefore an increased level of interest), and yellow and green areas showing fewer gaze points (and therefore a less engaged visual system. Areas without coloring were likely not attended to at all. Areas of Interest (AOI) Areas of Interest, also referred to as AOIs, areuser-definedsubregionsof a displayed stimulus. Extracting metrics for separate AOIs might come in handy when evaluating the performance of two or more specific areas in the same video, picture, website or program interface. This can be performed to compare groups of participants, conditions, or different features within the same scene Based on fixation position (where?) and timing information (when?) you can generate a fixation sequence. This is dependent onwhere respondents look and how much time they spend, and provides insight into the order of attention, telling you where respondents looked first, second, third etc",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_9"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". This is dependent onwhere respondents look and how much time they spend, and provides insight into the order of attention, telling you where respondents looked first, second, third etc. This is a commonly used metric in eye tracking research as it reflects salient elements (elements that stand out in terms of brightness, hue, saturation etc.) in the display or environment that are likely to catch attention. The time to first fixation indicates theamount of time it takes a respondent to look at a specific AOI from stimulus onset. TTFF can indicate both bottom-up stimulus driven searches (a flashy company label catching immediate attention, for example) as well as top-down attention driven searches (respondents actively decide to search for certain elements or areas on a website, for example). TTFF is a basic yet very valuable metric in eye tracking. Time spentquantifies the amount of time that respondents spent looking at an AOI. As respondents have to blend out other stimuli in the visual periphery that could be equally interesting, the amount of time spent often indicates motivation and conscious attention (prolonged visual attention at a certain region clearly points to a high level of interest, while shorter times indicate that other areas on screen or in the environment might be more catchy). The respondent count describeshow many of your respondents actually guided their gaze towards a specific AOI. A higher count shows that the stimulus is widely attended to, while a low count shows that little attention is paid to it. – More than meets the eye With the core tools at hand, you‘re perfectly equipped to track the basics. You can now find out where, when and what people look at, and even what they fail to see. So far, so good. Now how about pushing your insights a bit further and stepping beyond the basics of eye tracking? Pupil size / dilation An increase in pupil size is referred to as pupil dilation, and a decrease in size is called pupil constriction",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_10"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Pupil size primarily responds to changes in light (ambient light) or stimulus material (e.g. video stimulus). However, if the experiment can account for light, other attributes can be derived from changes in pupil size. Two common properties areemotional arousal(referring to the amount of emotional engagement) andcognitive workload(which refers to how mentally taxing a stimulus is). In most casespupillary responses are used as a measure for emotional arousal. However, be careful with overreaching conclusions as pupillary responses alone don’t give any indication of whether arousal arises from a positive (“yay“!) or negative stimulus (“nay!“). Distance to the screen Along with pupil size, eye trackers also measure the distance to the screen and the relative position of the respondent. Leaning forwards or backwards in front of a remote device is tracked directly and can reflectapproach-avoidance behavior. However, keep in mind that interpreting the data is always very specific to the application. Ocular Vergence Most eye trackers measure the positions of the left and right eyes independently. This allows the extraction ofvergence, i.e., whether left and right eyes move together or apart from each other. This phenomenon is just a natural consequence of focusing near and far. Divergence often happens when our mind drifts away, when losing focus or concentration. It can be picked up instantly by measuring inter-pupil distance. Blinks Eye tracking can also provide essential information oncognitive workloadby monitoring blinks. Cognitively demanding tasks can be associated with delays in blinks, the so-calledattentional blink. However, many other insights can be derived from blinks. A very low frequency of blinks, for example, is usually associated with higher levels of concentration. A rather high frequency is indicative of drowsiness and lower levels of focus and concentration",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_11"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". A rather high frequency is indicative of drowsiness and lower levels of focus and concentration. Objective answers with eye tracking To date, eye tracking is the only method in human behavior research that makes it possible to objectively measure and quantify eye movements.With eye tracking, you can tap into nonconscious mental processing. Eye tracking can be used to assess which elements in your product design or advertisement catch attention, and allow you to obtain insights into your respondents‘ individual preferences by observing which elements they dwell on over time. Eye motion is tightly linked to visual attention. As a matter of fact, you just can‘t move your eyes without moving attention. You can however certainly shift attention without moving your eyes. While eye tracking can tell us what people look at and what theysee, it can’t tell us what peopleperceive. Think of this classic example: You open the fridge in search of a milk carton. While it is right in front of you, you can‘t seem to find it. You keep looking until you close the door empty-handed. You have to take your coffee black that day. What happened exactly in this scenario? Even though you saw the carton(and this is what eye tracking data would tell us), you were probably not paying sufficient attention to actuallyperceivethat it was right in front of your eyes. You simply overlooked the bottle, although eye tracking data would have told you otherwise. Take a moment and transfer the milk bottle problem to website testing, for example. Imagine your carefully designed call to action just doesn’t cut it as hoped. While eye tracking can certainly reveal if your website visitors actively guide their gaze toward the call to action (let‘s assume they do), it won’t tell you if they really perceive the flashy button that you want them to click. The figure below illustrates how nonconscious processing of a stimulus occurs before our conscious processing kicks in",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_12"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". The figure below illustrates how nonconscious processing of a stimulus occurs before our conscious processing kicks in. We can miss the milk carton in the fridge because we don’t spend long enough attending to it to become consciously aware of it. As an objective measure, eye tracking indicates: So what‘s the bottom line? Eye tracking gives incredible insights into where we direct our eye movements at a certain time and how those movements are modulated by visual attention and stimulus features (size, brightness, color, andlocation). However, tracking gaze positions alone doesn’t tell us anything particular about the cognitive processes and the emotional states that drive eye movements. In these cases, eye tracking needs to be complemented by other biosensors to capture the full picture of human behavior in that very moment. Let’s find out how. – Next level insights Why combine eye tracking with other biosensors? Take the milk carton situation. Although you looked straight at the carton, you didn’t realize it was standing right in front of you. Based on eye tracking alone we would have argued that, as your gaze was directed towards the item, you must have seen it. The most intuitive way to validate our assumption might be to just ask you – “did you see the milk carton?”. Primarily due to the relatively low costs, surveys are in fact a very common research tool to consolidate gaze data withself-reportsof personal feelings, thoughts, and attitudes. Of course, people might not be aware of what they are really thinking – they weren’t consciously aware of the milk carton after all, and this extends to other stimuli too. The power of self-reports is also limited when it comes to the disclosure of sensitive personal information (alcohol, drugs, sexual behavior etc.)",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_13"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". The power of self-reports is also limited when it comes to the disclosure of sensitive personal information (alcohol, drugs, sexual behavior etc.). Keep in mind that when working with self-reports, any delay between action and recollection introduces artifacts – asking immediately after closing the fridge might yield a different response (“nope, I didn’t see it!”) compared to one week later (“um, I don‘t remember!”). Take your study from ordinary to extraordinary We could have used webcam-basedfacial expression analysisto monitor your emotional valence while staring into the fridge, for example. A confused expression while scanning the items in the fridge followed by a sad expression would have told us how you were feeling when realizing that the milk was missing. On the other hand, a slight smile would tell a different story. Furthermore, we could have quantified your level of emotional arousal and stress levels based on the changes in skin conductance (measured as galvanic skin response) or your heart rate (as measured by electrocardiography). On top of this, we could have usedelectroencephalography(EEG) to capture your cognitive and motivational state as it is the ideal tool to identify fluctuations in workload (“have I looked everywhere?”), engagement (“I have to find this bottle!”), or even drowsiness levels. Of course, this example is a simplification of the interpretation of physiological data. In most research scenarios you will have to consider and control for many more factors that might have a significant impact on what you are investigating. Each biosensor can reveal a specific aspect of human cognition, emotion, and behavior. Depending on your individual research question, consider combining eye tracking with two or more additional biosensors in order to gain meaningful insights into the dynamics of attention, emotion, and motivation. What‘s the gain? The true power of eye tracking unfolds as it is combined with other sources of data to measure complex dependent variables",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_14"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". What‘s the gain? The true power of eye tracking unfolds as it is combined with other sources of data to measure complex dependent variables. These 5 biosensors are a perfect complement to eye tracking. Which metrics can be extracted from the different systems? Have a look. Facial Expression Analysis Facial expression analysis is a non-intrusive method to assess emotional reactions throughout time. While facial expressions can measure the presence of an emotion (valence), they can’t measure the intensity of that emotion (arousal). EEG Electroencephalography is a technique that measures the electrical activity of the brain at the scalp. EEG provides information about brain activity during task performance or stimulus exposure. It allows for the analysis of brain dynamics that provide information about the levels ofengagement (arousal), motivation, frustration, cognitive workload. Other metrics can also be measured that are associated with stimulus processing, action preparation, and execution. GSR (EDA) Galvanic skin response (or electrodermal activity) monitors the electrical activity across the skin generated by physiological or emotional arousal. Skin conductance offers insights into the respondents’nonconscious arousalwhen being confronted with emotionally loaded stimulus material EMG Electromyographic sensors monitor the electric activity of muscles. You can use EMG to monitormuscular responsesto any type of stimulus material to extract even subtle activation patterns associated with emotional expressions (facial EMG) or consciously controlled hand/finger movements. ECG & PPG Electrocardiography (ECG) and photoplethysmography (PPG) allow for recording of the heart rate or pulse. You can use this data to obtain insights into respondents’physical state, anxiety and stress levels (arousal), and how changes in physiological state relate to their actions and decisions. – Perfect research every time Failure or complications in studies most often occur due to small mistakes that could have easily been avoided",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_15"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". – Perfect research every time Failure or complications in studies most often occur due to small mistakes that could have easily been avoided. Often this happens because researchers and staff just didn’t know about the basics to avoid running into issues. Conducting an eye tracking study involves juggling with a lot of moving parts. A complex experimental design, new respondents, different technologies, different hardware pieces, different operators. It can of course be quite challenging at times. We’ve all been there. But don‘t worry, we’ve got your back. The following are our 6 top tips for a smooth lab experience in eye tracking research. 1. Environment and lighting conditions Have a dedicated space for running your study. Find an isolated room that is not used by others so you can keep your experimental setup as consistent for each participant as possible. Make sure to place all system components on a table that doesn’t wobble or shift. For eye tracking, lighting conditions are essential. Avoid direct sunlight coming through windows (close the blinds!) as sunlight contains infrared light that can affect the quality of the eye tracking measurements. Avoid brightly lit rooms (no overhead light). Ideally, use ambient light. It‘s particularly important to keep the lighting levels consistent when measuring levels of pupil dilation (pupillometry) – this goes for both the luminance of the stimulus and the brightness of the room. Be aware that long experiments might cause dry eyes, resulting in drift. Try to keep noise from the surrounding environment (rooms, corridors, streets) at a minimum, as it could distract the respondent and affect measurement validity. 2. Work with a dual screen configuration Work with a dual screen configuration – one screen for the respondent for stimulus presentation (which ideally remains black until stimulus material appears) and one screen for the operator (which the respondent should not be able to see) to control the experiment and monitor data acquisition",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_16"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". A dual screen setup allows you to detect any issues with the equipment during the experiment, without interfering with the respondent’s experience. 3. Clean your computer before getting started Clean your computer from things you don‘t need. Disable anything that could interfere with your computer, e.g. make sure to turn off your antivirus software so it doesn’t pop up during the experiment and use CPU resources. Disconnect your computer from the internet during data collection if it’s not required for the experiment to avoid any potential interruptions. It can also be a good idea to disable your screen saver. 4. Ensure all people involved are properly trained It is essential that the people involved in data collection are trained on the systems used so that they have a level of knowledge that allows them to run a study smoothly. Generally, training is important for any kind of position in the lab. Having to train people before the testing process is advantageous and allows you to prepare for potential mishaps or missteps made during the experiment. 5. Always use protocols Always have protocols! It is essential to have any instructions or any documentation that is associated with setting up and/or running a study at the lab readily available in written form. Try to have templates for every step of the research process. Literally. Don‘t underestimate the importance of documentation in institutions such as a university. It‘s common practice for research assistants to switch labs after a certain amount of time – protocols are true lifesavers as they keep track of anything from management to study execution; therefore, make sure that new lab members can jump right in and are able to perform on the fly in line with lab routines. 6. Simplify your technology setup Chances are that you need a couple of different biosensors to run your study. To make sure that they interact well and are compatible with each other, use as few vendors as possible for both harand software",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_17"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". To make sure that they interact well and are compatible with each other, use as few vendors as possible for both harand software. In the ideal setup everything would be integrated into a single software platform. Having to switch between different operating systems or different computers can cause difficulties. Keep in mind that it‘s easier to train lab members on a single software than on multiple. The equation is simple: Having a single software platform decreases the amount of training needed, simplifies the setup and reduces the risk of human error. Also, in case of problems and support issues it is more convenient to deal with one vendor and have a direct contact person than to be shuffled around between vendors because no single point of contact is responsible. – Hardware suited to your need At this point you might think that all eye tracking systems are pretty much the same. Their only job is to track where people are looking, so how much can they vary? Actually, quite a lot. The use of eye tracking is increasing all the time, and to keep pace with demand, new systems are continually released. Amidst all the manufacturer specifications it can be difficult to stay current on the available options and evaluate which eye tracker is best for your needs Begin with the obvious: 1)Will your respondents be seated in front of a computer during the session? Go for a screen-based eye tracker. Do your respondents need to move freely in a natural setting? Choose a head-mounted system that allows for head and body mobility. 2)We have covered this already, but here it is again: Even though it‘s a cheaper solution, it‘s best to avoid webcam-based eye trackers. Yes, we know the temptation of a good bargain – however,when it comes to eye trackers it‘s absolutely worth spending a bit of extra moneyif you‘re aiming for worthwhile measurement accuracy. 3)Make sure the eye tracker you purchase meets the specifications required to answer your research questions",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_18"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". 3)Make sure the eye tracker you purchase meets the specifications required to answer your research questions. Have a look at the key specifications to the right that can help you find a suitable eye tracker. Measurement precision:Measured in degrees. Standard is about 0.5 degree. Low-end hardware starts at around 1.0 degree, medium 0.5 degree, and high end at 0.1 degree or even less. Trackability:How much of the population can be tracked? The best systems track around 95% of the population, low end systems less. Sampling rate:How many times per second is the eye position measured? Typical value range is 30-60 Hz. Special research equipment records at around 120-1000+ Hz. Headbox size:To what extent is the respondent allowed to move in relative distance to the eye tracker? A good system will typically allow around 11 inches (28cm) of movement in each direction. Recapture rate:How fast does the eye tracker detect the eye position after the eyes have been out of sight for a moment (e.g. during a blink)? Integrated or standalone:Is the eye tracking hardware integrated into the monitor frame? Standalone eye trackers usually offer high-end specifications, but are also typically a bit more complex to set up. Does your provider offer support?With most eye trackers, you can usually run your eye tracker out of the box. To get started, however, a live training is helpful to learn the ropes. Even along the way, a little expert advice often comes in handy. Does your provider offer that kind of support? What about online support? And how long does it take them to reply when you need it the most? Seriously, good support is priceless. Hardware is only half the battle – finding the right software solution. Of course, hardware is only half the battle. Before you can kick off your eye tracking research, you definitely need to think about which recording and data analysis software to use. Usually, separate software is required for data acquisition and data processing",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_19"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Usually, separate software is required for data acquisition and data processing. Although some manufacturers offer integrated solutions, you will most likely have to export the raw data to a dedicated analysis software for data inspection and further processing. So which eye tracking software solution is the one you need? Let’s first look at the usual struggles that are encountered with eye tracking software. Struggle 1:Eye tracking software either records or analyzes Usually, separate software is necessary for data recording and data processing. Despite automated procedures, proper data handling requires careful manual checks along the way. This recommended checking procedure is time-consuming and prone to error. Struggle 2:Eye tracking software is bound to specific eye trackers Typically, eye tracking sofand hardware are paired. One software is only compatible with one specific eye tracker, so if you want to mix and match devices or software even within one brand, you could soon hit a wall. Also, be aware that you will need separate software for remote and mobile eye trackers. Not only does operating several software solutions require expert training beforehand, it might even prevent you from switching from one system to another. Worst case scenario? Your lab will stick to outdated trackers and programs even though the latest generation of devices and software might offer improved usability and extended functionality. Struggle 3:Eye tracking software is limited to certain stimulus categories Usually, eye tracking systems don ́t allow the recording of eye movements across different conditions and types of stimuli. You will have to use one software for static images and videos, a different software for websites and screen captures, and yet another software for scenes or mobile tracking. Struggle 4:Eye tracking software can be complex to use Eye tracking software can be quite complex to use",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_20"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Struggle 4:Eye tracking software can be complex to use Eye tracking software can be quite complex to use. You have to be familiar with all relevant software-controlled settings for eye tracker sampling rate, calibration, gaze or saccade/fixation detection etc. In the analytic framework, you have to know how to generate heat maps, select Areas of Interest (AOIs) or place markers. Statistical knowledge is recommended to analyze and interpret the final results. Struggle 5:Eye tracking software rarely supports different biosensors eye tracking software almost always is just that – it tracks eyes but rarely connects to other biosensors to allow recordings of emotional arousal and valence. You will therefore need to use different recording software for your multimodal research. As you most likely will have to set up each system individually, a considerable amount of technical skill is required. If you happen to be a tech whiz, you might be on the safe side. If not, you could run into serious issues even before getting started. Also, be aware that you have to make sure that the different data streams are synchronized. Only then you can analyze how different biosensor data relate to each other. Ideally, your eye tracking software: The iMotions platform is an easy to usesoftware solution for study design, multisensor calibration, data collection, and analysis. Out of the box, iMotions supports over 50 leading eye trackers and biosensors including EEG, GSR, ECG, EMG, and facial expression analysis along with surveys for multimodal human behavior research",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_21"
  },
  {
    "document_type": "online_article",
    "title": "Eye Tracking: The Complete Pocket Guide",
    "author": "Morten Pedersen",
    "source": "https://imotions.com/blog/learning/best-practice/eye-tracking/",
    "date_published": "2022-08-02 08:00:00+00:00",
    "flag": "",
    "chunk_text": ". Out of the box, iMotions supports over 50 leading eye trackers and biosensors including EEG, GSR, ECG, EMG, and facial expression analysis along with surveys for multimodal human behavior research. From start to finish, iMotions has got you covered: • Run your multimodal study onone single computer • Forget about complex setups: iMotions requires minimal technical skills and effort foreasy experimental setupand data acquisition • Get real-time feedback on calibration quality forhighest measurement accuracy • Draw on unlimited resources:Plug and play any biosensorand synchronize it with any type of stimulus (images, videos, websites, screen recordings, surveys, real-life objects, you name it) • Receiveimmediate feedback on data qualitythroughout the recording across respondents • Worried about data synchronization? Don ́t be. While you can work on what really matters, iMotions takes care of thesynchronization of data streams across all sensors Want to learn how iMotions can help you with your eye tracking questions and research setup? Feel free toreach outand one of our solutions specialists will get right back to you. The Complete Pocket Guide Published Last edited About the author Bryn Farnsworth See what is next in human behavior research Follow our newsletter to get the latest insights and events send to your inbox. Consumer Insights Morten Pedersen Best Practice Bryn Farnsworth Best Practice Divya Seernani Consumer Insights Morten Pedersen Consumer Insights Morten Pedersen Consumer Insights Morten Pedersen Academia Morten Pedersen Consumer Insights Morten Pedersen Publications Guides Webinars Case Stories Explore Blog Categories",
    "chunk_id": "Human_computer_interaction_eye_tracking_the_complete_pocket_guide.json_chunk_22"
  },
  {
    "document_type": "AU_course_page",
    "title": "Human Computer Interaction",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/114898/Human-Computer-Interaction",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": "Human Computer Interaction Spring semester 2023 Course Catalogue Save course ECTS 10 Forms of instruction Lecture etc. Forms of examination Take-home assignment (Assign) etc. Language of instruction English Level Master Location Aarhus Use arrow keys on your keyboard to explore Course content Read more see description of qualifications Description of qualifications Read more Purpose: The purpose of the course is to introduce students to the field of human-computer interaction. The emphasis in the course will be on the application of theories and methods from cognitive science to the field of HCI. Students will learn to apply methods from cognitive science in the analysis, and evaluation of information technology designed for human use, and they will learn how to apply theories from cognitive science to inform the design of new human-computer systems. The course includes an introduction to human-computer interaction from the perspective of cognitive science. The course will focus on how knowledge of human cognitive processes, and different methodologies can inform the evaluation and design of information technology systems intended for human use. It will also discuss opportunities and limitations of using artificial systems in direct interaction with humans. This course builds on earlier theoretical and methodological courses in cognitive science, to enable students to understand basic design principles used in the development of information technology systems. Academic objectives: In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge: - explain common theories in human-computer interaction - describe opportunities and limitations of human-computer interaction",
    "chunk_id": "Human_computer_interaction_human_computer_interaction.json_chunk_1"
  },
  {
    "document_type": "AU_course_page",
    "title": "Human Computer Interaction",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/114898/Human-Computer-Interaction",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": ". Skills: - apply statistical tools and measurement techniques to evaluate information technology systems with respect to usability and user experience - apply theories of cognitive function in the design of new information technology systems - discuss implications of information technology systems for human cognition and behavior. Competences: - make informed suggestions for the design and use of human interaction systems. See all ECTS 10 Level Master Semester MA, 2nd semester Language of instruction English Type of course Ordinary Primary programme Master's Degree Programme in Cognitive Science Department School of Communication and Culture Faculty Arts Location Aarhus STADS UVA code 147222U008 Copy UVA code Teaching Forms of instruction Lecture and classroom instruction Instructor See all Marc Malmdorf Andersen Institut for Kultur og Samfund - Interacting Minds (IMC) Comments on the form of instruction Read more Lectures and classroom instruction Language of instruction: The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Literature Read more Will be announced on Brightspace at the start of the semester. Examination Forms of examination Take-home assignment (Assign) and oral Form of co-examination Internal co-examination Assessment 7-point grading scale Take-home assignment (Assign) Permitted exam aids Not specified Oral Permitted exam aids Not specified Duration 30 minute(s) Comments Read more Ordinary exam and re-examination: The examination consists of an individual oral exam based on a synopsis. The synopsis can be written individually or in a group up to 4 students. Length of the synopsis: For 1 student: 4-7 standard pages. For 2 students: 8-14 standard pages For 3 students: 12-21 standard pages For 4 students: 16-28 standard pages The synopsis must be submitted in the Digital Exam system before the deadline set in the examination plan",
    "chunk_id": "Human_computer_interaction_human_computer_interaction.json_chunk_2"
  },
  {
    "document_type": "AU_course_page",
    "title": "Human Computer Interaction",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/114898/Human-Computer-Interaction",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": ". The oral exam is based on the student’s presentation of the synopsis followed by a dialogue between the student and the examiner in which the rest of the course syllabus is included. Duration: 30 minutes (including assessment). The assessment is based on an overall evaluation of the synopsis and the oral presentation.",
    "chunk_id": "Human_computer_interaction_human_computer_interaction.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Fitts’s Law The time to acquire a target is a function of the distance to and size of the target. Key Takeaways • Touch targets should be large enough for users to accurately select them. • Touch targets should have ample spacing between them. • Touch targets should be placed in areas of an interface that allow them to be easily acquired. Overview Usability is a key aspect of good design. It implies ease of use, which means the interface should be easy for users to understand and navigate. Interaction should be painless and straightforward, requiring minimal effort. The time it takes for users to move to and engage with an interactive object is a critical metric. It’s important that designers size and position interactive objects appropriately to ensure they are easily selectable and meet user expectations with regard to the selectable region—a challenge compounded by the differing precision of the range of input methods available today (mouse, finger, etc.). To aid in this endeavor we can apply Fitts’s law, which states that the time it takes for a user to engage with an object is relative to its size and the distance to it. In other words, as the size of an object increases, the time to select it goes 13 1 Paul M. Fitts, “The Information Capacity of the Human Motor System in Controlling the Amplitude of Movement,” Journal of Experimental Psychology 47, no. 6 (1954): 381–91. down. Additionally, the time to select an object decreases as the distance that a user must move to select it decreases. The opposite is true as well: the smaller and further away an object is, the more time it takes to accurately select it. This rather obvious concept has far-reaching implications, which we’ll unpack in this chapter. We’ll also take a look at some supporting examples",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This rather obvious concept has far-reaching implications, which we’ll unpack in this chapter. We’ll also take a look at some supporting examples. Origins The origins of Fitts’s law can be traced back to 1954, when American psycholgist Paul Fitts predicted that the time required to rapidly move to a target area is a function of the ratio between the distance to the target and the width of the target ( Figure 2-1 ). Today, it’s regarded as one of the most successful and influential mathematical models of human motion, and it’s widely used in ergonomics and human–computer interaction to model the act of pointing, either physically or virtually. 1 Figure 2-1. Diagram depicting Fitts’s law Fitts also proposed an index of difficulty metric to quantify the difficulty of a target selection task in which the distance to the center of the target ( D ) is like a signal and the tolerance or width of the target ( W ) is like noise: ID = log 2 2 D W KEY CONSIDERATION Touch Targets Fitts’s law was established as a model for understanding human movment in the physical world before the invention of the graphical user interface, but it can also be applied to movement through a digital inteface. There are three key considerations that we can derive from Fitts’s law. First, touch targets should be large enough that users can easily dicern them and accurately select them. Second, touch targets should have ample space between them. Last, touch targets should be placed in areas of an interface that allow them to be easily acquired. As obvious as it might seem, touch target sizing is of vital impotance: when touch targets are too small, it takes users longer to engage them. The recommended size varies ( Table 2-1 ), but all recommendtions indicate awareness of the importance of sizing. Table 2-1",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The recommended size varies ( Table 2-1 ), but all recommendtions indicate awareness of the importance of sizing. Table 2-1. Minimum touch target size recommendations Company/Org Size Human Interface Guidelines (Apple) 44 × 44 pt Material Design Guidelines (Google) 48 × 48 dp Web Content Accessibility Guidelines (WCAG) 44 × 44 CSS px Nielsen Norman Group 1 × 1 cm It’s important to keep in mind that these recommendations are minmums. Designers should aim to exceed these target touch sizes wheever possible to decrease the need for precision. Adequate touch target size not only ensures interactive elements are easily selectable but also can reinforce users’ perception that the interface is easy to use. Small touch targets add to the perception that an interface is less usable, even if the user is able to avoid errors when attempting to select a target. 2 Kiran Dandekar, Balasundar I. Raju, and Mandayam A. Srinivasan, “3-D Finite-Element Models of Human and Monkey Fingertips to Investigate the Mechanics of Tactile Sense,” Journal of Biomechanical Engineeing 125, no. 5 (2003): 682–91. 3 Steven Hoober, “Design for Fingers, Touch, and People, Part 1,” UXmatters, March 6, 2017, https:// www.uxmatters.com/mt/archives/2017/03/design-for-fingers-touch-and-people-part-1.php . Another consideration that affects the usability of interactive elments is the spacing between them. When the space between elements is too small, the likelihood of touch target errors increases. MIT Touch Lab conducted a study that showed that the average adult human’s figer pad is 10–14 mm and the average fingertip is 8–10 mm. 2 It’s inevitble that a user will partially touch outside touch targets at least some of the time—and if additional touch targets are too close they might be accidentally selected, causing frustration and decreasing the user’s peception of the interface’s usability",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To mitigate the false activation that can happen when targets are too close, Google’s Material Design guidlines recommend that “touch targets should be separated by 8 dp [density-independent pixels] of space or more to ensure balanced infomation density and usability.” In addition to sizing and spacing, the position of touch targets is key to how easily selectable they are. Placing touch targets in areas of the screen that are harder to reach will in turn make them harder to select. What isn’t always obvious is where exactly these hard-to-reach areas of a screen are, as it changes depending on the context of the user, the device used, etc. Take smartphones, for instance, which come in a range of form factors and which people hold in a variety of ways depending on the task and availability of both hands. Some areas of the screen can become dificult to reach when holding the device in one hand and using the thumb to select items, while holding the phone in one hand and selecting elments with the other greatly reduces this difficulty. Even with onhanded use, accuracy does not increase linearly from the lower right to the upper left of the screen; according to research by Steven Hoober, 3 people prefer to view and touch the center of the smartphone screen, and this is where accuracy is the highest ( Figure 2-2 ). They also tend to focus on the center of the screen, as opposed to scanning from the upper left to the lower right as is common on desktop devices. Figure 2-2. Smartphone touch accuracy (illustration based on research by Steven Hoober) Examples We’ll begin by looking at a common example of Fitts’s law: form text labels. By associating a text label element with an input, designers and developers can ensure that taps or clicks on the label will perform the same function as selecting the input ( Figure 2-3 ). This native feature effectively expands the surface area of the form input, making it easier for users to focus on the input with less precsion",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This native feature effectively expands the surface area of the form input, making it easier for users to focus on the input with less precsion. The net effect is a better user experience for desktop and mobile users alike. Figure 2-3. Touch target area on text label and form input Continuing with forms, another common example of Fitts’s law can be found in the placement of form submission buttons. These buttons are usually positioned in close proximity to the last form input ( Figure 2-4 ), because buttons that are intended to complete an action (such as filling out a form) should be close to the active element. This positioning not only ensures that the two types of input are visually related but also ensures that the distance the user must travel from the last form input to the submission button is minimal. Figure 2-4. Form submission buttons are placed in close proximity to the last form input The spacing between interactive elements is also an important consideration. Take, for example, the connection request confirmation screen in LinkedIn’s iOS app ( Figure 2-5 ), which places the “accept” and “deny” actions together on the right side of a dialog. The actions are so close together that users must make a significant effort to focus on selecting the action they wish to perform without accidentally selecting the other. In fact, each time I see this screen I know it means I have to switch to using two hands to avoid misselecting “accept” with my thumb. Smartphones, laptops, and desktop computers aren’t the only interfaces we interact with on a daily basis. Take, for example, infotainment systems, which can be found in the vehicles many use every day. The Tesla Model 3 features a 15 ʺ display mounted directly on the dashboard. Most of the vehicle’s controls are located on this screen and do not provide haptic feedback when the user engages with them. This of course requires the driver to divert their attention from the road to the screen to access these controls, so Fitts’s law is of critical importance",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This of course requires the driver to divert their attention from the road to the screen to access these controls, so Fitts’s law is of critical importance. Figure 2-5. Lack of ample space between actions decreases usability (source: LinkedIn, 2019) The Model 3 follows Fitts’s law, providing ample space between the items in the bottom navigation bar ( Figure 2-6 ). This mitigates the risk of accidental seletion of adjacent actions. Figure 2-6. Providing sufficient space between items increases usability, minimizing the chances of selecting the wrong action (source: Tesla, 2019) I mentioned thumb zones with regard to touch target positioning earlier, and how positioning touch targets in hard-to-reach areas of the interface makes them harder to select. With the arrival of the larger iPhone 6 and iPhone 6 Plus, Apple introduced a feature that aimed to mitigate the difficulty of one-handed use. The feature, called Reachability , enables users to quickly bring items at the top of the screen down to the lower half of the screen via a simple gesture ( Figure 2-7 ). It effectively enables easy access to parts of the screen that would otherwise be difficult for one-handed users to interact with. Figure 2-7. The iPhone’s Reachability feature enables easy access to the top half of the screen (source: Apple, 2019) Conclusion A key responsibility we have as designers is to ensure the interfaces we create augment human capabilities and experiences, and don’t distract from or deter them. Mobile interfaces are especially susceptible to Fitts’s law due to the limited screen real estate available. We can ensure interactive elements are easily selectble by making them large enough for users to both discern them and accurately select them, providing ample space between controls to avoid accidental selection of adjacent actions and placing them in areas of the interface that allow them to be easily selected.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-13-22.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "How Many Test Users in a Usability Study?",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/how-many-test-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "June 3, 20122012-06-03 Share If you want a single number, the answer is simple:test 5 users in a qualitative usability study.Testing with 5 people lets you find almost as many usability problems as you'd find using many more test participants. This answer has been the same since I started promoting \"discount usability engineering\" in 1989. Doesn't matter whether you test websites, intranets, PC applications, or mobile apps. With5 users, you almost always get close to user testing's maximum benefit-cost ratio. As with any human factors issue, however, there areexceptions: However, these exceptions shouldn't worry you much: the vastmajority of your user research should be qualitative— that is, aimed at collectinginsights to drive your design, not numbers to impress people in PowerPoint. The main argument for small tests is simplyreturn on investment: testing costs increase with each additional study participant, yet the number of findings quickly reaches the point of diminishing returns. There's little additional benefit to running more than 5 people through the same study; ROI drops like a stone with a biggerN. And if you have a big budget? Yay! Spend it on additional studies, not more users in each study. Sadly, most companies insist on running bigger tests. During theUX Conference, I surveyed 217 participants about the practices at their companies. The average response was that they used11 test participantsper round of user testing — more than twice the recommended size. Clearly, I need to better explain the benefits of small-Nusability testing. \"A big website has millions of users.\" Doesn't matter for the sample size, even if you were doing statistics. An opinion poll needs the same number of respondents to find out who will be elected mayor of Pittsburgh or president of France. The variance in statistical sampling is determined by the sample size, not the size of the full population from which the sample was drawn",
    "chunk_id": "Human_computer_interaction_how_many_test_users_in_a_usability_study.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "How Many Test Users in a Usability Study?",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/how-many-test-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The variance in statistical sampling is determined by the sample size, not the size of the full population from which the sample was drawn. In user testing, we focus on a website's functionality to see which design elements are easy or difficult to use. The evaluation of a design element's quality is independent of how many people use it. (Conversely, the decision about whether to fix a design flaw should certainly consider how much use it'll get: it might not be worth the effort to improve a feature that has few users; better to spend the effort recoding something with millions of users.) \"A big website has hundreds of features.\" This is an argument for running severaldifferent tests— each focusing on a smaller set of features — not for having more users in each test. You can't ask any individual to test more than a handful of tasks before the poor user is tired out. Yes, you'll need more users overall for a feature-rich design, but you need to spread these users across many studies, each focusing on a subset of your research agenda. \"We have several different target audiences.\" This can actually be a legitimate reason for testing a larger user set because you'll need representatives of each target group. However, this argument holds only if the different users are actually going to behave in completely different ways. Some examples from our projects include When the users and their tasks are this different, you're essentially running a new test for each target audience, and you'll need close to 5 users per group. Typically, you can get away with 3–4 users per group because the user experience will overlap somewhat between the two groups. With, say, a financial site that targets novice, intermediate, and experienced investors, you might test 3 of each, for a total of 9 users — you won't need 15 users total to assess the site's usability. \"The site makes so much money that even the smallest usability problem is unacceptable.\" Rich companies certainly have anROI case to spend more on usability",
    "chunk_id": "Human_computer_interaction_how_many_test_users_in_a_usability_study.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "How Many Test Users in a Usability Study?",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/how-many-test-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". \"The site makes so much money that even the smallest usability problem is unacceptable.\" Rich companies certainly have anROI case to spend more on usability. Even if they spend \"too much\" on each quality improvement, they'll make even more back because of the vast amounts of money flowing through the user interface. However, even the highest-value design projects will still optimize their ROI by keeping each study small and conducting many more studies than a lower-value project could afford. The basic point is that it's okay to leave usability problems behind in any one version of the design as long as you're employing aniterative design processwhere you'll design and test additional versions. Anything not fixed now will be fixed next time. If you have many things to fix, simply plan for a lot of iterations. The end result will be higher quality (and thus higher business value) due to the additional iterations than from testing more users each time. The following chart summarizes 83 of Nielsen Norman Group's recentusability consulting projects. Each dot is one usability study and shows how many users we tested and how many usability findings we reported to the client. (The chart includes only normal qualitative studies; we also run competitive studies and benchmark measurements, and conduct other types of research not shown here.) Why did we run more users in the first place, given that I certainly believe my own research results showing the superiority of small-Ntesting? Three reasons: The last point also explains why the true answer to \"how many users\" can sometimes be much smaller than 5. If you have anAgile-style UX processwith very low overhead, your investment in each study is so trivial that the cost–benefit ratio is optimized by a smaller benefit",
    "chunk_id": "Human_computer_interaction_how_many_test_users_in_a_usability_study.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "How Many Test Users in a Usability Study?",
    "author": "Jakob Nielsen",
    "source": "https://www.nngroup.com/articles/how-many-test-users/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". If you have anAgile-style UX processwith very low overhead, your investment in each study is so trivial that the cost–benefit ratio is optimized by a smaller benefit. (It might seem counterintuitive to get more return on investment by benefiting less from each study, but this savings occurs because the smaller overhead per study lets you run so many more studies that the sum of numerous small benefits becomes a big number.) For reallylow-overhead projects, it's often optimal to test as few as2 users per study. For some other projects, 8 users — or sometimes even more — might be better. For most projects, however, you should stay with the tried-and-true: 5 users per usability test. Plan, conduct, and analyze your own studies, whether in person or remote Research Essential skills and tools for exceptional management Management Define, share, and implement design operations Management Enable cookiesto watch NN/g videos Usability Testing with 5 Users: ROI Criteria (video 2 of 3) 5 Reasons to Test Even When You “Know” the Answer Hoa Loranger·4 min A Case for Returning to In-Person Usability Testing Hoa Loranger·6 min What Is User Research? Caleb Sponheim·3 min Qualitative Usability Testing: Study Guide Kate Moran·5 min Group Notetaking for User Research Susan Farrell·10 min Avoid Leading Questions to Get Better Insights from Participants Amy Schade·4 min Employees as Usability-Test Participants Angie Li·5 min Team Members Behaving Badly During Usability Tests Hoa Loranger·7 min Checklist for Planning Usability Studies Hoa Loranger·5 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_how_many_test_users_in_a_usability_study.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "von Restorff Effect When multiple similar objects are present, the one that differs from the rest is most likely to be remembered. Key Takeaways • Make important information or key actions visually distinctive. • Use restraint when placing emphasis on visual elements to avoid them competing with one another and to ensure salient items don’t get mistakenly identified as ads. • Don’t exclude those with a color vision deficiency or low vision by relying exclusively on color to communicate contrast. • Carefully consider users with motion sensitivity when using motion to communicate contrast. Overview Thousands of years of evolution have given humans an incredibly sophisticated system of vision and cognitive processing. We can identify objects in fractions of a second, we possess superior pattern processing capabilities in comparison to other living things, and we have an innate ability to spot small differences in 77 1 Mark P. Mattson, “Superior Pattern Processing Is the Essence of the Evolved Human Brain,” Frontiers in Neuroscience 8(2014): 265. 2 Hedwig von Restorff,“Über die Wirkung von Bereichsbildungen im Spurenfeld,” Psychologische Foschung 18 (1933): 299–342. 3 Shelley E. Taylor and Susan T. Fiske, “Salience, Attention, and Attribution: Top of the Head Phenomena,” in Advances in Experimental Social Psychology , vol. 11, ed. Leonard Berkowitz (New York: Academic Press, 1978), 249–88. objects. 1 These traits have proven valuable for the survival of our species, and they remain with us to this day, affecting how we perceive and process the world around us. Our focus is not only dictated by the goals we seek to accomplish, but also directed by these instinctual abilities. They also affect how we encode information in memory, and therefore our ability to recall items and events at a later point—recognition is prioritized over recall. When it comes to digital interfaces, an interesting consideration is the tedency of contrasting elements to draw our attention faster",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When it comes to digital interfaces, an interesting consideration is the tedency of contrasting elements to draw our attention faster. A primary challenge we have as designers is managing what users will focus on in an interface, while supporting them in achieving their goals. On the one hand, visual emphasis can be used to guide users toward a goal by capturing their attention. On the other hand, too many points of visual emphasis will compete with one another and make it harder for people to find the information they need. Color, shape, size, position, and motion are all factors that come into play in directing the attention of users, and we must carefully consider each of these when building interfaces. Origins The von Restorff effect is named after German psychiatrist and pediatrician Hewig von Restorff, who found in a 1933 study employing the isolation paradigm that participants presented with a list of categorically similar items best remebered ones that were distinctly different. 2 In other words, memory is improved for items of a set that are visually or conceptually isolated from the other items. While von Restorff wasn’t the first to investigate the effects of this paradigm on memory, it became closely associated with her and the study of distinctiveness. Her initial findings would later be corroborated by research, such as that by Sheley Taylor and Susan Fiske (1978), suggesting that people are drawn to salient, novel, surprising, or distinctive stimuli. 3 4 Klaus Oberauer, “Working Memory and Attention—A Conceptual Analysis and Review,” Journal of Cognition 2, no. 1 (2019): 36. 5 Kara Pernice, “Banner Blindness Revisited: Users Dodge Ads on Mobile and Desktop,” Nielsen Norman Group, April 22, 2018, https://www.nngroup.com/articles/banner-blindness-old-and-new-findings . PSYCHOLOGY CONCEPT Selective Attention, Banner Blindness, and Change Blindness Simply put, humans live in a world of distraction. Every day, at any given moment, we are subjected to a plethora of sensory information",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Every day, at any given moment, we are subjected to a plethora of sensory information. While driving, at work, when attending a social event or simply shopping online, most people have a multitude of signals competing for their attention. Objects within our field of view might be visible, but we don’t always see them. The reason for this is because attention plays a fundamental role in how we perceive the world around us. In order to maintain focus on information that is important or relevant to the task at hand, we often filter out information that isn’t relevant. In other words, our ability to focus on the things around us is limited in terms of capacity and durtion, so we focus on relevant information to the detriment of nonrelevant information. It’s a survival instinct known in cognitive psychology as selective attention , and it’s critical not only to how we humans perceive the world around us but also to how we process sensory information in critical moments that could mean the difference between life and death. Like we saw in Chapter 4 regarding Miller’s law and the capacity of short-term memory, attention is also a limited resource. Though there are different ways of conceptualizing memory and attention, there is broad agreement in the psychological community that working memory is closely related to attention. 4 The implications of this with regard to diital products and services is significant, since the interfaces people inteact with must guide their attention, prevent them from being overwhelmed or distracted, and help them in finding the relevant infomation or action. One example of selective attention that is common in digital intefaces is the user behavior known as banner blindness . Banner blindness describes the tendency for people to ignore elements that they perceive to be advertisements, and it is a strong and robust phenomenon that’s been documented across three decades",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Banner blindness describes the tendency for people to ignore elements that they perceive to be advertisements, and it is a strong and robust phenomenon that’s been documented across three decades. 5 When you consider banner blindness in the context of our limited capacity for attention, it makes sense that we’d ignore anything that we don’t typically find helpful (e.g., digital ads). Instead, people are more likely to search for items that help them achieve their goals—especially design patterns such as navigation, search bars, headlines, links, and buttons (as Jakob’s law dictates, they will also instinctively look for these items in common locations). Even legitimate content elements may be ignored if they remotely resemble ads or are placed in close proximity to them. Therefore, it’s good to be aware of when visually differentiating content could inadvertently lead to it being mistaken for an ad. Related to banner blindness is change blindness , which describes the tendency for people to fail to notice significant changes when they lack strong enough visual cues, or when their attention is focused elswhere. Since our attention is a limited resource, we often ignore informtion we deem irrelevant in order to complete tasks efficiently. Because our attention is focused on what appears to be most salient, we may overlook even major differences introduced elsewhere. If it’s important that the user be aware of certain changes to the interface of a product or service, we should take care to ensure that their attention is drawn to the elements in question. Examples As you can imagine, examples of the von Restorff effect can be found in every digital product and service, some of which make use of it more effectively than others. The need to make specific elements or content visually distinct is fundmental in design. When this technique is used sparingly and strategically, the contrast that it affords not only helps draw attention but also directs people to the most valuable information",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When this technique is used sparingly and strategically, the contrast that it affords not only helps draw attention but also directs people to the most valuable information. A common example of this visual phenomenon can be found in the design of interactive elements such as buttons, text links, and the like. The visual diffeentiation of these elements can help draw people’s attention and inform them of the actions available to them, guiding users in completing tasks and preventing them from taking actions they didn’t intend to. To demonstrate, take a look at the example in Figure 8-1 , which depicts two versions of a confirmation modal: one with buttons that are visually indistinct from one another, and another with emphasis placed on the most important button. The lack of visual contrast in the modal on the left could easily lead to people accidentally selecting the wrong action. By placing a visual emphasis on the destructive action, the version on the right will not only help guide users who want to delete their accounts to the corect option but will also help those who don’t intend to delete their accounts avoid accidentally selecting this option. For extra safety, there is also a warning icon included in the header of the modal on the right to help draw attention and communicate the importance of the content within the modal. Figure 8-1. Example of how contrast can be used to draw attention to important actions and help users avoid accidentally selecting the wrong option Let’s take the button example a step further and look at an interface that moves beyond the simple use of color to create contrast. The floating action buton (FAB), a design pattern introduced by Google’s Material Design ( Figure 8-2 ), “performs the primary, or most common, action on a screen.” By providing guidelines around the design of this element, its placement on the screen, and what actions it should perform, Google ensures its consistency across various products and services",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As a result, it has become a familiar pattern that people recognize and that therefore helps to guide their experience (so this is also an example of Jakob’s law in action). Figure 8-2. Examples of the floating action button from Material Design (sources: Gmail and Google Calendar, 2019) Another common example of the von Restorff effect can be found in pricing tables. Subscription plans are available for most of the services we use, and often companies will emphasize one option over the others. To achieve this emphasis, designers frequently differentiate the option they want to call out by adding viual cues. Take, for example, Dropbox, which places emphasis on the “advanced” option ( Figure 8-3 ) through the use of color (an accent color is applied to the “Try free” button), shape (the card appears slightly larger due to the “Best value” elment at the top), and position (placing the card in the center of the display). Figure 8-3. An example of the von Restorff effect in a pricing table (source: Dropbox, 2019) The von Restorff effect can also be seen in design elements intended to grab our attention. Take notifications, for example ( Figure 8-4 ), which are meant to inform users when something requires them to take action. These ubiquitous elements can be found in almost every app or service and are designed to grab our attention, for better or worse. Figure 8-4. Employing the von Restorff effect to call attention to notifications (source: iOS, 2019) We can extend the thinking behind the von Restorff effect and apply it to design going beyond singular elements as well. Take, for example, news wesites, which commonly place emphasis on featured content in order to make it stand out against lots of other headlines, images, and ads ( Figure 8-5 ). The cosistent pattern you’ll notice on these websites is the use of scale to create contrast between the featured content and adjacent content. The reader’s attention is drawn to the information that breaks out of the implied columns of content",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The reader’s attention is drawn to the information that breaks out of the implied columns of content. As these examples illustrate, visual contrast can be created in many ways. Color is a common way to differentiate elements, but it is by no means the only way to create contrast. Scale, shape, negative space, and motion are additional properties that can make specific elements or content stand out in comparison to adjacent information. Figure 8-5. News websites often use scale to emphasize featured headlines (sources [clockwise from top left]: Bloomberg, ProPublica, the New York Times, and the Boston Globe, 2019) KEY CONSIDERATION Moderation and Accessibility There are some important things to consider when it comes to applying the von Restorff effect in design. The first is when and how often contrast should be created—this effect should be used with intention, and not overused. The only thing worse than no contrast is way too much, which can not only dilute the power of the elements or content that you intended to stand out but also visually overwhelm people. It’s wise to use restraint when placing emphasis on visual elements to ensure they don’t compete with each other. The importance of moderation is even clearer when we take into account factors such as banner blindness and change blindness. If the visual emphasis on content causes it to be mistakenly identified as an advertisement, then it’s likely to be ignored. Additionally, if too many items are emphasized, people are less likely to notice important informtion or changes when they occur: they may either be distracted or autmatically tune out the “noise.” The next consideration is accessibility. It’s crucial to have an awarness of which visual properties you are using to create contrast and how they affect different people. Take, for example, people with color vision deficiency, who are unable to distinguish certain shades of color (or, in some cases, see any color)",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Take, for example, people with color vision deficiency, who are unable to distinguish certain shades of color (or, in some cases, see any color). For these users, relying solely on color to communicate visual contrast is problematic and will result in a less than ideal user experience. Further, vision impairments such as cataracts can affect how people perceive detail and difference, causing them to miss subtle differentiations between elements. In addition to these considertions, it’s important that we ensure sufficient color contrast exists between foreground and background elements to help those who have trouble seeing specific colors or have an impairment that results in low vision. Contrast is also sometimes provided through the use of motion, but it’s important to consider how this might affect users with a vestibular disorder, or with any disease, damage, or injury to the system connected to the inner ear and brain that processes the sensory information involved with controlling balance and eye movements. Take, for example, those with benign paroxysmal positional vertigo (BPPV) or labyrinthitis, in whom motion can trigger dizziness, nausea, headaches, or worse. In addition, motion can affect those with epilepsy and migraine sensitivties. We must carefully consider when and how we use motion in our designs to ensure users with motion sensitivity aren’t negatively affected. Conclusion The von Restorff effect is a powerful guideline for how to use contrast to direct people’s attention to the most relevant content. It can help to inform our design decisions when we want to place emphasis on critical or important actions or information, and help to ensure that users of our products and services can quickly identify what they need to achieve their goals. Contrast can also become problematic when not used with restraint. When designers differentiate elements visually, the effect is to draw the user’s attention",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Contrast can also become problematic when not used with restraint. When designers differentiate elements visually, the effect is to draw the user’s attention. If too many elements are visally competing with each other, their power is diluted and they no longer stand out amongst the other elements. Additionally, we must be aware of how visual properties we use to create contrast are perceived by those with vision deficiecies and how they might affect people with motion sensitivities.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-77-85.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Controlling Researcher Effects We take a look at some subtle yet pervasive researcher effects, at ways they can bias the outcome of UX research, and at what we can do to control their influence. nothing is more guaranteed to shatter the illusion of reality that’s building up nicely in that best seller you’re reading, than the author suddenly appeaing out of the blue and talking to you directly. Grandly known as “authorial intrusion,” it’s the moment the author yanks the reader’s attention out of the story, reminds us of the real world, and breaks the golden rule of fiction wriing and journalism: Keep yourself out of the story. It’s right up there with the film crew recording its own reflection in a passing shop window. Research has its own version of these unfortunate moments. They happen when the researcher blunders into view and ruins things by influencing the outcome of a study. We call these blunders experimenter effects . Experimenter effects contaminate the research process, but we know their origin. They are almost always the result of the experimenter having prior expectations about the hypothesis of a study. Some classic examples remind us how insidious experimenter effects can be. Recall the case of Clever Hans the counting horse. When given numbers to add, subtract, multiply, or divide, Hans would tap out the correct answer with his hoof. Psychologist oskar Pfungst showed that, rather than doing arithmtic, Hans was simply picking up involuntary ideomotor cues from his trainer when the correct number of hoof taps was reached. When asked a math prolem his trainer did not know the answer to, Hans’ performance collapsed. And it’s not just horses. In 1963, psychologists Lucian Cordaro and James Ison 16 asked two groups of college students to observe and count head turns and body contractions made by planaria (flatworms). one group of students was led to believe the target behaviors would occur infrequently, while the other group was led to expect a high rate of head turns and contractions",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". one group of students was led to believe the target behaviors would occur infrequently, while the other group was led to expect a high rate of head turns and contractions. The flatworms in each group were identical and (as is often the case with flatworms) had no expectations about anything. Sure enough, the group expecting a high rate of body movements reported a much higher count than the group expecing a lower rate—an outcome carried entirely by experimenter expectation. Robert Rosenthal’s extensive work on experimenter effects 17 (an investigtion spanning 30 years and including both animal and human behavioral research) reveals that 70 % of experimenter effects influence outcomes in favor of the researcher’s hypothesis. Experimenter effects are also common in UX and market research, though we’ll switch to calling them “researcher effects” as most user experience methods are not really experiments in the conventional sense. Field studies, focus groups, interviews and usability tests are all susceptible to researcher effects because researchers have expectations, and because UX and marketing research create social situations, and because being truly objective is difficult. The Double-Blind Science has a powerful antidote to researcher effects: the Double-Blind. In a double-blind study, neither the participant nor, critically, the expermenter (nor anyone involved in moderating, observing, recording or anlyzing the data) knows the research hypothesis, or knows which condition participants are assigned to, or which design is the “before” and which is the “after,” which product is “ours” and which is “theirs,” etc. A double-blind eliminates the most problematic researcher effects and is effectively used in clinical trials and in debunking pseudoscientific claims. Can we apply this to UX research? Unfortunately, conducting a doublblind in UX research, while not impossible, is very challenging in pratice and is not something we are likely to see",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Can we apply this to UX research? Unfortunately, conducting a doublblind in UX research, while not impossible, is very challenging in pratice and is not something we are likely to see. A UX researcher typically works with the development team on a daily basis and has been instrumetal in guiding design. In any evaluative situation, there’s no way the UX researcher can suddenly have no knowledge or prior expectation of study conditions. Bringing in an external UX researcher doesn’t solve the problem either, because this person also must know the design or product at least well enough to conduct an effective study. And remember, the double-blind must extend to data recording, analyzing, interpreting, and reporting the results. In most cases, running a double-blind would turn a quick user experience study into a major covert operation. The double-blind may be the gold standard but, if we can’t use it, how else might we prevent researcher effects? The first step is to raise awareness among project teams that researcher effects exist, and that they can have serious consequences. The second step, since we can’t totally eliminate these effects, is to find ways to control them. Here are some ways that researchers influence the outcomes of their own research, and some thoughts on how we might control biases in these situations. Biases When Interacting with Participants These are the “Clever Hans” biases that stem from unintended communictions with the participant before and during a study. They result from verbal and non-verbal cues and gestures that influence the participant’s thinking or behavior during the UX research, and they become damaging when they systematically favor one particular outcome. For example, Philip observed a study during which the moderator did a good job of remaining neutral when introducing competitor designs, but she leaned forward and nodded wheever she spoke about the sponsor’s design",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In reality there are an almost infinite number of biasing behaviors that can creep into a study, from the blatantly obvious, “We hope you’ll only have good things to say about our product,” (we’re not making that one up, we actually heard a researcher say this), to the almost imperceptible smile when the participant clicks the correct button in an interface. other inflencing behaviors can include the researcher’s mood and demeanor, tone of voice, frowning, sighing, tensing up, relaxing, shuffling on their chair, raising their eyebrows, and pulling faces. Even note-taking can introduce a bias (“oh dear, he just wrote something in his notebook, I must have done it wrong”). And this is before we even start to think about the biasing effects of leading and loaded questions, reassurances such as, “You’re not the only one to have done that wrong today,” or paraphrasing the particpant’s remarks with a little extra topspin, or allowing one’s own opinions to creep into the dialogue. We can’t eliminate all of these biasing factors: They are far too pervasive, and we would end up behaving like automatons if we tried to monitor ouselves down to this micro-behavioral level. So what is the solution? Since a double-blind test is not possible, here are some techniques that can help: • Standardize everything : the research protocol, the moderator script, the questions, and so on. Follow the same protocol in the same way for every participant in every condition. Present task scenarios on cards for the participant to read aloud. Stick to the script. • Have a second researcher monitor the first researcher : Monitor for “protocol drift.” • Stay out of the participant’s line of sight : In summative usability testing, leave the room if you can during a task. • Practice : Run mock studies that focus on controlling biasing behaviors. Video-record yourself administering a study or moderating an interview",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Practice : Run mock studies that focus on controlling biasing behaviors. Video-record yourself administering a study or moderating an interview. Critically analyze your performance, and have a colleague help you note any systematic biasing behaviors and inadvertent signals you may be giving. Biases When Recording, Interpreting and Reporting Findings Researcher effects can contaminate a study in a number of ways, such as: • Systematic data recording errors. • Participant actions or responses that are given undue weight. • Jotting down what you thought the participant meant rather than what she actually said. • Trying to interpret data on the fly and too early into a test. • Making careless recording errors. • Failing to pay attention. • Struggling to keep up with the participant. • Making copying and data-entry errors. Subsequent data interpretation can also fall foul of confirmation bias . This is the tendency to prioritize evidence that fits well with what we already believe, while ignoring evidence that doesn’t. Even highly experienced researchers can fall into this cognitive trap. Biases toward a positive outcome can also influence report writing and research presentations. In the scientific community similar pressures to suceed exist, such that negative results are less likely to be submitted for publcation and, if submitted, are less likely to be accepted than positive results. In consumer research, we’ve seen obviously negative findings given a ludicrously positive spin in a final research presentation with summary headlines such as: “Five of the 20 people really liked the new concept.” Researchers doing this risk their credibility. Business stakes are far too high to risk being misled by a researcher contriving a “feel-good” effect. This may sound unusual, but you should not care what the outcome of your research is. You should only care that your research design and data are bulletproof and will stand up to the most rigorous scrutiny. Let the chips fall where they may",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You should only care that your research design and data are bulletproof and will stand up to the most rigorous scrutiny. Let the chips fall where they may. It’s not your job to guarantee a happy ending. Here are some checks we’ve found helpful: • Decide on the data logging procedure ahead of the study. Use a maageable number of data codes to categorize events. Practice using them and document them in a formal test plan. • Record objective data where possible: for example, task completion rates and time on task. • Agree any pass/fail criteria ahead of the study, not afterwards. • It may not be possible to have a “blind” UX researcher, but it may be possible to have “blind” data loggers. Have at least two data loggers (or note-takers) so that you can check for inter-scorer reliability and compare notes. • Record verbatim what participants say, not what you think they mean. • Avoid trying to interpret the data during the study. • Double-check your data coding, data entry and any statistical analysis. • Ask a research colleague to read your final report, or presentation slides, and give critical feedback. Sponsorship Biases Biases toward the company sponsoring the research studies are common. In the pharmaceutical industry, for example, industry-funded trials are about four times more likely to report positive rather than negative results 18 ; and studies sponsored by pharmaceutical companies are more likely to have oucomes favoring the sponsor than are studies with other sponsors. 19 Philip recently witnessed a deliberate example of sponsorship bias, motvated perhaps by a misguided desire for a happy outcome, when he was invited to review a project in which a third party “independent” research company was carrying out long-term in-home trials of a new domestic appliance product",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". He was astonished to discover that the owner of the research company had planted herself into the study as a test participant, and that for every item on every quetionnaire over a three-month period, she gave the product the highest possible five-star positive rating. In her eagerness to please the client, this researcher had clearly lost sight of the purpose of the in-home research, which was to uncover problems so that they could be fixed before the product was launched. Internal company pressures, often stemming from the ever-escalating colective belief that the project cannot possibly fail (otherwise why are we still doing it?), can create a working environment intolerant of any negative oucomes. Much of this pressure comes from testing or carrying out research too late, and it can be defused by doing research and testing early and often, and by keeping the stakeholders closely involved so that they can make course corrections and mitigate risk before things reach the point of no return. Again, if a double-blind can be employed, this can effectively put a “firwall” between the funding source and the research team. no one involved in conducting and reporting the study would know who the sponsoring company was. But, as we have noted, in UX research this is nearly impossible, and somtimes you just have to stand by your research data, bite the bullet and break the bad news, sponsor or no sponsor. However, you might be surprised at the reaction you get. Philip once had to present negative UX research data that he knew would deliver a death blow to a $ 28 million project. To his surprise, the relief in the room was palpable. It was the final piece of evidence that gave the company the confidence to pull the plug and stop wasting more money. Why You Need to Fix Your Bias Blind Spot Bias works like a Trojan horse. It hides in our UX research toolbox, slips past our defenses, and operates from the inside. Everyone has a bias blind spot but we are less likely to detect bias in ourselves than in others",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It hides in our UX research toolbox, slips past our defenses, and operates from the inside. Everyone has a bias blind spot but we are less likely to detect bias in ourselves than in others. Carey Morewedge, associate professor of marketing at Boston University, says 20 : “People seem to have no idea how biased they are. Whether a good decision maker or a bad one, everyone thinks that they are less biased than their peers. This suceptibility to the bias blind spot appears to be pervasive, and is unrelated to people’s intelligence, self-esteem, and actual ability to make unbiased judments and decisions.” Unlike in the academic science community, where research is rigorously peer reviewed and scoured for methodological flaws, even shot down if it cannot pass muster, most UX and market research is seldom subjected to such intense scrutiny. Things are simply moving too fast. Headline findings are often all that most stakeholders see, and findings are largely taken on trust that the results are what they appear to be. Decisions are quickly made and the show moves on down the road. But false research outcomes can cost a company millions of dollars. In the absence of a double-blind methodology, your strongest weapon against researcher bias may simply be the awareness that it exists. THINK LIKE A UX RESEARCHER • Think of the various stages in UX research, from planning a study through to taking action on the findings. What different biases could creep in at each stage? Are some of these stages more vulnerable to researcher bias than others? • Consider the situation where people from the development team are observing your research",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Think Like a UX Researcher: How to Observe Users, Influence Design, and Shape Business Strategy",
    "author": "David Travis and Philip Hodgson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Think Like a UX Researcher (David Travis, Philip Hodgson).pdf",
    "date_published": "2018-12-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If they discuss their observations with you after the session, are they biasing your recollection of events? or is this a useful way of highlighting observations you may have missed? • Do you think the way a UX researcher approaches design experments meets the standards that a scientist would expect of a “cotrolled” experiment? Why, or why not? Is UX research a science? Does UX research need to adopt the scientific method to offer value? • We say that it’s nearly impossible to run a double-blind UX research study. But could we come close to a double-blind study if we used remote, unmoderated usability tools? • Whenever you work with humans, there is a tendency to empthize with some people more deeply than with others. Some paticipants in your research may be more likable, more articulate, or more needful of a good solution than others. How can you prevent this from biasing the way you interpret and report the results?",
    "chunk_id": "Human_computer_interaction_think_like_a_ux_researcher_how_to_observe_users,_influence_design,_and_shape_business_strategy_page-128-134.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Tesler’s Law Tesler’s law, also known as the law of conservation of complexity, states that for any system there is a certain amount of complexity that cannot be reduced. Key Takeaways • All processes have a core of complexity that cannot be designed away and therefore must be assumed by either the system or the user. • Ensure as much as possible of the burden is lifted from users by dealing with inherent complexity during design and development. • Take care not to simplify interfaces to the point of abstraction. Overview Who should bear the burden of complexity within an application or a process— the user, or the designers and developers? This is a fundamental question when considering the design of user interfaces and, more broadly, how humans inteact with technology. A key objective for designers is to reduce complexity for the people that use the products and services we help to build, yet there is some inherent complexity in every process. Inevitably we reach a point at which coplexity cannot be reduced any further but only transferred from one place to another. At this point, it finds its way either into the user interface or into the processes and workflows of designers and developers. 87 Origins The origins of Tesler’s law can be traced back to the mid-1980s, when Larry Teler, a computer scientist at Xerox PARC, was helping to develop the language of interaction design—a set of principles, standards, and best practices for defining the structure and behavior of interactive systems that was key to the development of the desktop and desktop publishing. Tesler observed that the way users inteacted with an application was just as important as the application itself. It was important, therefore, to reduce the complexity of both the application and the user interface. However, Tesler realized that within any application or process, there is an inherent amount of complexity that cannot be removed or hidden",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". However, Tesler realized that within any application or process, there is an inherent amount of complexity that cannot be removed or hidden. This complexity needs to be dealt with in one of two places: development (and, by extension, design) or user interaction. Examples One common way to illustrate Tesler’s law is through the humble email. When you write an email, there are two required pieces of information: who the mesage is from (you), and to whom it should be sent. The email cannot be sent if either of these is missing, and therefore it’s a necessary complexity. To reduce this complexity, a modern email client will do two things: prepopulate the sender (it can do this because it is aware of your email address), and provide suggestions for the recipient as you begin to type their address, based on prior emails and/or your contacts ( Figure 9-1 ). The complexity isn’t entirely gone; it’s just abstracted away to reduce the effort required of the user. In other words, the experience of writing an email is made a little simpler by moving the complexity of filling in the sender’s and, if possible, the recipient’s address to the email client, which was designed and developed by a team that assumed that burden of complexity when building it. Figure 9-1. Modern email clients reduce complexity by populating the “from” line and suggesting the “to” line based on prior emails (source: Gmail, 2019) Taking that a step further, Gmail now leverages artificial intelligence (AI) within your emails through a feature called Smart Compose ( Figure 9-2 ). This intelligent feature can scan what you’ve typed and use that content to suggest words and phrases to finish your sentences, thus saving you additional typing and time. It should be noted that Smart Compose is not the first time-saving feture introduced to Gmail by way of AI—there’s also Smart Reply , which scans an email for context and suggests several relevant quick reply options. Figure 9-2",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 9-2. Example of Gmail’s Smart Compose feature (source: Gmail, 2019) Another place that Tesler’s law can commonly be observed is in the ubiqutous checkout process found on online shopping sites. Purchasing items online requires customers to provide lots of repetitive information, including billing and shipping details. To simplify this process for customers, it is common to see online stores enable users to have their shipping address inherit the information from their billing address ( Figure 9-3 ). This option simplifies the checkout prcess for customers in many cases, because it prevents them from having to enter duplicate information for shipping. The resulting experience for customers has been effectively simplified, while the complexity required to enable the feature has shifted to the designers and developers responsible for implementing it up front. Simplifying the checkout process even further are services such as Apple Pay ( Figure 9-4 ), which makes paying for items both online and in person even eaier for customers. Once they’ve set up an account, people using Apple Pay or similar payment services can purchase items simply by selecting the option duing checkout and verifying the details of their purchase—no need to enter any additional information. The customer experience thus becomes significantly less complex, with the complexity again shifted to the designers and developers responsible for the service. Figure 9-3. The ability to inherit a shipping address from billing details within an ecommerce checkout simplifies the process and removes the need to type redundant information Figure 9-4. Apple Pay makes the checkout process as easy as selecting the payment option and verifying your purchase (source: Apple, 2019) Retail is an area where you can find many innovative ways to abstract coplexity away from users. Take, for example, Amazon’s Go stores ( Figure 9-5 ), which provide a checkout-free shopping experience",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Take, for example, Amazon’s Go stores ( Figure 9-5 ), which provide a checkout-free shopping experience. First appearing as an experment in downtown Seattle, they are now popping up in major metropolitan areas all over the United States. With the Amazon Go app installed on their smarphone, a customer can simply check in with the app, grab what they need, and walk out, without ever needing to wait in line, scan their items, or even pay in the store. A little later, the customer receives a receipt, and their Amazon account is charged. Figure 9-5. The first Amazon Go store in Seattle (source: Wikipedia, 2019; photographer, Brianc333a) The dizzying array of technology involved in a checkout-free shopping exprience like that found in Amazon Go stores is nothing short of astounding. Advanced technology like machine learning, computer vision, and AI must be deeply integrated to allow for people to simply walk into the store, grab the items they wish to purchase, and then walk out. While the friction of shopping is dratically reduced for customers, the complexity that comes along with it must be absorbed by the designers and developers responsible for ensuring it all works. KEY CONSIDERATION When Simplicity Turns to Abstraction An important goal for designers is to eliminate unnecessary complexity for the users of the products and services they design—to become elgant simplifiers. After all, good user experiences are often those that feel easy and intuitive, and where obstructions that might deter people from achieving their goals are removed. But there is a balance that must be struck when striving for simplicity, and it’s important not to take it too far. When an interface has been simplified to the point of abstraction, there is no longer enough information available for users to make informed decisions",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When an interface has been simplified to the point of abstraction, there is no longer enough information available for users to make informed decisions. In other words, the amount of visual information prsented has been reduced in order to make the interface seem less coplex, but this has led to a lack of sufficient cues to help guide people through a process or to the information they need. Take, for example, iconography. Icons can help simplify an interface by providing an alternative way to communicate visual information that takes up less space than text labels, but they can also lead to ambiguity ( Figure 9-6 ). This is especially true when icons are not accompanied by text labels and are therefore left to interpretation. With a few exceptions, icons seldom hold universal meaning—they can mean different things to different people. To add to the ambiguity, icons that are associated with certain actions don’t always perform the same action from one interface to the next. When iconography is used and does not convey clear meaing or perform a consistent action, it becomes visual noise and an obstcle to completing tasks. Figure 9-6. Iconography can sometimes mean different things to different people Conclusion Tesler’s law is important for designers to be aware of because it relates to a fudamental challenge we face throughout our work: how we manage complexity. We must first acknowledge that with any process there will be a necessary amount of complexity that cannot be removed, no matter how simplified the prcess becomes as a result of the design process. Everything from a humble email to a highly sophisticated checkout process will have inherent complexity that must be managed. As designers, we have a responsibility to remove inherent complexity from our interfaces, or else we ship that complexity to our users. This can result in confusion, frustration, and a bad user experience",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This can result in confusion, frustration, and a bad user experience. Where possible, designers and developers should handle complexity, while taking care not to oversimplify to the point of abstraction.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-87-96.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "Human-computer interaction(HCI) is an area of research and practice that emerged in the early 1980s, initially as a specialty area in computer science embracing cognitive science and human factors engineering. HCI has expanded rapidly and steadily for three decades, attracting professionals from many other disciplines and incorporating diverse concepts and approaches. To a considerable extent, HCI now aggregates a collection of semi-autonomous fields of research and practice in human-centered informatics. However, the continuing synthesis of disparate conceptions and approaches to science and practice in HCI has produced a dramatic example of how different epistemologies and paradigms can be reconciled and integrated in a vibrant and productive intellectual project. Until the late 1970s, the only humans who interacted with computers were information technology professionals and dedicated hobbyists. This changed disruptively with the emergence of personal computing in the later 1970s. Personal computing, including both personal software (productivity applications, such as text editors and spreadsheets, and interactive computer games) and personal computer platforms (operating systems, programming languages, and hardware), made everyone in the world a potential computer user, and vividly highlighted the deficiencies of computers with respect tousabilityfor those who wanted to use computers as tools. Author/Copyright holder: Steven Weyhrich. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in the copyright terms below. Author/Copyright holder: Courtesy of Grubitzsch (geb. Raphael), Waltraud. Copyright terms and licence:CC-Att-SA-3 (Creative Commons Attribution-ShareAlike 3.0) Figure 2.1A-B: Personal computing rapidly pushed computer use into the general population, starting in the later 1970s. However, the non-professional computer user was often subjected to arcane commands and system dialogs",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". However, the non-professional computer user was often subjected to arcane commands and system dialogs. The challenge of personal computing became manifest at an opportune time. The broad project of cognitive science, which incorporatedcognitive psychology,artificial intelligence, linguistics, cognitive anthropology, and the philosophy of mind, had formed at the end of the 1970s. Part of the programme of cognitive science was to articulate systematic and scientifically informed applications to be known as \"cognitive engineering\". Thus, at just the point when personal computing presented the practical need for HCI, cognitive science presented people, concepts, skills, and a vision for addressing such needs through an ambitious synthesis of science and engineering. HCI was one of the first examples of cognitive engineering. Author/Copyright holder: Card, Moran and Newell. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Figure 2.2: The Model Human Processor was an early cognitive engineering model intended to help developers apply principles from cognitive psychology. This was facilitated by analogous developments in engineering and design areas adjacent to HCI, and in fact often overlapping HCI, notably human factors engineering and documentation development. Human factors had developed empirical and task-analytic techniques for evaluating human-system interactions in domains such as aviation and manufacturing, and was moving to address interactive system contexts in which human operators regularly exerted greater problem-solving discretion. Documentation development was moving beyond its traditional role of producing systematic technical descriptions toward a cognitive approach incorporating theories of writing, reading, and media, with empiricaluser testing. Documents and other information needed to be usable also. Author/Copyright holder: MIT Press. Copyright terms and licence: All Rights Reserved. Reproduced with permission",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Documents and other information needed to be usable also. Author/Copyright holder: MIT Press. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Figure 2.3: Minimalist information emphasized supporting goal-directed activity in a domain. Instead of topic hierarchies and structured practice, it emphasized succinct support for self-directed action and for recognizing and recovering from error. Other historically fortuitous developments contributed to the establishment of HCI. Software engineering, mired in unmanageable software complexity in the 1970s (the “software crisis”), was starting to focus on nonfunctional requirements, including usability and maintainability, and on empirical software development processes that relied heavily on iterative prototyping and empirical testing. Computer graphics and information retrieval had emerged in the 1970s, and rapidly came to recognize that interactive systems were the key to progressing beyond early achievements. All these threads of development in computer science pointed to the same conclusion: The way forward for computing entailed understanding and better empowering users. These diverse forces of need and opportunity converged around 1980, focusing a huge burst of human energy, and creating a highly visible interdisciplinary project. The original and abiding technical focus of HCI was and is the concept ofusability. This concept was originally articulated somewhat naively in the slogan \"easy to learn, easy to use\". The bluntsimplicityof this conceptualization gave HCI an edgy and prominent identity in computing. It served to hold the field together, and to help it influence computer science and technology development more broadly and effectively. However, inside HCI the concept of usability has been re-articulated and reconstructed almost continually, and has become increasingly rich and intriguingly problematic",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". However, inside HCI the concept of usability has been re-articulated and reconstructed almost continually, and has become increasingly rich and intriguingly problematic. Usability now often subsumes qualities like fun, well being, collective efficacy, aesthetic tension, enhancedcreativity, flow, support for human development, and others. A more dynamic view of usability is one of a programmatic objective that should and will continue to develop as our ability to reach further toward it improves. Author/Copyright holder: ©. Copyright terms and licence: All Rights Reserved. Used without permission under the Fair Use Doctrine (as permission could not be obtained). See the \"Exceptions\" section (and subsection \"allRightsReserved-UsedWithoutPermission\") on the pagecopyright notice. Figure 2.4: Usability is an emergent quality that reflects the grasp and the reach of HCI. Contemporary users want more from a system than merely “ease of use”. Although the original academic home for HCI was computer science, and its original focus was on personal productivity applications, mainly text editing and spreadsheets, the field has constantly diversified and outgrown all boundaries. It quickly expanded to encompass visualization, information systems, collaborative systems, the system development process, and many areas of design. HCI is taught now in many departments/faculties that address information technology, including psychology, design, communication studies, cognitive science, information science, science and technology studies, geographical sciences, management information systems, and industrial, manufacturing, and systems engineering. HCI research and practice draws upon and integrates all of these perspectives. A result of this growth is that HCI is now less singularly focused with respect to core concepts and methods, problem areas and assumptions about infrastructures, applications, and types of users",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". A result of this growth is that HCI is now less singularly focused with respect to core concepts and methods, problem areas and assumptions about infrastructures, applications, and types of users. Indeed, it no longer makes sense to regard HCI as a specialty of computer science; HCI has grown to be broader, larger and much more diverse than computer science itself. HCI expanded from its initial focus on individual and genericuser behaviorto include social and organizational computing,accessibilityfor the elderly, the cognitively and physically impaired, and for all people, and for the widest possible spectrum of human experiences and activities. It expanded from desktop office applications to include games, learning and education, commerce, health and medical applications, emergency planning and response, and systems to support collaboration and community. It expanded from early graphical user interfaces to include myriad interaction techniques and devices, multi-modal interactions, tool support for model-baseduser interfacespecification, and a host of emerging ubiquitous, handheld andcontext-awareinteractions. There is no unified concept of an HCI professional. In the 1980s, the cognitive science side of HCI was sometimes contrasted with the software tools and user interface side of HCI. The landscape of core HCI concepts and skills is far more differentiated and complex now. HCI academic programs train many different types of professionals:user experience designers, interaction designers, user interface designers, application designers, usability engineers, user interface developers, application developers, technical communicators/online information designers, and more. And indeed, many of the sub-communities of HCI are themselves quite diverse",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". And indeed, many of the sub-communities of HCI are themselves quite diverse. For example, ubiquitous computing (aka ubicomp) is subarea of HCI, but it is also a superordinate area integrating several distinguishable subareas, for examplemobile computing, geo-spatial information systems, in-vehicle systems, community informatics, distributed systems, handhelds, wearable devices, ambient intelligence, sensor networks, and specialized views ofusability evaluation, programming tools and techniques, and application infrastructures. The relationship between ubiquitous computing and HCI is paradigmatic: HCI is the name for a community of communities. Author/Copyright holder: User Experience Professionals Association. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Author/Copyright holder: Envis Precisely. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Figure 2.5 A-B: Two visualizations of the variety of disciplinary knowledge and skills involved in contemporary design of human-computer interactions Indeed, the principle that HCI is a community of communities is now a point of definition codified, for example, in the organization of major HCI conferences and journals. The integrating element across HCI communities continues to be a close linkage of critical analysis of usability, broadly understood, with development of novel technology and applications. This is the defining identity commitment of the HCI community. It has allowed HCI to successfully cultivate respect for the diversity of skills and concepts that underlie innovative technology development, and to regularly transcend disciplinary obstacles. In the early 1980s, HCI was a small and focused specialty area. It was a cabal trying to establish what was then a heretical view of computing",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". In the early 1980s, HCI was a small and focused specialty area. It was a cabal trying to establish what was then a heretical view of computing. Today, HCI is a vast and multifaceted community, bound by the evolving concept of usability, and the integrating commitment to value human activity and experience as the primary driver in technology. Given the contemporary shape of HCI, it is important to remember that its origins are personal productivity interactions bound to the desktop, such as word processing and spreadsheets. Indeed, one of biggest design ideas of the early 1980s was the so-called messy desk metaphor, popularized by the Apple Macintosh: Files and folders were displayed as icons that could be, and were scattered around the display surface. The messy desktop was a perfect incubator for the developing paradigm of graphical user interfaces. Perhaps it wasn’t quite as easy to learn and easy to use as claimed, but people everywhere were soon double clicking, dragging windows and icons around their displays, and losing track of things on their desktop interfaces just as they did on their physical desktops. It was surely a stark contrast to the immediately prior teletype metaphor of Unix, in which all interactions were accomplished by typing commands. Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Figure 2.6: The early Macintosh desktop metaphor: Icons scattered on the desktop depict documents and functions, which can be selected and accessed (as System Disk in the example) Even though it can definitely be argued that the desktop metaphor was superficial, or perhaps under-exploited as a design paradigm, it captured imaginations of designers and the public. These were new possibilities for many people in 1980, pundits speculated about how they might change office work",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_7"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". These were new possibilities for many people in 1980, pundits speculated about how they might change office work. Indeed, the tsunami of desktop designs challenged, sometimes threatened the expertise and work practices of office workers. Today they are in the cultural background. Children learn these concepts and skills routinely. As HCI developed, it moved beyond the desktop in three distinct senses. First, the desktop metaphor proved to be more limited than it first seemed. It’s fine to directly represent a couple dozen digital objects as icons, but this approach quickly leads to clutter, and is not very useful for people with thousands of personal files and folders. Through the mid-1990s, HCI professionals and everyone else realized that search is a more fundamental paradigm than browsing for finding things in a user interface. Ironically though, when early World Wide Web pages emerged in the mid-1990s, they not only dropped the messy desktop metaphor, but for the most part dropped graphical interactions entirely. And still they were seen as a breakthrough in usability (of course, the direct contrast was to Unix-style tools like ftp and telnet). The design approach of displaying and directly interacting with data objects as icons has not disappeared, but it is no longer a hegemonic design concept. Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Figure 2.7: The early popularity of messy desktops for personal information spaces does not scale. The second sense in which HCI moved beyond the desktop was through the growing influence of the Internet on computing and on society. Starting in the mid-1980s, email emerged as one of the most important HCI applications, but ironically, email made computers and networks into communication channels; people were not interactingwithcomputers, they were interacting with other peoplethroughcomputers",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_8"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Tools and applications to support collaborative activity now include instant messaging, wikis, blogs, online forums, social networking, social bookmarking and tagging services, media spaces and other collaborative workspaces, recommender and collaborative filtering systems, and a wide variety of online groups and communities. New paradigms and mechanisms for collective activity have emerged including online auctions, reputation systems, soft sensors, and crowd sourcing. This area of HCI, now often called social computing, is one of the most rapidly developing. Author/Copyright holder: ©. Copyright terms and licence: All Rights Reserved. Used without permission under the Fair Use Doctrine (as permission could not be obtained). See the \"Exceptions\" section (and subsection \"allRightsReserved-UsedWithoutPermission\") on the pagecopyright notice. Author/Copyright holder: Courtesy of Larry Ewing. Copyright terms and licence: CC-Att-3 (Creative Commons Attribution 3.0 Unported). Author/Copyright holder: GitHub Inc. Copyright terms and licence: All Rights Reserved. Used without permission under the Fair Use Doctrine (as permission could not be obtained). See the \"Exceptions\" section (and subsection \"allRightsReserved-UsedWithoutPermission\") on the pagecopyright notice. Figure 2.8 A-B-C: A huge and expanding variety of social network services are part of everyday computing experiences for many people. Online communities, such as Linux communities and GitHub, employ social computing to produce high-quality knowledge work. The third way that HCI moved beyond the desktop was through the continual, and occasionally explosive diversification in the ecology of computing devices. Before desktop applications were consolidated, new kinds of device contexts emerged, notably laptops, which began to appear in the early 1980s, and handhelds, which began to appear in the mid-1980s",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_9"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Before desktop applications were consolidated, new kinds of device contexts emerged, notably laptops, which began to appear in the early 1980s, and handhelds, which began to appear in the mid-1980s. One frontier today is ubiquitous computing: The pervasive incorporation of computing into human habitats — cars, home appliances, furniture, clothing, and so forth. Desktop computing is still very important, though the desktop habitat has been transformed by the wide use of laptops. To a considerable extent, the desktop itself has moved off the desktop. Author/Copyright holder: Courtesy of Andrew Stern. Copyright terms and licence: CC-Att-SA-3 (Creative Commons Attribution-ShareAlike 3.0) Author/Copyright holder: Courtesy of United States Federal Government. Copyright terms and licence: pd (Public Domain (information that is common property and contains no original authorship)). Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Figure 2.9 A-B-C: Computing moved off the desktop to be everywhere all the time. Computers are in phones, cars, meeting rooms, and coffee shops. The focus of HCI has moved beyond the desktop, and its focus will continue to move. HCI is a technology area, and it is ineluctably driven to frontiers of technology and application possibility. The special value and contribution of HCI is that it will investigate, develop, and harness those new areas of possibility not merely as technologies or designs, but as means for enhancing human activity and experience. The movement of HCI off the desktop is a large-scale example of a pattern of technology development that is replicated throughout HCI at many levels of analysis. HCI addresses the dynamic co-evolution of the activities people engage in and experience, and the artifacts — such as interactive tools and environments — that mediate those activities",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_10"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". HCI addresses the dynamic co-evolution of the activities people engage in and experience, and the artifacts — such as interactive tools and environments — that mediate those activities. HCI is about understanding and critically evaluating the interactive technologies people use and experience. But it is also about how those interactions evolve as people appropriate technologies, as their expectations, concepts and skills develop, and as they articulate new needs, new interests, and new visions and agendas for interactive technology. Reciprocally, HCI is about understanding contemporary human practices and aspirations, including how those activities are embodied, elaborated, but also perhaps limited by current infrastructures and tools. HCI is about understanding practices and activity specifically as requirements and design possibilities envisioning and bringing into being new technology, new tools and environments. It is about exploring design spaces, and realizing new systems and devices through the co-evolution of activity and artifacts, the task-artifact cycle. Author/Copyright holder: Courtesy of John M. Carroll. Copyright terms and licence: CC-Att-SA-3 (Creative Commons Attribution-ShareAlike 3.0) Figure 2.10: Human activities implicitly articulate needs, preferences and design visions. Artifacts are designed in response, but inevitably do more than merely respond. Through the course of their adoption andappropriation, new designs provide new possibilities for action and interaction. Ultimately, this activity articulates further human needs, preferences, and design visions. Understanding HCI as inscribed in a co-evolution of activity and technological artifacts is useful. Most simply, it reminds us what HCI is like, that all of the infrastructure of HCI, including its concepts, methods, focal problems, and stirring successes will always be in flux",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_11"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Most simply, it reminds us what HCI is like, that all of the infrastructure of HCI, including its concepts, methods, focal problems, and stirring successes will always be in flux. Moreover, because the co-evolution of activity and artifacts is shaped by a cascade of contingent initiatives across a diverse collection of actors, there is no reason to expect HCI to be convergent, or predictable. This is not to say progress in HCI is random or arbitrary, just that it is more like world history than it is like physics. One could see this quite optimistically: Individual and collective initiative shapes what HCI is, but not the laws of physics. Author/Copyright holder: Palo Alto Research Center Incorporated (PARC) - a Xerox company. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Figure 2.11: Smalltalk was a programming language and environment project in Xerox Palo Alto Research Center in the 1970s. The work of a handful of people, it became the direct antecedent for the modern graphical user interface. A second implication of the task-artifact cycle is that continual exploration of new applications and application domains, new designs and design paradigms, new experiences, and new activities should remain highly prized in HCI. We may have the sense that we know where we are going today, but given the apparent rate of co-evolution in activity and artifacts, our effective look-ahead is probably less than we think. Moreover, since we are in effect constructing a future trajectory, and not just finding it, the cost of missteps is high. The co-evolution of activity and artifacts evidences strong hysteresis, that is to say, effects of past co-evolutionary adjustments persist far into the future. For example, many people struggle every day with operating systems and core productivity applications whose designs were evolutionary reactions to misanalyses from two or more decades ago",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_12"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". For example, many people struggle every day with operating systems and core productivity applications whose designs were evolutionary reactions to misanalyses from two or more decades ago. Of course, it is impossible to always be right with respect to values and criteria that will emerge and coalesce in the future, but we should at least be mindful that very consequential missteps are possible. Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Figure 2.12 A-B: The Drift Table is an interactive coffee table; aerial views of England and Wales are displayed the porthole on top; placing and moving objects on the table causes the aerial imagery to scroll. This design is intended to provoke reaction and challenge thinking about domestic technologies. The remedy is to consider many alternatives at every point in the progression. It is vitally important to have lots of work exploring possible experiences and activities, for example, on design and experience probes andprototypes. If we focus too strongly on theaffordancesof currently embodied technology we are too easily and uncritically accepting constraints that will limit contemporary HCI as well as all future trajectories. Author/Copyright holder: Apple Computer, Inc. Copyright terms and licence: All Rights Reserved. Used without permission under the Fair Use Doctrine (as permission could not be obtained). See the \"Exceptions\" section (and subsection \"allRightsReserved-UsedWithoutPermission\") on the pagecopyright notice. Author/Copyright holder: Courtesy of Antonio Zugaldia. Copyright terms and licence: CC-Att-SA-2 (Creative Commons Attribution-ShareAlike 2.0 Unported)",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_13"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Author/Copyright holder: Courtesy of Antonio Zugaldia. Copyright terms and licence: CC-Att-SA-2 (Creative Commons Attribution-ShareAlike 2.0 Unported). Figure 2.13 A-B: Siri, the speech-based intelligent assistant for Apple’s iPhone, and theaugmented realityglasses of Goggle’s Project Glass are recent examples of technology visions being turned into everyday HCI experiences. HCI is not fundamentally about the laws of nature. Rather, it managesinnovationto ensure that human values and human priorities are advanced, and not diminished through new technology. This is what created HCI; this is what led HCI off the desktop; it will continue to lead HCI to new regions of technology-mediated human possibility. This is why usability is an open-ended concept, and can never be reduced to a fixed checklist. The contingent trajectory of HCI as a project in transforming human activity and experience through design has nonetheless remained closely integrated with the application and development of theory in the social and cognitive sciences. Even though, and to some extent because the technologies and human activities at issue in HCI are continually co-evolving, the domain has served as a laboratory and incubator for theory. The origin of HCI as an early case study in cognitive engineering had an imprinting effect on the character of the endeavor. From the very start, the models, theories and frameworks developed and used in HCI were pursued as contributions to science: HCI has enriched every theory it has appropriated. For example, the GOMS (Goals, Operations, Methods, Selection rules) model, the earliest native theory in HCI, was a more comprehensive cognitive model than had been attempted elsewhere in cognitive science and engineering; the model human processor included simple aspects of perception, attention,short-term memoryoperations, planning, and motor behavior in a single model",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_14"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". But GOMS was also a practical tool, articulating the dual criteria of scientific contributionplusengineering and design efficacy that has become the culture of theory and application in HCI. Author/Copyright holder: Bonnie E. John. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Author/Copyright holder: Bonnie E. John. Copyright terms and licence: All Rights Reserved. Reproduced with permission. See section \"Exceptions\" in thecopyright termsbelow. Figure 2.14 A-B: CogTool analyzes demonstrations of user tasks to produce a model of the cognitive processes underlying task performance; from this model it predicts expert performance times for the tasks. The focus of theory development and application has moved throughout the history of HCI, as the focus of the co-evolution of activities and artifacts has moved.?á Thus, the early information processing-based psychological theories, like GOMS, were employed to model the cognition and behavior of individuals interacting with keyboards, simple displays, and pointing devices. This initial conception of HCI theory was broadened as interactions became more varied and applications became richer. For example, perceptual theories were marshaled to explain how objects are recognized in a graphical display, mental model theories were appropriated to explain the role of concepts — like the messy desktop metaphor — in shaping interactions, active user theories were developed to explain how and why users learn and making sense of interactions. In each case, however, these elaborations were both scientific advances and bases for better tools and design practices. This dialectic of theory and application has continued in HCI",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_15"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". In each case, however, these elaborations were both scientific advances and bases for better tools and design practices. This dialectic of theory and application has continued in HCI. It is easy to identify a dozen or so major currents of theory, which themselves can by grouped (roughly) into three eras: theories that view human-computer interaction as information processing, theories that view interaction as the initiative of agents pursuing projects, and theories that view interaction as socially and materially embedded in rich contexts. To some extent, the sequence of theories can be understood as a convergence of scientific opportunity and application need: Codifying and using relatively austere models made it clear what richer views of people and interaction could be articulated and what they could contribute; at the same time, personal devices became portals for interaction in the social and physical world, requiring richer theoretical frameworks for analysis and design. Author/Copyright holder: Unknown (pending investigation). Copyright terms and licence: Unknown (pending investigation). See section \"Exceptions\" in thecopyright termsbelow. Figure 2.15: Through the past three decades, a series of theoretical paradigms emerged to address the expanding ambitions of HCI research, design, and product development. Successive theories both challenged and enriched prior conception of people and interaction. All of these theories are still relevant and still in use today in HCI. The sequence of theories and eras is of course somewhat idealized. People still work on GOMS models; indeed, all of the major models, theories and frameworks that ever were employed in HCI are still in current use. Indeed, they continue to develop as the context of the field develops. GOMS today is more a niche model than a paradigm for HCI, but has recently been applied in research on smart phone designs and human-robot interactions",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_16"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". GOMS today is more a niche model than a paradigm for HCI, but has recently been applied in research on smart phone designs and human-robot interactions. The challenge of integrating, or at least better coordinating descriptive and explanatory science goals with prescriptive and constructive design goals is abiding in HCI. There are at least three ongoing directions — traditional application of ever-broader and deeper basic theories, development of local, sometimes domain dependent proto-theories within particular design domains, and the use of design rationale as a mediating level of description between basic science and design practice. One of the most significant achievements of HCI is its evolving model of the integration of research and practice. Initially this model was articulated as a reciprocal relation between cognitive science and cognitive engineering. Later, it ambitiously incorporated a diverse science foundation, notably social and organizational psychology, Activity Theory, distributed cognition, and sociology, and a ethnographic approaches human activity, including the activities of design and technology development and appropriation. Currently, the model is incorporating design practices and research across a broad spectrum, for example, theorizing user experience and ecological sustainability. In these developments, HCI provides a blueprint for a mutual relation between science and practice that is unprecedented. Although HCI was always talked about as a design science or as pursuing guidance for designers, this was construed at first as a boundary, with HCI research and design as separate contributing areas of professional expertise. Throughout the 1990s, however, HCI directly assimilated, and eventually itself spawned, a series of design communities. At first, this was a merely ecumenical acceptance of methods and techniques laying those of beyond those of science and engineering",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_17"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". At first, this was a merely ecumenical acceptance of methods and techniques laying those of beyond those of science and engineering. But this outreach impulse coincided with substantial advances in user interface technologies that shifted much of the potential proprietary value of user interfaces into graphical design and much richer ontologies of user experience. Somewhat ironically, designers were welcomed into the HCI community just in time to help remake it as a design discipline. A large part of this transformation was the creation of design disciplines and issues that did not exist before. For example,user experience designandinteraction designwere not imported into HCI, but rather were among the first exports from HCI to the design world. Similarly, analysis of the productive tensions between creativity and rationale in design required a design field like HCI in which it is essential that designs have an internal logic, and can be systematically evaluated and maintained, yet at the same time provoke new experiences and insights.?á Design is currently the facet of HCI in most rapid flux. It seems likely that more new design proto-disciplines will emerge from HCI during the next decade. No one can accuse HCI of resting on laurels. Conceptions of how underlying science informs and is informed by the worlds of practice and activity have evolved continually in HCI since its inception. Throughout the development of HCI, paradigm-changing scientific and epistemological revisions were deliberately embraced by a field that was, by any measure, succeeding intellectually and practically. The result has been an increasingly fragmented and complex field that has continued to succeed even more. This example contradicts the Kuhnian view of how intellectual projects develop through paradigms that are eventually overthrown. The continuing success of the HCI community in moving its meta-project forward thus has profound implications, not only for human-centered informatics, but for epistemology",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_18"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The continuing success of the HCI community in moving its meta-project forward thus has profound implications, not only for human-centered informatics, but for epistemology. There are many highly readable descriptions of the disciplinary landscape in which early HCI developed: 1980 volume of the journal Cognitive Science provides a vivid picture of the foundations of cognitive science as they were being built (http://csjarchive.cogsci.rpi.edu/1980v04/index.html); F. Brooks’ bookThe Mythical Man-Month(1975, Addison-Wesley) is an insightful analysis of software engineering, and the original source for the idea that iterative prototyping is inevitable in the design and development of complex software; J. Foley and A. van Dam’s bookComputer Graphics(1982, Addison Wesley) describes the early field of computer graphics as a root of what would become human-computer interaction. Vivid primary information about the founding of HCI - the proceedings of the 1982 US Bureau of Standards Conference in Gaithersburg, Maryland, are available in the ACM Digital Library athttp://dl.acm.org/citation.cfm?id=800049 Several histories of HCI have been published: Carroll, J.M. (1997) Human-Computer Interaction: Psychology as a science of design.Annual Review of Psychology, 48,61-83.?á (Co-published (slightly revised) inInternational Journal of Human-Computer Studies, 46,501-522). Grudin, J. (2012) A Moving Target: The evolution of Human-computer Interaction. In J. Jacko (Ed.),Human-computer interaction handbook: Fundamentals, evolving technologies, and emerging applications. (3rd edition).Taylor & Francis. Myers, B.A. (1998) A Brief History of Human Computer Interaction Technology.ACM interactions. Vol. 5, no. 2, March. pp. 44-54. The leading HCI textbooks also include some discussion of history (see below). There is some dispute as to how to address the evolution of usability",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_19"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Vol. 5, no. 2, March. pp. 44-54. The leading HCI textbooks also include some discussion of history (see below). There is some dispute as to how to address the evolution of usability. In this overview, I take a historical view that the concept itself is evolving, analogous to way physics has treated its fundamental concepts, such as gravity and mass. See also Carroll, J.M. (2004) Beyond fun.ACM interactions, 11(5),38-40. The ACM Special Interest Group on Computer-Human Interaction (SIGCHI), and its CHI Conference, one of the most general and significant HCI conferences, now is explicitly organized into communities that manage pieces of the technical program (http://www.sigchi.org/communities). In fall of 2012, these communities included CCaA (Creativity, Cognition and Art), CSCW (Computer-Supported Cooperative Work), EICS (Engineering Interactive Computer Systems), HCI and Sustainability, HCI Education, HCI4D (HCI for Development), Heritage Matters, Latin American HCI, Pattern Languages and HCI, Research-practice Interaction, UbiComp (Ubiquitous Computing), and UIST (User Interface Software and Tools). An even more diverse view of HCI can be appreciated by investigating HCI activities and interest groups embedded in professional communitiesother thanACM: the Design Research Society (designresearchsociety.org), the Association for Information Systems (sighci.org), the Human Factors and Ergonomics Society (hfes.org), the Society for Technical Communication (stc.org), the AIGA (aiga.org), International Communication Association (icahdq.org), the Interaction Design Association (https://www.ixda.org/), the IEEE Professional Communication Society (pcs.ieee.org), the European Association of Work and Organizational Psychology (eawop2013.org), and many others. Further relevant material in the Encyclopedia of Human-Computer Interaction can be found inchapters 1, 3, 8, 13, 15, 19, 21, and 22. A classic discussion of the desktop metaphor is Apple Human Interface Guidelines:Apple&Raskin, J",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_20"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". A classic discussion of the desktop metaphor is Apple Human Interface Guidelines:Apple&Raskin, J. (1992).Macintosh Human Interface Guidelines.Addison-Wesley Professional.ISBN0-201-62216-5. An early critique of the Macintosh user interface paradigm is: Gentner, D. and Nielsen, J. (1996) The Anti-Mac interface,Communications of the ACM39, 8 (August), 70-82. The emergence of collaboration, mobility, and new types of user devices and interactions as major themes driving “HCI beyond the desktop” are discussed widely, of course; here are some starting points: Horn, D.B., Finholt, T.A., Birnholtz, J.P., Motwani, D. and Jayaraman, S. (2004) Six degrees of jonathan grudin: a social network analysis of the evolution and impact of CSCW research. InProceedings of the 2004 ACM conference on Computer supported cooperative work(CSCW '04). ACM, New York, NY, USA, 582-591. Luff, P. and Heath, C. (1998) Mobility in collaboration. InProceedings of the 1998 ACM conference on Computer supported cooperative work(CSCW '98). ACM, New York, NY, USA, 305-314. Shaer, O. and Hornecker, E. (2010) Tangible User Interfaces: Past, Present, and Future Directions.Found. Trends Hum.-Comput. Interact.3, 1-2 (January), 1-137. Waller V. and Johnston, R.B. (2009) Making ubiquitous computing available.Commun. ACM52, 10 (October 2009), 127-130. Further relevant material in the Encyclopedia of Human-Computer Interaction can be found inchapters 4, 14, 23, and 27. I use the term “task-artifact cycle” here, as originally introduced in a 1991 paper with Wendy Kellogg and Mary Beth Rosson, though think “activity” better conveys what I mean than “task”. Not surprisingly, the terminology of the task-artifact cycle is itself an example of how HCI shifts under its own foundations; see Carroll, J.M., Kellogg, W.A., & Rosson, M.B. (1991) The task-artifact cycle.?á In J.M. Carroll (Ed.),?áDesigning Interaction: Psychology at the human-computer interface.?áNew York: Cambridge University Press, pages 74-102",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_21"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". (1991) The task-artifact cycle.?á In J.M. Carroll (Ed.),?áDesigning Interaction: Psychology at the human-computer interface.?áNew York: Cambridge University Press, pages 74-102. A good reference for the history of the Smalltalk project is Kay, A.C. (1996) The early history of Smalltalk. InHistory of programming languages---II, Thomas J. Bergin, Jr. and Richard G. Gibson, Jr. (Eds.). ACM, New York, NY, USA 511-598. For more discussion of the drift table project, see Boucher, A. and Gaver, W. 2006. Developing the drift table.interactions13, 1 (January 2006), 24-27. For more discussion of the general point being emphasized through the example of the drift table, see Sengers, P. and W. Gaver, W. (2006) Staying open to interpretation: engaging multiple meanings in design and evaluation. InProceedings of the 6th conference on Designing Interactive systems(DIS '06). ACM, New York, NY, USA, 99-108. Further relevant material in the Encyclopedia of Human-Computer Interaction can be found inchapters 7, 12, 14, and 20. I edited a book of theory overviews (Carroll, 2003), referenced below. It is available online (http://www.sciencedirect.com/science/book/9781558608085). I am currently curating theory overviews in the Synthesis Lectures on Human-Centered Informatics (http://www.morganclaypool.com/toc/hci/1/1). John, B.E. (2011) Using predictive human performance models to inspire and support UI design recommendations. InProceedings of the 2011 annual conference on Human factors in computing systems(CHI '11). ACM, New York, NY, USA, 983-986. Further relevant material in the Encyclopedia of Human-Computer Interaction can be found in chapters5, 6, 9, 11, 16, 1 7, 24, 25, 26, and 28. The work I refer to on creativity and design rationale is collected in a book, Carroll (2012), reference below",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_22"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The work I refer to on creativity and design rationale is collected in a book, Carroll (2012), reference below. A nice example, I think, of the theoretical multi-vocality I describe in this section can be appreciated by contrasting these three treatments ofaestheticsin HCI design (all are published in theSynthesis Lectures on Human-Centered Informatics, http://www.morganclaypool.com/toc/hci/1/1): Hassenzahl, M. (2010). Experience Design: Technology for All the Right Reasons. Sutcliffe, A. (2009) Designing for User Engagement: Aesthetic and Attractive User Interfaces Wright, P. and McCarthy, J. (2010) Experience-Centered Design: Designers, Users, and Communities in Dialogue. My reference to Kuhn regarding the development of science and knowledge is: Kuhn, T.S. (1962)The Structure of Scientific Revolutions. Chicago: University of Chicago Press. Kuhn, T.S. (1977)The Essential Tension: Selected Studies in Scientific Tradition and Change. Chicago and London: University of Chicago Press. The number of important monographs is just too large to list, so I have concentrated in the list below on a few significant textbooks. Readers should also check theHCI Bibliography, theHCC Education Digital Library, theACM Digital Library, and theSynthesis Series of lectures on human-centered informatics. These are the three most comprehensive textbooks: Dix, A.J.,Finlay, J.E.,Abowd, G.D. andBeale, R. (2003).Human-Computer Interaction (3rd Edition).Prentice Hall Rogers, Y.,Sharp, H. andPreece, J.J. (2011)Interaction Design: Beyond Human-Computer Interaction (3rd ed.).John Wiley and Sons Shneiderman, B. andPlaisant, C. (2009).Designing the User Interface: Strategies for Effective Human-Computer Interaction (5th ed.).Addison-Wesley Several texts present more specialized views of HCI. Carroll (2003) collected a set of introductory papers on major theories used in HCI. L??wgren and Stolterman (2007) present a design perspective on HCI",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_23"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Carroll (2003) collected a set of introductory papers on major theories used in HCI. L??wgren and Stolterman (2007) present a design perspective on HCI. Rosson and Carroll (2002) emphasize a software engineering view of HCI using a set of case studies to convey an engineering process view of usability. Tidwell (2011) presents a pattern-based approach to user interface design. Carroll, John M. (ed.) (2003).HCI Models, Theories, and Frameworks: Toward a Multidisciplinary Science.Morgan Kaufmann L??wgren, J. and Stolterman, E. (2007)Thoughtful Interaction Design: A Design Perspective on Information Technology.MIT Press. Rosson, M.B. andCarroll, J.M. (2002).Usability Engineering: Scenario-Based Development of Human Computer Interaction.Morgan Kaufmann. Tidwell, J. (2011)Designing Interfaces (2nd ed.).O’Reilly Media. The leading general journal for HCI is theACM Transactions on Computer-Human Interaction. However, there are many other well-established journals of roughly equivalent quality:Human-Computer Interaction(emphasizes design research),Interacting With Computers,International Journal of Human-Computer Studies,Behaviour and Information Technology,International Journal of Human-Computer Interaction,Journal of Computer-Supported Cooperative Work. Recently, Association for Information Systems has initiated aTransactions on Human-Computer Interaction.Morgan-Claypool publishes a monograph series,Synthesis Lectures on Human-Centered Informatics. My personal perspectives on the emergence and development of HCI are elaborated in several other articles, monographs, and introductions to edited books: Carroll, J.M. ?á(1995)?á Introduction: The scenario perspective on system development.?á In Carroll, J.M. (Ed.),Scenario-based design: Envisioning work and technology in system development.New York: John Wiley & Sons, pp 1-17. Carroll, John M",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_24"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". (Ed.),Scenario-based design: Envisioning work and technology in system development.New York: John Wiley & Sons, pp 1-17. Carroll, John M. (1997) Human-Computer Interaction: Psychology as a Science of Design.Annual Review of Psychology, 48,61-83.?á Co-published (slightly revised) inInternational Journal of Human-Computer Studies, 46(4), 501-522 Carroll, J.M. (1998) Reconstructing minimalism. In J.M. Carroll (Ed.)Minimalism beyond?á “The Nurnberg Funnel”.M.I.T. Press Carroll, J.M. (2000)?áMaking use: Scenario-based design of human-computer interactions.?áMIT Press.?á Japanese edition published in 2003 by Kyoritsu Publishing; translated by Professor Kentaro Go. Carroll, John M. (2002). Human-Computer Interaction. In: (ed.),MacMillan Encyclopedia of Cognitive Science. Macmillan-Nature Publishing Group. Carroll, John M. (2004). Beyond fun.Interactions, 11(5), 38-40 Carroll, J.M. (2010) Narrating the Future:Scenariosand the Cult of Specification. In Selber, S. (Ed.),Rhetorics And Technologies: New directions in writing and communication.University of South Carolina Press, pp. 134-147. Carroll, J.M. (2010). Conceptualizing a possible discipline of Human-Computer Interaction.Interacting with Computers, 22, 3-12. Carroll, J.M. (2012)The neighborhood in the Internet: Design research projects in community informatics.Routledge. Carroll, J.M. (2012) Creativity and Rationale: The Essential Tension, in J.M. Carroll (Ed.)Creativity and rationale: Enhancing human experience by design.Springer, pages 1-10",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_25"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Carroll, J.M. (2012) Creativity and Rationale: The Essential Tension, in J.M. Carroll (Ed.)Creativity and rationale: Enhancing human experience by design.Springer, pages 1-10. 20112010200920082007200620052004200320022001200019991998199719961995199419931992199119901989198819871986198519831982 200920072003200320012001199919971995199319911989 201220122012201120102008200620042004200220001998199619941992199019881986 20122012201120102009200820072007200720062005200420032003200220012000199919981997199619951994199319921991199019891988 20102008200620042002200220002000 2012201020092008200620062005200420032002200120001998199719961995199419931992199119891988198719861985 2010200920072005200320011999199719951993199119901988198619841982 201220102008200620042002200019971995 Next conference is coming up08 Jun 2016in Brisbane, Australia 201120092007200520021999 Myers, Brad A. (1998): A Brief History of Human-Computer Interaction Technology. InInteractions, 5 (2) pp. 44-54 We believe in Open Access and thedemocratization of knowledge. Unfortunately, world-class educational materials such as this page are normally hidden behind paywalls or in expensive textbooks. If you want this to change,cite this book chapter, link to us, orjoin usto help usdemocratize design knowledge! With 186,761 graduates, the Interaction Design Foundation is the biggest online design school globally. We were founded in 2002. Reach us athello@interaction-design.orgor through ouronline contact form. Have questions? Check ourfrequently asked questions. Join315,773designers and getweekly inspiration and design tipsin your inbox. Experience the full potential of our site that remembers your preferences and supports secure sign-in. Governs the storage of data necessary for maintaining website security, user authentication, and fraud prevention mechanisms. Saves your settings and preferences, like your location, for a more personalized experience. We share user ID with Bugsnag and NewRelic to help us track errors and fix issues",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_26"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Saves your settings and preferences, like your location, for a more personalized experience. We share user ID with Bugsnag and NewRelic to help us track errors and fix issues. Collects anonymous data on how you navigate and interact, helping us make informed improvements. Differentiates real visitors from automated bots, ensuring accurate usage data and improving your website experience. Lets us tailor your digital ads to match your interests, making them more relevant and useful to you. Stores information for better-targeted advertising, enhancing your online ad experience. Permits storing data to personalize content and ads across Google services based on user behavior, enhancing overall user experience. Allows for content and ad personalization across Google services based on user behavior. This consent enhances user experiences. Enables personalizing ads based on user data and interactions, allowing for more relevant advertising experiences across Google services. Receive more relevant advertisements by sharing your interests and behavior with our trusted advertising partners. Enables better ad targeting and measurement on Meta platforms, making ads you see more relevant. Tracks conversions, retargeting, and web analytics for LinkedIn ad campaigns, enhancing ad relevance and performance. Enhances LinkedIn advertising through server-side event tracking, offering more accurate measurement and personalization. Tracks ad performance and user engagement, helping deliver ads that are most useful to you. Share on: or copy link Simply copy and paste the text below into your bibliographic reference list, onto your blog, or anywhere else. You can also just hyperlink to this book chapter. Download our free ebook “The Basics of User Experience Design” to learn about core concepts of UX design. In 9 chapters, we'll cover: conducting user interviews, design thinking, interaction design, mobile UX design, usability, UX research, and many more! Enjoy unlimited downloads of our literature",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_27"
  },
  {
    "document_type": "online_article",
    "title": "Human Computer Interaction - brief intro",
    "author": "Unknown",
    "source": "https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/human-computer-interaction-brief-intro",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". In 9 chapters, we'll cover: conducting user interviews, design thinking, interaction design, mobile UX design, usability, UX research, and many more! Enjoy unlimited downloads of our literature. Our online textbooks are written by 100+ leading designers, bestselling authors and Ivy League professors. Download our free ebook “The Basics of User Experience Design” to learn about core concepts of UX design. In 9 chapters, we'll cover: conducting user interviews, design thinking, interaction design, mobile UX design, usability, UX research, and many more!",
    "chunk_id": "Human_computer_interaction_human_computer_interaction_-_brief_intro.json_chunk_28"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Hick’s Law The time it takes to make a decision increases with the number and coplexity of choices available. Key Takeaways • Minimize choices when response times are critical to increase decision time. • Break complex tasks into smaller steps in order to decrease cogntive load. • Avoid overwhelming users by highlighting recommended options. • Use progressive onboarding to minimize cognitive load for new users. • Be careful not to simplify to the point of abstraction. Overview One of the primary functions we have as designers is to synthesize information and present it in a way that doesn’t overwhelm the people who use the products and services we design. We do this because we understand, almost instinctively, that redundancy and excessiveness create confusion. This confusion is probleatic when it comes to creating products and services that feel intuitive. Instead we should enable people to quickly and easily accomplish their goals. We risk causing confusion when we don’t completely understand the goals and costraints of the people using the product or service. Ultimately, our objective is to 23 understand what the user seeks to accomplish so that we can reduce or eliminate anything that doesn’t contribute to them successfully achieving their goal(s). We in essence strive to simplify complexity through efficiency and elegance. What is neither efficient nor elegant is when an interface provides too many options. This is a clear indication that those who created the product or service do not entirely understand the needs of the user. Complexity extends beyond just the user interface; it can be applied to processes as well. The absence of a distintive and clear call to action, unclear information architecture, unnecessary steps, too many choices or too much information—all of these can be obstacles to users seeking to perform a specific task. This observation directly relates to Hick’s law, which predicts that the time it takes to make a decision increases with the number and complexity of choices available",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This observation directly relates to Hick’s law, which predicts that the time it takes to make a decision increases with the number and complexity of choices available. Not only is this principle fundamental to decision making, but it’s critcal to how people perceive and process the user interfaces we create. We’ll look at some examples of how this principle relates to design, but first let’s look at its origins. Origins Hick’s law was formulated in 1952 by psychologists William Edmund Hick and Ray Hyman, who were examining the relationship between the number of stiuli present and an individual’s reaction time to any given stimulus. What they found was that increasing the number of choices available logarithmically increses decision time. In other words, people take longer to make a decision when given more options to choose from. It turns out there is an actual formula to reresent this relationship: RT = a + b log 2 ( n ) ( Figure 3-1 ). This formula calculates response time (RT) based on the number of stimuli present ( n ) and two arbitrary measurable constants that depend on the task ( a , b ). Fortunately, we don’t need to understand the math behind this formula to grasp what it means. The concept is straightforward when applied to design: the time it takes for users to interact with an interface directly correlates to the nuber of options available to interact with. This implies that complex or busy intefaces result in longer decision times for users, because they must first process the options that are available to them and then choose which is the most relevant in relation to their goal. When an interface is too busy, actions are unclear or difficult to identify, and critical information is hard to find, a larger amount of brain power is required to find what we are looking for. This leads us to our key concept for Hick’s law: cognitive load. Figure 3-1",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This leads us to our key concept for Hick’s law: cognitive load. Figure 3-1. Diagram representing Hick’s law PSYCHOLOGY CONCEPT Cognitive Load When engaging with a digital product or service, a user must first learn how it works and then determine how to find the information they are looking for. Understanding how to use the navigation (or sometimes even finding it), processing the page layout, interacting with UI elements, and entering information into forms all require mental resources. While this learning process is happening, the user must also maintain focus on what they intended to do in the first place. Depending on how easy an interface is to use, the latter can be quite a challenge. The amount of mental resources needed to understand and interact with an interface is known as cognitive load . You can think of it like memory in a phone or laptop: run too many apps and the battery begins to drain and the device slows down, or worst of all, it crashes. The amount of processing power available determines performance, and this depends on memory—a finite resource. Our brains work similarly: when the amount of information coming in exceeds the space we have available, we struggle mentally to keep up— tasks become more difficult, details are missed, and we begin to feel overwhelmed. Our working memory, the buffer space ( Figure 3-2 ) available for storing information relevant to the current task, has a spcific number of slots in which to store information. If the tasks at hand require more space than is available, we begin to lose information from our working memory to accommodate this new information. Figure 3-2. Working memory buffer illustration This becomes problematic when the information lost is critical to the task that someone wishes to perform or is related to the information they want to find. Tasks will become more difficult and users might start to feel overwhelmed, ultimately leading to frustration or even task abadonment—both symptoms of a bad user experience",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Tasks will become more difficult and users might start to feel overwhelmed, ultimately leading to frustration or even task abadonment—both symptoms of a bad user experience. Examples Now that we have an understanding of Hick’s law and cognitive load, let’s take a look at some examples that demonstrate this principle. There are examples of Hick’s law in action everywhere, but we’ll start with a common one: remote controls. As the number of features available in TVs increased over the decades, so did the options available on their corresponding remotes. Eventually, we ended up with remotes so complex that using them required either muscle memory from repeated use or a significant amount of mental processing. This led to the phnomenon known as “grandparent-friendly remotes.” By taping off everything except for the essential buttons, grandkids were able to improve the usability of remotes for their loved ones, and they also did us all the favor of sharing them online ( Figure 3-3 ). In contrast, today we have smart TV remotes: the streamlined cousins of the previous examples, simplifying the controls to only those that are absolutely neessary ( Figure 3-4 ). The result is a remote that doesn’t require a substantial amount of working memory and therefore incurs much less cognitive load. The complexity is transferred to the TV interface itself, where information can be effectively organized and progressively disclosed within menus. Figure 3-3. Modified TV remotes that simplify the “interface” (sources: Sam Weller via Twitter, 2015 [left]; Luke Hannon via Twitter, 2016 [right]) Figure 3-4. A smart TV remote, which simplifies the controls to only those absolutely necessary (source: Digital Trends, 2018) Now that we’ve seen some examples of Hick’s law at work in the physical world, let’s shift our focus to the digital. As we’ve seen already, the number of choices available can have a direct impact on the time it takes to make a decision",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As we’ve seen already, the number of choices available can have a direct impact on the time it takes to make a decision. We can ensure better user experiences by providing the right choices at the right time rather than presenting all the possible choices all the time. An excellent example of this can be found with Google Search, which provides the varying means of filtering results by type (all, images, videos, news, etc.) only after you’ve begun your search ( Figure 3-5 ). This helps to keep people focused on the more meaningful task at hand, rather than their being overwhelmed with decisions at the outset. Figure 3-5. Google simplifies the initial task of searching (left) and provides the ability to filter results only after the search has begun (right) (source: Google, 2020) Let’s take a look at another example of Hick’s law. Onboarding is a crucial but risky process for new users, and few nail it as well as Slack ( Figure 3-6 ). Instead of dropping users into a fully featured app after subjecting them to a few onboarding slides, a bot (Slackbot) is used to engage users and prompt them to learn about the messaging features in a risk-free way. To prevent new users from feeling overwhelmed, Slack hides all features except for the messaging input. Once users have learned how to message with Slackbot, they are progressively introduced to additional features. Figure 3-6. Screenshot from Slack’s progressive onboarding experience (source: Slack, 2019) This is an effective way to onboard users because it mimics the way we actually learn: we build upon each previous step and add to what we already know. By revealing features at just the right time, we can enable our users to adapt to complex workflows and feature sets without feeling overwhelmed. TECHNIQUE Card Sorting As we’ve seen in the previous examples, the number of choices can have a critical impact on the time it takes for people to make a decision. This is especially important when it comes to enabling users to find the informtion they need",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is especially important when it comes to enabling users to find the informtion they need. Too many items can lead to more cognitive load for users, especially if the choices aren’t clear. Conversely, with too few choices it becomes more difficult for them to identify which item is the most likely to lead them to the information they’re looking for. One particularly usful method for identifying users’ expectations when it comes to informtion architecture is card sorting . This handy research method is great for figuring out how items should be organized according to people’s mental models: simply have the participants organize topics within groups that make the most sense to them ( Figure 3-7 ). 1 In a closed exercise, the groups are predefined by the researcher. 2 Jakob Nielsen, “Card Sorting: Pushing Users Beyond Terminology Matches,” Nielsen Norman Group, August 23, 2009, https://www.nngroup.com/articles/card-sorting-terminology-matches . Figure 3-7. Card sorting The steps required during this exercise are relatively straightforward. While there are a variety of approaches to card sorting (closed versus open, moderated and unmoderated), they all follow the same general process. The following are the steps that make up a moderated open card sorting exercise, 1 which is the most common type: 1. Identify topics. The first step is to identify the topics that the paticipants will be asked to organize. These topics should represent the main content within your information architecture, with each item written on an individual card (the exercise can also be coducted digitally). It’s recommended that you avoid labeling topics with the same words, which can bias participants and lead them to group these items together. 2 2. Organize topics. The next step is to have the participants organize the topics, one at a time, into groupings that make sense to them. It’s common to have participants think out loud during this phase, which can provide valuable insight into their thought processes. 3. Name categories",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It’s common to have participants think out loud during this phase, which can provide valuable insight into their thought processes. 3. Name categories. Once the topics have been organized into groups, ask the participants to name each group they created based on the term they think best describes it. This step is partiularly valuable because it reveals what each participant’s mental model is and will be helpful when determining what to eventually label categories within your information architecture. 4. Debrief participants (optional). The optional but recommended final step during an open card sorting exercise is to ask the particpants to explain their rationale for each of the groupings they crated. This enables you to uncover why each participant made the decisions they did, identify any difficulties they experienced, and gather their thoughts on any topics that might have remained unsorted. KEY CONSIDERATION Oversimplification As we’ve seen, simplifying an interface or process helps to reduce the cognitive load for users and increases the likelihood that they’ll complete their task and achieve their goal. But it’s also important to consider when simplification can negatively affect the user experience—more specifcally, when we simplify to the point of abstraction, and it’s no longer clear what actions are available, what the next steps are, or where to find spcific information. A common example of this is the use of iconography as a way to communicate critical information about possible actions ( Figure 3-8 ). Using icons has a lot of advantages: they provide visual interest, they save space, they present excellent targets for taps or clicks, and they can provide quick recognition if they hold universal meaning. The challenge is that truly universal icons are rare, and icons often mean different things to different people. While relying on icons to convey information can help to simplify an interface, it can also make it harder to perform tasks or find information",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While relying on icons to convey information can help to simplify an interface, it can also make it harder to perform tasks or find information. This is especially true if the icons aren’t immediately recognizable to users, who more often than not will have a wide spetrum of knowledge and experience. Figure 3-8. Screenshot of the app bar from Facebook’s iOS app (source: Facebook, 2019) Another complicating factor is that similar icons may be used to reresent different actions or information, sometimes in complete oppostion, from one product or service to another. There is no icon standardization body that regulates what icons can be used where in websites or apps, which means how they are used is left to the discretion of the designers and their teams. We know that an icon can represent diferent things to different people, but what about when the same icon reresents different actions? Since there is no standardization, the functionality attached to an icon can vary from one digital experience to another. Take, for example, the “heart” and “star” icons: they typically indicate the ability to favorite, like, bookmark, or rate an item, but they may sometimes simply indicate a featured item. Not only does the meaning and functionality of these two icons vary across different proucts and services, but they also often compete with each other. This obviously results in confusion and an increase in the cognitive load on users, because the icons’ meaning is hard to interpret precisely. Adding contextual clues helps users to identify the options that are open to them and the relevance of the information available to the tasks they wish to perform. In the case of iconography, studies have shown that simply adding text labels to accompany icons will provide clarity and aid users with both discovery and recognition. This practice is even more critical when using icons for important elements such as navigation ( Figure 3-9 )",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This practice is even more critical when using icons for important elements such as navigation ( Figure 3-9 ). The addition of text labels effectively reduces the abstration of the icons alone by including additional information to help convey meaning and increase usability. Figure 3-9. Text labels accompany icons in the navigation on the Twitter web app (source: Twitter, 2019) Conclusion Hick’s law is a key concept in user experience design because it’s an underlying factor in everything we do. When an interface is too busy, actions are unclear or difficult to identify, and critical information is hard to find, a higher cognitive load is placed on users. Simplifying an interface or process helps to reduce the mental strain, but we must be sure to add contextual clues to help users identify the options available and determine the relevance of the information available to the tasks they wish to perform. It’s important to remember that each user has a goal, whether it’s to buy a product, understand something, or simply learn more about the content. I find the process of reduction, or eliminating any element that isn’t helping the user achieve their goal, a critical part of the design process. The less they have to think about what they need to do to reach their goal, the more likely it is they will achieve it. We touched on the role of memory in user experience design with cognitive load. Next up, we’ll further explore memory and its importance with Miller’s law.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-23-34.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 31 Designing with the Mind in Mind . https://doi.org/10.1016/B978-0-12-818202-4.00003-9 Copyright © 2021 Elsevier Inc. All rights reserved. Chapter 2 showed how our visual system is optimized to perceive structure accoring to the Gestalt principles of visual perception. Perceiving structure in our enviroment helps us make sense of objects and events quickly. Chapter 2 also explained that when people are navigating through software or websites, they don’t scrutinize screens carefully and read every word. They scan quickly for information relevant to their goals. This chapter (1) shows that when information is presented in a terse, structured way, it is easier for people to scan and understand and (2) explains how best to structure information. STRUCTURED INFORMATION IS EASIER TO SCAN Consider two presentations of the same information about an airline flight reservtion. The first presentation is unstructured prose text; the second is structured text in outline form (see Fig. 3.1 ). The structured presentation of the reservation can be scanned and understood much more quickly than the prose presentation. We Seek and Use Visual Structure 3 Unstructured: You are booked on United flight 237, which departs from Auckland at 14:30 on Tuesday 15 Oct and arrives at San Francisco at 11:40 on Tuesday 15 Oct. Structured: Flight: United 237, Auckland San Francisco Depart: 14:30 Tue 15 Oct Arrive: 11:40 Tue 15 Oct FIGURE 3.1 Structured presentation of airline reservation information is easier to scan. Next, consider two ways to present a list. As above, the first presentation is unstrutured prose text; the second is structured using bullets (see Fig. 3.2 ). The bulleted list is faster to scan and understand than the prose paragraph. That would also be true if the list were numbered. The more structured and brief the presentation of information, the more quickly and easily people can scan and comprehend it",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That would also be true if the list were numbered. The more structured and brief the presentation of information, the more quickly and easily people can scan and comprehend it. Look at the Driver License Renewals, Duplicates, and Changes page from the California Department of Motor Vehicles (see Fig. 3.3 ). Even with a bulleted list, the wordy, repetitive links slow users down and “bury” the important words they need to see. FIGURE 3.2 Structured (bulleted) list is easier to scan. FIGURE 3.3 Driver License Renewals, Duplicates, and Changes page of the California Department of Motor Vehicles (DMV) website (January 2020) buried important information in a wordy prose paragraph and repetitive list. Compare that with a less wordy, more structured hypothetical design that factors out needless repetition and marks as links only those words that represent options (see Fig. 3.4 ). The revision provides the same information and options as the website’s actual page but is easier to scan. Displaying search results is another situation in which structuring data and avoiing repetitive “noise” can improve people’s ability to scan quickly and find what they seek. In 2006, search results at HP.com included so much repeated navigation data and metadata for each retrieved item that they were useless. Recent HP websites eliminate the repetition and structure the results, making them easier to scan and more useful (see Fig. 3.5 ). Of course, for information displays to be easy to scan, it is not enough merely to make them terse, structured, and nonrepetitious. They must also conform to the rules of graphic design, some of which were presented in Chapter 2. For example, a prerelease version of a mortgage calculator on a real estate website presented its results in a table that violated at least two important rules of graphic design (see Fig. 3.6A ). First, people usually read (online or offline) from top to bottom, but the labels for calculated amounts were below their corresponing values",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.6A ). First, people usually read (online or offline) from top to bottom, but the labels for calculated amounts were below their corresponing values. Second, the labels were just as close to the value below as to their own value, so proximity (see Chapter 2) could not be used to perceive that labels were grouped with their values. To understand this mortgage results table, users had to scrutinize it carefully and slowly figure out which labels went with which numbers. The revised design, in contrast, allows users to perceive the correspondence between labels and values without conscious thought (see Fig. 3.6B ). FIGURE 3.4 Possible (fictional) improvement of the “Driver License Renewals, Duplicates, and Changes” page of the California Department of Motor Vehicles (DMV) website, eliminating unneeded text and making it easier to scan. VISUAL HIERARCHY HELPS PEOPLE FIND RELEVANT INFORMATION One of the most common ways to structure information presentations is to provide a visual hierarchy—an arrangement that: l breaks the information into distinct sections and breaks large sections into subsections; l labels each section and subsection prominently and in such a way that content is clearly identified; (A) (B) FIGURE 3.5 In 2006, HP.com’s site search produced repetitious, “noisy” results (A), but later (2019) was improved (B). 360 0.00 Mortgage Summary Monthly Payment $ 1,840.59 Number of Payments Total of Payments $ 662,611.22 Interest Total $ 318,861.22 Tax Total $ 93,750.00 PMI Total $ Pay-off Date Sep 2037 (A) (B) FIGURE 3.6 (A) Mortgage summary presented by a software mortgage calculator; (B) an improved design. l presents the sections and subsections as a hierarchy, with higher-level sections presented more strongly than lower-level ones. A visual hierarchy allows people, when scanning information, to quickly separate what is relevant to their goals from what is irrelevant and to focus their attention on the relevant information",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A visual hierarchy allows people, when scanning information, to quickly separate what is relevant to their goals from what is irrelevant and to focus their attention on the relevant information. They find what they are looking for more quickly because they can easily skip everything else. Try it for yourself. Look at the two information displays in Fig. 3.7 and find the information about prominence. How much longer does it take you to find it in the nonhierarchical presentation? The examples in Fig. 3.7 show the value of visual hierarchy in a textual, read-only information display. Visual hierarchy is equally important in interactive control panels and forms—perhaps even more so. Compare dialogue boxes from two different sofware products (see Fig. 3.8 ). The “Generate SoundTrack” dialogue box of the music app Band-in-a-Box has poor visual hierarchy, making it hard for users to scan it and find settings quickly. Adobe Acrobat Pro’s “Print” dialogue box, by contrast, has better visual hierarchy, so users can quickly find the desired settings. BEYOND VISUAL HIERARCHY : INFORMATION HIERARCHY Visual hierarchy should actually be called “information hierarchy” because the basic design principle applies more broadly than just visual design. Organizing information hierarchically, using headings, subheadings, and other groupings to define sections Create a Clear Visual Hierarchy Organize and prioritize the contents of a page by using size, prominence, and content relationships. Let’s look at these relationships more closely: • Size. The more important a headline is, the larger its font size should be. Big bold headlines help to grab the user’s attention as they scan the Web page. • Content Relationships. Group similar content types by displaying the content in a similar visual style, or in a clearly defined area. • Prominence. The more important the headline or content, the higher up the page it should be placed",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Prominence. The more important the headline or content, the higher up the page it should be placed. The most important or popular content should always be positioned prominently near the top of the page, so users can view it without having to scroll too far. Create a Clear Visual Hierarchy Organize and prioritize the contents of a page by using size, prominence, and content relationships. Let’s look at these relationships more closely. The more important a headline is, the larger its font size should be. Big bold headlines help to grab the user’s attention as they scan the Web page. The more important the headline or content, the higher up the page it should be placed. The most important or popular content should always be positioned prominently near the top of the page, so users can view it without having to scroll too far. Group similar content types by displaying the content in a similar visual style, or in a clearly defined area. (A) (B) FIGURE 3.7 Find the advice about prominence in each of these displays. Prose text format (A) forces users to read or skim everything. Visual hierarchy (B) lets users skip irrelevant content and go straight to the information most relevant to their goals. (A) (B) FIGURE 3.8 Visual hierarchy in interactive control panels and forms lets users find settings quickly: (A) Banin-a-Box (2017—bad) and (B) Adobe Acrobat Pro (2019–good). and subsections, can help even sight-impaired and blind users find what they are looking for. Consider a blind person using a screen reader to search the screens shown in Fig. 3.7 for the information about “prominence” or trying to find a particular setting on the dialogue boxes shown in Fig. 3.8 . With Fig. 3.7A , blind users would have to command their screen reader to read methodically through most of the information before reaching the desired content, but with the information separated with headings into sections as in Fig. 3.7B , blind users can have their screen reader read just the section headings, skipping content in irrelevant sections",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.7B , blind users can have their screen reader read just the section headings, skipping content in irrelevant sections. That would get them much more quickly to the information about “prominence.” Similarly, with Band-in-a-Box’s “Generate SoundTrack” dialogue box (see Fig. 3.8A ), blind users would have to laboriously tab through most of the settings to find the one they want, but with Acrobat Pro’s “Print” dialogue box (see Fig. 3.8B ), assuming the area headings are marked as headings, they could quickly find the relevant section, then look in it for the control they want. To help Web developers and designers create information displays that are useable by people with visual impairments, the World Wide Web Consortium (W3C) created the Web Content Accessibility Guidelines (WCAG 2.0) (W3C, 2008a). The WCAG 2.0 guidelines advise designers to using cascading style sheets to mark text sections and subsections, and screen divisions and subdivisions, with appropriate-level headings (W3C, 2008b). “CHUNKING” HELPS PEOPLE SCAN AND ENTER DATA Even small amounts of data can be made easier to scan if they are structured. Two examples are telephone and credit card numbers (see Fig. 3.9 ). Such numbers should be broken into parts—often called “chunks”—to make them easier to scan and remember. The right size for chunks was determined by psychologists and humafactors researchers decades ago when telephone numbers and credit card numbers were introduced (e.g., Miller, 1956). It is based on the capacity of human short-term memory (Moran, 2016) (see also Chapter 7). FIGURE 3.9 Telephone and credit card numbers are easier to scan, understand, and remember when separated into “chunks.” Unfortunately, many of today’s digital presentations of phone and credit card nubers neither chunk the numbers nor allow users to include spaces, hyphens, paretheses, or other punctuation (see Fig. 3.10A )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.10A ). This makes it harder for people to scan a number or verify that they typed it correctly and so is considered a user-interface design blooper (Johnson, 2007). To be more user-friendly, a user interface should support chunking of numbers by letting users separate them into parts using spaces or other punctuation (see Fig. 3.10B ). Alternatively, designers can support chunking of long numbers by providing a separate field for each part of the number. Separate data fields can provide useful visual structure even when the data to be entered is not, strictly speaking, a number. Dates and phone numbers are an example of situations in which segmented fields can improve readability and help prevent data entry errors (see Fig. 3.11 ). (A) (B) FIGURE 3.10 (A) At KP.org (2020), credit card numbers cannot include spaces or other punctuation, making them hard to scan and verify. (B) At Amazon.com (2020), spaces are added automatically as users enter the number. FIGURE 3.11 BankOfAmerica.com (2020) chunks phone numbers into three parts, making it impossible to enter an incorrectly formatted number. “Chunking” data helps people scan, comprehend, and recall not only long nubers but also other types of data. It makes data conform better to the limitations of human memory and attention, which are discussed more fully in Chapters 7 and 8. EVEN MORE STRUCTURE FOR INPUT: DATA-SPECIFIC CONTROLS A step-up in structure from segmented text fields is data-specific controls. Designers can combine text fields and other controls to create hybrid controls for getting values of specific types from users. With data-specific controls, users cannot enter invalid data, so one possible type of user error is eliminated. For example, most airlines these days use a text field and a calendar control to let customers choose flight dates (see Fig. 3.12 ). Structured data-specific controls can also be constructed from menus. For example, Southwest.com used Month, Day, and Year menus to build a date-of-birth control (see Fig. 3.13A )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.12 ). Structured data-specific controls can also be constructed from menus. For example, Southwest.com used Month, Day, and Year menus to build a date-of-birth control (see Fig. 3.13A ). Menus can also be combined with text fields, as in Google’s date-of-birth control (see Fig. 3.13B ). (B) (A) FIGURE 3.12 For getting flight dates, most airlines use a hybrid control: text field + calendar. (A) AirNewZealand.com (B) mobile.Southwest.com . IMPORTANT TAKEAWAYS l Presenting information in a terse, structured manner makes it easier for people to scan, understand, and remember. Eliminate needless words and repetition. Use bullets and numbered lists instead of paragraphs of text. l Visual hierarchy—breaking content up into clearly labeled sections and subsetions—is a “best practice” way to structure information. l Hierarchical structuring of information, using section headings and labels that screen readers can recognize as headings, helps people with visual impairments find their desired content. l Chunking data—such as phone numbers, credit card numbers, serial numbers— helps people scan, enter, and remember it. (A) (B) FIGURE 3.13 Birthdate control: (A) built with menus on Southwest.com (2020); (B) built with menus + text fields on Google.com (2020). Used by permission, www.OKCancel.com.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-31-42.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Street signs and Breadcrumbs DESIGNING NAVIGATION I t’s a fact: People won’t use your Web site if they can’t find their way around it. You know this from your own experience as a Web user. If you go to a site and can’t find what you’re looking for or figure out how the site is organized, you’re not likely to stay long—or come back. So how do you create the proverbial “clear, simple, and consistent” navigation? Scene from a mall Picture this: It’s Saturday afternoon and you’re headed for the mall to buy a chainsaw. As you walk through the door at Sears, you’re thinking, “Hmmm. Where do they keep chainsaws?” As soon as you’re inside, you start looking at the department names, high up on the walls. (They’re big enough that you can read them from all the way across the store.) TOOLS HOUSEWARES LAWN AND GARDEN “Hmmm,” you think, “Tools? Or Lawn and Garden?” It could be either one, but you’ve got to start somewhere so you head in the direction of Tools. When you reach the Tools department, you start looking at the signs at the end of each aisle. —TALKING HEADS, “ONCE IN A LIFETIME” POWER TOOLS HAND TOOLS SANDING AND GRINDING When you think you’ve got the right aisle, you start looking at the individual products. If it turns out you’ve guessed wrong, you try another aisle, or you may back up and start over again in the Lawn and Garden department. By the time you’re done, the process looks something like this: Basically, you use the store’s navigation systems (the signs and the organizing hierarchy that the signs embody) and your ability to scan shelves full of products to find what you’re looking for. Of course, the actual process is a little more complex",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Of course, the actual process is a little more complex. For one thing, as you walk in the door you usually devote a few microseconds to a crucial decision: Are you going to start by looking for chainsaws on your own or are you going to ask someone where they are? It’s a decision based on a number of variables—how familiar you are with the store, how much you trust their ability to organize things sensibly, how much of a hurry you’re in, and even how sociable you are. Still think you’re in the right department? NO NOT YET YES NO YES Enter Store Look for the cash register Pay up Look for exit sign Look for the right department Look for the right aisle Look for the product Find it? YES thoroughly Frustrated $ When we factor this decision in, the process looks something like this: Note that even if you start looking on your own, if things don’t pan out there’s a good chance that eventually you’ll end up asking someone for directions anyway. Still think you’re in the right department? NO NOT YET YES NO YES Ask someone first? Enter Store Look for the cash register Pay up Look for exit sign Look for the right department Find a clerk Ask Credible answer? Look for the aisle Look for the product Look for the right aisle Look for the product Find it? YES YES thoroughly Frustrated? YES NO NO Find a smarter looking clerk had enough? NOT YET $ NO YES Find it? Web Navigation 101 In many ways, you go through the same process when you enter a Web site. ¢ You’re usually trying to find something. In the “real” world it might be the emergency room or a family-size bottle of ketchup. On the Web, it might be a pair of headphones or the name of the actor in Casablanca who played the headwaiter at Rick’s. 1 ¢ You decide whether to ask first or browse first. The difference is that on a Web site there’s no one standing around who can tell you where things are. The Web equivalent of asking directions is searching—typing a description of what you’re looking for in a search box and getting back a list of links to places where it might be",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The Web equivalent of asking directions is searching—typing a description of what you’re looking for in a search box and getting back a list of links to places where it might be. Some people (Jakob Nielsen calls them “search-dominant” users) will almost always look for a search box as soon as they enter a site. (These may be the same people who look for the nearest clerk as soon as they enter a store.) 1 S. Z. “Cuddles” Sakall, born Eugene Sakall in Budapest in 1884. Ironically, most of the character actors who played the Nazi-hating denizens of Rick’s Café were actually famous European stage and screen actors who landed in Hollywood after fleeing the Nazis. Other people (Nielsen’s “link-dominant” users) will almost always browse first, searching only when they’ve run out of likely links to click or when they have gotten sufficiently frustrated by the site. For everyone else, the decision whether to start by browsing or searching depends on their current frame of mind, how much of a hurry they’re in, and whether the site appears to have decent browsable navigation. ¢ If you choose to browse, you make your way through a hierarchy, using signs to guide you. Typically, you’ll look around on the Home page for a list of the site’s main sections (like the store’s department signs) and click on the one that seems right. Then you’ll choose from the list of subsections. With any luck, after another click or two you’ll end up with a list of the kind of thing you’re looking for. Then you can click on the individual links to examine them in detail, the same way you’d take products off the shelf and read the labels. ¢ Eventually, if you can’t find what you’re looking for, you’ll leave. This is as true on a Web site as it is at Sears. You’ll leave when you’re convinced they haven’t got it or when you’re just too frustrated to keep looking",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is as true on a Web site as it is at Sears. You’ll leave when you’re convinced they haven’t got it or when you’re just too frustrated to keep looking. Here’s what the process looks like: Enter Site Feel like browsing? Find a search box Click on a section Scan results for likely matches Type your query Click on a subsection Find it? Think you’re in the right section? Check them out Credible results? Look for whatever it is Devise a better query Find it? YES had enough? leave unhappy leave happy thoroughly frustrated? NO NO YES NO YES YES NOT YET ALMOST YES YES NO NO YES NOT YET The unbearable lightness of browsing Looking for things on a Web site and looking for them in the “real” world have a lot of similarities. When we’re exploring the Web, in some ways it even feels like we’re moving around in a physical space. Think of the words we use to describe the experience—like “cruising,” “browsing,” and “surfing.” And clicking a link doesn’t “load” or “display” another page—it “takes you to” a page. But the Web experience is missing many of the cues we’ve relied on all our lives to negotiate spaces. Consider these oddities of Web space: ¢ No sense of scale. Even after we’ve used a Web site extensively, unless it’s a very small site we tend to have very little sense of how big it is (50 pages? 1,000? 17,000?). 2 For all we know, there could be huge corners we’ve never explored. Compare this to a magazine, a museum, or a department store, where you always have at least a rough sense of the seen/unseen ratio. The practical result is that it’s very hard to know whether you’ve seen everything of interest to you in a site, which means it’s hard to know when to stop looking. 3 ¢ No sense of direction. In a Web site, there’s no left and right, no up and down. We may talk about moving up and down, but we mean up and down in the hierarchy—to a more general or more specific level. ¢ No sense of location. In physical spaces, as we move around we accumulate knowledge about the space",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". ¢ No sense of location. In physical spaces, as we move around we accumulate knowledge about the space. We develop a sense of where things are and can take shortcuts to get to them. We may get to the chainsaws the first time by following the signs, but the next time we’re just as likely to think, “Chainsaws? Oh, yeah, I remember where they were: right rear corner, near the refrigerators.” 2 Even the people who manage Web sites often have very little idea how big their sites really are. 3 This is one reason why it’s useful for links that we’ve already clicked on to display in a different color. It gives us some small sense of how much ground we’ve covered. 3 And then head straight to them. But on the Web, your feet never touch the ground; instead, you make your way around by clicking on links. Click on “Power Tools” and you’re suddenly teleported to the Power Tools aisle with no traversal of space, no glancing at things along the way. When we want to return to something on a Web site, instead of relying on a physical sense of where it is we have to remember where it is in the conceptual hierarchy and retrace our steps. This is one reason why bookmarks—stored personal shortcuts—are so important, and why the Back button is the most used button in Web browsers. It also explains why the concept of Home pages is so important. Home pages are—comparatively—fixed places. When you’re in a site, the Home page is like the North Star. Being able to click Home gives you a fresh start. This lack of physicality is both good and bad. On the plus side, the sense of weightlessness can be exhilarating and partly explains why it’s so easy to lose track of time on the Web—the same as when we’re “lost” in a good book. On the negative side, I think it explains why we use the term “Web navigation” even though we never talk about “department store navigation” or “library navigation.” If you look up navigation in a dictionary, it’s about doing two things: getting from one place to another, and figuring out where you are",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". FIRST TIME Subsequent visits I think we talk about Web navigation because “figuring out where you are” is a much more pervasive problem on the Web than in physical spaces. We’re inherently lost when we’re on the Web, and we can’t peek over the aisles to see where we are. Web navigation compensates for this missing sense of place by embodying the site’s hierarchy, creating a sense of “there.” Navigation isn’t just a feature of a Web site; it is the Web site, in the same way that the building, the shelves, and the cash registers are Sears. Without it, there’s no there there. The moral? Web navigation had better be good. The overlooked purposes of navigation Two of the purposes of navigation are fairly obvious: to help us find whatever it is we’re looking for and to tell us where we are. But navigation has some other equally important—and easily overlooked— functions: ¢ It tells us what’s here. By making the hierarchy visible, navigation tells us what the site contains. Navigation reveals content! And revealing the site may be even more important than guiding or situating us. ¢ It tells us how to use the site. If the navigation is doing its job, it tells you implicitly where to begin and what your options are. Done correctly, it should be all the instructions you need. (Which is good, since most users will ignore any other instructions anyway.) ¢ It gives us confidence in the people who built it. Every moment we’re in a Web site, we’re keeping a mental running tally: “Do these guys know what they’re doing?” It’s one of the main factors we use in deciding whether to bail out and deciding whether to ever come back. Clear, well-thought-out navigation is one of the best opportunities a site has to create a good impression. Web navigation conventions Physical spaces like cities and buildings (and even information spaces like books and magazines) have their own navigation systems, with conventions that have evolved over time like street signs, page numbers, and chapter titles",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The conventions specify (loosely) the appearance and location of the navigation elements so we know what to look for and where to look when we need them. Putting them in a standard place lets us locate them quickly, with a minimum of effort; standardizing their appearance makes it easy to distinguish them from everything else. For instance, we expect to find street signs at street corners, we expect to find them by looking up (not down), and we expect them to look like street signs (horizontal, not vertical). We also take it for granted that the name of a building will be above or next to its front door. In a grocery store, we expect to find signs near the ends of each aisle. In a magazine, we know there will be a table of contents somewhere in the first few pages and page numbers somewhere in the margin of each page—and that they’ll look like a table of contents and page numbers. Think of how frustrating it is when one of these conventions is broken (when magazines don’t put page numbers on advertising pages, for instance). Although their appearance can vary significantly, these are the basic navigation conventions for the Web: Footer navigation Page name “You are here” indicator Site ID Utilities Se ctions Local navigation (Things at the current level) Don’t look now, but I think it’s following us Web designers use the term persistent navigation (or global navigation ) to describe the set of navigation elements that appear on every page of a site. Done right, persistent navigation should say—preferably in a calm, comforting voice: “The navigation is over here. Some parts will change a little depending on where you are, but it will always be here, and it will always work the same way.” Just having the navigation appear in the same place on every page with a consistent look gives you instant confirmation that you’re still in the same site—which is more important than you might think. And keeping it the same throughout the site means that (hopefully) you only have to figure out how it works once",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". And keeping it the same throughout the site means that (hopefully) you only have to figure out how it works once. Persistent navigation should include the four elements you most need to have on hand at all times: X YZ Corp. Home Products News Support About XYZ Sign in Contact Search Utilities Sections Site ID We’ll look at each of them in a minute. But firstDid I say every page? I lied. There is one exception to the “follow me everywhere” rule: forms. On pages where a form needs to be filled in, the persistent navigation can sometimes be an unnecessary distraction. For instance, when I’m paying for my purchases on an e-commerce site, you don’t really want me to do anything but finish filling in the forms. The same is true when I’m registering, subscribing, giving feedback, or checking off personalization preferences. For these pages, it’s useful to have a minimal version of the persistent navigation with just the Site ID, a link to Home, and any Utilities that might help me fill out the form. Now I know we’re not in Kansas The Site ID or logo is like the building name for a Web site. At Sears, I really only need to see the name on my way in; once I’m inside, I know I’m still in Sears until I leave. But on the Web—where my primary mode of travel is teleportation—I need to see it on every page. OK. Now I’m in South Park OK. I’m still in South Park And now I’m on Facebook In the same way that we expect to see the name of a building over the front entrance, we expect to see the Site ID at the top of the page—usually in (or at least near) the upper left corner. 4 Why? Because the Site ID represents the whole site, which means it’s the highest thing in the logical hierarchy of the site. This site Sections of this site Subsections Sub-subsections, etc. This page Areas of this page Items on this page And there are two ways to get this primacy across in the visual hierarchy of the page: either make it the most prominent thing on the page, or make it frame everything else",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Since you don’t want the ID to be the most prominent element on the page (except, perhaps, on the Home page), the best place for it—the place that is least likely to make me think—is at the top, where it frames the entire page. And in addition to being where we would expect it to be, the Site ID also needs to look like a Site ID. This means it should have the attributes we would expect to see in a brand logo or the sign outside a store: a distinctive typeface and a graphic that’s recognizable at any size from a button to a billboard. 4 on Web pages written for left-to-right reading languages. Everything else Site ID The Sections The Sections—sometimes called the primary navigation —are the links to the main sections of the site: the top level of the site’s hierarchy. In some designs the persistent navigation will also include space to display the secondary navigation: the list of subsections in the current section. In others, pointing at a section name or clicking on it reveals a dropdown menu. And in others, clicking takes you to the front page of the section, where you’ll find the secondary navigation. The Utilities Utilities are the links to important elements of the site that aren’t really part of the content hierarchy. These are things that either can help me use the site (like Sign in/Register, Help, a Site Map, or a Shopping Cart) or provide information about its publisher (like About Us and Contact Us). XYZ Corp. Home Products News Support About XYZ Sign in Contact Sections Subsections Utilities XYZ Corp. Bivalves Lug Nuts Protein Shakes Home Products News Support About XYZ Sign in Contact XYZ Corp. Home Products News Support About XYZ Sign in Contact Like the signs for the facilities in a store, the Utilities list should be slightly less prominent than the Sections. Utilities will vary for different types of sites",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Utilities will vary for different types of sites. For a corporate or e-commerce site, for example, they might include any of the following: As a rule, the persistent navigation can accommodate only four or five Utilities— the ones users are likely to need most often. If you try to squeeze in more than that, they tend to get lost in the crowd. The less frequently used leftovers belong in the footer: the small text links at the bottom of each page. Just click your heels three times and say, “There’s no place like home” One of the most crucial items in the persistent navigation is a button or link that takes me to the site’s Home page. Having a Home button in sight at all times offers reassurance that no matter how lost I may get, I can always start over, like pressing a Reset button or using a “Get out of Jail Free” card. Almost all Web users expect the Site ID to be a button that can take you to the Home page. I think it’s also a good idea to include Home with the main sections of the site. About Us Archives Checkout Company Info Contact Us Customer Service Discussion Boards Downloads Directory Forums FAQs Help Home Investor Relations How to Shop Jobs My _______ News Order Tracking Press Releases Privacy Policy Register Search Shopping Cart Sign in Site Map Store Locator Your Account A way to search Given the power of searching and the number of people who prefer searching to browsing, unless a site is very small and very well organized, every page should have either a search box or a link to a search page. And unless there’s very little reason to search your site, it should be a search box. Keep in mind that for a large percentage of users their first official act when they reach a new site will be to scan the page for something that matches one of these three patterns: It’s a simple formula: a box, a button, and either the word “Search” or the universally recognized magnifying glass icon. Don’t make it hard for them—stick to the formula. In particular, avoid ¢ Fancy wording",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Don’t make it hard for them—stick to the formula. In particular, avoid ¢ Fancy wording. They’ll be looking for the word “Search,” so use the word Search, not Find, Quick Find, Quick Search, or Keyword Search. (If you use “Search” as the label for the box, use the word “Go” as the button name.) ¢ Instructions. If you stick to the formula, anyone who has used the Web for more than a few days will know what to do. Adding “Type a keyword” is like saying, “Leave a message at the beep” on your voice mail message: There was a time when it was necessary, but now it just makes you sound clueless. ¢ Options. If there is any possibility of confusion about the scope of the search (what’s being searched: the site, part of the site, or the whole Web), by all means spell it out. But think very carefully before giving me options to limit the scope (to search just the current section of the site, for instance). And also be wary of providing options for how I specify what I’m searching for (search by title or by author, for instance, or search by part number or by product name). I seldom see a case where the potential payoff for adding options to the persistent search box is worth the cost of making me figure out what the options are and whether I need to use them (i.e., making me think). If you want to give me the option to scope the search, give it to me when it’s useful—when I get to the search results page and discover that searching everything turned up far too many hits, so I need to limit the scope. Secondary, tertiary, and whatever comes after tertiary It’s happened so often I’ve come to expect it: When designers I haven’t worked with before send me preliminary page designs so I can check for usability issues, I almost inevitably get a flowchart that shows a site four levels deepand sample pages for the Home page and the top two levels",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". XYZ loves you! Products News Support About XYZ Products > News > Products Hardware Software > Support > About XYZ XYZ Software > News > Products Hardware Software > Support > About XYZ Home Second-level page Subsection page I keep flipping the pages looking for more, or at least for the place where they’ve scrawled “Some magic happens here,” but I never find even that. I think this is one of the most common problems in Web design (especially in larger sites): failing to give the lower-level navigation the same attention as the top. In so many sites, as soon as you get past the second level, the navigation breaks down and becomes ad hoc . The problem is so common that it’s actually hard to find good examples of third-level navigation. Why does this happen? Partly, I think, because good multi-level navigation is just plain hard to design— given the limited amount of space on the page and the number of elements that have to be squeezed in. Partly because designers usually don’t even have enough time to figure out the first two levels. Partly because it just doesn’t seem that important. (After all, how important can it be? It’s not primary. It’s not even secondary.) And there’s a tendency to think that by the time people get that far into the site, they’ll understand how it works. And then there’s the problem of getting sample content and hierarchy examples for lower-level pages. Even if designers ask, they probably won’t get them, because the people responsible for the content usually haven’t thought things through that far, either. But the reality is that users usually end up spending as much time on lowelevel pages as they do at the top. And unless you’ve worked out top-to-bottom navigation from the beginning, it’s very hard to graft it on later and come up with something consistent. The moral? It’s vital to have sample pages that show the navigation for all the potential levels of the site before you start arguing about the color scheme. Page names, or Why I love to drive in L.A",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Page names, or Why I love to drive in L.A. If you’ve ever spent time in Los Angeles, you understand that it’s not just a song lyric—L.A. really is a great big freeway. And because people in L.A. take driving seriously, they have the best street signs I’ve ever seen. In L.A., ¢ Street signs are big. When you’re stopped at an intersection, you can read the sign for the next cross street. ¢ They’re in the right place —hanging over the street you’re driving on, so all you have to do is glance up. Now, I’ll admit I’m a sucker for this kind of treatment because I come from Boston, where you consider yourself lucky if you can manage to read the street sign while there’s still time to make the turn. The result? When I’m driving in L.A., I devote less energy and attention to dealing with where I am and more to traffic, conversation, and listening to All Things Considered . I love driving in L.A. Page names are the street signs of the Web. Just as with street signs, when things are going well I may not notice page names at all. But as soon as I start to sense that I may not be headed in the right direction, I need to be able to spot the page name effortlessly so I can get my bearings. Covington Road Los Angeles Boston Russett Rd There are four things you need to know about page names: ¢ Every page needs a name. Just as every corner should have a street sign, every page should have a name. Designers sometimes think, “Well, we’ve highlighted the page name in the navigation. That’s good enough.” It’s a tempting idea because it can save space, and it’s one less element to work into the page layout, but it’s not enough. You need a page name, too. ¢ The name needs to be in the right place. In the visual hierarchy of the page, the page name should appear to be framing the content that is unique to this page. (After all, that’s what it’s naming—not the navigation or the ads, which are just the infrastructure.) ¢ The name needs to be prominent",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". (After all, that’s what it’s naming—not the navigation or the ads, which are just the infrastructure.) ¢ The name needs to be prominent. You want the combination of position, size, color, and typeface to make the name say “This is the heading for the entire page.” In most cases, it will be the largest text on the page. Page Name Unique page content Page Name Unique page content Page Name Unique page content ¢ The name needs to match what I clicked. Even though nobody ever mentions it, every site makes an implicit social contract with its visitors: The name of the page will match the words I clicked to get there. In other words, if I click on a link or button that says “Hot mashed potatoes,” the site will take me to a page named “Hot mashed potatoes.” It may seem trivial, but it’s actually a crucial agreement. Each time a site violates it, I’m forced to think, even if only for milliseconds, “Why are those two things different?” And if there’s a major discrepancy between the link name and the page name or a lot of minor discrepancies, my trust in the site—and the competence of the people who publish it—will be diminished. Of course, sometimes you have to compromise, usually because of space limitations. If the words I click on and the page name don’t match exactly, the important thing is that (a) they match as closely as possible, and (b) the reason for the difference is obvious. For instance, if I click buttons labeled “Gifts for Him” and “Gifts for Her” and get pages titled “Gifts for Men” and “Gifts for Women,” even though the wording isn’t identical they feel so equivalent that I’m not going to think about the difference. WHAT I CLICKWHAT I GETLug Nuts Names match. Comfort, trust, no thought required. Names don’t match. Frustration, loss of trust",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". WHAT I CLICKWHAT I GETLug Nuts Names match. Comfort, trust, no thought required. Names don’t match. Frustration, loss of trust. Nuts Spare parts (No mention of Lug Nuts on the page) Error 404 Page not found “You are here” One of the ways navigation can counteract the Web’s inherent “lost in space” feeling is by showing me where I am in the scheme of things, the same way that a “You are here” indicator does on the map in a shopping mall—or a National Park. On the Web, this is accomplished by highlighting my current location in whatever navigation bars, lists, or menus appear on the page. In this example, the current section (Bedroom) and subsection (Lighting) have both been “marked.” Looks like I’m in bedroom lighting ©2000. The New Yorker Collection from cartoonbank.com. All Rights Reserved. There are a number of ways to make the current location stand out: The most common failing of “You are here” indicators is that they’re too subtle. They need to stand out; if they don’t, they lose their value as visual cues and end up just adding more noise to the page. One way to ensure that they stand out is to apply more than one visual distinction—for instance, a different color and bold text. Too-subtle visual cues are actually a very common problem. Designers love subtle cues, because subtlety is one of the traits of sophisticated design. But Web users are generally in such a hurry that they routinely miss subtle cues. In general, if you’re a designer and you think a visual cue is sticking out like a sore thumb, it probably means you need to make it twice as prominent. Put a pointer next to it Change the text color Use bold text Reverse the button Change the button color Breadcrumbs Like “You are here” indicators, Breadcrumbs show you where you are. They’re called Breadcrumbs because they’re reminiscent of the trail of crumbs Hansel dropped in the woods so he and Gretel could find their way back home",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". They’re called Breadcrumbs because they’re reminiscent of the trail of crumbs Hansel dropped in the woods so he and Gretel could find their way back home. 5 Breadcrumbs show you the path from the Home page to where you are and make it easy to move back up to higher levels in the hierarchy of a site. For a long time, Breadcrumbs were an oddity, found only in sites that were really just enormous databases with very deep hierarchies. But these days they show up in more and more sites, sometimes in lieu of well-thought-out navigation. 5 In the original story, H & G’s stepmother persuades their father to lose them in the forest during lean times so the whole family won’t have to starve. The suspicious and resourceful H spoils the plot by dropping pebbles on the way in and following them home. But the next time (!) H is forced to use breadcrumbs instead, which prove to be a less-than-suitable substitute since birds eat them before H & G can retrace their steps. Eventually the tale devolves into attempted cannibalism, grand larceny, and immolation, but basically it’s a story about how unpleasant it is to be lost. Done right, Breadcrumbs are self-explanatory, they don’t take up much room, and they provide a convenient, consistent way to do two of the things you need to do most often: back up a level or go Home. They’re most useful in a large site with a deep hierarchy. Here are a few best practices for implementing them: ¢ Put them at the top. Breadcrumbs seem to work best if they’re at the top of the page. I think this is probably because it literally marginalizes them— making them seem like an accessory, like page numbers in a book or magazine. ¢ Use > between levels. Trial and error seems to have shown that the best separator between levels is the “greater than” character ( > ), probably because it visually suggests forward motion down through the levels. ¢ Boldface the last item. The last item in the list should be the name of the current page, and making it bold gives it the prominence it deserves",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". ¢ Boldface the last item. The last item in the list should be the name of the current page, and making it bold gives it the prominence it deserves. And because it’s the page that you’re on, naturally it’s not a link. Three reasons why I still love tabs I haven’t been able to prove it (yet), but I strongly suspect that Leonardo da Vinci invented tab dividers sometime in the late 15th century. As interface devices go, they’re clearly a product of genius. Tabs are one of the very few cases where using a physical metaphor in a user interface actually works. Like the tab dividers in a three-ring binder or tabs on folders in a file drawer, they divide whatever they’re sticking out of into sections. And they make it easy to open a section by reaching for its tab (or, in the case of the Web, clicking on it). I think they’re an excellent and underused navigation choice. Here’s why I like them: ¢ They’re self-evident. I’ve never seen anyone—no matter how “computer illiterate”—look at a tabbed interface and say, “Hmmm. I wonder what those do?” ¢ T hey’re hard to miss. When I do usability tests, I’m surprised at how often people can overlook horizontal navigation bars at the top of a Web page. But tabs are so visually distinctive that they’re hard to overlook. And because they’re hard to mistake for anything but navigation, they create the kind of obvious-at-a-glance division you want between navigation and content. ¢ They’re slick. Web designers are always struggling to make pages more visually interesting. If done correctly, tabs can add polish and serve a useful purpose. If you’re going to use tabs, though, you have to do them right. For tabs to work to full effect, the graphics have to create the visual illusion that the active tab is in front of the other tabs. This is the main thing that makes them feel like tabs—even more than the distinctive tab shape. To create this illusion, the active tab needs to be a different color or contrasting shade, and it has to physically connect with the space below it",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To create this illusion, the active tab needs to be a different color or contrasting shade, and it has to physically connect with the space below it. This is what makes the active tab “pop” to the front. BAD: No connection, no pop. BETTER: Connected, but no contrast. Limited pop. BEST: Duck! It’s coming right at you. Try the trunk test Now that you have a feeling for all of the moving parts, you’re ready to try my acid test for good Web navigation. Here’s how it goes: Imagine that you’ve been blindfolded and locked in the trunk of a car, then driven around for a while and dumped on a page somewhere deep in the bowels of a Web site. If the page is well designed, when your vision clears you should be able to answer these questions without hesitation: ¢ What site is this? (Site ID) ¢ What page am I on? (Page name) ¢ What are the major sections of this site? (Sections) ¢ What are my options at this level? (Local navigation) ¢ Where am I in the scheme of things? (“You are here” indicators) ¢ How can I search? Why the Goodfellas motif? Because it’s so easy to forget that the Web experience is often more like being abducted than following a garden path. When you’re designing pages, it’s tempting to think that people will reach them by starting at the Home page and following the nice, neat paths you’ve laid out. But the reality is that we’re often dropped down in the middle of a site with no idea where we are because we’ve followed a link from a search engine, a social networking site, or email from a friend, and we’ve never seen this site’s navigation scheme before. And the blindfold? You want your vision to be slightly blurry, because the true test isn’t whether you can figure it out given enough time and close scrutiny. The standard needs to be that these elements pop off the page so clearly that it doesn’t matter whether you’re looking closely or not. You want to be relying solely on the overall appearance of things, not the details",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "Dont Make Me Think, Revisited - A Common Sense Approach to Web Usability 3rd Edition",
    "author": "Steven Krug",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Dont Make Me Think, Revisited A Common Sense Approach to Web Usability 3rd Edition.pdf",
    "date_published": "2014",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You want to be relying solely on the overall appearance of things, not the details. Here’s how you perform the trunk test: Step 1: Choose a page anywhere in the site at random, and print it. Step 2: Hold it at arm’s length or squint so you can’t really study it closely. Step 3: As quickly as possible, try to find and circle each of these items: ¢ Site ID ¢ Page name ¢ Sections (Primary navigation) ¢ Local navigation ¢ “You are here” indicator(s) ¢ Search Try it on your own site and see how well it works. Then ask some friends to try it, too. You may be surprised by the results.",
    "chunk_id": "Human_computer_interaction_dont_make_me_think,_revisited_a_common_sense_approach_to_web_usability_3rd_edition_page-54-83.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "The Public Is The Priority",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\The Public Is The Priority.pdf",
    "date_published": "2023-02-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Click to edit Master title style The Public Is The Priority: Making Decisions Using The Software Engineering Code Of Ethics D o n a l d G o t t e r b a r n K e i t h W. M i l l e r Ye a r o f P u b l i c a t i o n : 2 0 0 9 Click to edit Master title style What is Code of Ethics? • Code of Ethics are often seen as a way to manage the behavior of members of profession. • Important challenges described by the author in this paper is how to use the code to balance multiple factors when deciding the best course of action. • The code can help the software developers to make ethical, complex and technical decisions which will be better for the public, profession and the developer as well. • Now, we will look into 3 reports which will demonstrate how a professional can use the code to make a decision whenever an ethical conflict arises. Click to edit Master title style Case 1: Design Risk • Imagine you work for a company that develops software for military and law enforcements agencies. • You are working on a project which involves TTWI (which is Through-The-Wall-Imaging) that uses impulse radar to see through hard surfaces like wood, plaster, concrete and brick walls. • Your company developed and marketed TTWI, which has become its most profitable product line. • Now, your new project is to develop a software which can tackle TTWI, called ANTI-TTWI. • If the second project is successful, your company will be able to sell TTWI to one company and ANTI-TTWI to its rival company. • Here, you will act as a middleman, who can develop both type of products and can sell them to rival companies. What will be your ethical obligations as a software professional in this situation? Click to edit Master title style Resolving the Issue • The ACM and IEEE Computer Society sponsored the development of a body of knowledge and ethical guidelines documenting the professional responsibilities and obligations of software engineers",
    "chunk_id": "Human_computer_interaction_the_public_is_the_priority.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "The Public Is The Priority",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\The Public Is The Priority.pdf",
    "date_published": "2023-02-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Representatives including industry, government, education and military compiled a set of guidelines to inspire software engineers. The code went extensive review process before approval. • The code summarizes the software engineering professional’s ethical aspirations and explains how these aspirations can affect the way software engineers act. It also informs the public about the responsibilities that are important to this profession and educates practitioners on the standards that society expects them to meet and what their peers strive for an expect of each other. Click to edit Master title style Principles of Code • The Code includes 8 principles that are arranged with the highest priority first: 1. Public 2. Client And Employer 3. Product 4. Judgement 5. Management 6. Profession 7. Colleagues 8. Self Click to edit Master title style Applying the Code • It is not always easy to apply a collection of abstract principles to make a solid decision. That’s why Code (which is mentioned in this paper) differs from many other code of ethics in two significant ways. 1. First, many codes are tied to the organization and only applicable to the organizations and members of that organization only. In contrast, the Software Engineering Code of Ethics and Professional Practice is a code of the profession and not a single organization. 2. Second, in addition to providing abstract principles the Code specifically addresses the problem of conflicting standards, which can help in making the ethical decision. Take an example of that ANTI-TTWI case, The code clearly states that as a computer professional you should be loyal to your employer. It also declares that as a software engineer, you should report any kind of problem that can put public in danger. 3. So, these two principles are contradictory in ANTI-TTWI case. Because TTWI can endanger the safety of public and ANTTTWI which can misdirects weapons. As well as you should be quiet and not to divulge any information about this because you are loyal to the employer",
    "chunk_id": "Human_computer_interaction_the_public_is_the_priority.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "The Public Is The Priority",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\The Public Is The Priority.pdf",
    "date_published": "2023-02-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As well as you should be quiet and not to divulge any information about this because you are loyal to the employer. Click to edit Master title style Code according to the paper • The preamble to the code states that: • Ethical tensions can best be addressed by thoughtful consideration of fundamental principles, rather than blind reliance on detailed regulations. These principles should influence software engineers to consider broadly who is affected by their work; to examine if they and their colleagues are treating other human beings with due respect; to consider how the public, if reasonably well informed, would view their decisions; to analyze how the least empowered will be affected by their decisions; and to consider whether their acts would be judged worthy of the ideal professional working as a software engineer. • In other words, this Code will not make a decision automatically and on its own. Professional judgement is still necessary. The software engineer is obligated to act in the public’s best interest, even if those actions oppose the interests of the company. Click to edit Master title style Applying Code to the ANTI-TTWI case • The main focus of the Code is on the welfare of the public. In all the judgements, concern for the health, safety of the public is primary principle. • The software engineer should act as much as possible in the interest of the company; however, software engineer must act in a way that enhances the public’s safety. • In this case, software engineer has several options like 1. Consulting with managers. 2. Seeking a second opinion from advisor or lawyers. 3. Conferring with executives. • But sometimes, if there is no other way then the Code clearly states that: The software engineer is obligated to act in public’s best interest, even if those actions oppose the interest of the company. • At this point, ANTI-TTWI Case will become a whistleblower case and the personal consequences for the engineer might be catastrophic in nature",
    "chunk_id": "Human_computer_interaction_the_public_is_the_priority.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "The Public Is The Priority",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\The Public Is The Priority.pdf",
    "date_published": "2023-02-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • At this point, ANTI-TTWI Case will become a whistleblower case and the personal consequences for the engineer might be catastrophic in nature. Click to edit Master title style Case 2: Who is in Control? • On 7 October 2008, a faulty computer sent a large passenger plane into a steep dive, which injured 51 passengers onboard. Another disaster that happened in 1994 was the crash of Airbus 310 which resulted in death of 75 passengers. • In the first case autopilot was on, but computer-generated faulty value. These incorrect value led to the flight control computer commanding a nose-down aircraft movement. After the autopilot was turned off, software gave the control to pilots. In the second case, pilot brought his daughter and son into the cockpit, letting them put their hands on the controls as the autopilot flew the plane. • In both the cases, media pinned the blame on human factors, which are easier to explain and might underreport the importance of technological problems. Click to edit Master title style Case 3: Disclaiming Responsibility • In August 2000, medical technicians at National Oncology Institute of Panama City modified the computerized cancer treatment software which will calculate the radiotherapy treatments. But by the late March 2001, 28 patients had been overexposed during the therapy which resulted in death of 17 patients. • First group of investigators, placed the responsibility on users who had “misused” the treatment planning software. But, additional group of investigators painted a different picture. Their report noted that the software manufacturer included a total disclaimer of responsibility for calculations accuracy. • In other words, all the data input was correct, but the technicians did not perform all internal test. They just assumed that the software would perform its function correctly. • The Code provides specific details about software practitioners’ obligations, and if they ignore those obligations, they are not acting in good faith as professionals",
    "chunk_id": "Human_computer_interaction_the_public_is_the_priority.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "The Public Is The Priority",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\The Public Is The Priority.pdf",
    "date_published": "2023-02-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • The Code provides specific details about software practitioners’ obligations, and if they ignore those obligations, they are not acting in good faith as professionals. Even though it was not intentional but however, their actions contributed to the injuries. Click to edit Master title style Conclusion • All 3 cases involved the potential for fatalities. Computers are very bad at doing what we want them to do, but they are very good at doing what we tell them to do. • The Software Engineering Code of Ethics and Professional Practice fulfills several functions. The code is meant to be inspirational; it encourages software engineers to undertake positive actions and resist pressure to act unethically. Click to edit Master title style Thank You Aditya Singh Rajput (G01326046)",
    "chunk_id": "Human_computer_interaction_the_public_is_the_priority.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": "The purpose of the Master’s degree programme in Cognitive Science is to develop student’s knowledge, skills, and competences in the analysis of human cognition and behaviour. The program provides students with theoretical knowledge of human cognitive function; skills and competencies in experimental methodology, including measurement of behaviour and neural function; and skills and competencies in computational and statistical modelling of human data. The program prepares students to use their learning in applied and research contexts. Graduates from the program will be qualified to Cognitive science graduates are expected to find employment in five mains sorts of professions Direct admissionCertain Bachelor's Degree Programmes from Aarhus University and various other Danish Universities have been determined to meet the admission requirements to this Master's Degree Programme in Cognitive Science: International and other qualificationsIf you have an international educational background, you must meet both the general admission requirement and the specific admission requirements: General admission requirementsYou must have a Bachelor's Degree Programme or equivalent that is comparable to a Danish Bachelor's Degree Programme in both level and duration (180 ECTS). For more information about how your Bachelor's Degree Programme is assessed, please see thenational database. Specific admission requirementsIf you meet the general admission requirement, the university continue on to assess whether or not you meet the specific admission requirements. You can be admitted to the Master's Degree Programme if the university assesses that your education has a level, scope, and content that corresponds to the academic requirements specified below. We cannot assess in advance whether your specific degree will meet the above requirements. Therefore, we recommend that you apply for the degree programme if you believe that you meet the academic requirements",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Therefore, we recommend that you apply for the degree programme if you believe that you meet the academic requirements. In this regard, it is a good idea to compare your degree programme with the degree programmes that provide direct admission. Language requirementsEnglish at B level at upper-secondary school or the equivalent is a requirement for admission:read about English language requirements. The following Bachelor's Degree Programme(s) from Aarhus University entitles you to an offer of admission to the Master's Degree Programme in Cognitive Science: Please note that you must apply for admission to the Master's Degree Programme within three years of completing your Bachelor's Degree Programme.Learn more aboutLegal right of admission These academic regulations take effect on 1. September (2022). As a result of this, the former academic regulations will be phased out. Master’s Degree Programme in Cognitive Science 2019 Master’s Degree Programme in Cognitive Science 2022 Grade is transferred Advanced cognitive neuroscience (10 ECTS) Advanced cognitive neuroscience (10 ECTS) Yes Natural language processing (10 ECTS) Natural language processing (10 ECTS) Yes Decision making (10 ECTS) Decision making (10 ECTS) Yes Advanced cognitive modeling (10 ECTS) Advanced cognitive modeling (10 ECTS) No Data science (10 ECTS) Data Science, Prediction, and Forecasting (10 ECTS) Yes Human computer interaction (10 ECTS) Human computer interaction (10 ECTS) Yes Profile courses (20 ECTS) Profile courses (20 ECTS) Nej Project placement (20 ECTS) Project placement (20 ECTS) No Thesis preparation (10 ECTS) Thesis preparation (10 ECTS) No Master’s thesis (30 ECTS) Master’s thesis (30 ECTS) Yes Students who are enrolled under the academic regulations must complete their degree programmes no later than six months after the prescribed period of study. The deadline for completing degree programmes (maximum duration of study) will be stated in the student self-service system",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The deadline for completing degree programmes (maximum duration of study) will be stated in the student self-service system. 2.1Degree programme rulesIn addition to the general rules (section 3) at the end of the academic regulations, the following rules apply to the programme: 2.1.1 Lecture participationLecture participation is an exam form and involves: Regular, active and satisfactory participation in the teaching. ?Regular? means participating in at least 75% of the teaching activities offered. At the start of the semester, the teacher will announce in writing (on Brightspace) the teaching activities in which students are expected to participate, and the requirements regarding what will be construed as active and satisfactory participation. 2.1.2 Language of teaching and examsThe language of the teaching and exams is English. 2.1.3 SyllabusRules regarding the syllabus can be found on the study portal. 2.1.4 Tables, graphs and figures in written examsTables, graphs and figures correspond to 800 characters each (regardless of their size) if they are produced by the students. Tables, graphs and figures taken from other sources correspond to 0 characters. Comment on study chart:Commencement of studies exam:Students who are enrolled on Track A at the summer admissions 2020 and later summer admissions must pass a Commencement of Studies exam. The examination is conducted via WISEflow. Students are informed of the Commencement of Studies exam via AU student post in connection with the commencement of studies.In the 3rd semester, the student can choose between:(1) 30 ECTS international electives passed at a university abroad(2) 20 ECTS profile courses, and 10 ECTS thesis preparation(3) 20 ECTS project placement, and 10 ECTS thesis preparation 30 ECTS Description of qualifications:Purpose:The purpose of the course is for students to acquire advanced knowledge about the structure and function of the brain, with a focus on how brain function contributes to cognitive function",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The focus of the course is on advanced experimental methods in cognitive neuroscience, and students will conduct their own cognitive neuroimaging/cognitive neurophysiology research. Students will learn advanced statistical methods for analysing data acquired from the measurement of neural processes, with a focus on modelling techniques for relating neural data to cognitive functions.The course includes 1) theory of neural and cognitive processes; 2) advanced statistical methodologies for analysing neuroimaging data; and 3) discussion of the theoretical relationships between neurobiological and cognitive brain processes.This course builds on students’ knowledge of cognition, and their skills and competencies in using statistical methods.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- describe the anatomy and physiology of the human brain, and explain the brain basis of cognitive functiocontrast different cognitive neuroscience methods in terms of their strengths and weaknesses, and use this knowledge to develop appropriate experimental research for investigating different cognitive functions of the brain.Skills:- run experiments using neuroimaging and/or neurophysiological measurement equipmenuse advanced statistical methods to make inferences about cognitive brain functions from neuroimaging and/or neurophysiological data.Competences:- independently identify the appropriate measurement technology and experimental designs for investigating different cognitive functionidentify cases in which statistical methods taught in the course can be applied to domains outside of cognitive neuroscience",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Knowledge:- describe the anatomy and physiology of the human brain, and explain the brain basis of cognitive functiocontrast different cognitive neuroscience methods in terms of their strengths and weaknesses, and use this knowledge to develop appropriate experimental research for investigating different cognitive functions of the brain. Skills:- run experiments using neuroimaging and/or neurophysiological measurement equipmenuse advanced statistical methods to make inferences about cognitive brain functions from neuroimaging and/or neurophysiological data. Competences:- independently identify the appropriate measurement technology and experimental designs for investigating different cognitive functionidentify cases in which statistical methods taught in the course can be applied to domains outside of cognitive neuroscience. Forms of instruction:LectureClassroom instruction Comments on form of instruction:The course combines lectures with practical exercises and workshops.Students produce a number of assignments during the course. At the start of the semester, the teacher will inform the students of the number of assignments, their form, length and deadlines both on Brightspace and orally. The assignments can provide the basis for various forms of feedback and further development related to the teaching, but the individual assignments are not assessed on a continuous basis. All or some of these assignments can provide the basis for the student’s exam.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. The course combines lectures with practical exercises and workshops.Students produce a number of assignments during the course. At the start of the semester, the teacher will inform the students of the number of assignments, their form, length and deadlines both on Brightspace and orally",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". At the start of the semester, the teacher will inform the students of the number of assignments, their form, length and deadlines both on Brightspace and orally. The assignments can provide the basis for various forms of feedback and further development related to the teaching, but the individual assignments are not assessed on a continuous basis. All or some of these assignments can provide the basis for the student’s exam. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment (Assign)Form of co-examination:Internal co-examinationAssessment form:7-point grading scaleComments:Ordinary exam and re-examination:The exam consists of a portfolio containing 3-7 of assignments. The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products. Depending on their length, and subject to the teacher’s approval, these products can replace some of the standard pages in the portfolio.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The complete portfolio must be submitted for assessment in WISEflow before the deadline set in the examination plan. Each student submits a portfolio. Take-home assignment (Assign) Form of co-examination:Internal co-examination Assessment form:7-point grading scale Comments:Ordinary exam and re-examination:The exam consists of a portfolio containing 3-7 of assignments. The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products. Depending on their length, and subject to the teacher’s approval, these products can replace some of the standard pages in the portfolio.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The complete portfolio must be submitted for assessment in WISEflow before the deadline set in the examination plan. Each student submits a portfolio. Description of qualifications:Purpose:The purpose of the course is to introduce students to advanced statistical methods used in the analysis of text and speech data. The course also introduces students to computational models used in speech and text recognition and prediction, and to models used to generate text and speech outputs in artificial intelligence systems, such as digital assistants and chat bots.The course addresses how we can approach theoretical and applied topics in the cognitive sciences using computational linguistics and natural language processing tools. Examples may include probabilistic topic modelling, sentiment analysis, and word2vec semantic analysis. The course also addresses key ethical topics that arise from the analysis of freely available natural language data, and in the development of natural language processing software and technologies.This course builds on students’ background knowledge in statistics and statistical programming, and introduces students to working with large data sets",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_7"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The course introduces students to ethical and philosophical topics.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- contrast different natural language processing methods in terms of their strengths and weaknesses in different use contextexplain how formal and computational analysis of natural language can provide insights into human cognition and behavioudiscuss ethical and philosophical issues connected to natural language processing technology applications.Skills:- identify relevant data sources for specific research and applied questioncorrectly choose and apply tools for analysing natural language data.Competences:- critically reflect on and discuss theoretical and empirical implications of using natural language processing techniquejustify the choice between relevant methods and analyses used for specific research questions within the field of natural language processincritically evaluate the appropriateness of a given method for a given natural language data set. Purpose:The purpose of the course is to introduce students to advanced statistical methods used in the analysis of text and speech data. The course also introduces students to computational models used in speech and text recognition and prediction, and to models used to generate text and speech outputs in artificial intelligence systems, such as digital assistants and chat bots. The course addresses how we can approach theoretical and applied topics in the cognitive sciences using computational linguistics and natural language processing tools. Examples may include probabilistic topic modelling, sentiment analysis, and word2vec semantic analysis. The course also addresses key ethical topics that arise from the analysis of freely available natural language data, and in the development of natural language processing software and technologies",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_8"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The course also addresses key ethical topics that arise from the analysis of freely available natural language data, and in the development of natural language processing software and technologies. This course builds on students’ background knowledge in statistics and statistical programming, and introduces students to working with large data sets. The course introduces students to ethical and philosophical topics. Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge:- contrast different natural language processing methods in terms of their strengths and weaknesses in different use contextexplain how formal and computational analysis of natural language can provide insights into human cognition and behavioudiscuss ethical and philosophical issues connected to natural language processing technology applications. Skills:- identify relevant data sources for specific research and applied questioncorrectly choose and apply tools for analysing natural language data. Competences:- critically reflect on and discuss theoretical and empirical implications of using natural language processing techniquejustify the choice between relevant methods and analyses used for specific research questions within the field of natural language processincritically evaluate the appropriateness of a given method for a given natural language data set. Forms of instruction:Classroom instruction Comments on form of instruction:Combination of classroom instruction, individual practical exercises and study group projects.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Combination of classroom instruction, individual practical exercises and study group projects. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_9"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment on topic of student's choiceForm of co-examination:Internal co-examinationAssessment form:7-point grading scaleComments:Ordinary exam and re-examination:The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 10-12 standard pagesLength for two students: 17-22 standard pagesLength for three students: 24-32 standard pagesLength for four students: 31-42 standard pagesThe scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner.The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_10"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Assessment is based on an overall assessment of the take-home assignment and the practical product. Take-home assignment on topic of student's choice Form of co-examination:Internal co-examination Assessment form:7-point grading scale Comments:Ordinary exam and re-examination:The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 10-12 standard pagesLength for two students: 17-22 standard pagesLength for three students: 24-32 standard pagesLength for four students: 31-42 standard pagesThe scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner.The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product. The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_11"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Assessment is based on an overall assessment of the take-home assignment and the practical product. Description of qualifications:Purpose:The purpose of the course is 1) to develop students’ knowledge of judgment, decision-making, and choice processes; 2) to develop students’ knowledge of decision making in relation to other cognitive functions (e.g. learning, memory, attention, social cognition); and 3) to introduce applied use cases for decision-making research.The course includes basic revision of important conceptual theories of judgment, decision-making, and choice; discussion of the psychological components of decision-making (e.g. learning, memory, attention, social cognition); discussion of philosophical, theoretical, and normative frameworks for assessing the quality of decision-making; and discussion of domains in which decision making research may be applied.The course provides students with conceptual knowledge to understand cognitive processes in judgement, choice, and decision making. These topics may also be covered in the advanced cognitive neuroscience course, and they may be revisited in advanced cognitive modelling.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- explain conceptual theories of decision-makinexplain how decision making relates to other cognitive processeexplain how empirical research on decision making relates to theory.Skills:- use philosophical, theoretical, or normative frameworks to assess decision makinanalyze experimental methodologies relevant for evaluating theories of decision making.Competences:- apply conceptual theories, normative frameworks, or formal methods learned in the course to real world decision making. Purpose:The purpose of the course is 1) to develop students’ knowledge of judgment, decision-making, and choice processes; 2) to develop students’ knowledge of decision making in relation to other cognitive functions (e.g",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_12"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". learning, memory, attention, social cognition); and 3) to introduce applied use cases for decision-making research.The course includes basic revision of important conceptual theories of judgment, decision-making, and choice; discussion of the psychological components of decision-making (e.g. learning, memory, attention, social cognition); discussion of philosophical, theoretical, and normative frameworks for assessing the quality of decision-making; and discussion of domains in which decision making research may be applied.The course provides students with conceptual knowledge to understand cognitive processes in judgement, choice, and decision making. These topics may also be covered in the advanced cognitive neuroscience course, and they may be revisited in advanced cognitive modelling. Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge:- explain conceptual theories of decision-makinexplain how decision making relates to other cognitive processeexplain how empirical research on decision making relates to theory. Skills:- use philosophical, theoretical, or normative frameworks to assess decision makinanalyze experimental methodologies relevant for evaluating theories of decision making. Competences:- apply conceptual theories, normative frameworks, or formal methods learned in the course to real world decision making. Forms of instruction:LectureClassroom instruction Comments on form of instruction:Lectures and classroom instructions.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Lectures and classroom instructions. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_13"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Lectures and classroom instructions. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment on topic of student's choiceForm of co-examination:External co-examinationAssessment form:7-point grading scaleComments:Ordinary exam and re-examination:The exam is a take-home assignment on a topic of the student’s choice. The topic and method used in the assignment must be relevant in relation to the content of the course and is subject to the approval of the teacher.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 12-15 standard pagesLength for two students: 19-25 standard pagesLength for three students: 26-35 standard pagesLength for four students: 33-45 standard pagesThe assignment must be submitted for assessment in WISEflow before the deadline set in the examination plan. Take-home assignment on topic of student's choice Form of co-examination:External co-examination Assessment form:7-point grading scale Comments:Ordinary exam and re-examination:The exam is a take-home assignment on a topic of the student’s choice. The topic and method used in the assignment must be relevant in relation to the content of the course and is subject to the approval of the teacher.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_14"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 12-15 standard pagesLength for two students: 19-25 standard pagesLength for three students: 26-35 standard pagesLength for four students: 33-45 standard pagesThe assignment must be submitted for assessment in WISEflow before the deadline set in the examination plan. Ordinary exam and re-examination:The exam is a take-home assignment on a topic of the student’s choice. The topic and method used in the assignment must be relevant in relation to the content of the course and is subject to the approval of the teacher.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 12-15 standard pagesLength for two students: 19-25 standard pagesLength for three students: 26-35 standard pagesLength for four students: 33-45 standard pagesThe assignment must be submitted for assessment in WISEflow before the deadline set in the examination plan. 30 ECTS Description of qualifications:Purpose:The purpose of the course is 1) to develop student’s ability to understand theoretical models of cognitive function (e.g. memory, decision-making); 2) to develop student’s ability to work with formal mathematical and computational models of verbally stated theories; and 3) to apply formal cognitive models to data. This will provide students with a deeper knowledge of conceptual theories of cognition, and with skills in formalising theory and applying it to data",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_15"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". This will provide students with a deeper knowledge of conceptual theories of cognition, and with skills in formalising theory and applying it to data. It will also prepare students to use theories from cognitive science in the development of software agents capable of performing simple cognitive tasks (i.e. artificial intelligence systems).The course includes 1) an introduction to general frameworks for formalising cognitive theories, including specific mathematical models; and 2) the application of advanced statistical methods for evaluating formal models of cognition, and for applying models to data.This course integrates conceptual and theoretical knowledge about cognitive functions with 1) formal modelling tools, and 2) statistical methods for fitting theory to data. The course builds on the decision-making course. The course places theoretical knowledge in cognitive science within the broader perspective of philosophical issues surrounding theory construction in cognitive science.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- describe common mathematical and computational approaches to modelling cognitioexplain the relationship between conceptual theories of cognition and mathematical/computational modelshow how specific mathematical and computational models express conceptual theoriereflect on philosophical issues surrounding theory construction in cognitive science.Skills:- use advanced statistical methods to evaluate and compare different models when applied to the same data sedesign simple software agents implementing theories of cognitive function.Competences:- formalise verbally stated conceptual theories of cognitive function. Purpose:The purpose of the course is 1) to develop student’s ability to understand theoretical models of cognitive function (e.g",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_16"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Purpose:The purpose of the course is 1) to develop student’s ability to understand theoretical models of cognitive function (e.g. memory, decision-making); 2) to develop student’s ability to work with formal mathematical and computational models of verbally stated theories; and 3) to apply formal cognitive models to data. This will provide students with a deeper knowledge of conceptual theories of cognition, and with skills in formalising theory and applying it to data. It will also prepare students to use theories from cognitive science in the development of software agents capable of performing simple cognitive tasks (i.e. artificial intelligence systems). Knowledge:- describe common mathematical and computational approaches to modelling cognitioexplain the relationship between conceptual theories of cognition and mathematical/computational modelshow how specific mathematical and computational models express conceptual theoriereflect on philosophical issues surrounding theory construction in cognitive science. Skills:- use advanced statistical methods to evaluate and compare different models when applied to the same data sedesign simple software agents implementing theories of cognitive function. Competences:- formalise verbally stated conceptual theories of cognitive function. Forms of instruction:LectureClassroom instruction Comments on form of instruction:Lectures and classroom instructionStudents produce a number of assignments during the course. At the start of the semester, the teacher will inform the students of the number of assignments, their form, length and deadlines both on Brightspace and orally. The assignments can provide the basis for various forms of feedback and further development related to the teaching, but the individual assignments are not assessed on a continuous basis. All or some of these assignments can provide the basis for the student’s exam.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_17"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Lectures and classroom instruction Students produce a number of assignments during the course. At the start of the semester, the teacher will inform the students of the number of assignments, their form, length and deadlines both on Brightspace and orally. The assignments can provide the basis for various forms of feedback and further development related to the teaching, but the individual assignments are not assessed on a continuous basis. All or some of these assignments can provide the basis for the student’s exam. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment (Assign)Form of co-examination:Internal co-examinationAssessment form:Passed /failedComments:Ordinary exam and re-examination:The exam consists of a portfolio containing 3-7 of assignments. The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products. Depending on their length, and subject to the teacher’s approval, these products can replace some of the standard pages in the portfolio.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The complete portfolio must be submitted for assessment in WISEflow before the deadline set in the examination plan. Each student submits a portfolio. Take-home assignment (Assign) Form of co-examination:Internal co-examination Assessment form:Passed /failed Comments:Ordinary exam and re-examination:The exam consists of a portfolio containing 3-7 of assignments. The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_18"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products. Depending on their length, and subject to the teacher’s approval, these products can replace some of the standard pages in the portfolio.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The complete portfolio must be submitted for assessment in WISEflow before the deadline set in the examination plan. Each student submits a portfolio. Ordinary exam and re-examination:The exam consists of a portfolio containing 3-7 of assignments. The number of assignments as well as their form and eventual length will be announced on Brightspace by the teacher at the start of the semester. The portfolio may include products. Depending on their length, and subject to the teacher’s approval, these products can replace some of the standard pages in the portfolio.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The complete portfolio must be submitted for assessment in WISEflow before the deadline set in the examination plan. Each student submits a portfolio. Description of qualifications:Purpose:The purpose of the course is to introduce students to advanced statistical and computational methods required for the analysis of large, complex, and naturalistic data sets. In particular, students are introduced to state-of-the-art methods for the analysis of time series. In this area, a particular focus is placed on prediction and forecasting",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_19"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". In particular, students are introduced to state-of-the-art methods for the analysis of time series. In this area, a particular focus is placed on prediction and forecasting. Examples of relevant data sets are publicly available unit sales time series from large multinational companies, financial market time series, behavioural time series from cognitive-scientific and neuroscientific experimental studies, social media network data, data from wearable technology, and data from interactive sensors. Students will also be able to recognise important ethical, social, and policy implications arising from the application of advanced statistical analysis to human data, and to reflect on the ethical choices that must be made when developing data processing and statistical analysis software.The course covers advanced tools in data science, time series analysis, prediction, and forecasting. The course relates these tools to experimental and lab methodology, and to theories of cognitive functions. The course also includes integrated discussion of the ethical, social, and policy implications of the use of data science.The course equips students with the computational tools to model and investigate human cognition and behaviour both at the individual and at the collective level",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_20"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The course prepares students for a wide range of data processing and analysis career paths.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- describe and contrast different computational and statistical methods for analysing large and complex data sets, in particular time series.Skills:- identify relevant data sources for specific research and applied questions, and integrate data sources as needeprepare and pre-process complex data for analysicreate visualisation of time series and other complex data.Competences:- clearly and effectively present the results of complex analyses, in a way that is understood by non-specialistcritically evaluate the appropriateness of a given method for a given data serecognise and analyse ethical, social, and policy issues arising from the use of data science methods and technology. Purpose:The purpose of the course is to introduce students to advanced statistical and computational methods required for the analysis of large, complex, and naturalistic data sets. In particular, students are introduced to state-of-the-art methods for the analysis of time series. In this area, a particular focus is placed on prediction and forecasting. Examples of relevant data sets are publicly available unit sales time series from large multinational companies, financial market time series, behavioural time series from cognitive-scientific and neuroscientific experimental studies, social media network data, data from wearable technology, and data from interactive sensors. Students will also be able to recognise important ethical, social, and policy implications arising from the application of advanced statistical analysis to human data, and to reflect on the ethical choices that must be made when developing data processing and statistical analysis software. The course covers advanced tools in data science, time series analysis, prediction, and forecasting",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_21"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The course covers advanced tools in data science, time series analysis, prediction, and forecasting. The course relates these tools to experimental and lab methodology, and to theories of cognitive functions. The course also includes integrated discussion of the ethical, social, and policy implications of the use of data science.The course equips students with the computational tools to model and investigate human cognition and behaviour both at the individual and at the collective level. The course prepares students for a wide range of data processing and analysis career paths. Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge:- describe and contrast different computational and statistical methods for analysing large and complex data sets, in particular time series. Skills:- identify relevant data sources for specific research and applied questions, and integrate data sources as needeprepare and pre-process complex data for analysicreate visualisation of time series and other complex data. Competences:- clearly and effectively present the results of complex analyses, in a way that is understood by non-specialistcritically evaluate the appropriateness of a given method for a given data serecognise and analyse ethical, social, and policy issues arising from the use of data science methods and technology. Forms of instruction:Lecture Comments on form of instruction:Lectures and classroom instruction.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Lectures and classroom instruction. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_22"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Lectures and classroom instruction. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment on topic of student's choiceForm of co-examination:Internal co-examinationAssessment form:7-point grading scaleComments:Ordinary exam and re-examination:The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 10-12 standard pagesLength for two students: 17-22 standard pagesLength for three students: 24-32 standard pagesLength for four students: 31-42 standard pagesThe scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner.The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_23"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Assessment is based on an overall assessment of the take-home assignment and the practical product. Take-home assignment on topic of student's choice Form of co-examination:Internal co-examination Assessment form:7-point grading scale Comments:Ordinary exam and re-examination:The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 10-12 standard pagesLength for two students: 17-22 standard pagesLength for three students: 24-32 standard pagesLength for four students: 31-42 standard pagesThe scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner.The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product. Ordinary exam and re-examination:The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product.The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_24"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section.Length for one student: 10-12 standard pagesLength for two students: 17-22 standard pagesLength for three students: 24-32 standard pagesLength for four students: 31-42 standard pages The scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner. The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product.The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product. Description of qualifications:Purpose:The purpose of the course is to introduce students to the field of human-computer interaction. The emphasis in the course will be on the application of theories and methods from cognitive science to the field of HCI. Students will learn to apply methods from cognitive science in the analysis, and evaluation of information technology designed for human use, and they will learn how to apply theories from cognitive science to inform the design of new human-computer systems.The course includes an introduction to human-computer interaction from the perspective of cognitive science. The course will focus on how knowledge of human cognitive processes, and different methodologies can inform the evaluation and design of information technology systems intended for human use",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_25"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The course will focus on how knowledge of human cognitive processes, and different methodologies can inform the evaluation and design of information technology systems intended for human use. It will also discuss opportunities and limitations of using artificial systems in direct interaction with humans.This course builds on earlier theoretical and methodological courses in cognitive science, to enable students to understand basic design principles used in the development of information technology systems.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- explain common theories in human-computer interactiodescribe opportunities and limitations of human-computer interaction.Skills:- apply statistical tools and measurement techniques to evaluate information technology systems with respect to usability and user experiencapply theories of cognitive function in the design of new information technology systemdiscuss implications of information technology systems for human cognition and behavior.Competences:- make informed suggestions for the design and use of human interaction systems. Purpose:The purpose of the course is to introduce students to the field of human-computer interaction. The emphasis in the course will be on the application of theories and methods from cognitive science to the field of HCI. Students will learn to apply methods from cognitive science in the analysis, and evaluation of information technology designed for human use, and they will learn how to apply theories from cognitive science to inform the design of new human-computer systems. Knowledge:- explain common theories in human-computer interactiodescribe opportunities and limitations of human-computer interaction",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_26"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Knowledge:- explain common theories in human-computer interactiodescribe opportunities and limitations of human-computer interaction. Skills:- apply statistical tools and measurement techniques to evaluate information technology systems with respect to usability and user experiencapply theories of cognitive function in the design of new information technology systemdiscuss implications of information technology systems for human cognition and behavior. Competences:- make informed suggestions for the design and use of human interaction systems. Forms of instruction:LectureClassroom instruction Comments on form of instruction:Lectures and classroom instructionLanguage of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Lectures and classroom instruction Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment (Assign), OralSpecific to form of examination: OralExam duration:30 minute(s)Form of co-examination:Internal co-examinationAssessment form:7-point grading scaleComments:Ordinary exam and re-examination:The examination consists of an individual oral exam based on a synopsis. The synopsis can be written individually or in a group up to 4 students.Length of the synopsis:For 1 student: 4-7 standard pages.For 2 students: 8-14 standard pagesFor 3 students: 12-21 standard pagesFor 4 students: 16-28 standard pagesThe synopsis must be submitted in WISEflow before the deadline set in the examination plan.The oral exam is based on the student’s presentation of the synopsis followed by a dialogue between the student and the examiner in which the rest of the course syllabus is included.Duration: 30 minutes (including assessment).The assessment is based on an overall evaluation of the synopsis and the oral presentation",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_27"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Take-home assignment (Assign), Oral Specific to form of examination: Oral Exam duration:30 minute(s) Form of co-examination:Internal co-examination Assessment form:7-point grading scale Comments:Ordinary exam and re-examination:The examination consists of an individual oral exam based on a synopsis. The synopsis can be written individually or in a group up to 4 students.Length of the synopsis:For 1 student: 4-7 standard pages.For 2 students: 8-14 standard pagesFor 3 students: 12-21 standard pagesFor 4 students: 16-28 standard pagesThe synopsis must be submitted in WISEflow before the deadline set in the examination plan.The oral exam is based on the student’s presentation of the synopsis followed by a dialogue between the student and the examiner in which the rest of the course syllabus is included.Duration: 30 minutes (including assessment).The assessment is based on an overall evaluation of the synopsis and the oral presentation. Ordinary exam and re-examination:The examination consists of an individual oral exam based on a synopsis. The synopsis can be written individually or in a group up to 4 students.Length of the synopsis:For 1 student: 4-7 standard pages.For 2 students: 8-14 standard pagesFor 3 students: 12-21 standard pagesFor 4 students: 16-28 standard pages The synopsis must be submitted in WISEflow before the deadline set in the examination plan.The oral exam is based on the student’s presentation of the synopsis followed by a dialogue between the student and the examiner in which the rest of the course syllabus is included.Duration: 30 minutes (including assessment).The assessment is based on an overall evaluation of the synopsis and the oral presentation",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_28"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". 30 ECTS In the third semester, the student can choose between:(1)30 ECTS international electives passed at a university abroad(2)20 ECTS profile courses, and 10 ECTS thesis preparation(3)20 ECTS project placement, and 10 ECTS thesis preparationInternational elective courses (30 ECTS):The objective of the international elective courses is to expand the degree programme’s academic scope at a university outside Denmark. The courses must be within the degree programme’s core field of study, and must comply with requirements for progression as well as being connected to the knowledge, skills and competences acquired during the first year of the Master’s degree programme. International elective courses must also include elements that are thematically or methodically relevant for the student’s Master’s thesis on semester 4 of the Master’s degree programme. Students must be able to participate actively in an academic educational environment outside Denmark and take responsibility for their own academic development in an unfamiliar educational environment.Courses are subject to the advance approval of the board of studies based on the description of objectives above.Courses are subject to the advance approval of the board of studies based on the description of objectives above.﻿Profile courses (20 ECTS):Students choose between the profile courses offered. A given semester’s range of profile courses is submitted by the respective boards of studies for approval by the dean before being published in the Aarhus University course catalogue one year before the courses start. Detailed stipulations regarding the form, content and exam format of each individual option are stated in the individual course descriptions in the catalogue",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_29"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Detailed stipulations regarding the form, content and exam format of each individual option are stated in the individual course descriptions in the catalogue. Detailed guidelines for application, prioritisation and admission to profile courses can be found on the websitehttps://studerende.au.dk/en/studies/subject-portals/arts/electivecourses,profilecoursesetc/profile-coursesSee the description ofProject placement and thesis preparation below: In the third semester, the student can choose between:(1)30 ECTS international electives passed at a university abroad(2)20 ECTS profile courses, and 10 ECTS thesis preparation(3)20 ECTS project placement, and 10 ECTS thesis preparation International elective courses (30 ECTS):The objective of the international elective courses is to expand the degree programme’s academic scope at a university outside Denmark. The courses must be within the degree programme’s core field of study, and must comply with requirements for progression as well as being connected to the knowledge, skills and competences acquired during the first year of the Master’s degree programme. International elective courses must also include elements that are thematically or methodically relevant for the student’s Master’s thesis on semester 4 of the Master’s degree programme. Students must be able to participate actively in an academic educational environment outside Denmark and take responsibility for their own academic development in an unfamiliar educational environment.Courses are subject to the advance approval of the board of studies based on the description of objectives above. Courses are subject to the advance approval of the board of studies based on the description of objectives above. ﻿Profile courses (20 ECTS):Students choose between the profile courses offered. A given semester’s range of profile courses is submitted by the respective boards of studies for approval by the dean before being published in the Aarhus University course catalogue one year before the courses start",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_30"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Detailed stipulations regarding the form, content and exam format of each individual option are stated in the individual course descriptions in the catalogue. Detailed guidelines for application, prioritisation and admission to profile courses can be found on the websitehttps://studerende.au.dk/en/studies/subject-portals/arts/electivecourses,profilecoursesetc/profile-courses See the description ofProject placement and thesis preparation below: Description of qualifications:Purpose:The project placement must be related to a subject in the field of cognitive science. Planned goals and activities should be specified in a work plan (max. 2 standard pages) jointly signed by all parties involved before the start of the placement.The placement lasts minimally three months and maximally four months. The project placement is recommended to be four days a week. The project placement should complement the knowledge, skills, and competences acquired through regular course work, and students should have opportunity to apply what they have learned in regular coursework to the placement.Example placements include educational or research institutions, administrative bodies, consultancy agencies, and private companies. The project course may be integrated with the MA thesis.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- demonstrate knowledge and understanding of the workplace and its taskrelate the project placement to cognitive science as an academic field.Skills:- communicate relevant concepts and methods from cognitive science in a way that is comprehensible for people from different academic backgroundparticipate in goal-directed activities shaped by the context, activities, and goals of an organization.Competences:- meet and balance work deadlineco-ordinate work with others within an organisatioapply concepts and methods from cognitive science in a larger (e.g. social, educational, political) context",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_31"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". social, educational, political) context. Purpose:The project placement must be related to a subject in the field of cognitive science. Planned goals and activities should be specified in a work plan (max. 2 standard pages) jointly signed by all parties involved before the start of the placement. The placement lasts minimally three months and maximally four months. The project placement is recommended to be four days a week. The project placement should complement the knowledge, skills, and competences acquired through regular course work, and students should have opportunity to apply what they have learned in regular coursework to the placement. Example placements include educational or research institutions, administrative bodies, consultancy agencies, and private companies. The project course may be integrated with the MA thesis. Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge:- demonstrate knowledge and understanding of the workplace and its taskrelate the project placement to cognitive science as an academic field. Skills:- communicate relevant concepts and methods from cognitive science in a way that is comprehensible for people from different academic backgroundparticipate in goal-directed activities shaped by the context, activities, and goals of an organization. Competences:- meet and balance work deadlineco-ordinate work with others within an organisatioapply concepts and methods from cognitive science in a larger (e.g. social, educational, political) context. Forms of instruction:Supervision Comments on form of instruction:Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_32"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Take-home assignment on topic of student's choiceForm of co-examination:Internal co-examinationAssessment form:Passed /failedComments:Ordinary examination and re-examination:The exam is an individual take-home assignment on a topic of the student’s choice.Based on the project placement, the student must write a report of 10-20 standard pages in length, including an analysis of a defined and academically relevant topic related to the project placement.The first part of the report must include a presentation of the workplace, the type of work carried out at the host organisation as well as the host organisation’s fundamental goals and motivations. It must also include a description of the student’s role in the organisation and the tasks the student has performed.The second part is the actual analysis, which is the main part of the report. Here, the student must analyse selected aspects of the project placement from an academic perspective. An academic perspective means theoretically and academically informed considerations on the current practice of the host organisation, its fundamental goals and/or reflections of the specific tasks which the student has performed during the project placement. If the project is a communication project, the report must also include a reflection on the factual and academic basis of the communicated information, and of the communication itself.The third part of the assignment includes a meta-academic reflection on the relation between the project placement and the field of cognitive science, including a discussion of relevant theory and methodology from cognitive science.The take-home assignment must be handed in for assessment in WISEflow by the date specified in the exam plan",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_33"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Take-home assignment on topic of student's choice Form of co-examination:Internal co-examination Assessment form:Passed /failed Comments:Ordinary examination and re-examination:The exam is an individual take-home assignment on a topic of the student’s choice.Based on the project placement, the student must write a report of 10-20 standard pages in length, including an analysis of a defined and academically relevant topic related to the project placement.The first part of the report must include a presentation of the workplace, the type of work carried out at the host organisation as well as the host organisation’s fundamental goals and motivations. It must also include a description of the student’s role in the organisation and the tasks the student has performed.The second part is the actual analysis, which is the main part of the report. Here, the student must analyse selected aspects of the project placement from an academic perspective. An academic perspective means theoretically and academically informed considerations on the current practice of the host organisation, its fundamental goals and/or reflections of the specific tasks which the student has performed during the project placement. If the project is a communication project, the report must also include a reflection on the factual and academic basis of the communicated information, and of the communication itself.The third part of the assignment includes a meta-academic reflection on the relation between the project placement and the field of cognitive science, including a discussion of relevant theory and methodology from cognitive science.The take-home assignment must be handed in for assessment in WISEflow by the date specified in the exam plan. Ordinary examination and re-examination:The exam is an individual take-home assignment on a topic of the student’s choice",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_34"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Ordinary examination and re-examination:The exam is an individual take-home assignment on a topic of the student’s choice. Based on the project placement, the student must write a report of 10-20 standard pages in length, including an analysis of a defined and academically relevant topic related to the project placement. The first part of the report must include a presentation of the workplace, the type of work carried out at the host organisation as well as the host organisation’s fundamental goals and motivations. It must also include a description of the student’s role in the organisation and the tasks the student has performed.The second part is the actual analysis, which is the main part of the report. Here, the student must analyse selected aspects of the project placement from an academic perspective. An academic perspective means theoretically and academically informed considerations on the current practice of the host organisation, its fundamental goals and/or reflections of the specific tasks which the student has performed during the project placement. If the project is a communication project, the report must also include a reflection on the factual and academic basis of the communicated information, and of the communication itself.The third part of the assignment includes a meta-academic reflection on the relation between the project placement and the field of cognitive science, including a discussion of relevant theory and methodology from cognitive science. The take-home assignment must be handed in for assessment in WISEflow by the date specified in the exam plan",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_35"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The take-home assignment must be handed in for assessment in WISEflow by the date specified in the exam plan. Description of qualifications:Purpose:The aim of the course is to enhance students' ability to write a thesis, which lives up to scientific standards in the subject.During the course, the students develop a problem statement, an outline, a description of the empirical basis of the thesis and a plan for completion of the thesis, including a plan for supervision.In the case of an intended product thesis, students must identify and specify the product that they intend to produce, and explain its function in relation to the problem statement of the thesis.Students must also identify academic sources for the thesis, including information about relevant scientific theory and methodology to be used in the thesis.Academic goals:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- demonstrate knowledge of the subject field, including theories, methods, research traditions and concepts that are relevant to the study undertaken in the thesis.Skills:- define and argue for the empirical basis of the thesis and, in the case of the product thesis, are able to specify the product to be developed in connection with the thesidevelop a solid and realistic strategy for carrying out the study within the time frame availablselect, systematize and review relevant literature in connection with the topic of the thesis.Competences:- demonstrate an ability to develop a project in which the methods, theories, and analyses chosen are logically consistent with one anotheargue for the connection of theoretical, analytical and methodological grip on a major taswork solution-oriented and process-oriented in relation to a specific project",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_36"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Purpose:The aim of the course is to enhance students' ability to write a thesis, which lives up to scientific standards in the subject.During the course, the students develop a problem statement, an outline, a description of the empirical basis of the thesis and a plan for completion of the thesis, including a plan for supervision.In the case of an intended product thesis, students must identify and specify the product that they intend to produce, and explain its function in relation to the problem statement of the thesis.Students must also identify academic sources for the thesis, including information about relevant scientific theory and methodology to be used in the thesis. Academic goals:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge:- demonstrate knowledge of the subject field, including theories, methods, research traditions and concepts that are relevant to the study undertaken in the thesis.Skills:- define and argue for the empirical basis of the thesis and, in the case of the product thesis, are able to specify the product to be developed in connection with the thesidevelop a solid and realistic strategy for carrying out the study within the time frame availablselect, systematize and review relevant literature in connection with the topic of the thesis.Competences:- demonstrate an ability to develop a project in which the methods, theories, and analyses chosen are logically consistent with one anotheargue for the connection of theoretical, analytical and methodological grip on a major taswork solution-oriented and process-oriented in relation to a specific project. Forms of instruction:LectureIndependent studySupervision Comments on form of instruction:Lecture, independent study, and supervision.Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Lecture, independent study, and supervision",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_37"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Lecture, independent study, and supervision. Language of instruction:The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Exam language:English Exam options:Lecture participationForm of co-examination:No co-examinationAssessment form:Passed /failedComments:Ordinary examination:The exam is passed by a collection of assignments. The collection will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form as well as the length and deadline for submission will be announced on Brightspace by the teacher at the start of the semester. The total length does not exceed 15 standard pages.Re-examination:The exam is a portfolio exam. The portfolio will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form and length will be announced on Brightspace by the teacher at the start of the semester.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The total length does not exceed 15 standard pages. The complete portfolio must be handed in for assessment in WISEflow by the date specified in the exam plan. Lecture participation Form of co-examination:No co-examination Assessment form:Passed /failed Comments:Ordinary examination:The exam is passed by a collection of assignments. The collection will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form as well as the length and deadline for submission will be announced on Brightspace by the teacher at the start of the semester. The total length does not exceed 15 standard pages.Re-examination:The exam is a portfolio exam",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_38"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The total length does not exceed 15 standard pages.Re-examination:The exam is a portfolio exam. The portfolio will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form and length will be announced on Brightspace by the teacher at the start of the semester.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for.The total length does not exceed 15 standard pages. The complete portfolio must be handed in for assessment in WISEflow by the date specified in the exam plan. Ordinary examination:The exam is passed by a collection of assignments. The collection will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form as well as the length and deadline for submission will be announced on Brightspace by the teacher at the start of the semester. The total length does not exceed 15 standard pages. Re-examination:The exam is a portfolio exam. The portfolio will consist of a problem formulation; a draft outline of a thesis; an indication of the empirical material; and an annotated bibliography. Their form and length will be announced on Brightspace by the teacher at the start of the semester.It must be possible to carry out an individual assessment. So if some parts of the portfolio have been produced by a group, it must be stated clearly which parts each student is responsible for, and which parts the group as a whole is responsible for. The total length does not exceed 15 standard pages. The complete portfolio must be handed in for assessment in WISEflow by the date specified in the exam plan",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_39"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The total length does not exceed 15 standard pages. The complete portfolio must be handed in for assessment in WISEflow by the date specified in the exam plan. 30 ECTS Description of qualifications:Purpose:The objective of the course is to produce a substantial piece of academic work which meets the academic standards of the programme. By working on their Master’s thesis, students develop in-depth knowledge of a topic they have chosen themselves which must be within the subject area, as well as knowledge of the relevant academic literature",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_40"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Students thereby develop the ability to apply the theories and methods of the programme to address a delimited thesis statement.Students also develop the ability to independently structure their own work process, including the ability to plan, organise and work on a substantial academic assignment, identify the need for the independent acquisition of new knowledge, and present their work in a clear and comprehensible manner.If the Master’s thesis takes the form of a product thesis, the student will develop the ability to create a connection between the analytical/theoretical/empirical point of departure and the product.If the Master’s thesis takes the form of a portfolio thesis, the student will develop the ability to produce academic texts which deal with one overall academic issue, as well as a synthesising presentation which presents relevant theoretical and methodical issues and perspectives in relation to the overall assignment.If the Master’s thesis takes the form of a group thesis, students will also develop their collaborative skills.Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to:Knowledge:- explain the theories and methods relevant to the thesiaccount for how the chosen topic, theories and methods relate to current research and/or practice within the field.Skills:- prepare a delimited thesis statement with a view to using it for academic analysiuse academic theories and methods critically and consciously, including choosing and applying relevant primary and secondary literaturwrite grammatically and linguistically correctly, use the citing and referencing practice relevant for the field, and communicate and argue in an objective, coherent and comprehensible mannerFor product theses, students must also demonstrate a clear connection between the analytical/theoretical/empirical point of departure and the productFor portfolio theses, students must also produce academic texts which deal with one overall academic issue, as",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_41"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": "between the analytical/theoretical/empirical point of departure and the productFor portfolio theses, students must also produce academic texts which deal with one overall academic issue, as well as a synthesising presentation which presents relevant theoretical and methodical issues and perspectives in relation to the overall assignmentFor group theses, students must also initiate and carry out mono-disciplinary and cross-disciplinary collaboration in an independent manner as well as taking professional responsibilityCompetences:- plan and carry out the thesis process independently within the given frameworpresent academic knowledge to a target group in a relevant manner",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_42"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". Purpose:The objective of the course is to produce a substantial piece of academic work which meets the academic standards of the programme. By working on their Master’s thesis, students develop in-depth knowledge of a topic they have chosen themselves which must be within the subject area, as well as knowledge of the relevant academic literature. Students thereby develop the ability to apply the theories and methods of the programme to address a delimited thesis statement. Students also develop the ability to independently structure their own work process, including the ability to plan, organise and work on a substantial academic assignment, identify the need for the independent acquisition of new knowledge, and present their work in a clear and comprehensible manner. If the Master’s thesis takes the form of a product thesis, the student will develop the ability to create a connection between the analytical/theoretical/empirical point of departure and the product. If the Master’s thesis takes the form of a portfolio thesis, the student will develop the ability to produce academic texts which deal with one overall academic issue, as well as a synthesising presentation which presents relevant theoretical and methodical issues and perspectives in relation to the overall assignment. If the Master’s thesis takes the form of a group thesis, students will also develop their collaborative skills",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_43"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". If the Master’s thesis takes the form of a group thesis, students will also develop their collaborative skills. Academic objectives:In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: For product theses, students must also demonstrate a clear connection between the analytical/theoretical/empirical point of departure and the product For portfolio theses, students must also produce academic texts which deal with one overall academic issue, as well as a synthesising presentation which presents relevant theoretical and methodical issues and perspectives in relation to the overall assignment For group theses, students must also initiate and carry out mono-disciplinary and cross-disciplinary collaboration in an independent manner as well as taking professional responsibility Competences:- plan and carry out the thesis process independently within the given frameworpresent academic knowledge to a target group in a relevant manner. Forms of instruction:Supervision Comments on form of instruction:Supervision Exam language:English Exam options:• Take-home assignment on topic of student's choiceForm of co-examination:External co-examinationAssessment form:7-point grading scaleComments:Monograph thesis:The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The topic of the thesis and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor.The thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment.The thesis may be written individually or in groups of up to three students",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_44"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The summary is included in the overall assessment.The thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 40 standard pages per student must be available for individual assessment.Total length for 1 student: 55-70 standard pagesTotal length for 2 students: 100-120 standard pagesTotal length for 3 students: 145-170 standard pagesTotal length for 4 students: 190-220 standard pages.Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment.• Take-home assignment on topic of student's choiceForm of co-examination:External co-examinationAssessment form:7-point grading scaleComments:Product thesis:The examination consists of a written assignment on a topic of the student’s choice and a related practical product. The written assignment must have a scope of 35-50 pages (excluding the front cover, table of contents, summary, bibliography and any appendices) and must contain theoretical and methodological reflections on the product and the production process if relevant. The written assignment and the practical product are assessed as a whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor.The product thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_45"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The summary is included in the overall assessment. If the thesis is written in English (or another foreign language), the summary may be written in Danish.The practical product must be finished to a level that partly demonstrates how the product is included in the thesis statement, and partly forms the basis for assessing the quality of the product based on the connection between the analytical/theoretical/empirical point of departure and the product. The scope and nature of the product must be agreed with and approved by the supervisor.The product thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 25 standard pages per student must be available for individual assessment.Total length for 1 student: 35-50 standard pagesTotal length for 2 students: 60-80 standard pagesTotal length for 3 students: 85-110 standard pagesTotal length for 4 students: 110-140 standard pages.Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment.• Take-home assignment (Assign)Form of co-examination:External co-examinationAssessment form:7-point grading scaleComments:Portfolio thesis:Portfolio theses are intended for PhD students on the 4+4 scheme. The examination is a written assignment on a topic of the student’s choice and consists of a portfolio with 2-5 written presentations as well as a synthesising presentation. The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The written presentations in the portfolio focus on selected parts of an overarching issue",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_46"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The written presentations in the portfolio focus on selected parts of an overarching issue. The concluding synthesising presentation must present relevant theoretical and methodological issues along with perspectives relating to the whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed with and approved by the thesis supervisor.The portfolio thesis must contain a summary of 1-2 standard pages per student in English. The summary is included in the overall assessment. If the thesis is written in English (or another foreign language), the summary may be written in Danish.Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment. • Take-home assignment on topic of student's choice Form of co-examination:External co-examination Assessment form:7-point grading scale Comments:Monograph thesis:The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The topic of the thesis and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor.The thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment.The thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 40 standard pages per student must be available for individual assessment.Total length for 1 student: 55-70 standard pagesTotal length for 2 students: 100-120 standard pagesTotal length for 3 students: 145-170 standard pagesTotal length for 4 students: 190-220 standard pages.Master’s thesis is only taken for assessment if the supervision plan has been approved",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_47"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment. Monograph thesis:The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The topic of the thesis and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor. The thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment. The thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 40 standard pages per student must be available for individual assessment. Total length for 1 student: 55-70 standard pagesTotal length for 2 students: 100-120 standard pagesTotal length for 3 students: 145-170 standard pagesTotal length for 4 students: 190-220 standard pages. Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt. If the master’s thesis does not contain a summary the assignment is not taken for assessment. • Take-home assignment on topic of student's choice Form of co-examination:External co-examination Assessment form:7-point grading scale Comments:Product thesis:The examination consists of a written assignment on a topic of the student’s choice and a related practical product. The written assignment must have a scope of 35-50 pages (excluding the front cover, table of contents, summary, bibliography and any appendices) and must contain theoretical and methodological reflections on the product and the production process if relevant. The written assignment and the practical product are assessed as a whole",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_48"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The written assignment and the practical product are assessed as a whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor.The product thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment. If the thesis is written in English (or another foreign language), the summary may be written in Danish.The practical product must be finished to a level that partly demonstrates how the product is included in the thesis statement, and partly forms the basis for assessing the quality of the product based on the connection between the analytical/theoretical/empirical point of departure and the product. The scope and nature of the product must be agreed with and approved by the supervisor.The product thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 25 standard pages per student must be available for individual assessment.Total length for 1 student: 35-50 standard pagesTotal length for 2 students: 60-80 standard pagesTotal length for 3 students: 85-110 standard pagesTotal length for 4 students: 110-140 standard pages.Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment. Product thesis:The examination consists of a written assignment on a topic of the student’s choice and a related practical product. The written assignment must have a scope of 35-50 pages (excluding the front cover, table of contents, summary, bibliography and any appendices) and must contain theoretical and methodological reflections on the product and the production process if relevant. The written assignment and the practical product are assessed as a whole",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_49"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The written assignment and the practical product are assessed as a whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed on and approved by the thesis supervisor. The product thesis must contain a summary of 1-2 standard pages in English. The summary is included in the overall assessment. If the thesis is written in English (or another foreign language), the summary may be written in Danish. The practical product must be finished to a level that partly demonstrates how the product is included in the thesis statement, and partly forms the basis for assessing the quality of the product based on the connection between the analytical/theoretical/empirical point of departure and the product. The scope and nature of the product must be agreed with and approved by the supervisor. The product thesis may be written individually or in groups of up to three students. Parts of the thesis, including the introduction, conclusion and summary, may be produced in collaboration, but at least 25 standard pages per student must be available for individual assessment. Total length for 1 student: 35-50 standard pagesTotal length for 2 students: 60-80 standard pagesTotal length for 3 students: 85-110 standard pagesTotal length for 4 students: 110-140 standard pages. Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt. If the master’s thesis does not contain a summary the assignment is not taken for assessment. • Take-home assignment (Assign) Form of co-examination:External co-examination Assessment form:7-point grading scale Comments:Portfolio thesis:Portfolio theses are intended for PhD students on the 4+4 scheme. The examination is a written assignment on a topic of the student’s choice and consists of a portfolio with 2-5 written presentations as well as a synthesising presentation",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_50"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The examination is a written assignment on a topic of the student’s choice and consists of a portfolio with 2-5 written presentations as well as a synthesising presentation. The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The written presentations in the portfolio focus on selected parts of an overarching issue. The concluding synthesising presentation must present relevant theoretical and methodological issues along with perspectives relating to the whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed with and approved by the thesis supervisor.The portfolio thesis must contain a summary of 1-2 standard pages per student in English. The summary is included in the overall assessment. If the thesis is written in English (or another foreign language), the summary may be written in Danish.Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt.If the master’s thesis does not contain a summary the assignment is not taken for assessment. Portfolio thesis:Portfolio theses are intended for PhD students on the 4+4 scheme. The examination is a written assignment on a topic of the student’s choice and consists of a portfolio with 2-5 written presentations as well as a synthesising presentation. The written assignment must have a scope of 55-70 standard pages (excluding the front cover, table of contents, summary, bibliography and any appendices). The written presentations in the portfolio focus on selected parts of an overarching issue. The concluding synthesising presentation must present relevant theoretical and methodological issues along with perspectives relating to the whole. The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed with and approved by the thesis supervisor",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_51"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". The thesis topic and the thesis statement must be relevant to the degree programme and must be agreed with and approved by the thesis supervisor. Master’s thesis is only taken for assessment if the supervision plan has been approved. The supervision plan must be approved prior to each examination attempt. If the master’s thesis does not contain a summary the assignment is not taken for assessment. The compulsory commencement of studies exam will be held before the end of September on the first semester of the Master’s degree programme.The exam generates 0 ECTS credits and is a pass/fail internal examination with no co-examiner. Two attempts to pass this exam are allowed. First-semester students will be registered for the exam automatically. The result will be posted in the student’s self-service system (mit.au.dk). Students who do not pass their first examination attempt will be registered automatically for their second attempt.The result of the second attempt will be posted in the student’s self-service system (mit.au.dk) no later than 28 September. Students who fail both their first and second attempt will be withdrawn from their degree programme with immediate effect and without further warning. Students may apply for advance approval of degree programme elements which are subsequently completed at another university or institution of higher education in Denmark or abroad. An approved application for advance approval of transfer credit obligates the student to document the results – both passed and failed degree programme elements, as soon as these marks are available. The board of studies may – if necessary – procure the necessary documentation directly. The student will then receive transfer credit for the passed degree programme elements.Applications for advance approval of transfer credit and submission of documentation for completed degree programme elements with a view to registration of transfer credit must take place in accordance with the rules published on the study portal",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_52"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". In addition, the board of studies may approve applications for transfer credit for degree programme elements which have been completed at an institution of higher education in Denmark or abroad.When the board of studies has approved the credit transfer of a passed course/course element from an institution of higher education in Denmark or abroad, credit must be transferred as ‘Passed’. If the course element in question is assessed in accordance with the 7-point grading scale at both institutions, the assessment is transferred with the grade given, cf. the Degree Programme Order and the Examination Order. Registering for compulsory courses, elective courses and examsRegistering for compulsory courses and elective courses:Students will be registered automatically for teaching in compulsory courses in the order of semesters prescribed in the academic regulations. Students are responsible for ensuring that they are always registered correctly for courses and elective courses. Students must register for and (when necessary) prioritise elective courses by the set deadline, which can be found on the study portal. After this deadline, elective courses become a compulsory part of the student’s degree programme and cannot be changed. Students who stick to the prescribed curriculum will be given priority when places are assigned for compulsory courses. After the deadline for registration, places can only be assigned to students if there is room in the classroom, and only until the start of the semester.Selection criteria for elective coursesThere may be a limit to the number of students admitted to elective courses, and the creation of such courses is contingent on the number of students who register for them. The criteria for allocating places on elective courses which only admit a limited number of students are stated on the study portal. (studerende.au.dk/arts). Registering for and withdrawing from ordinary exams and re-examinations:Students will be registered automatically for ordinary exams in compulsory courses",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_53"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". (studerende.au.dk/arts). Registering for and withdrawing from ordinary exams and re-examinations:Students will be registered automatically for ordinary exams in compulsory courses. Students are responsible for ensuring that they are always registered correctly for exams. Students are responsible for registering for re-examinations by the set deadline, which can be found on the study portal and in the exam plan. Students can only register for re-examinations if they have already tried to pass the ordinary exam before the re-examination. Students can withdraw from exams using the self-service system at mitstudie.au.dk, following the procedure described on the study portal. Students cannot withdraw from exams after the deadline, and the failure to take part in an exam for which they are registered will count as a failed attempt to pass it. The deadlines for registration and withdrawal can be found on the study portal and in the exam plan. Students cannot withdraw from the Master’s thesis. Forms of exam for ordinary exams and re-examinationsThe form of exam for ordinary exams and re-examinations is stated in the academic regulations. The ordinary form of exam is offered immediately after courses have been taught.Students taking exams in the re-examination period will be examined using the form of re-examination that has been specified. If the ordinary exam and the re-examination both use the same form of exam, students are entitled to apply for a re-examination in the ordinary exam period immediately after the course teaching has been completed. Deadlines are stated on the study portal. Students should not expect any requests submitted after the deadline to be granted. Students can choose between the specified forms of exam for their Master’s thesis. Options for project-oriented courses are stated in the description of the individual study element. It is possible to take project placement with more than one project host if it is deemed to live up to all requirements for project placement",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_54"
  },
  {
    "document_type": "online_article",
    "title": "Master's Degree Programme in Cognitive Science (2022)",
    "author": "Aarhus University",
    "source": "https://eddiprod.au.dk/EDDI/webservices/DokOrdningService.cfc?method=visGodkendtOrdning&dokOrdningId=17274&sprog=en",
    "date_published": "2022",
    "flag": "",
    "chunk_text": ". It is possible to take project placement with more than one project host if it is deemed to live up to all requirements for project placement. Complaints about exams should be submitted to the university. For the application to be processed, it must be made in writing. The complaint must state both the reason for the complaint and what the complainant expects to achieve. Complaints about exams must be submitted within fourteen days after the deadline for the publication of exam results, cf. the Examination Order. More information about exam complaints is available on the study portal. 08.11.2024:Adjustment of the display of \"Admission requirements and prerequisites\" and \"Students with a legal right of admission\" 01.09.2024:Updating of general ruleAdded description of model for Calculation of average grade pointThe rules on cheating on exams have been changed so that Generative Articifial Intelligence (GAI) is a permitted aid at exams, unless the use of aids is restricted in specific courses. Admission requirements: Addition of requirements for applicants withOther qualifications Master’s thesis: Clarification of summary as formal requirement 01.03.2024:Due to a change of exam system, the mentioning of Digital Exam has been replaced with WISEflow 16.01.2024:Updating of general rules 01.09.2023:Updating of general rulesUpdate of:Section 2.1: Change of description of the rules for prerequisite requirementsThesis: Clarification requirement for submission of a supervision plan 13.01.2023: - Corrected mistake on length of oral exam for Human Computer Interaction.",
    "chunk_id": "Human_computer_interaction_master's_degree_programme_in_cognitive_science_(2022).json_chunk_55"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 57 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00005-2 Copyright © 2021 Elsevier Inc. All rights reserved. Chapter 4 explained how the human visual system differs from a digital camera in the way it detects and processes color. Our visual system also differs from a camera in its resolution. On a digital camera’s photosensor, photoreceptive elements are spread uniformly in a tight matrix, so spatial resolution is constant across the entire image frame. The human visual system is not like that. This chapter explains why: l Stationary items in muted colors presented in the periphery of a person’s visual field often will not be noticed. l Motion in the periphery is usually noticed. RESOLUTION OF THE FOVEA COMPARED WITH THE PERIPHERY The spatial resolution of the human visual field drops greatly from the center to the edges for three reasons: l Pixel density . Each eye has six to seven million retinal cone cells. They are packed much more tightly in the center of our visual field—a small region called the fovea— than at the edges of the retina (see Fig. 5.1 ). The fovea has about 158,000 cone cells in each square millimeter. The rest of the retina has only 9000 cone cells per square millimeter. l Data compression . Cone cells in the fovea connect 1:1 to the ganglial neuron cells that begin the processing and transmission of visual data, while elsewhere on the retina, multiple photoreceptor cells (cones and rods) connect to each ganglion cell. In technical terms, information from the visual periphery is copressed (with data loss) before transmission to the brain, while information from the fovea is not compressed. Our Peripheral Vision is Poor 5 l Processing resources . The fovea is only about 1% of the retina, but the brain’s visual cortex devotes about 50% of its area to processing input from the fovea. The other half of the visual cortex processes data from the remaining 99% of the retina",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The other half of the visual cortex processes data from the remaining 99% of the retina. The result is that our vision has much, much greater resolution in the center of our visual field than elsewhere (Lindsay and Norman, 1972). Stated in developer jargon: in the center 1% of your visual field (i.e., the fovea), you have a high-resolution TIFF, and everywhere else, you have only a low-resolution JPEG. That is nothing like a digital camera. To visualize how small the fovea is compared with your entire visual field, hold your arm straight out and look at your thumb. Your thumbnail, viewed at arm’s length, coresponds approximately to the fovea (Ware, 2008). While you have your eyes focused on the thumbnail, everything else in your visual field falls outside your fovea. In the fovea, people with normal vision have very high resolution: they can resolve several thousand dots within that region—better resolution than many of today’s pocket digital cameras. Just outside the fovea, the resolution is already down to a few dozen dots per inch viewed at arm’s length. At the edges of our vision, the “pixels” of our visual system are as large as a melon (or human head) at arm’s length (see Fig. 5.2 ). Even though our eyes have more rods than cones—125 million versus 6 to 7 milion—peripheral vision has much lower resolution than foveal vision. This is because while most of our cone cells are densely packed in the fovea (in 1% of the retina’s area), the rods are spread out over the rest of the retina (in 99% of the retina’s area). In people with normal vision, peripheral vision is about 20/200, which in the United States is considered legally blind. Think about that: in the periphery of your visual Blind spot s d o R s d o R s e n o C s e n o C 180,000 160,000 140,000 120,000 Number of receptors per square millimeter 100,000 80,000 60,000 40,000 20,000 0 70 60 50 40 30 20 10 0 Angle (deg) 10 20 30 40 50 60 70 80 FIGURE 5.1 Distribution of photoreceptor cells (cones and rods) across the retina",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". (From Lindsay, P., Norman, D.A., 1972. Human Information Processing. Academic Press, New York and London.) field—which is most of it—you are legally blind. Here is how brain researcher David Eagleman (2012, p. 23) describes it: The resolution in your peripheral vision is roughly equivalent to looking through a frosted shower door, and yet you enjoy the illusion of seeing the periphery clearlyWherever you cast your eyes appears to be in sharp focus, and therefore you assume the whole visual world is in focus. If our peripheral vision has such low resolution, one might wonder why we don’t see the world in a kind of tunnel vision where everything is out of focus except what we are directly looking at now. Instead, we seem to see our surroundings sharply and clearly all around us. We experience this illusion because our eyes move rapidly and constantly about three times per second even when we don’t realize it, focusing our fovea on selected pieces of our environment. Our brain fills in the rest in a gross, impressionistic way based on what we know and expect. 1 Our brain does not have to maintain a high-resolution mental model of our environment because it can order the eyes to sample and resample details in the environment as needed (Clark, 1998). For example, as you read this page, your eyes dart around, scanning and reading. No matter where on the page your eyes are focused, you have the impression of viewing a complete page of text because, of course, you are. 1 Our brains also fill in perceptual gaps that occur during rapid (saccadic) eye movements, when vision is suppressed (see Chapter 14). FIGURE 5.2 The resolution of our visual field is high in the center but much lower at the edges. (Right image from Vision Research, vol. 14, 1974. Elsevier.) But now imagine this: you are viewing the page on a computer screen. The coputer is tracking your eyes and knows where your fovea is focused. The computer shows the correct text for that spot, but everywhere else the computer shows radom meaningless text",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The coputer is tracking your eyes and knows where your fovea is focused. The computer shows the correct text for that spot, but everywhere else the computer shows radom meaningless text. As your fovea zips around the page, the computer instantly updates each area where your fovea pauses to show the correct text there, while the previous position of your fovea returns to textual noise. Amazingly, experiments have shown that people rarely notice this: not only can they read, they believe that they are viewing a full page of meaningful text (Clark, 1998). However, it does slow people’s reading even if they don’t realize it (Larson, 2004). Perhaps the best demonstration that our peripheral vision does not see as much detail as our foveal vision is Ninio’s Extinction Illusion (see Fig. 5.3 ). Try to count the dots. Only dots that are in the center of your visual field are visible even though the entire grid appears to be in sharp focus. This shows that what we perceive as higresolution peripheral vision is actually an artificial construction of our visual system filling in what it expects to be there. The fact that retinal cone cells are distributed tightly in and near the fovea, and sparsely in the periphery of the retina, affects not only spatial resolution but also color resolution. We can discriminate colors better in the center of our visual field than at the edges. Another interesting fact about our visual field is that it has a gap—a small area (blind spot) in which we see nothing. The gap corresponds to the spot on our retina where the optic nerve and blood vessels exit the back of the eye (see Fig. 5.1 ). There are no retinal rod or cone cells at that spot, so when the image of an object in our FIGURE 5.3 Ninio’s extinction illusion. You see dots only by looking directly at them. visual field happens to fall on that part of the retina, we don’t see it",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You see dots only by looking directly at them. visual field happens to fall on that part of the retina, we don’t see it. We usually don’t notice this hole in our vision because our brain fills it in with the surrounding content, like a graphic artist using Photoshop to fill in a blemish on a photograph by copying nearby background pixels. People sometimes experience the blind spot when they gaze at stars. As you look at one star, a nearby star may disappear briefly into the blind spot until you shift your gaze. You can also observe the gap by trying the exercise in Fig. 5.4 . Some people have other gaps resulting from imperfections on the retina, retinal damage, or brain strokes that affect the visual cortex, 2 but the optic nerve gap is an imperfection everyone shares. IS THE VISUAL PERIPHERY GOOD FOR ANYTHING? It seems that the fovea is better than the periphery at just about everything. One might wonder why we have peripheral vision. What is it good for? Our peripheral vision serves three important functions: it guides our fovea toward objects and events that match our goals, it detects motion and guides our fovea there, and it lets us see better in the dark. Function 1: guides fovea toward objects matching goals Remember what Chapter 1 explained: our perception is biased by our goals. Periperal vision provides low-resolution cues to guide our eye movements so that our fovea visits the interesting and crucial parts of our visual field. Our eyes don’t scan our envronment randomly. They move so as to focus our fovea on important things—things related to our goals and to possible threats—the most important ones (usually) first. Think of peripheral vision as our visual system’s “forward patrol,” primed to notice important things and report them to “central command.” Thus, the fuzzy cues in the periphery of our visual field provide the data that helps our brain plan where to move our eyes and attention",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, when we scan a medicine label for a “use by” date, a fuzzy blob in the periphery with the vague form of a date is enough to cause an eye movement that lands the fovea there to allow us to check it. If we are browsing a produce market 2 See VisualSimulations.com . FIGURE 5.4 To “see” the retinal gap, cover your left eye, hold this book near your face, and focus your right eye on the + . Move the book slowly away from you, staying focused on the + . The @ will disappear at some point. looking for strawberries, a blurry reddish patch at the edge of our visual field draws our eyes and our attention, even though sometimes it will be radishes instead of straberries. If we hear an animal growl nearby, a fuzzy animal-like shape in the corner of our eye will be enough to zip our eyes in that direction, especially if the shape is moing toward us (see Fig. 5.5 ). And as shown in Chapter 1 , where we look on a web page or app screen depends on what on the display matches our goals. How peripheral vision guides and augments central, foveal vision is discussed more in the “ Visual Search is Linear Unless Targets ‘Pop’ in the Periphery ” section later in this chapter. Function 2: detects motion A related guiding function of peripheral vision is that it is good at detecting motion. Anything that moves in our visual periphery, even slightly, is likely to draw our attetion—and hence our fovea—toward it. The reason for this phenomenon is that our ancestors—including prehuman ones—were selected for their ability to spot food and avoid predators. As a result, even though we can move our eyes under conscious, intentional control, some of the mechanisms that control where our eyes look are preconscious, involuntary, and very fast. What if we have no reason to expect that there might be anything interesting in a certain spot in the periphery, 3 and nothing in that spot attracts our attention? Our eyes may never move our fovea to that spot, so we may never see what is there",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3 See Chapter 1 on how expectations bias our perception. FIGURE 5.5 A moving shape at the edge of our vision draws our eye: it could be food, or it might consider us food. Function 3: lets us see better in the dark A third function of peripheral vision is to allow us to see in low-light conditions— for example, on starlit nights, in caves, around campfires, etc. These were conditions under which vision evolved and in which people—like the animals that preceded them on Earth—spent much of their time until the invention of the electric light bulb in the 1800s. Just as the rods are less functional in well-lighted conditions (see Chapter 4 ), the cones don’t function very well in low light, so our rods take over. Low-light, rods-only vision is called scotopic vision . An interesting fact is that because there are no rods in the fovea, you can see objects better in low-light conditions (e.g., faint stars) if you don’t look directly at them. EXAMPLES FROM COMPUTER USER INTERFACES The low acuity of our peripheral vision explains why users of software and websites fail to notice error messages in some applications and websites. When someone clicks a button or link, that is usually where his or her fovea is positioned. Everything on the screen not within 1–2 cm of the click location (assuming a normal computer viewing distance) is in peripheral vision, where resolution is low. If after the click, an error message appears in the periphery, it should not be surprising if the person does not notice it. For example, at Informaworld.com , the former online publication website of Informa Healthcare, if a user entered an incorrect username or password and clicked “Sign In,” an error message appeared in a “message bar” far away from where the users’ eyes were most likely focused (see Fig. 5.6 ). The red word “Error” might appear in the user’s peripheral vision as a small reddish blob, which would help draw the eyes in that direction",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5.6 ). The red word “Error” might appear in the user’s peripheral vision as a small reddish blob, which would help draw the eyes in that direction. However, the red blob might fall into a gap in the viewer’s visual field and not be noticed at all. An alert message displayed by Delta Airlines’ (2017) website is similarly easy to miss (see Fig. 5.7 ). Customers sometimes encountered it when clicking through seaselection steps of the flight check-in process. Users’ attention almost certainly would be focused on the Previous Flight/First Flight buttons at the lower right of the screen. The alert message is not as far from where site visitors would be looking as for the one at Informaworld.com , but it is tiny and insufficiently highlighted. Consider the sequence of events from a user’s point of view. The user enters a usename and password and then clicks “Sign In.” The page redisplays with blank fields. The user thinks “Huh? I gave it my login information and hit ‘Sign In,’ didn’t I? Did I hit the wrong button?” The user reenters the username and password and clicks “Sign In” again. The page redisplays with empty fields again. Now the user is really confused. The user sighs (or curses), sits back in his chair, and lets his eyes scan the screen. Sudenly noticing the error message, the user says “A-ha! Has that error message been there all along?” (UURU0HVVDJH )RYHD FIGURE 5.6 The error message from the former Taylor & Francis Informaworld website (2010) for a faulty login appeared in peripheral vision, where most users would probably not see it. FIGURE 5.7 Error message at Delta.com (2017). Can you spot it? Even when an error message is placed nearer to the center of the viewer’s visual field than in the preceding examples, other factors can diminish its visibility. For exaple, from about 2003 to2008, 4 the website of Airborne Express (now part of DHL) signaled a login failure by displaying an error message in red just above the Login ID field (see Fig. 5.8 )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For exaple, from about 2003 to2008, 4 the website of Airborne Express (now part of DHL) signaled a login failure by displaying an error message in red just above the Login ID field (see Fig. 5.8 ). The error message was entirely in red and fairly near the “Login” button where a user’s eyes and attention would be focused. Can you think of any resons people might not initially see it? 4 Although this example is old, it is still the best example of this issue I have found. FIGURE 5.8 This error message for an invalid login was easy to miss even though it is not far from the “Login” button. Why? FIGURE 5.9 Simulation of a user’s visual field while the fovea is fixed on the “Login” button. One reason is that the error message was still in peripheral vision, not the fovea. The fovea is small: just a centimeter or two on a computer screen, assuming the user is a normal distance from the screen. A second reason is that the error message was not the only red thing near the top of the page. The page title was also red. Resolution in the periphery is very low. When the error message appeared, a user’s peripheral vision might not register any change—there was a red blotch there before, and with the error message, there still was (see Fig. 5.9 ). If the page title were black or any other color besides red, the red error message would be more likely to be noticed, even though it appeared in the periphery of the user’s visual field. COMMON METHODS OF MAKING MESSAGES VISIBLE Several common and well-known methods can ensure that error messages are seen: l Put it where users are looking . People focus on predictable places when interacting with graphical user interfaces. In Western societies, people tend to traverse forms and control panels from upper left to lower right. While moving the screen pointer, people usually look either at where it is or where they are moving it to. When people click a button or link, they can usually be assumed to be looking directly at it, at least for a few moments afterward",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When people click a button or link, they can usually be assumed to be looking directly at it, at least for a few moments afterward. Designers can use this predictability to position error messages near where they expect users to be looking. l Mark the error . Somehow, mark the error prominently to indicate clearly that something is wrong. Often this can be done by simply placing the error message near what it refers to, unless that would place the message too far from where users are likely to be looking. l Use an error symbol . Make errors or error messages more visible by marking them with an error symbol, such as , , , or . l Reserve red for errors . By convention, interactive computer systems use the color red to connote alert , danger , error , etc. Using red for any other information on a computer display risks misinterpretation. But suppose you are designing a website for Stanford University, which has red as its school color. Or suppose you are designing for a Chinese market, where red is considered an auspicious, postive color. What do you do? Use another color for errors, mark them with error symbols, or use stronger methods (see the next section). The most recent Taylor & Francis login error screen uses several of these tecniques (see Fig. 5.10 ). Compare it with their earlier website (see Fig. 5.6 ). A bigger, bolder font for the error message and an error symbol would improve this, but it is much better than in the 2010 site. FIGURE 5.10 In the latest Taylor & Francis website (2020), the error message for a faulty login is displayed near where users will be looking, and both the error message and the login field are highlighted in red (compare with Fig. 5.6 ). FIGURE 5.11 Salesforce.com’s mobile site displays error messages prominently, midscreen. Salesforce.com’s mobile app displays error messages in a way that makes them hard to miss (see Fig. 5.11 ). They are marked with an error symbol and displayed in red in the middle of the screen",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Salesforce.com’s mobile app displays error messages in a way that makes them hard to miss (see Fig. 5.11 ). They are marked with an error symbol and displayed in red in the middle of the screen. HEAVY ARTILLERY FOR MAKING USERS NOTICE MESSAGES If the common, conventional methods of making users notice messages are not enough, three stronger methods are available to user-interface designers: a pop-up message in an error dialogue box, use sound (e.g., a beep), and wiggle or blink briefly. However, these methods, while very effective, have significant negative effects, so they should be used sparingly and with great care. Method 1: a pop-up message in an error dialogue box Displaying an error message in a dialogue box sticks it right in the user’s face, making it hard to miss. Error dialogue boxes interrupt the user’s work and demand immedate attention. That is good if the error message signals a critical condition, but it can annoy people if such an approach is used for a minor message, such as confirming the execution of a user-requested action. The annoyance of pop-up messages rises with the degree of modality. Nonmodal pop-ups allow users to ignore them and continue working. Application-modal poups block any further work in the application that displayed the error but allow users to interact with other software on their computer. System-modal pop-ups block any user action until the dialogue has been dismissed. Application-modal pop-ups should be used sparingly—for example, only when application data may be lost if the user doesn’t attend to the error. System-modal pop-ups should be used extremely rarely—basically only when the system is about to crash, taking hours of work with it, or if people will die if the user misses the error message. On the Web, an additional reason to avoid pop-up error dialogue boxes is that some people set their browsers to block all pop-up windows. If your website relies on poup error messages, some users may never see them",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If your website relies on poup error messages, some users may never see them. REI.com has an example of a pop-up dialogue being used to display an error mesage. The message is displayed when someone who is registering as a new customer omits any required field in the form (see Fig. 5.12 ). Is this an appropriate use of a pop-up dialogue? The improved Taylor & Francis error message display (see Fig. 5.10 ) shows that data entry errors can be signaled well without pop-up dialogues, so REI. com’s use of them seems a bit heavy-handed. Examples of more appropriate use of error dialogue boxes come from Microsoft Excel (see Fig. 5.13A ) and Adobe InDesign (see Fig. 5.13B ). In both cases, loss of data is at stake. Method 2: use sound (e.g., a beep) When a computer beeps, it tells its user something has happened that requires attetion. The person’s eyes reflexively begin scanning the screen for whatever caused the beep. This can allow users to notice an error message someplace other than where they were just looking, such as in a standard error message box on the display. That is the value of beeping. FIGURE 5.12 REI.com’s pop-up dialogue box signals required data that was omitted. It is hard to miss, but perhaps overkill. (A) (B) FIGURE 5.13 Appropriate pop-up error dialogues: (A) Microsoft Excel and (B) Adobe InDesign. However, imagine many people in a cubicle work environment or a classroom, all using an application that signals all errors and warnings by beeping. Such a worplace would be very annoying to say the least. Worse, people would not be able to tell whether their own computer or someone else’s was beeping. The opposite situation is noisy work environments (e.g., factories or computer server rooms), where auditory signals emitted by an application might be masked by ambient noise. Even in nonnoisy environments, some computer users simply prefer quiet and mute the sound on their computers or turn it way down",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Even in nonnoisy environments, some computer users simply prefer quiet and mute the sound on their computers or turn it way down. For these reasons, signaling errors and other conditions with sound are remedies that can be used only in very special, controlled situations. Computer games often use sound to signal events and conditions. In games, sound isn’t annoying; it is expected. Its use in games is widespread, even in game arcades where doens of machines are all banging, roaring, buzzing, clanging, beeping, and playing music at once. (Well, it is annoying to parents who have to go into the arcades and endure all the screeching and booming to retrieve their kids, but the games aren’t designed for them.) Method 3: wiggle or blink briefly As described earlier in this chapter, our peripheral vision is good at detecting motion, and motion in the periphery causes reflexive eye movements that bring the motion into the fovea. User-interface designers can make use of this by wiggling or flashing messages briefly when they want to ensure that users see them. It does not take much motion to trigger eye movement toward it. Just a tiny bit of motion is enough to make a viewer’s eyes zip over in that direction. Millions of years of evolution have had quite an effect. As an example of using motion to attract users’ eye attention, Apple’s iCloud online service briefly shakes the entire dialogue box horizontally when a user enters an invalid username or password (see Fig. 5.14 ). In addition to clearly indicating “No” (like a person shaking his head), this attracts the users’ eyeballs, guaranteed. Because after all, the motion in the corner of your eye might be a leopard. The most common use of blinking in computer user interfaces (other than advetisements) is in menu bars. When an action (e.g., Edit or Copy) is selected from a menu, it usually blinks once before the menu closes to confirm that the system “got” the command—that is, that the user didn’t miss the menu item. This use of blinking is very common",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This use of blinking is very common. It is so quick that most computer users aren’t even aware of it, but if menu items didn’t blink once, we would have less confidence that we actually selected them. Motion and blinking, like pop-up dialogue boxes and beeping, must be used sparingly. Most experienced computer users consider wiggling, blinking objects oscreen to be annoying. Most of us have learned to ignore displays that blink because many such displays are advertisements. Conversely, a few computer users have attetional impairments that make it difficult for them to ignore something blinking or wiggling. Therefore, if wiggling or blinking is used, it should be brief—it should last about a quarteto a half-second, no longer. Otherwise, it quickly goes from an unconscious attention-grabber to a conscious annoyance. Use heavy-artillery methods sparingly to avoid habituating your users There is one final reason to use the preceding heavy-artillery methods sparingly (i.e., only for critical messages): to avoid habituating your users. When pop-ups, sound, motion, and blinking are used too often to attract users’ attention, a psychological phenomenon called habituation sets in (see Chapter 1 ). Our brain pays less and less attention to any stimulus that occurs frequently. It is like the old fable of the boy who cried “Wolf!” too often. Eventually, the vilagers learned to ignore his cries, so when a wolf actually did come, his cries went unheeded. Overuse of strong attention-getting methods can cause important mesages to be blocked by habituation. VISUAL SEARCH IS LINEAR UNLESS TARGETS “POP” IN THE PERIPHERY As explained earlier, one function of peripheral vision is to drive our eyes to focus the fovea on important things—those that match our goals or that might be a threat. For example, objects moving in our peripheral vision fairly reliably “pull” our eyes and FIGURE 5.14 Apple’s iCloud shakes the dialogue box briefly on login errors to attract a user’s fovea toward it. attention toward them",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". attention toward them. Similarly, fuzzy, blurred images related to our current goals also attract our attention. This is why our perception—in this case our visual perception— is biased by our goals (see Chapter 1 ). Peripheral vision is a crucial component in visual search despite its low spatial and color resolution. When we are looking for an object, our entire visual system, including the periphery, primes itself to detect that object. It does so by sensitizing neural networks running from the retina to the visual cortex of the brain. The neural networks are sensitized to detect features of the sought object (Treisman and Gelade, 1980; Wolfe, 1994; Wolfe and Gray, 2007). However, how helpful the periphery is in aiding visual search depends strongly on the identifying features of the sought object and how distinct those features are from the features of other objects in our visual field. Look quickly at Fig. 5.15 and find the Z. To find the Z, you had to scan carefully through the characters until your fovea landed on it. In the lingo of vision researchers, the time to find the Z is linear : it depends approximately linearly on the number of distracting characters and the postion of the Z among them. Why? The features that define the shape of the letter Z—vetical lines and a diagonal line—do not distinguish it from the surrounding letters, so our peripheral vision cannot spot it. We can only find it when our fovea lands on it. In designer jargon, letter shape does not “pop out” (“pop” for short) in peripheral vision. Now look quickly at Fig. 5.16 and find the bold character. That was much easier (i.e., faster), wasn’t it? You did not have to scan your fovea carefully through all the characters. Your periphery quickly detected the boldness and determined its location, and because that is what you were seeking, your visual system moved your fovea there. Your periphery could not determine exactly what was bold—that is beyond its resolution and abilities—but it did locate the boldness",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Your periphery could not determine exactly what was bold—that is beyond its resolution and abilities—but it did locate the boldness. In vision-researcher lingo, the periphery was primed to look for boldness in parallel over its entire area, and boldness is a distinctive feature of the target, so searching for a bold target is nonlinear. In designer lingo, we say that boldness “pops” in the peripery, assuming that only the target is bold. Color “pops” even more strongly. Compare counting the L’s in Fig. 5.17 with couning the red characters in Fig. 5.18 . FIGURE 5.15 Finding the Z requires scanning carefully through the characters. What else makes things “pop” in the periphery? As described earlier, the periphery easily detects motion, so motion “pops.” Generalizing from boldness , we also can say that font weight “pops,” because if all but one of the characters on a display were bold, the non bold character would stand out. In general, a visual target will “pop out” in your periphery if it differs from surrounding objects in features that the loresolution peripheral vision can detect. The more distinctive the features of the target, the more it “pops,” assuming the periphery can detect those features. FIGURE 5.16 Finding the bold letter does not require scanning through everything. FIGURE 5.17 Counting L’s is hard—letter shape doesn’t “pop” among characters. FIGURE 5.18 Counting red characters is easy because color “pops.” Using peripheral “pop” in design Designers use peripheral “pop” to focus the attention of a product’s users as well as to allow users to find information faster. Chapter 3 described how visual hierarchy— titles, headings, boldness, bullets, and indenting—can make it easier for users to spot and extract the information they need from text. Glance back at Fig. 3.7B in Chapter 3 and see how the headings and bullets make the topics and subtopics “pop” so readers can go right to them. Many interactive systems use color to indicate status, usually reserving red for prolems",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Many interactive systems use color to indicate status, usually reserving red for prolems. Online maps and most vehicle GPS devices mark traffic jams with red so they stand out (see Fig. 5.19 ). Systems for controlling air traffic mark potential collisions FIGURE 5.19 Google Maps uses color to show traffic conditions. Red indicates traffic jams. in red (see Fig. 5.20 ). Applications for monitoring servers and networks use color to show the health status of assets or groups of them (see Fig. 5.21 ). These are all uses of peripheral “pop” to make important information stand out and visual search nonlinear. FIGURE 5.20 Air traffic control systems often use red to make potential collisions “pop” out. FIGURE 5.21 Paessler’s monitoring tool uses color to show the health of network components. When there are many possible targets Sometimes in displays of many items, any of them could be what the user wants. Examples include command menus (see Fig. 5.22A ) and app pallets (see Fig. 5.22B ). Assume that the application cannot anticipate which item or items a user is likely to want and highlight them. That is a fair assumption for today’s applications. 5 Are users doomed to have to search linearly through such displays for the item they want? 5 But in the not-too-distant future it might not be. (A) (B) FIGURE 5.22 (A) Microsoft Word tools menu and (B) macOS application pallet. That depends. Designers can try to make each item so distinctive that when a specific one is the user’s target, the user’s peripheral vision will spot it among all the other items. Designing distinctive sets of icons is hard—especially when the set is large—but it can be done (see Johnson et al., 1989). Designing sets of icons so distinctive that they can be distinguished in peripheral vision is very hard, but not impossible",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Designing sets of icons so distinctive that they can be distinguished in peripheral vision is very hard, but not impossible. For example, if a user goes to the macOS application pallet to open his or her calendar, a white rectangular blob in the periphery with something black in the middle is more likely to attract the user’s eye than a blue circular blob (see Fig. 5.22B ). The trick is to not get too fancy and detailed with the icons—give each a distinctive color and gross shape. On the other hand, if the potential targets are all words, as in command menus (see Fig. 5.22A ), visual distinctiveness is not an option. In textual menus and lists, visual search will be linear, at least at first. With practice, users learn the positions of frequently used items in menus, lists, and pallets, so searching for particular items is no longer linear. That is why applications should never move items around in menus, lists, or pallets. Doing that prevents users from learning item positions, thereby dooming them to search linearly forever. Therefore, the use of “dynamic menus” is considered a major user-interface design mistake (Johnson, 2007). IMPORTANT TAKEAWAYS l Unlike digital cameras, the resolution of human vision is much higher in a small area in the middle of our visual field than it is everywhere else. The small area is called the fovea , and it makes up only about 1% of our visual field. Our peripheral vision has very low resolution. l Peripheral vision guides our eyes and attention toward objects and events that either match our goals or represent possible threats. Peripheral vision can detect motion and tends to move our eyes toward whatever is moving even though it cannot identify what is moving. l Peripheral vision is good in low-light situations. l Some visual features “pop out” in peripheral vision, and some do not. Font weight pops. Color pops. Motion pops. Letter shape does not pop",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Peripheral vision is good in low-light situations. l Some visual features “pop out” in peripheral vision, and some do not. Font weight pops. Color pops. Motion pops. Letter shape does not pop. l Designing based on the strengths and weaknesses of peripheral vision: l Place new, important, or changed information where users will be looking—in or near where their fovea is positioned. Information placed elsewhere may not be noticed. l Use color, motion, a distinctive shape, etc. to make important information “pop” in peripheral vision to attract users’ fovea and attention. Red is comonly used to attract attention to error messages. l Overuse of any stimulus causes people to habituate to it, diminishing its ability to attract attention. Pop-up messages can in principle force users to attend to them, but they are often overused, and therefore many experienced users of digital technology have learned to ignore them. l Sound can attract a user’s attention, but it can also be annoying, especially in environments shared with other people.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-57-78.json_chunk_19"
  },
  {
    "document_type": "online_article",
    "title": "Natural Mappings and Stimulus-Response Compatibility in User Interface Design",
    "author": "Katie Sherwin",
    "source": "https://www.nngroup.com/articles/natural-mappings/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "October 14, 20182018-10-14 Share Have you ever played a video game in which the controls were reversed? In other words, right meant left and left meant right? If you did, you probably know that it’s not easy. It is an example of stimulus–response incompatibility. Back in the 1950s, the psychologists Paul Fitts (well known in human–computer interaction for thelaw that bears his name) and Charles Seeger ran a series of experiments which demonstrated this concept. They asked people to respond to a stimulus that appeared on their left or right side by pressing two buttons: Fitts found that participants were faster in the first condition, which presented a stimulus–response compatibility. Definition:Stimulus–response compatibilityis the degree to which the physical arrangement of the stimuli matches the location of the expected responses. Stimulus–response compatibility is highly relevant for design. When an object or an interface violates it, its usability decreases significantly. Consider these two stoves. Which knob controls which burner? Does one stove make you feel more confident in your guess? Consider this picture I took not long ago of a ticket machine at a train station. After paying with a credit card, the machine asked for my ZIP code as a security measure. But, to my confusion, the yellow numbered buttons were not touchable. Dumbfounded, I repeatedly tapped the yellow on-screen numbers, thinking that maybe the screen was frozen or had a slow response time. Then, I noticed the column of metal buttons on either side of the screen. Each column was numbered 1-6. To my surprise (and horror), pressing the button marked ‘1’ on the right column, caused a ‘6’ to appear in the zip code field. Pressing the metal button for 2 caused a ‘7’ to appear on the screen. I couldn’t believe it. It felt physically and cognitively unnatural to press one number to get a different number to appear. My slow reaction time to press each button was a direct result of stimulus–response incompatibility",
    "chunk_id": "Human_computer_interaction_natural_mappings_and_stimulus-response_compatibility_in_user_interface_design.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Natural Mappings and Stimulus-Response Compatibility in User Interface Design",
    "author": "Katie Sherwin",
    "source": "https://www.nngroup.com/articles/natural-mappings/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". My slow reaction time to press each button was a direct result of stimulus–response incompatibility. The concept of stimulus–response compatibility can be extended beyond its spatial dimension. This is exactly what Don Norman does in his book “The Design of Everyday Things”. Definition:Natural mappingrefers to a design in which the system’s controls represent or correspond to the desired outcome. When controls map to the actions that will result, systems are faster to learn and easier to remember. Natural mappings bridge thegulf of execution, helping users to understand how a system can be used and what actions are required to accomplish their goal. Designers can create natural mappings in several ways, including the three common patterns below: For a system to be easy to learn, people need to quickly understand how it works, what actions are possible, and how to accomplish their goals. Designs that have a high stimulus–response compatibility and take advantage of natural mappings can help users act faster and with more confidence. Fitts, P. M., & Seeger, C. M. (1953). \"S-R compatibility: spatial characteristics of stimulus and response codes.\"Journal of Experimental Psychology, 46(3), 199-210. Lakoff, G. & Johnson, M. (2003).Metaphors we live by. Chicago: University of Chicago Press. Norman, D. (2013).The design of everyday things. New York, New York: Basic Books",
    "chunk_id": "Human_computer_interaction_natural_mappings_and_stimulus-response_compatibility_in_user_interface_design.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Natural Mappings and Stimulus-Response Compatibility in User Interface Design",
    "author": "Katie Sherwin",
    "source": "https://www.nngroup.com/articles/natural-mappings/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Lakoff, G. & Johnson, M. (2003).Metaphors we live by. Chicago: University of Chicago Press. Norman, D. (2013).The design of everyday things. New York, New York: Basic Books. Steering Law for Cursor and Mouse Movements in a GUI Tunnel Lexie Kane·3 min Fitts's Law Lexie Kane·2 min Observe, Test, Iterate, and Learn (Don Norman) Don Norman·4 min Signal–to–Noise Ratio Xinyi Chen·6 min Computer-Assisted Embarrassment Susan Farrell·10 min The Role of Enhancement in Web Design Raluca Budiu·7 min Why Designers Think Users Are Lazy: 3 Human Behaviors Kara Pernice·8 min Apple's products are getting harder to use because they ignore principles of design Don Norman· Don’t Prioritize Efficiency Over Expectations: The Case of Autosave Aurora Harley·7 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_natural_mappings_and_stimulus-response_compatibility_in_user_interface_design.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Miller’s Law The average person can keep only 7 ( ± 2) items in their working memory. Key Takeaways • Don’t use the “magical number seven” to justify unnecessary design limitations. • Organize content into smaller chunks to help users process, understand, and memorize easily. • Remember that short-term memory capacity will vary per indiviual, based on their prior knowledge and situational context. Overview It’s likely that many designers will have heard of Miller’s law, but there is also a high probability that their understanding of it is inaccurate. This commonly miunderstood heuristic has frequently been cited as justification for design decsions such as “the number of navigation items must be limited to no more than seven” and so forth. While there is value in limiting the number of options avaiable to users (see Chapter 3 ), it is misleading and inaccurate to attribute such dogma to Miller’s law. In this chapter, we’ll explore the origins of Miller’s “magcal number seven” and the real value Miller’s law has to provide UX designers. 35 1 George A. Miller, “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Prcessing Information,” Psychological Review 63, no. 2 (1956): 81–97. Origins Miller’s law originates from a paper published in 1956 by cognitive psychologist George Miller titled “The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information.” 1 Miller, a professor at Havard University’s Department of Psychology, discussed in his paper the coincdence between the limits of one-dimensional absolute judgment and the limits of short-term memory. Miller observed that memory span in young adults was approximately limited to 7, regardless of the stimuli consisting of vastly different amounts of information. This led him to the conclusion that bits, the basic unit of information, don’t affect memory span as much as the number of information chunks being memorized",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-35-42.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This led him to the conclusion that bits, the basic unit of information, don’t affect memory span as much as the number of information chunks being memorized. The term “chunks” in cognitive psychology refers to collections of basic familiar units that have been grouped together and stored in a person’s memory. Miller’s paper is often interpreted as arguing that the number of objects an average human can hold in short-term memory is 7±2. Miller himself only used the expression “the magical number seven” rhetorically and was surprised by its frequent misinterpretation. Later research on short-term memory and working memory revealed that memory span is not a constant even when measured in “chunks.” PSYCHOLOGY CONCEPT Chunking Miller’s fascination with short-term memory and memory span centered not on the number seven, but on the concept of chunking and our ability to memorize information accordingly. He found that the size of the chunks did not seem to matter—seven individual words could be held in short-term memory as easily as seven individual letters. While there are factors that influence how many chunks a given individual can retain (context, familiarity with the content, specific capacity), the takeaway is the same: human short-term memory is limited, and chunking helps us retain information more effectively. When applied to UX design, chunking informs an incredibly valuable approach to content. When we chunk content in design, we are effetively making it easier to comprehend. Users can then scan the content, identify the information that aligns with their goal(s), and consume that information to achieve their goal(s) more quickly. By structuring content into visually distinct groups with a clear hierarchy, we can align the infomation we present with how people evaluate and process digital content. Next, we’ll take a look at a few ways this can be achieved. Examples The simplest example of chunking can be found in how we format phone nubers",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-35-42.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Next, we’ll take a look at a few ways this can be achieved. Examples The simplest example of chunking can be found in how we format phone nubers. Without chunking, a phone number would just be a long string of digits— significantly more than seven—making it difficult to process and remember. A phone number that has been formatted (chunked) is much easier to both process and memorize ( Figure 4-1 ). Figure 4-1. A (US) phone number with and without chunking applied Let’s move on to a slightly more complex example. When browsing the web, you’ll inevitably be faced with the dreaded “wall of text” ( Figure 4-2 )—content that’s characterized by a lack of hierarchy or formatting and that exceeds the appropriate line length. It can be compared to the unformatted phone number example just given, but on a larger scale. This content is more difficult to scan and process, which has the effect of increasing the cognitive load on users. Figure 4-2. “Wall of text” example (source: Wikipedia, 2019) When we compare this example with content that has formatting, hierarchy, and appropriate line lengths applied, the contrast is significant. Figure 4-3 is an improved version of the same content. Headings and subheadings have been added to provide hierarchy, whitespace has been used to break the content into discernable sections, line length has been reduced to improve readability, text links have been underlined, and key words have been highlighted to provide cotrast with the surrounding text. Figure 4-3. “Wall of text” improved with hierarchy, formatting, and appropriate line lengths (source: Wikipedia, 2019) Now let’s take a look at how chunking is applied in a broader context. Chunking can be used to help users understand underlying relationships by grouping content into distinctive modules, applying rules to separate content, and providing hierarchy ( Figure 4-4 ). Especially in information-dense experieces, chunking can be leveraged to provide structure to the content",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-35-42.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Especially in information-dense experieces, chunking can be leveraged to provide structure to the content. Not only is the result more visually pleasing, but it’s also more scannable. Users who are skiming the latest headlines to determine which is worthy of their attention can quickly scan the page and make a decision. Figure 4-4. Example of chunking applied to dense information (source: Bloomberg, 2018) While chunking is incredibly useful for bringing order to information-dense experiences, it can be found in many other places as well. Take, for example, ecommerce websites such as Nike.com ( Figure 4-5 ), which uses chunking to group information related to each product. While individual elements might not share a background or surrounding border, they are visually chunked by their proximity to one another (product image, title, price, type of product, and finally total colors available). Additionally, Nike.com leverages chunking to group related filters in the lefthand sidebar. Figure 4-5. Chunking is commonly used to group products and filters on ecommerce websites (source: Nike.com , 2019) These examples demonstrate how chunking can be used to visually organize any content for easier comprehension. It helps those who are consuming the content to understand the underlying relationships and information hierarchy. What chunking does not do is dictate a specific limit on the number of items that can be shown at a given time or within a group. Rather, it’s simply a method for organizing content that makes it easier to quickly identify important information. KEY CONSIDERATION The Magical Number Seven Miller’s law is occasionally misunderstood to mean that there is a spcific limit to the number of items that can be stored and processed in short-term memory at one time (7 ± 2), and therefore the number of related interface elements should be limited to this range. A common example in regard to which this law is mistakenly cited is that of adjacent elements such as navigation links",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-35-42.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A common example in regard to which this law is mistakenly cited is that of adjacent elements such as navigation links. Perhaps you’ve heard someone metion in the past that navigation links must be limited to seven, citing Miler’s law as the justification. In reality, design patterns such as navigation menus don’t require people to memorize them—the choices available to them via the navigation menu are visible at all times. In other words, there’s no usability gain provided by limiting these links to a specific number. As long as the menu is designed effectively, users will be able to quickly identify the relevant link—the only memorization necessary is what their actual goal is. We’ll return to Nike.com and examine the primary navigation menu ( Figure 4-6 ) to see an example. As you can see, the navigation links nuber well beyond seven, yet it is still easy to scan the list thanks to clear categorization and the use of whitespace and vertical dividers to seprate subgroupings. Figure 4-6. Despite not limiting menu items to seven, the navigation on Nike.com is easily comprehensible (source: Nike.com , 2019) Miller’s findings were centered around the limitations of short-term memory and how it can be optimized by organizing bits of information into meaningful chunks. The actual limit to the number of chunks that can be stored will vary per individual based on their knowledge of the information, and there is even research that suggests the average limit is lower than what Miller’s research determined. Conclusion The sheer volume of information around us is growing at an exponential rate— but we humans have a finite amount of mental resources available to process that information. The inevitable overload that can occur has a direct effect on our abiity to complete tasks. Miller’s law teaches us to use chunking to organize content into smaller clusters to help users process, understand, and memorize easily.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-35-42.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "With Power Comes Responsibility In the previous chapters, we looked at how to leverage psychology to build more intuitive, human-centered products and experiences. We identified and explored some key principles from psychology that can be used as a guide for designing for how people actually are, instead of forcing them to conform to technology. This knowledge can be quite powerful for designers, but with power comes responsibility. While there’s nothing inherently wrong with leveraging the insights from behavioral and cognitive psychology to help create better designs, it’s critical that we consider how products and services have the potential to undermine the goals and objectives of the people using them , why accountability is critical for those creating those products and services, and how we can slow down and be more intentional. How Technology Shapes Behavior The first step in making more responsible design decisions is to acknowledge and understand the ways in which the human mind is susceptible to persuasive technology and how behavior can be shaped. There are a number of studies that provide a glimpse into the fundamentals of behavior shaping, but perhaps none are as influential or foundational as those conducted by American psychologist, behaviorist, author, inventor, and social philosopher B. F. Skinner. Through a process he called “operant conditioning,” Skinner studied how behaviors could be learned and modified by creating an association between a particular behavior and a consequence. Using a laboratory apparatus that came to be named after him ( Figure 11-1 ), Skinner studied how the behavior of animals could be shaped by teaching them to perform desired actions in response to specific stimuli in an isolated environment. His earliest experiments involved placing a hungry rat into 107 1 B. F. Skinner, The Behavior of Organisms: An Experimental Analysis (New York: Appleton-Century, 1938)",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". His earliest experiments involved placing a hungry rat into 107 1 B. F. Skinner, The Behavior of Organisms: An Experimental Analysis (New York: Appleton-Century, 1938). the chamber and observing it while it discovered that a food pellet would be dipensed when it came into contact with a lever on one side. 1 After a few chance occurrences, the rat quickly learned the association between pushing the lever and receiving food, and each time it was put in the box it would go straight to the lever—a clear demonstration of how positive reinforcement increases the likelhood of behavior being repeated. Skinner also experimented with negative reiforcement by placing a rat inside the chamber and subjecting it to an unpleasant electrical current, which would be turned off when the lever was pressed. Much like his previous experiments that rewarded the rats with food, the animal learned to avoid the current quickly by going straight to the lever once placed in the box. Figure 11-1. B. F. Skinner’s operant conditioning chamber, also known as the “Skinner box” (source: Skinner, 1938) 2 C. B. Ferster and B. F. Skinner, Schedules of Reinforcement (New York: Appleton-Century-Crofts, 1957). 3 Natasha Dow Schüll, Addiction by Design: Machine Gambling in Las Vegas (Princeton, NJ: Princeton Unversity Press, 2012). Skinner later discovered that different patterns of reinforcement affected the speed and frequency at which the animals would perform the desired behavior. 2 For example, rats that were rewarded with food each time they pressed the lever would press it only when they became hungry, and rats that were rewarded too infrequently would stop pressing the lever altogether. By contrast, rats that were rewarded with food in unpredictable patterns would repeatedly press the lever and continue doing so without reinforcement for the longest time. In other words, the rats’ behavior could most effectively be shaped by reinforcing it at varable times, as opposed to every time or not frequently enough",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In other words, the rats’ behavior could most effectively be shaped by reinforcing it at varable times, as opposed to every time or not frequently enough. Too much or too little reinforcement led to the animals losing interest, but random reinforcement led to impulsive, repeated behavior. Fast-forward to today, and it’s clear that Skinner’s research has been applied beyond the isolated box that bears his name. It can also be observed with human subjects in casinos around the world, where you’ll find slot machines that have perfected operant conditioning. These machines are an excellent modern-day example of the Skinner box: gamblers pay to pull a lever, occasionally being rewarded for doing so. In her book Addiction by Design , 3 cultural anthropologist Natasha Dow Schüll explores the world of machine-aided gambling and describes how slot machines are designed to mesmerize people into a state of “continuous productivity” in order to extract maximum value through a continual feedback loop. Additionally, their activity is often recorded into a data system that creates a risk profile for each player, informing the casino observers how much they can lose and still feel satisfied. When a player approaches their algorithmcally calculated “pain point,” casinos often dispatch a “luck ambassador” to suplement the holding power of the slot machine by dispensing meal coupons, show tickets, gambling vouchers, and other incentives. It’s a stimulus-response loop that’s optimized to keep people in front of the machines, repeatedly pulling the levers and spending money—all while being tracked in order to maximize time on device. Digital products and services have also been known to employ various metods with the goal of shaping human behavior, and we can see examples in many of the apps we use every day",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Digital products and services have also been known to employ various metods with the goal of shaping human behavior, and we can see examples in many of the apps we use every day. Everything from keeping you on a site for as long as possible to nudging you to make a purchase or tempting you to share content is 4 Michael Winnick, “Putting a Finger on Our Phone Obsession,” dscout, June 16, 2016, https:// blog.dscout.com/mobile-touches . behavior that can be shaped through reinforcement at the right time. Let’s take a closer look at some of the more common methods technology employs to shape behavior, whether intentionally or unintentionally. INTERMITTENT VARIABLE REWARDS Skinner demonstrated that random reinforcement on a variable schedule is the most effective way to influence behavior. Digital platforms can also shape behaior through the use of variable rewards, and this can be observed each time we check our phones for notifications, scroll through a feed, or pull to refresh. The results are similar to what Skinner observed in his lab: studies show that the average person interacts with their smartphone over 2,500 times a day and some up to 5,400 times, amounting to 2–4 hours each day. 4 Let’s look at a specific example that demonstrates variable rewards: pull to refresh ( Figure 11-2 ). This common interaction pattern is used by many mobile apps to enable the loading of new content by swiping down on the screen when at the top of a content feed. It doesn’t take a stretch of the imagination to see the similarities between this and a slot machine—not only in the physical interaction, but also in the variable “reward” it generates. Figure 11-2. Pull-to-refresh example on Twitter (source: Twitter, 2020) 5 Catalina L. Toma and Jeffrey T. Hancock, “Self-Affirmation Underlies Facebook Use,” Personality and Social Psychology Bulletin 39, no. 3 (2013): 321–31. INFINITE LOOPS Infinite loops like autoplay videos ( Figure 11-3 ) and infinite scrolling feeds are designed to maximize time on site by removing friction",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3 (2013): 321–31. INFINITE LOOPS Infinite loops like autoplay videos ( Figure 11-3 ) and infinite scrolling feeds are designed to maximize time on site by removing friction. Without the need for the user to make a conscious decision to load more content or play that next video, companies can ensure that passive consumption on their sites or apps continues uninterrupted. Ads are typically interspersed with the looping content, so more time on site means more ads viewed—a model that is significantly more effective in generating revenue than displaying static ads. Figure 11-3. YouTube autoplays the next video (source: YouTube, 2019) SOCIAL AFFIRMATION We humans are inherently social creatures. The drive to fulfill our core needs for a sense of self-worth and integrity extends to our lives on social media, 5 where we seek social rewards ( Figure 11-4 ). Each “like” or positive comment we receive on content we post online temporarily satisfies our desire for approval and beloning. Such social affirmation delivers a side dish of dopamine, the chemical prduced by our brains that plays a key role in motivating behavior. 6 Yabing Liu, Krishna P. Gummadi, Balachander Krishnamurthy, and Alan Mislove, “Analyzing Facebook Prvacy Settings: User Expectations vs. Reality,” in IMC ’11: Proceedings of the 2011 ACM SIGCOMM Internet Measurement Conference (New York: Association for Computing Machinery, 2011), 61–70. Figure 11-4. Facebook’s “like” button, first introduced in 2009 and now a ubiquitous feature in social media (source: Facebook, 2020) DEFAULTS Default settings matter when it comes to choice architecture because most peple never change them. These settings therefore have incredible power to steer decisions, even when people are unaware of what’s being decided for them. For example, a 2011 study found that Facebook’s default privacy settings ( Figure 11-5 ) matched users’ expectations only 37% of the time, leading to their content and personal information being visible to more people than they expected. 6 Figure 11-5",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6 Figure 11-5. Facebook’s privacy settings (source: Facebook, 2020) 7 Isaac Dinner, Eric Johnson, Daniel Goldstein, and Kaiya Liu, “Partitioning Default Effects: Why People Choose Not to Choose,” Journal of Experimental Psychology: Applied 17, no. 4 (2011): 332–41. Despite these potential mismatches, studies suggest that default options often lead people to rationalize their acceptance and reject alternatives. 7 (LACK OF) FRICTION Another way to shape behavior with digital products and services is to remove as much friction as possible—especially friction around actions you want people to take. In other words, the easier and more convenient you make an action, the more likely people will be to perform that action and form a habit around it. Take, for example, Amazon Dash buttons ( Figure 11-6 ), small electronic devices that enabled customers to order frequently used products simply by pressing a button, without even visiting the Amazon website or app. The physical buttons have since been deprecated in favor of digital-only versions, but this example illustrates just how far companies will go to shape behavior by attempting to remove as many obstacles as possible. Figure 11-6. An example of Amazon’s now-deprecated Dash button (source: Amazon, 2019) RECIPROCITY Reciprocation, or the tendency to repay the gestures of others, is a strong impulse we share as human beings. It’s a social norm we’ve come to value and even rely on as a species. It’s also a strong determining factor of human behavior that can be exploited, intentionally or not. Technology can tap into our impulse to reciprocate the gestures of others and shape our behavior as a result. Take, for example, LinkedIn, which notifies people when others have endorsed them for a skill ( Figure 11-7 ). More often than not, this leads to the recipient of the endorsment not only accepting the gesture but also feeling obliged to respond with their own endorsement. The end result is more time spent on the platform by both people, and more profit for LinkedIn",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The end result is more time spent on the platform by both people, and more profit for LinkedIn. Figure 11-7. LinkedIn skill endorsement notification (source: LinkedIn, 2020) DARK PATTERNS Dark patterns are yet another way technology can be used to influence behavior, by making people perform actions that they didn’t intend to for the sake of increasing engagement or to convince users to complete a task that is not in their best interest (make a larger purchase, share unnecessary information, accept marketing communications, etc.). Unfortunately, these deceptive techniques can be found all over the internet. In a 2019 study, researchers from Princeton Unversity and the University of Chicago analyzed about 11,000 shopping websites looking for evidence of dark patterns. Their findings were nothing short of alarming: they identified 1,818 instances of dark patterns, with the more popular 8 Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan, “Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites,” in Proceeings of the ACM on Human-Computer Interaction , vol. 3 (New York: Association for Computing Machnery, 2019), 1–32. sites in the sample being more likely to feature them. 8 To illustrate, consider 6pm.com , which makes use of the scarcity pattern to indicate that limited quantties of a product are available, increasing its perceived desirability. The company does this by displaying a low-stock message when people choose product options, to make it always seem that the item is in imminent danger of selling out ( Figure 11-8 ). Figure 11-8. An example of the scarcity dark pattern (source: 6pm.com , 2019) These are only some of the more common methods by which technology can be used to shape behavior in subtle ways",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 11-8. An example of the scarcity dark pattern (source: 6pm.com , 2019) These are only some of the more common methods by which technology can be used to shape behavior in subtle ways. Data collected about user behavior can be used to fine-tune how a system responds to an individual, and these methods are constantly increasing in sophistication and accuracy, while the psychological hardware we share as humans remains the same. Now, more than ever, it’s important that designers consider the ethics of influencing behavior. Why Ethics Matter Now let’s explore why exploitative technology should matter to those in the tecnology industry. It seems as if digital technology grows increasingly more embedded in our daily lives with each passing year. Since the arrival of the smarphone and other “smart” devices, we’ve become more and more reliant on the miniaturized computers we keep in our pockets, wear on our wrists, embed in our clothing, or carry in our bags. Everything from transportation and accommdation to food and consumer goods is just a few taps and swipes away, all thanks to these convenient little digital companions. The convenience these devices 9 Adrian Ward, Kristen Duke, Ayelet Gneezy, and Maarten Bos, “Brain Drain: The Mere Presence of One’s Own Smartphone Reduces Available Cognitive Capacity,” Journal of the Association for Consumer Research 2, no. 2 (2017): 140–54. 10 Melissa Hunt, Rachel Marx, Courtney Lipson, and Jordyn Young, “No More FOMO: Limiting Social Media Decreases Loneliness and Depression,” Journal of Social and Clinical Psychology 37, no. 10 (2018): 751–68. bring us is liberating and empowering, but it is not without consequence. Somtimes companies with the best of intentions create technology that ultimately produces unintended results. GOOD INTENTIONS, UNINTENDED CONSEQUENCES Companies seldom set out to create harmful products and services",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". GOOD INTENTIONS, UNINTENDED CONSEQUENCES Companies seldom set out to create harmful products and services. When Facbook introduced the “like” button in 2009, they probably didn’t intend for it to become such an addictive feedback mechanism, providing a small dopamine hit of social affirmation to users who found themselves returning to the app time and time again to measure their self-worth. They probably also didn’t intend for people to spend so many hours mindlessly scrolling through their news feeds once infinite scrolling was introduced. Snapchat probably didn’t intend for its fiters to change how many see themselves or present themselves to others, or to drive some to pursue cosmetic surgery in an effort to recreate the look provided by the filters in the app. They surely didn’t intend for their disappearing videos to be used for sexual harassment or to become a haven for sexual predators. Sadly, I could fill a whole chapter with examples like these—but I think you get the point. It’s hard to imagine any of these companies intended the negative consequences that resulted from the services they provided or features they introduced. And yet those consequences did occur, and the harm created by these examples and countless others is not excusable just because it was unintended by the creators. Things have moved so fast in the technology industry that we haven’t always had time to see the things that have been broken in the process. Now the research is starting to catch up and enlighten us about the lasting effects of “prgress.” It appears that the mere presence of our smartphones reduces our avaiable cognitive capacity, even when the devices are turned off",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 9 Additionally, links have been made between social media use and its disturbing effects on some of society’s most vulnerable: increases in depression and loneliness in young adults 10 and a rise in suicide-related outcomes or deaths among 11 Jean Twenge, Thomas Joiner, Megan Rogers, and Gabrielle Martin, “Increases in Depressive Symptoms, Suicide-Related Outcomes, and Suicide Rates Among U.S. Adolescents After 2010 and Links to Increased New Media Screen Time,” Clinical Psychological Science 6, no. 1 (2018): 3–17. adolescents. 11 Unfortunate side effects like these continue to surface as researcers take a closer look at the ways in which technology is impacting people’s lives and society as a whole. THE ETHICAL IMPERATIVE Human vulnerabilities often get exploited on digital platforms that lose sight of the human problems that they once sought to solve. The same technology that enables us to so easily purchase, connect, or consume can also distract us, affect our behavior, and impact the relationships we have with others around us. Pschology and its application in user experience design plays a critical role in all of this: behavior design is useful for keeping people “hooked,” but at what cost? When did “daily active users” or “time on site” become a more meaningful meric than whether a product is actually helping people achieve their goals or faciltating meaningful connections? Ethics must be an integral part of the design process, because without this check and balance, there may be no one advocating for the end user within the companies and organizations creating technology. The commercial imperatives to increase time on site, streamline the consumption of media and advertising, or extract valuable data don’t match up with human objectives of accomplishing a task, staying connected with friends or family, and so on. In other words, the coporate goals of the business and the human goals of the end user are seldom aligned, and more often than not designers are a conduit between them",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In other words, the coporate goals of the business and the human goals of the end user are seldom aligned, and more often than not designers are a conduit between them. If behavior can be shaped by technology, who holds the companies that build tecnology to account for the decisions they make? It’s time that designers confront this tension and accept that it’s our resposibility to create products and experiences that support and align with the goals and well-being of users. In other words, we should build technology that auments the human experience rather than replacing it with virtual interaction and rewards. The first step in making ethical design decisions is to acknowledge how the human mind can be exploited. We must then take accountability for the tecnology we help to create and ensure it respects people’s time, attention, and oveall digital well-being. No longer is “moving fast and breaking things” an acceptable means of building technology—instead, we must slow down and be intentional with the technology we create, and consider how it’s impacting peple’s lives. Slow Down and Be Intentional To ensure we are building products and services that support the goals of the people using them, it’s imperative that ethics are integrated into the design prcess. The following are a few common approaches to ensuring the human part of “human-centered design” remains at the forefront. THINK BEYOND THE HAPPY PATH Scenarios provide a frame of reference for designers—they’re essential to defiing critical features and functionality that must be available when a person uses a product or service. Unfortunately, teams that move fast and break things tend to focus exclusively on the idealized scenarios that provide the path of least resisance. These “happy paths” are by their very nature devoid of use cases for when things go wrong outside of simple technical errors",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These “happy paths” are by their very nature devoid of use cases for when things go wrong outside of simple technical errors. Technology that scales without considering scenarios that stray from the happy path become ticking time bombs that leave the people existing outside these idealized scenarios vulnerable. A better approach is to change the definition of the minimum viable product (MVP) to focus on nonideal scenarios first, as opposed to the path of least resisance. By placing edge cases at the center of our thinking, we can ensure that we’re creating more resilient products and services that consider the most vunerable cases by default. DIVERSIFY TEAMS AND THINKING Homogeneous teams often have difficulty identifying blind spots that exist ouside their own shared life experiences. This inevitably leads to less-resilient proucts and services, which can have disastrous results when things go wrong. To avoid the pitfalls of homogeneous thinking, there are a number of things that teams responsible for building technology can do. First, they can ensure they’re as diverse as possible—a team comprising different genders, races, ages, and backgrounds brings a broader spectrum of human experience into the design process from the outset. It’s also important to ensure the personas derived from target audience research aren’t exclusively focused on the user segments consiered essential for an MVP—the more diverse the audience you design for, the more likely it is that you’ll catch blind spots before they become bigger problems. LOOK BEYOND DATA Quantitative data tells us lots of useful things, such as how quickly people are performing tasks, what they are looking at, and how they are interacting with the system. What this data doesn’t tell us is why users are behaving a certain way or how the product is impacting their lives. It’s critical to consider other metrics in order to gain insight into the why , and to do that we must both listen and be receptive to our users",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It’s critical to consider other metrics in order to gain insight into the why , and to do that we must both listen and be receptive to our users. This means getting out from behind a screen, talking with them, and then using this qualitative research to inform how we evolve the design in an impactful way. Technology has the ability to significantly affect people’s lives, and it’s crucial that we ensure that impact is positive. It’s our responsibility to create products and experiences that support and align with the goals and well-being of users. We can make ethical design decisions by acknowledging how the human mind can be exploited and take accountability for our work by thinking beyond the happy path scenarios, building more diverse teams, and talking with users to gain qualitative feedback on how the products and experiences we build affect their lives.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-107-119.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": "81 Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites ARUNESH MATHUR, Princeton University, USA GUNES ACAR, Princeton University, USA MICHAEL J. FRIEDMAN, Princeton University, USA ELENA LUCHERINI, Princeton University, USA JONATHAN MAYER, Princeton University, USA MARSHINI CHETTY, University of Chicago, USA ARVIND NARAYANAN, Princeton University, USA Dark patterns are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making unintended and potentially harmful decisions. We present automated techniques that enable experts to identify dark patterns on a large set of websites. Using these techniques, we study shopping websites, which often use dark patterns to influence users into making more purchases or disclosing more information than they would otherwise. Analyzing ∼ 53K product pages from ∼ 11K shopping websites, we discover 1,818 dark pattern instances, together representing 15 types and 7 broader categories. We examine these dark patterns for deceptive practices, and find 183 websites that engage in such practices. We also uncover 22 third-party entities that offer dark patterns as a turnkey solution. Finally, we develop a taxonomy of dark pattern characteristics that describes the underlying influence of the dark patterns and their potential harm on user decision-making. Based on our findings, we make recommendations for stakeholders including researchers and regulators to study, mitigate, and minimize the use of these patterns. CCS Concepts: • Human-centered computing → Empirical studies in HCI ; HCI theory, concepts and models ; • Social and professional topics → Consumer products policy ; • Information systems → Browsers . Additional Key Words and Phrases: Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation ACM Reference Format: Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan. 2019. Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites. Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 81 (November 2019), 32 pages. https://doi.org/10.1145/3359183 Authors’ addresses: Arunesh Mathur, Princeton University, 304 Sherrerd Hall, Princeton, NJ, 08544, USA, amathur@cs. princeton.edu; Gunes Acar, Princeton University, 320 Sherrerd Hall, Princeton, NJ, 08544, USA, gunes@princeton.edu; Michael J. Friedman, Princeton University, 35 Olden Street, Princeton, NJ, 08544, USA, mjf4@princeton.edu; Elena Lucherini, Princeton University, 312 Sherrerd Hall, Princeton, NJ, 08544, USA, elucherini@cs.princeton.edu; Jonathan Mayer, Princeton University, 307 Sherrerd Hall, Princeton, NJ, 08544, USA, jonathan.mayer@princeton.edu; Marshini Chetty, University of Chicago, 355 John Crerar Library, Chicago, IL, 60637, USA, marshini@uchicago.edu; Arvind Narayanan, Princeton University, 308 Sherrerd Hall, Princeton, NJ, 08544, USA, arvindn@cs.princeton.edu. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. 2573-0142/2019/11-ART81 $15.00 https://doi.org/10.1145/3359183 Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. arXiv:1907.07032v2 [cs.HC] 20 Sep 2019 81:2 Arunesh Mathur et al",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. arXiv:1907.07032v2 [cs.HC] 20 Sep 2019 81:2 Arunesh Mathur et al. 1 INTRODUCTION Dark patterns [ 32 , 48 ] are user interface design choices that benefit an online service by coercing, steering, or deceiving users into making decisions that, if fully informed and capable of selecting alternatives, they might not make. Such interface design is an increasingly common occurrence on digital platforms including social media websites [ 46 ], shopping websites [ 32 ], mobile apps [ 5 , 31 ], and video games [ 85 ]. At best, dark patterns annoy and frustrate users. At worst, they can mislead and deceive users, e.g., by causing financial loss [ 1 , 2 ], tricking users into giving up vast amounts of personal data [ 46 ], or inducing compulsive and addictive behavior in adults [ 74 ] and children [ 21 ]. While prior work [ 31 , 32 , 38 , 48 ] has provided taxonomies to describe the existing types of dark patterns, there is no large-scale evidence documenting their prevalence, or a systematic and descriptive investigation of how the different types of dark patterns harm users. Collecting this information would allow us to first examine where, how often, and the technical means by which dark patterns appear; second, it would allow us to compare and contrast how various dark patterns influence users. In doing so, we can develop countermeasures against dark patterns to both inform users and protect them from such patterns. Further, given that many of these patterns are potentially unlawful, we can also aid regulatory agencies in addressing and mitigating their use. In this paper, we present an automated approach that enables experts to identify dark patterns at scale on the web",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". In this paper, we present an automated approach that enables experts to identify dark patterns at scale on the web. Our approach relies on (1) a web crawler, built on top of OpenWPM [ 25 , 40 ]—a web privacy measurement platform—to simulate a user browsing experience and identify user interface elements; (2) text clustering to extract all user interface designs from the resulting data; and (3) inspecting the resulting clusters for instances of dark patterns. We also develop a taxonomy so that researchers can share descriptive and comparative terminology to explain how dark patterns subvert user decision-making and lead to harm. We base this taxonomy on the characteristics of dark patterns as well as the cognitive biases they exploit in users. While our automated approach generalizes, we focus this study on shopping websites, which are used by an overwhelming majority of people worldwide [ 41 ]. Dark patterns found on these websites trick users into signing up for recurring subscriptions and making unwanted purchases, resulting in concrete financial loss. We use our web crawler to visit the ∼ 11K most popular shopping websites worldwide, create a large data set of dark patterns, and document their prevalence. Our data set contains several new instances and variations of previously documented dark patterns [ 32 , 48 ]. Finally, we use our taxonomy of dark pattern characteristics to classify and describe the patterns we discover. We have five main findings: • We discovered 1,818 instances of dark patterns on shopping websites, which together reprsent 15 types of dark patterns and 7 broad categories. • These 1,818 dark patterns were found on 1,254 of the ∼ 11K shopping websites ( ∼ 11.1%) in our data set. Shopping websites that were more popular, according to Alexa rankings [ 9 ], were more likely to feature dark patterns",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Shopping websites that were more popular, according to Alexa rankings [ 9 ], were more likely to feature dark patterns. These numbers represent a lower bound on the total number of dark patterns on these websites, since our automated approach only examined text-based user interfaces on a sample of product pages per website. • In using our taxonomy to classify the dark patterns in our data set, we discovered that the majority are covert , deceptive , and information hiding in nature. Further, many patterns exploit cognitive biases, such as the default and framing effects. These characteristics and biases collectively describe the consumer psychology underpinnings of the dark patterns we identified. • We uncovered 234 instances of dark patterns—across 183 websites—that exhibit deceptive behavior. We highlight the types of dark patterns we encountered that rely on deception. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:3 • We identified 22 third-party entities that provide shopping websites with the ability to create and implement dark patterns on their sites. Two of these entities openly advertised practices that enable deceptive messages. Through this study, we make the following contributions: • We contribute automated measurement techniques that enable expert analysts to discover new or revisit existing instances of dark patterns on the web. As part of this contribution, we make our web crawler and associated technical artifacts available on GitHub 1 . These can be used to conduct longitudinal measurements on shopping websites or be re-purposed for use on other types of websites (e.g., travel and ticket booking websites). • We create a data set and measure the prevalence of dark patterns on 11K shopping websites",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". • We create a data set and measure the prevalence of dark patterns on 11K shopping websites. We make this data set of dark patterns and our automated techniques publicly available 2 to help researchers, journalists, and regulators raise awareness of dark patterns [ 21 ], and to help develop user-facing tools to combat these patterns. • We contribute a novel descriptive taxonomy that provides precise terminology to characterize how each dark pattern works. This taxonomy can aid researchers and regulators to better understand and compare the underlying influence and harmful effects of dark patterns. • We document the third-party entities that enable dark patterns on websites. This list of third parties can be used by existing tracker and ad-blocking extensions (e.g., Ghostery, 3 Adblock Plus 4 ) to limit their use on websites. 2 RELATED WORK 2.1 Online Shopping and Influencing User Behavior Starting with Hanson and Kysar, numerous scholars have examined how companies abuse users’ cognitive limitations and biases for profit, a practice they call market manipulation [ 50 ]. For instance, studies have shown that users make different decisions from the same information based on how it is framed [ 80 , 81 ], giving readily accessible information greater weight [ 79 ], and becoming susceptible to impulsively changing their decision the longer the reward from their decision is delayed [ 28 ]. Some argue that because users are not always capable of acting in their own best interests, some forms of ‘paternalism’—a term referring to the regulation or curation of the user’s options—may be acceptable [ 78 ]. However, determining the kinds of curation that are acceptable is less straightforward, particularly without documenting the practices that already exist. More recently, Calo has argued that market manipulation is exacerbated by digital marketplaces since they posses capabilities that increase the chance of user harm culminating in financial loss, loss of privacy, and the ability to make independent decisions [ 34 ]",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". For example, unlike brick-anmortar stores, digital marketplaces can capture and retain user behavior information, design and mediate user interaction, and proactively reach out to users. Other studies have suggested that certain elements in shopping websites can influence impulse buying behavior [ 60 , 86 ]. For instance, perceived scarcity, social influence (e.g., ‘social proof’—informing users of others’ behavior—and shopping with others [ 33 , 61 ]) can all lead to higher spending. More recently, Moser et al. conducted a study [ 65 ] to measure the prevalence of elements that encourage impulse buying. They identified 64 such elements—e.g., product reviews/ratings, discounts, and quick add-to cart buttons—by manually scraping 200 shopping websites. 1 https://github.com/aruneshmathur/dark-patterns 2 https://webtransparency.cs.princeton.edu/dark-patterns 3 https://ghostery.com 4 https://adblockplus.com Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:4 Arunesh Mathur et al. 2.2 Dark Patterns in User Interface Design Coined by Brignull in 2010, dark patterns is a catch-all term for how user interface design can be used to adversely influence users and their decision-making abilities. Brignull described dark patterns as ‘tricks used in websites and apps that make you buy or sign up for things that you didn’t mean to’, and he created a taxonomy of dark patterns using examples from shopping and travel websites to help raise user awareness. The taxonomy documented patterns such as ‘Bait and Switch’ (the user sets out to do one thing, but a different, undesirable thing happens instead), and ‘Confirmshaming’ (using shame tactics to steer the user into making a choice). 2.2.1 Dark Pattern Taxonomies. A growing number of studies have expanded on Brignull’s oriinal taxonomy more systematically to advance our understanding of dark patterns",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 2.2.1 Dark Pattern Taxonomies. A growing number of studies have expanded on Brignull’s oriinal taxonomy more systematically to advance our understanding of dark patterns. Conti and Sobiesk [ 38 ] were the first to create a taxonomy of malicious interface design techniques, which they defined as interfaces that manipulate, exploit, or attack users. While their taxonomy contains no examples and details on how the authors created the taxonomy are limited, it contains seeral categories that overlap with Brignull’s dark patterns, including ‘Confusion’ (asking the user questions or providing information that they do not understand) and ‘Obfuscation’ (hiding desired information and interface elements). More recently, Bösch et al. [ 31 ] presented a similar, alternative breakdown of privacy-specific dark patterns as ‘Dark Strategies’, uncovering new patterns: ‘Forced Registration’ (requiring account registration to access some functionality) and ‘Hidden Legalese Stipulations’ (hiding malicious information in lengthy terms and conditions). Finally, Gray et al. [ 48 ] presented a broader categorization of Brignull’s taxonomy and collapsed many patterns into categories such as ‘Nagging’ (repeatedly making the same request to the user) and ‘Obstruction’ (preventing the user from accessing functionality). While these taxonomies have focused on the web, researchers have also begun to examine dark patterns in specific application domains. For instance, Lewis [ 57 ] analyzed design patterns in the context of web and mobile applications and games, and codified those patterns that have been successful in making apps ‘irresistible’, such as ‘Pay To Skip’ (in-app purchases that skip levels of a game). In another instance, Greenberg et al. [ 49 ] analyzed dark patterns and ‘antipatterns’— interface designs with unintentional side-effects on user behavior—that leverage users’ spatial relationship with digital devices",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". [ 49 ] analyzed dark patterns and ‘antipatterns’— interface designs with unintentional side-effects on user behavior—that leverage users’ spatial relationship with digital devices. They introduced patterns such as ‘Captive Audience’ (inserting unrelated activities such as an advertisement during users’ daily activities) and ‘Attention Grabber’ (visual effects that compete for users’ attention). Finally, Mathur et al. [ 63 ] discovered that most affiliate marketing on social media platforms such as YouTube and Pinterest is not disclosed to users (the ‘Disguised Ads’ dark pattern). 2.2.2 Dark Patterns and User Decision-making. A growing body of work has drawn connections between dark patterns and various theories of human decision-making in an attempt to explain how dark patterns work and cause harm to users. Xiao and Benbasat [ 84 ] proposed a theoretical model for how users are affected by deceptive marketing practices in online shopping, including affective mechanisms (psychological or emotional motivations) and cognitive mechanisms (perceptions about a product). In another instance, Bösch et al. [ 31 ] used Kahneman’s Dual process theory [ 79 ] which describes how humans have two modes of thinking—‘System 1’ (unconscious, automatic, possibly less rational) and ‘System 2’ (conscious, rational)—and noted how ‘Dark Strategies’ exploit users’ System 1 thinking to get them to make a decision desired by the designer. Lastly, Lewis [ 57 ] linked each of the dark patterns described in his book to Reiss’s Desires, a popular theory of psychological motivators [ 72 ]. Finally, a recent study by the Norwegian Consumer Council (Frobrukerrådet) [ 46 ] examined how interface designs on Google, Facebook, and Windows 10 make Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:5 it hard for users to exercise privacy-friendly options. The study highlighted the default options and framing statements that enable such dark patterns",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Dark Patterns at Scale 81:5 it hard for users to exercise privacy-friendly options. The study highlighted the default options and framing statements that enable such dark patterns. 2.3 Comparison to Prior Work Our study differs from prior work in two ways. First, while prior work has largely focused on creating taxonomies of the types of dark patterns either based on anecdotal data [ 31 , 32 ] or data collected from users’ submissions [ 38 , 48 ], we provide large-scale evidence documenting the presence and prevalence of dark patterns in the wild. Automated measurements of this kind have proven useful in discovering various privacy and security issues on the web—including third-party tracking [ 25 , 40 ] and detecting vulnerabilities of remote third-party JavaScript libraries [ 68 ]—by documenting how and on which websites these issues manifest, thus enabling practical solutions to counter them. Second, we expand on the insight offered by prior work about how dark patterns affect users. We develop a comprehensive taxonomy of dark pattern characteristics (Section 3) that concretely explains the underlying influence and harmful effects of each dark pattern. Finally, while prior work has shed light on impulse buying on shopping websites, the focus of our work is on dark patterns. While there is some overlap between certain types of dark patterns and impulse buying features of shopping websites [ 65 ], the majority of impulse buying elements are not dark patterns. For instance, offering returns and exchanges for products, or showing multiple images of a product [ 65 ] do not constitute dark patterns: even though they play a role in persuading users into purchasing products, they do not fundamentally subvert user decision-making in a manner that benefits shopping websites and retailers",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 3 A TAXONOMY OF DARK PATTERN CHARACTERISTICS Our taxonomy explains how dark patterns affects user decision-making based on their charateristics as well as the cognitive biases in users—deviations from rational behavior justified by some ‘biased’ line of reasoning [ 51 ]—they exploit to their advantage. We ground this taxonomy in the literature on online manipulation [ 34 , 77 , 83 ] and by studying the types of dark patterns highlighted in previous work [32, 48]. Our taxonomy consists of the following five dimensions: • Asymmetric : Does the user interface design impose unequal weights or burdens on the available choices presented to the user in the interface? 5 For instance, a website may present a prominent button to accept cookies on the web but make the opt-out button less visible, or even hide it in another page. • Covert : Is the effect of the user interface design choice hidden from users? That is, does the interface design to steer users into making specific purchases without their knowledge? For instance, a website may leverage the decoy effect [ 52 ] cognitive bias, in which an additional choice—the decoy—is introduced to make certain other choices seem more appealing. Users may fail to recognize the decoy’s presence is merely to influence their decision making, making its effect covert. • Deceptive : Does the user interface design induce false beliefs either through affirmative misstatements, misleading statements, or omissions? For instance, a website may offer a discount to users that appears to be limited-time, but actually repeats when the user refreshes the website’s page. Users may be aware that the website is trying to offer them a discount; however, they may not realize that they do not have a limited time to take advantage of the deal. This false belief affects users’ decision-making i.e., they may act differently if they knew that the sale is recurring. 5 We narrow the scope of asymmetry to only refer to explicit choices in the interface. Proc. ACM Hum.-Comput. Interact., Vol. 3, No",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 5 We narrow the scope of asymmetry to only refer to explicit choices in the interface. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:6 Arunesh Mathur et al. • Hides Information : Does the user interface obscure or delay the presentation of necessary information to the user? For instance, a website may not disclose additional charges for a product to the user until the very end of their checkout. • Restrictive : Does the user interface restrict the set of choices available to users? For instance, a website may only allow users to sign up for an account with existing social media accounts so they can gather more information about them. Many types of dark patterns operate by exploiting cognitive biases in users. In Section 5, we draw an explicit connection between each type of dark pattern we encounter and the cognitive biases it exploits. The biases we refer to in our findings are: (1) Anchoring Effect [ 79 ]: The tendency of individuals to overly rely on an initial piece of information—the ‘anchor’—in future decisions. (2) Bandwagon Effect [ 75 ]: The tendency of individuals to value something more because others seem to value it. (3) Default Effect [ 54 ]: The tendency of individuals to stick with options that are assigned to them by default due to inertia. (4) Framing Effect [ 80 ]: The tendency of individuals to reach different decisions from the same information depending on how it is presented. (5) Scarcity Bias [ 64 ]: The tendency of individuals to place a higher value on things that are scarce. (6) Sunk Cost Fallacy [ 29 ]: The tendency of individuals to continue an action if they have invested resources into it, even if that action might make them worse off. 4 METHOD Dark patterns may manifest in several different locations inside websites, and they can rely heavily upon interface manipulation, such as changing the hierarchy of interface elements or prioritizing certain options over others using different colors",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". However, many dark patterns are often present on users’ primary interaction paths in an online service or website (e.g., when purchasing a product on a shopping website, or when a game is paused after a level is completed). Further, multiple instances of a type of dark pattern share common traits such as the text they display (e.g., in the ‘Confirmshaming’ dark pattern—which tries to shame the user into making a particular choice— many messages begin with No thanks ). Our technique relies on automating the primary interaction path of websites, extracting textual interface elements present in this path, and finally, grouping and organizing these—using clustering—for an expert analyst to sift through. While our method generalizes to different types of websites, we focus on shopping websites in this study. We designed a web crawler capable of navigating users’ primary interaction path on shopping websites: making a product purchase. Our crawler aligned closely with how an ordinary user would browse and make purchases on shopping websites: discover pages containing products on a website, add these products to the cart, and check out. We describe these steps, and the data we collected during each visit to a website below. Figure 1 illustrates an overview of our method. We note that only analyzing textual information in this manner restricts the set of dark patterns we can discover, making our findings a lower bound on the dark patterns employed by shopping websites. We leave detecting other kinds of dark patterns—those that are enabled using style, color, and other non-textual features—to future work, and we discuss possible approaches in Section 6. 4.1 Creating a Corpus of Shopping Websites We used the following criteria to evaluate existing lists of popular shopping websites, and, eventually, construct our own: (1) the list must be representative of the most popular shopping websites globally, Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:7 and (2) the list must consist of shopping websites in English so that we would have the means to analyze the data collected from the websites. We retrieved a list of popular websites worldwide from Alexa using the Top Sites API [ 9 ]. Alexa is a web traffic analysis company that ranks and categorizes websites based on statistics it collects from users of its toolbar. We used the Top Sites list because it is more stable and is based on monthly traffic and not daily rank, which fluctuates often [ 73 ] The list contained 361,102 websites in total, ordered by popularity rank. 6 We evaluated two website classification services to extract shopping websites from this list of the most popular websites: Alexa Web Information Service [ 10 ] and WebShrinker [ 23 ]. We evaluated the classification accuracy of these services using a random sample of 500 websites from our list of 361K websites, which we manually labeled as ‘shopping’ or ‘not shopping’. We considered a website to be a shopping website if it was offering a product for purchase. Of the 500 websites in our sample, we labeled 57 as ‘shopping’ and 443 as ‘not shopping’. We then evaluated the performance of both classifiers against this ground truth. Table 3 in the Appendix summarizes the classifiers’ results. Compared to Webshrinker, Alexa’s classifications performed poorly on our sample of websites (classification accuracy: 89% vs. 94%), with a strikingly high false negative rate (93% vs. 18%). Although Webshrinker had a slightly higher false positive rate (0.2% vs. 0.4%), we used methods to determine and remove these false positives as we describe in Section 4.2.1. We subsequently used Webshrinker to classify our list of 361K websites, obtaining a list of 46,569 shopping websites",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We subsequently used Webshrinker to classify our list of 361K websites, obtaining a list of 46,569 shopping websites. To filter out non-English websites, we downloaded home pages of each site using Selenium [ 8 ] and ran language detection on texts extracted from the pages using the polyglot Python library [ 4 ]. Our final data set contained 19,455 English language shopping websites. We created this filtered list in August 2018. 4.2 Data Collection with a Website Crawl We conducted all our crawls from the Princeton University campus using two off-the-shelf coputers, both equipped with 16G of memory and quad-core CPUs. Our crawler’s exploration of each shopping website mimicked a typical user’s primary interaction path on a shopping website— starting with one of its product pages. Therefore, the first step in our website crawl was to determine ways to automatically identify product URLs from shopping websites. 4.2.1 Discovering Product URLs on Shopping Websites. To effectively extract product URLs from shopping websites, we iteratively designed and built a Selenium-based web crawler that contained a classifier capable of distinguishing product URLs from non-product URLs. At first, we build a naïve depth-first crawler that, upon visiting a website’s home page, determined the various URLs on the page, selected one URL at random, and then repeated this process from the selected URL. Using this crawler, we assembled a data set of several thousand URLs from visiting a random sample of 100 websites from our data set of 19K shopping websites. We manually labeled a sample of these URLs either as ‘product’ or ‘non-product’ URLs, and created a balanced data set containing 714 labeled URLs in total. We trained a Logistic Regression classifier on this data set of labeled URLs using the SGDClassifier class from scikit-learn [ 71 ]",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We trained a Logistic Regression classifier on this data set of labeled URLs using the SGDClassifier class from scikit-learn [ 71 ]. We extracted several relevant features from this data set of URLs, icluding the length of a URL, the length of its path, the number of forward slashes and hyphens in 6 We did not use Alexa’s list of Top/Shopping websites [ 22 ] because of two issues. First, its criteria of categorization are not fully disclosed. Second, most of the websites in the list had an average monthly rank > 500,000, which we did not consider to be representative of the most popular websites worldwide. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:8 Arunesh Mathur et al. 361 K Websites From Alexa Top Sites 47 K Shopping Websites Webshrinker Classifier polyglot Language Classifier 19 K English Shopping Websites Corpus CreaDon Data CollecDon Product Page Crawler 53 K Product Pages From 11 K Shopping Websites Checkout Crawler • 13 M Segments • HTTP Requests & Responses • HAR Files • HTML Sources • Page Screenshots Data Analysis Hierarchical Clustering using HDBSCAN Manual ExaminaDon Dark PaSerns Fig. 1. Overview of the shopping website corpus creation, data collection using crawling, and data analysis using hierarchical clustering stages. its path, and whether its path contained the words ‘product’ or ‘category’. We used 90% of the URLs for training and obtained an 83% average classification accuracy using five-fold cross validation. We embedded this classifier into our original Selenium-based web crawler to help guide its crawl. As a result, rather than selecting and visiting URLs at random, the crawler first used the classifier to rank the URLs on a page by likelihood of being product URLs, and then visited the URL with the highest likelihood. The crawler declared a URL as product if its page contained an ‘Add to cart’ or similar button",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". The crawler declared a URL as product if its page contained an ‘Add to cart’ or similar button. We detected this button by assigning a weighted score to visible HTML elements on a page based on their size, color, and whether they matched certain regular expressions (e.g., ‘Add to bag|cart|tote|’). This check also helped us weed out any false positives that may have resulted from the classification of shopping websites using Webshrinker (Section 4.1). We tuned the crawler’s search process to keep its crawl tractable. The crawler returned to the home page after flagging a product URL. It did not visit a given URL more than two times to avoid exploring the same URLs, and it stopped after visiting 100 URLs or spending 15 minutes on a site. We determined these termination limits by running test crawls on random samples of shopping websites. Finally, we opted to extract no more than five product pages from each shopping website. To evaluate our crawler’s performance, we randomly sampled 100 shopping websites from our corpus of 19K shopping websites and examined the product URLs the crawler returned for each of these websites. For 86 of those 100 websites, our crawler successfully extracted and returned legitimate product pages where they were present, and it returned no product pages where there Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:9 were not any. For the remaining 14 websites, the crawler either timed out because the website was no longer reachable, the website included a step that the crawler could not handle (e.g., the website required selecting a country of origin), or the ‘Add to cart’ button was incorrectly detected. We then used the crawler on all of the 19K shopping websites, and in total we gathered 53,180 product pages from 11,286 shopping websites. 4.2.2 Simulating Product Purchase Flows",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We then used the crawler on all of the 19K shopping websites, and in total we gathered 53,180 product pages from 11,286 shopping websites. 4.2.2 Simulating Product Purchase Flows. To simulate a user’s typical shopping flow—which icluded selecting certain product options (e.g., size or color), adding the product to the cart, viewing the cart, and checking out—we designed and built an interactive ‘checkout crawler’. We based our checkout crawler on OpenWPM, a fully instrumented browser platform that is designed to conduct large-scale privacy and web-tracking measurement studies [ 40 ]. We extended OpenWPM in a number of ways to interact with the product pages we collected previously, including identifying various interface elements using scoring functions similar to the ones we described in Section 4.2. Each of these functions would output the most likely ‘Add to cart’ buttons, ‘View cart’ buttons, and ‘Checkout’ buttons, which the crawler would click in–order across multiple pages. Because websites do not follow uniform HTML markup and design, our crawler needed to account for a variety of design alternatives and edge cases to simulate user interaction, such as dismissing popup dialogs, and identifying and interacting with product options (e.g., selecting a size and color for a t-shirt) to add a product to cart. We collected three types of data during this crawl for each product page. First, we saved the page source on visit. Second, we took screenshots each time the state of the page changed (e.g., clicking a button or selecting a product option). Third, we extended OpenWPM’s HTTP instrumentation to store HTTP Archive (HAR) [ 13 ]) files for each crawled page since HAR files are not limited to HTTP headers and contain full response contents that can be used for further analysis. To evaluate our crawler’s performance, we randomly sampled 100 product pages from the crawl in Section 4.2.1 and examined whether our crawler was able to simulate a user’s shopping flow",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". To evaluate our crawler’s performance, we randomly sampled 100 product pages from the crawl in Section 4.2.1 and examined whether our crawler was able to simulate a user’s shopping flow. In 66 of the 100 pages, our crawler reached the checkout page successfully. In 14 of the remaining 34, the crawler was able to add the product to cart but it was unable to proceed to the cart page; most often this was the result of complex product interaction (e.g., selecting the dimensions of a rug), which our crawler was not designed to perform. In the remaining 20 cases, either we produced Selenium exceptions, or failed to discover cart and checkout buttons. We then used the crawler on all of the 53K product pages. We divided the 53K product URLs into two equal-length lists to reduce the total crawling time. These crawls took approximately 90 hours to complete. 4.2.3 Capturing Meaningful Text Using Page Segmentation. The checkout crawler divided all the pages it visited into meaningful page segments to help discover dark patterns. These segments can be thought of as ‘building blocks’ of web pages, representing meaningful smaller sections of a web page. These formed the basic units for our data analysis and clustering. We defined segments as visible HTML elements that contained no other block-level elements [ 6 ] and contained at least one text element—that is, elements of type TEXT_NODE [ 19 ]. However, since websites may use a virtually endless variety of markup and designs, we iteratively developed our segmentation algorithm, testing it on samples of shopping websites and accounting for possible edge cases. Algorithm 1 and Figure 11 in the Appendix detail the segmentation algorithm and illustrate its output for one web page, respectively. Before segmenting each web page, the crawler waited for the page to load completely, also accounting for the time needed for popup dialogs to appear. However, web pages may also display text from subsequent user interactions, and with dynamically loaded content (e.g., a countdown timer)",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". However, web pages may also display text from subsequent user interactions, and with dynamically loaded content (e.g., a countdown timer). To capture possible segments from such updates to the web page during a crawl—no matter how minor or transient—we integrated the Mutation Summary [ 3 ] library into our checkout crawler. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:10 Arunesh Mathur et al. The Mutation Summary library combines DOM MutationObserver events [ 18 ] into compound event summaries that are easy to process. When the checkout crawler received a new Mutation Summary representing updates to the page, it segmented (Algorithm 1) this summary and stored the resulting segments. For each segment, we stored its HTML Element type, its element text (via innerText ), its dimensions and coordinates on the page, and its style including its text and background colors. Our crawls resulted in ∼ 13 million segments across the 53K product URL pages. 4.3 Data Analysis with Clustering We employed hierarchical clustering to discover dark patterns from the data set of segments. Our use of clustering was not to discover a set of latent constructs in the data but rather to organize the segments in a manner that would be conducive to scanning, making it easier for an expert analyst to sift through the clusters for possible dark patterns. 4.3.1 Data Preprocessing. Many of the ∼ 13 million segments collected during our crawls were duplicates, such as multiple ‘Add to cart’ segments across multiple websites. Since we only used text-based features for our analyses, we retained unique pieces of text across the websites in our data set (e.g., one segment containing the text ‘Add to cart’ across all the websites in our data set). We also replaced all numbers with a placeholder before performing this process to further reduce duplicates. This preprocessing reduced the set of segments by 90% to ∼ 1.3 million segments. 4.3.2 Feature Representations and Hierarchical Clustering",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". This preprocessing reduced the set of segments by 90% to ∼ 1.3 million segments. 4.3.2 Feature Representations and Hierarchical Clustering. Before performing clustering, we tranformed the text segments into a Bag of Words (BoW) representation. Each entry in the resulting BoW matrix ( M ij ) indicated the number of times token j appeared in segment i . 7 We filtered all stop words 8 and punctuation—except currency symbols, since these are indicative of product price—from the list of tokens, and further only retained tokens that appeared in at least 100 segments. This resulted in a vocabulary of 10,133 tokens. Given this large size of our vocabulary—and thus the dimensions of the segment-token matrix— we performed Principal Component Analysis (PCA) on the BoW matrix. We retained 3 components from the PCA, which together captured more than 95% of the variance in the data. We used the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDSCAN) algorithm [ 35 ] implemented in the HDBSCAN Python library [ 14 ] to extract clusters from this data. We chose HDBSCAN over other clustering algorithms since it is robust to noise in the data, and it allows us to vary the minimum size of the clusters ( min_cluster_size ). We varied a total of four passes at clustering: two min_cluster_size values (5 and 10) × two distance metrics (Manhattan distance or L1 norm, and Euclidean distance or L2 norm). We picked sufficiently small values for the min_cluster_size parameter to keep the size of the noise cluster small and to avoid coercing segments into one cluster. The clustering output across the BoW input was nearly the same. As expected, a min_cluster_size of 10 resulted in a larger noise cluster compared to a min_cluster_size of 5—but only marginally larger regardless of the distance metric. However, since the min_cluster_size of 10 produced significantly fewer clusters, we picked its output over the others. It contained 10,277 clusters",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". However, since the min_cluster_size of 10 produced significantly fewer clusters, we picked its output over the others. It contained 10,277 clusters. 7 We did not use the Term Frequency-Inverse Document Frequency (TF-IDF) representation as upon clustering, it resulted in anywhere between 70%-75% of the segments being classified as noise. We believe this may have been because of the incorrect IDF scaling factor since the segments were not all drawn from a pool of independent observations—i.e., multiple segments originated from the same website 8 Using Python NLTK [30] Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:11 Alexa rank % Websites with >= 1 Dark Pattern 0e+00 1e+05 2e+05 3e+05 4e+05 0 5 10 15 20 Fig. 2. Distribution of the dark patterns we discovered over the Alexa rank of the websites. Each bin indicates the percentage of shopping websites in that bin that contained at least one dark pattern. 4.3.3 Examining and Analyzing the Clusters. Once the clustering was complete, we made two passes through the data. The goal of pass one was to include clusters that contained any segments that might manifest as dark patterns. In this pass, one researcher scanned the clusters and identified possible clusters of interest, recording all those clusters that represented specific types of user interfaces (e.g., login choices, cart totals), website characteristics (e.g., stock notifications), and product options (e.g., small/medium/large) that generally appear on shopping websites. This step filtered down the clusters from 10,277 to 1,768. In pass two, we extracted all the websites that corresponded to these segments for further examination. The research team used the literature on dark patterns [ 32 , 48 , 69 ] and impulse buying [ 65 ], and media coverage of high-pressure sales and marketing tactics (e.g., [ 15 ]) to create a shared understanding of possible dark patterns using the examples cited in these works to guide our thinking",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". In order to validate the coding of clusters, two researchers examined a sample of 200 of the 1,768 clusters, and recorded any dark patterns they encountered. The researchers also examined each website’s set of screenshots and visited the websites to gain context and additional information surrounding the segments (e.g., discovering practices associated with the flagged pattern). To measure agreement between the researchers, we computed Cohen’s kappa between the segments that were recorded—resulting in a score of 0.74. The team discussed and resolved all disagreements, and one researcher then examined the remaining clusters in the same manner. The team then discussed the resulting dark patterns, and iteratively grouped them into types and broader categories. 4.4 Detecting Deceptive Dark Patterns We further examined many of the dynamic dark patterns—those patterns that displayed transient values (e.g., a countdown timer)—for deceptive practices. To this end, we used our checkout crawler to ‘monitor’ the websites containing dark patterns of interest once every four hours for a period of five days. We combined this data with several dark pattern-specific heuristics—which we describe in the following sections—to uncover instances of deceptive practices. 5 FINDINGS In total, we discovered 1,818 instances of dark patterns from 1,254 ( ∼ 11.1%) websites in our data set of 11K shopping websites. Given that (1) our crawler only explored the product pages, cart pages, and checkout pages of websites, (2) our analyses only took text-based user interfaces into Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:12 Arunesh Mathur et al. Table 1. Categories and types of dark patterns along with their description, prevalence, and definitions",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Legend: = Always, G# = Sometimes, # = Never Category Type Description # Instances # Websites Asymmetric? Covert? Deceptive? Hides Info? Restrictive? Cognitive Biases Sneaking Sneak into Basket Adding additional products to users’ shoping carts without their consent 7 7 # # G # # Default Effect Hidden Costs Revealing previously undisclosed charges to users right before they make a purchase 5 5 # # G # # Sunk Cost Fallacy Hidden Subscription Charging users a recurring fee under the pretense of a one-time fee or a free trial 14 13 # # G # # None Urgency Countdown Timer Indicating to users that a deal or discount will expire using a counting-down timer 393 361 # G # G # # # Scarcity Bias Limited-time Message Indicating to users that a deal or sale will expire will expire soon without specifying a deadline 88 84 # G # # # Scarcity Bias Misdirection Confirmshaming Using language and emotion (shame) to steer users away from making a certain choice 169 164 # # # # Framing Effect Visual Interference Using style and visual presentation to steer users to or away from certain choices 25 24 G # G # # # Anchoring & Fraing Effect Trick Questions Using confusing language to steer users into making certain choices 9 9 # # # Default & Framing Effect Pressured Selling Pre-selecting more expensive variations of a product, or pressuring the user to accept the more expensive variations of a product and related products 67 62 G # G # # # # Anchoring & Default Effect, Scarcity Bias Social Proof Activity Message Informing the user about the activity on the website (e.g., purchases, views, visits) 313 264 # G # G # # # Bandwagon Effect Testimonials Testimonials on a product page whose orgin is unclear 12 12 # # G # # # Bandwagon Effect Scarcity Low-stock Message Indicating to users that limited quantities of a product are available, increasing its dsirability 632 581 # G # G # G # # Scarcity Bias High-demand Message Indicating to users that a product is in higdemand and likely to sell out soon, increaing its desirability 47 43 # G # # #",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": "its dsirability 632 581 # G # G # G # # Scarcity Bias High-demand Message Indicating to users that a product is in higdemand and likely to sell out soon, increaing its desirability 47 43 # G # # # # Scarcity Bias Obstruction Hard to Cancel Making it easy for the user to sign up for a service but hard to cancel it 31 31 # # # G # None Forced Action Forced Enrollment Coercing users to create accounts or share their information to complete their tasks 6 6 # # # None account, this number represents a lower-bound estimate of the prevalence of dark patterns",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We divide our discussion of the findings by first illustrating the categories of dark patterns revealed by our analyses, and then by describing our findings on the ecosystem of third-parties that enable dark patterns. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:13 5.1 Categories of Dark Patterns Our analyses revealed 15 types of dark patterns contained in 7 broader categories. Where applicable, we use the dark pattern labels proposed by Gray et al. [ 48 ] and Brignull [ 32 ] to describe these types and categories. Table 1 summarizes our findings, highlighting the number of separate instances of dark patterns found for each type. Figure 2 shows the distribution of the websites containing dark patterns over their Alexa ranks. The distribution suggests that dark patterns are more likely to appear on popular websites (Speaman’s Rho = -0.62, p < 0 . 0001). In the following sections, we describe the various categories and types of dark patterns we discovered. 5.1.1 Sneaking. Coined by Gray et al. in their taxonomy [ 48 ], ‘Sneaking’ refers to the category of dark patterns that attempt to misrepresent user actions, or hide/delay information that, if made available to users, they would likely object to. We observed three types of the Sneaking dark pattern: Sneak into Basket [ 32 ], Hidden Costs [ 32 ], and Hidden Subscription (Brignull’s Forced Continuity [32]) on 23 shopping websites. Figure 3 highlights instances of these three types. Sneak into Basket . The ‘Sneak into Basket’ dark pattern adds additional products to users’ shopping carts without their consent, often promoting the added products as ‘bonuses’ and ‘necesary’. Sneak into Basket exploits the default effect cognitive bias in users, with the website behind it hoping that users will stick with the products it adds to cart. One instance of Sneak into Basket is shown in Figure 3a, where adding a bouquet of flowers to the shopping cart on avasflowers.net also adds a greeting card",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". One instance of Sneak into Basket is shown in Figure 3a, where adding a bouquet of flowers to the shopping cart on avasflowers.net also adds a greeting card. In another instance on laptopoutlet.co.uk —not shown in the figure— adding an electronic product, such as a laptop, to the shopping cart also adds product insurance. Other websites, such as cellularoutfitter.com , add additional products (e.g., a USB charger) to the shopping cart using pre-selected checkboxes. While such checkboxes could be deselected by a vigilant user, the additional products would be added by default in the absence of any intervention. In our data set, we found a total of 7 instances of the Sneak into Basket dark pattern. Using our taxonomy of dark pattern characteristics, we classify Sneak into Basket as at least partially deceptive (it incorrectly represents the nature of the action of adding an item to the shopping cart) and information hiding (it deliberately disguises how the additional products were added to cart from users) in nature. However, it is not covert : users can visibly see and realize that the website included additional products to their shopping carts. Hidden Costs . The ‘Hidden Costs’ dark pattern reveals new, additional, and often unusually high charges to users just before they are about to complete a purchase. Examples of such charges include ‘service fees’ or ‘handling costs’. Often these charges are only revealed at the end of a checkout process, after the user has already filled out shipping/billing information, and consented to terms of use. The Hidden Costs dark pattern exploits the sunk cost fallacy cognitive bias: users are likely to feel so invested in the process that they justify the additional charges by completing the purchase to not waste their effort. Figure 3b shows the Hidden Costs dark pattern on proflowers.com , where the ‘Care & Handling’ charge of $2.99 is revealed immediately before confirming the order. In our data set, we found a total of 5 instances of the Hidden Costs dark pattern",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". In our data set, we found a total of 5 instances of the Hidden Costs dark pattern. Using our taxonomy of dark pattern characteristics, we classify Hidden Costs as at least partially deceptive (it relies on minimizing and delaying information from users), and thus also information hiding in nature. Like Sneak into Basket, Hidden Costs is not covert : users can visibly see and realize that the website included additional charges. Hidden Subscription . The ‘Hidden Subscription’ dark pattern charges users a recurring fee under the pretense of a one-time fee or a free trial. Often, if at all, users become aware of the Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:14 Arunesh Mathur et al. (a) Sneak into Basket on avasflowers.net . Despite requesting no greeting cards, one worth $3.99 is automatcally added. (b) Hidden Costs on proflowers .com . The Care & Handling charge ($2.99) is disclosed on the last step. (c) Hidden Subscription on wsjwine.com . Left: The website fails to disclose that the Advantage service is an annual subscription worth $89 unless the user clicks on Learn More . Right: The service in cart. Fig. 3. Three types of the Sneaking category of dark patterns. recurring fee once they are charged several days or months after their purchase. For instance, we discovered that wsjwine.com offers users an Advantage service which appears to be a one-time payment of $89 but renews annually, as shown in Figure 3c. Further, Hidden Subscription often appears with the ‘Hard to Cancel’ dark pattern—which we describe in Section 5.1.6—thereby making the recurring charges harder to cancel than signing up for them. In our data set, we found a total of 14 instances of Hidden Subscription dark pattern. Using our taxonomy of dark pattern characteristics, we classify Hidden Subscription as at least partially deceptive (it misleads users about the nature of the initial offer) and information hiding (it withholds information about the recurring fees from users) in nature",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 5.1.2 Urgency. ‘Urgency’ refers to the category of dark patterns that impose a deadline on a sale or deal, thereby accelerating user decision-making and purchases [ 27 , 37 , 53 , 69 ]. Urgency dark patterns exploit the scarcity bias in users—making discounts and offers more desirable than they would otherwise be, and signaling that inaction would result in losing out on potential savings. These dark patterns create a potent ‘fear of missing out’ effect particularly when combined with the Social Proof (Section 5.1.4) and Scarcity (Section 5.1.5) dark patterns. We observed two types of the Urgency dark pattern: Countdown Timers and Limited-time Messages on 437 shopping websites across their product, cart, and checkout pages. In product pages, these indicated deadlines about site-wide sales and coupons, sales on specific products, or shipping deadlines; in cart pages, they indicated deadlines about product reservation (e.g., ‘Your cart will expire in 10:00 minutes, please check out now’) and coupons, urging users to complete their purchase. Figure 4 highlights instances of these two types. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:15 (a) Countdown Timer on mattressfirm.com . The header displays a Flash Sale where the majority of discounted products remain the same on a day-to-day basis. (b) Countdown Timer on justfab.com . The offer is available even after the timer expires. (c) Limited-time Message on chicwish.com . The website claims the sale will end ‘soon’ without stating a deadline. Fig. 4. Two types of the Urgency category of dark patterns. Countdown Timers . The ‘Countdown Timer’ dark pattern is a dynamic indicator of a deadline, counting down until the deadline expires. Figures 4a and 4b show the Countdown Timer dark pattern on mattressfirm.com and justfab.com , respectively. One indicates the deadline for a recurring Flash Sale , the other a Member Exclusive",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Figures 4a and 4b show the Countdown Timer dark pattern on mattressfirm.com and justfab.com , respectively. One indicates the deadline for a recurring Flash Sale , the other a Member Exclusive . In our data set, we found a total of 393 instances of the Countdown Timer dark pattern. Deceptive Countdown Timers. Using the visit-and-record method described in Section 4.4, we examined the countdown timers in our data set for deceptive practices. We stitched the screenshots of each countdown timer from the repeated visits of our crawler to a website into a video, and viewed the resulting videos to observe the behavior of the timers. We considered a countdown timer deceptive if (1) the timer reset after timeout with the same offer still valid, or (2) the timer expired but the offer it claimed was expiring was still valid even following expiration. In our data set, we discovered a total of 157 instances of deceptive Countdown Timers on 140 shopping websites. One such example is shown in Figure 4b on justfab.com , where the advertised offer remains valid even after the countdown timer of 60 minutes expires. Using our taxonomy of dark pattern characteristics, we classify Countdown Timers as partially covert (it creates a heightened sense of immediacy, unbeknownst to at least some users), and sometimes deceptive (it can mislead users into believing an offer is expiring when in reality it is not) in nature. Limited-time Messages . Unlike Countdown Timers, the ‘Limited-time Message’ dark pattern is a static urgency message without an accompanying deadline. By not stating the deadline, websites withhold information from users, and thus misrepresent the nature of the offer [ 20 ]. Figure 4c shows an instance of the Limited-time Message dark pattern on chicwish.com , where the advertised sale is stated to end ‘soon’ with no mention of the end date. For every such instance we discovered, we verified that the shopping website made no disclosure about the accompanying deadline (e.g., in Proc. ACM Hum.-Comput. Interact., Vol",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". For every such instance we discovered, we verified that the shopping website made no disclosure about the accompanying deadline (e.g., in Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:16 Arunesh Mathur et al. (a) Confirmshaming on radioshack.com . The option to dismiss the popup is framed to shame the user into avoiding it. (b) Visual Interference on greenfingers.com . The option to opt out of marketing communication is grayed, making it seem uavailable even though it can be clicked. (c) Trick Questions on newbalance.co.uk . Opting out of marketing communication requires ticking the checkbox. (d) Pressured Selling on 1800flowers.com . The most expensive product is the default. Fig. 5. Four types of the Misdirection category of dark patterns. the fine print and in the terms of sale pages). In our data set, we discovered a total of 88 instances of the Limited-time Message dark pattern. Using our taxonomy of dark pattern characteristics, we classify Limited-time Messages as at least partially covert (similar to Countdown Timers), and information hiding (unlike Countdown Timers, they do not reveal the deadline in their offers) in nature. 5.1.3 Misdirection. The ‘Misdirection’ category of dark patterns uses visuals, language, and emtion to steer users toward or away from making a particular choice. Misdirection functions by exploiting different affective mechanisms and cognitive biases in users without actually restricting the set of choices available to users. Our version of the Misdirection dark pattern is inspired by Brignull’s original Misdirection dark pattern [ 32 ]. However, while Brignull considered Misdirection to occur exclusively using stylistic and visual manipulation, we take a broader view of the term, also including Misdirection caused by language and emotional manipulation",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We observed four types of the Misdirection dark pattern: Confirmshaming [ 32 ], Trick Quetions [ 32 ], Visual Interference [ 48 ], and Pressured Selling on 244 shopping websites. Figure 5 highlights instances of these four types. Confirmshaming . Coined by Brignull [ 32 ], the ‘Confirmshaming’ dark pattern uses language and emotion to steer users away from making a certain choice. Confirmshaming appeared most often in popup dialogs that solicited users’ email addresses in exchange for a discount, where the option to decline the offer—which the website did not want users to select—was framed as a shameful choice. Examples of such framing included ‘No thanks, I like paying full price’, ‘No thanks, I hate saving money’, and ‘No thanks, I hate fun & games’. By framing the negative option as such, the Confirmshaming dark pattern exploits the framing effect cognitive bias in users and shame, a powerful behavior change agent [ 58 ]. Figure 5a shows one instance of the Confirmshaming dark pattern on radioshack.com . In our data set, we found a total of 169 such instances. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:17 Using our taxonomy of dark pattern characteristics, we classify Confirmshaming as asymmetric (the opt-out choice shames users into avoiding it) in nature. However, Confirmshaming is not covert , since users can visibly see and realize that the design is attempting to influence their choice. Visual Interference . The ‘Visual Interference’ dark pattern uses style and visual presentation to influence users into making certain choices over others (Brignull’s original description of Misdirection [ 32 ]). Although we excluded style information in our clustering analysis, we extracted these patterns as a consequence of examining the text the patterns displayed. In some instances, websites used the Visual Interference dark pattern to make certain courses of action more prominent over others",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". In some instances, websites used the Visual Interference dark pattern to make certain courses of action more prominent over others. For example, the subscription offering on exposedskincare.com is stylistically more prominent and emphasized than the non-subscription offering. In other instances, websites used visual effects on textual descriptions to inflate the discounts available for products. For example, websites such as dyson.co.uk and justfab.com offered free gifts to users, and then used these gifts to inflate the savings on users’ purchases in the checkout page—even when the originally selected product was not on discount. In one instance on greenfingers.com , we discovered that the option to decline marketing communication is greyed out, creating an illusion that the option is unavailable or disabled even though it can be clicked, as shown in Figure 5b. In our data set, we found a total of 25 instances of the Visual Interference dark pattern. Using our taxonomy of dark pattern characteristics, we classify Visual Interference as sometimes asymmetric (in some instances it creates unequal choices, steering users into one choice over the other), covert (users may not realize the effect the visual presentation has had on their choice), and sometimes deceptive (e.g., when a website presents users with a ‘lucky draw’ from a list of potential deals, but the draw process is deterministic unbeknownst to users) in nature. Trick Questions . Also originating from Brignull’s taxonomy [ 32 ], the ‘Trick Questions’ dark pattern uses confusing language to steer users into making certain choices. Like Confirmshaming, Trick Questions attempt to overcome users’ propensity to opt out of marketing and promotional messages by subtly inverting the entire opt-out process",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Like Confirmshaming, Trick Questions attempt to overcome users’ propensity to opt out of marketing and promotional messages by subtly inverting the entire opt-out process. Most often, websites achieved this effect by introducing confusing double negatives (e.g., ‘Uncheck the box if you prefer not to receive email updates’), or by using negatives to alter expected courses of action, such as checking a box to opt out (e.g., ‘We would like to send you emails. If you do not wish to be contacted via email, please ensure that the box is not checked’). We note here that we only considered an opt-out choice as a Trick Question dark pattern when it was misleading, such as when the user has to check a box and the text began with an affirmative statement about the undesirable practice (e.g., ‘We want to send you marketing email’) since these would more likely be missed by users as opposed to ones that began with the opt-out choice (e.g., ‘Please tick here to opt-out of’). 9 Trick Questions exploits the default and framing effect cognitive biases in users, who become more susceptible to a choice they erroneously believe is aligned with their preferences. Figure 5c shows one instance of Trick Questions on newbalance.co.uk . In our data set, we found a total of 9 such instances, occurring most often during the checkout process when collecting user information to complete purchases. Using our taxonomy of dark pattern characteristics, we classify Trick Questions as asymmetric (opting out is more burdensome than opting in) and covert (users fail to understand the effect of their choice as a consequence of the confusing language) in nature. 9 We note that while Gray et al. [ 48 ] consider the latter as Trick Questions, we do not take that stance. However, we do consider all opt-out messages as concerning. We discovered 23 instances of opt-out choices that did not begin with an affirmative statement in total. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:18 Arunesh Mathur et al. Pressured Selling . The ‘Pressured Selling’ dark pattern refers to defaults or often high-pressure tactics that steer users into purchasing a more expensive version of a product ( upselling ) or into purchasing related products ( cross-selling ). The Pressured Selling dark pattern exploits a variety of different cognitive biases, such as the default effect, the anchoring effect, and the scarcity bias to drive user purchasing behavior. Figure 5d shows one such instance on 1800flowers.com , where the largest flower bouquet is selected by default. The dark pattern makes the most expensive option the point of comparison—an ‘anchor’—and thus increases the probability of users overlooking the least expensive option [ 70 ]. In another instance, on fashionworld.co.uk , the website opened popup dialogs that the user had to explicitly decline immediately after adding a product to cart. These dialogs urged users to buy more ‘Hot sellers’, ‘Deals’, and ‘Bundled’ products. In our data set, we found a total of 67 instances of the Pressured Selling dark pattern. Using our taxonomy of dark pattern characteristics, we classify Pressured Selling as sometimes asymmetric (it pushes users towards accepting more expensive product options) and at least partially covert (users fail to realize that they have purchased a more expensive product than they would have, had they been defaulted with the least expensive product to begin with) in nature. 5.1.4 Social Proof. According to the social proof principle, individuals determine the correct action and behavior for themselves in a given situation by examining the action and behavior of others [ 37 , 69 ]. The ‘Social Proof’ dark pattern uses this influence to accelerate user decision-making and purchases, exploiting the bandwagon effect cognitive bias to its advantage",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". The ‘Social Proof’ dark pattern uses this influence to accelerate user decision-making and purchases, exploiting the bandwagon effect cognitive bias to its advantage. Studies have shown that individuals are more likely to impulse buy when shopping with their peers and families [61]. We observed two types of the Social Proof dark pattern: Activity Notifications and Testimonials of Uncertain Origin on 275 websites across their product and cart pages. In all these instances, the Social Proof messages indicated other users’ activities and experiences shopping for products and items. Figure 6 highlights instances of these two types. Activity Notifications . The ‘Activity Notification’ dark pattern is a transient, often recurring and attention grabbing message that appears on product pages indicating the activity of other users. These can be grouped into different categories: dynamic and periodic messages that indicated other users just bought a product (e.g., ‘Abigail from Michigan just bought a new stereo system’); static or dynamic text to indicate how many users have a specific item in their cart (e.g., ‘35 people added this item to cart’); and similar text to indicate how many users have viewed a product (e.g., ‘90 people have viewed this product’). Figures 6a, 6b, and 6c highlight three instances of Activity Notification on tkmaxx.com , thredup.com , and jcpenney.com , respectively. In our data set, we found a total of 313 such instances. Deceptive Activity Notifications. We examined the Activity Notification messages in our data set for deceptive practices. To facilitate our analysis, we manually inspected the page source of each shopping website that displayed these notifications to verify their integrity. We ignored all those notifications that were generated server-side since we had limited insight into how and whether they were truly deceptive",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We ignored all those notifications that were generated server-side since we had limited insight into how and whether they were truly deceptive. We considered an instance of Activity Notification to be deceptive if the content it displayed—including any names, locations statistics, counts—was falsely generated or made misleading statements. In our data set, we discovered a total of 29 instances of deceptive Activity Notifications on 20 shopping websites. The majority of these websites generated their deceptive notifications in a random fashion (e.g., using a random number generator to indicate the number of users who are ‘currently viewing’ a product) and others hard-coded previously generated notifications, meaning they never changed. One notable case was thredup.com as shown in Figure 6b, where the website Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:19 (a) Activity Notification on tkmaxx.com . The message indcates how many people added the product to the cart in the last 72 hours. (b) Activity Notification on thredup.com . The message always signals products as if they were sold recently (‘just saved’), even in the case of old purchases. (c) Activity Notification on jcpenney.com . The message indicates the number of people who viewed the product in the 24 hours along with the quantity left in stock. (d) Testimonials of Uncertain Origin on coolhockey.com . We found the same tetimonials on ealerjerseys.com with diferent customer names. Fig. 6. Two types of the Social Proof category of dark patterns. generated messages based on fictitious names and locations for an unvarying list of products that was always indicated to be ‘just sold’",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 6. Two types of the Social Proof category of dark patterns. generated messages based on fictitious names and locations for an unvarying list of products that was always indicated to be ‘just sold’. Using our taxonomy of dark pattern characteristics, we classify Activity Notifications as partially covert (in instances where the notifications are site-wide for example, users may fail to understand their effect on their choices) and sometimes deceptive (the content of notifications can be deceptively generated or misleading) in nature. Testimonials of Uncertain Origin . The ‘Testimonials of Uncertain Origin’ dark pattern refers to the use of customer testimonials whose origin or how they were sourced and created is not clearly specified. For each instance of this dark pattern, we made two attempts to validate its origin. First, we inspected the website to check if it contained a form to submit testimonials. Second, we performed exact searches of the testimonials on a search engine ( google.com ) to check if they appeared on other websites. Figure 6d shows one instance on coolhockey.com , where we found the same set of testimonials on ealerjerseys.com with different customer names attached to them. In our data set, we found a total of 12 instances of this pattern. 5.1.5 Scarcity. ‘Scarcity’ refers to the category of dark patterns that signal the limited availability or high demand of a product, thus increasing its perceived value and desirability [ 37 , 55 , 62 , 69 ]. We observed two types of the Scarcity dark pattern: ‘Low-stock Messages’ and ‘High-demand Messages’ on 609 shopping websites across their product and cart pages. In both pages, they indicated the limited availability of a product or that a product was in high demand and thus likely to become unavailable soon. Figure 7 highlights instances of these two types. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:20 Arunesh Mathur et al. (a) Low-stock Message on 6pm.com",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:20 Arunesh Mathur et al. (a) Low-stock Message on 6pm.com . Left: Choosing product options shows Only 3 left in stock . Right: The out-of-stock product makes it seem that it just sold out. (b) Low-stock on orthofeet .com . Appears for all products. (c) High-demand Message on fashionnova.com . The message appears for all products in the cart. Fig. 7. Two types of the Scarcity category of dark patterns. Low-stock Messages . The ‘Low-stock Message’ dark pattern signals to users about limited quantities of a product. Figure 7a shows an instance of this pattern on 6pm.com , displaying the precise quantity in stock. In our data set, we found a total of 632 instances of the Low-stock Message dark pattern. However, not all of these instances displayed stock quantities. 49 of these instances only indicated that stock was limited or low, without displaying the exact quantity, resulting in uncertainty, increased desirability of products, and impulse buying behavior in users. Figure 7b shows one such instance on orthofeet.com . Deceptive Low-stock Messages. We examined all the Low-stock Message dark patterns for dceptive practices using the method described in Section 4.4. From the resulting data, we ignored those websites whose stock amounts remained the same between visits, reasoning that those are unlikely to be indicative of deceptive practices. We then manually examined the remaining sites and identified how the stock information was generated. In our data set, we discovered a total of 17 instances of deceptive Low-stock Messages on 17 shopping websites. On further examination, we observed that 16 of these sites decremented stock amounts in a recurring, deterministic pattern according to a schedule, and the one remaining site ( forwardrevive.com ) randomly generated stock values on page load",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Exactly 8 of these sites used third-party JavaScript libraries to generate the stock values, such as Hurrify [ 17 ] and Booster [ 11 ]. Both of these are popular plugins for Shopify—one of the largest e-Commerce companies—based websites. The remaining websites injected stock amounts through first-party JavaScript or HTML. Besides the use—or non-use—of numeric data and deception, Low-stock Messages can be cocerning in other ways. For example, we observed that several websites, such as 6pm.com and orthofeet.com , displayed Low-stock Messages for nearly all their products—stating ‘Only X left’ and ‘Hurry, limited quantities left!’ respectively. The former, in particular, showed a ‘Sorry, this is out of stock. You just missed it’ popup dialog for every product that was sold out, even if it had already been out of stock in the previous days. Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:21 (a) Hard to Cancel on sportsmanguide.com . The website only discloses in the terms PDF file that canceling the recurring service requires calling customer service. (b) Hard to Cancel on savagex.com . The website discloses upfront that the recurring service can only be canceled through customer care. Fig. 8. The Hard to Cancel type from the Obstruction category of dark patterns. Using our taxonomy of dark pattern characteristics, we classify Low-stock Messages as partially covert (it creates a heightened sense of impulse buying, unbeknownst to users), sometimes deceptive (it can mislead users into believing a product is low on stock when in reality it is not, creating false scarcity), and sometimes information hiding (in some instances, it does not explicitly specify the stock quantities at hand) in nature. High-demand Messages . The ‘High-demand Message’ dark pattern signals to users that a product is in high demand, implying that it is likely to sell out soon",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". High-demand Messages . The ‘High-demand Message’ dark pattern signals to users that a product is in high demand, implying that it is likely to sell out soon. Figure 7c shows one such instance on fashionnova.com on the cart page, indicating that the products in the cart are selling out quickly. In our data set, we found a total of 47 instances of the High-demand dark pattern; 38 of these instances appeared consistently, regardless of the product displayed on the website, or regardless of the items in cart. As with Low-stock Messages, we classify High-demand Messages as partially covert . 5.1.6 Obstruction. ‘Obstruction’, coined by Gray et al. [ 48 ], refers to the category of dark patterns that make a certain action harder than it should be in order to dissuade users from taking that action. We observed one type of the Obstruction dark pattern: ‘Hard to Cancel’—a pattern similar to Brignull’s Roach Motel dark pattern [ 32 ]—on 31 websites. Obstruction makes it easy for users to sign up for recurring subscriptions and memberships, but it makes it hard for them to subsequently cancel the subscriptions. More often than not, shopping websites did not disclose upfront to users that canceling the subscription or membership could not be completed in the same manner they signed up for the memberships in the first place. For example, as shown in Figure 8a, sportsmansguide.com prmotes a ‘buyer’s club’ discount membership price and makes it easy for users to sign up for the annual recurring membership, as they are under the impression they can ‘cancel anytime.’ However, sportsmansguide.com ’s terms of service reveal that the membership can only be cancelled by caling their customer service. In rare instances, as shown in Figure 8b, websites such as savagex.com disclosed upfront that cancellation required calling customer service. Using our taxonomy of dark pattern characteristics, we classify Hard to Cancel as restrictive (it limits the choices users can exercise to cancel their services) in nature",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Using our taxonomy of dark pattern characteristics, we classify Hard to Cancel as restrictive (it limits the choices users can exercise to cancel their services) in nature. In cases where websites do Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:22 Arunesh Mathur et al. (a) Forced Enrollment on musiciansfriend.com . Agreeing to the terms of use also requires agreeing to receive emails and promotions. (b) Forced Enrollment on therealreal.com . Browsing the website requires creating an account even without making a purchase. Fig. 9. The Forced Enrollment type from the Forced Action category of dark patterns. not disclose their cancellation policies upfront, Hard to Cancel also becomes information hiding (it fails to inform users about how cancellation is harder than signing up) in nature. 5.1.7 Forced Action. ‘Forced Action’ refers to the category of dark patterns—originally proposed by Gray et al. [ 48 ]—that require users to take certain additional and tangential actions to complete their tasks. We observed one type of the Forced Action dark pattern, ‘Forced Enrollment’, on 6 websites. This type of dark pattern explicitly coerces users into signing up for marketing communication, or creates accounts to surrender users’ information. By using the Forced Enrollment dark pattern, online services and websites collected more information about their users than they might otherwise consent to—resulting from an all-or-nothing proposition. On four out of six websites, the Forced Enrollment dark pattern manifested as a checkbox in the user interface, requiring users to simultaneously consent to the terms of service and to receiving marketing emails as part of the consent process. Figure 9a shows one such instance on musiciansfriend.com",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Figure 9a shows one such instance on musiciansfriend.com . In another instance of the Forced Enrollment on therealreal.com —as shown in Figure 9b—the website displayed a popup dialog that prevented users from viewing product offerings on the website without creating an account—even if users eventually decide against making a purchase. Using our taxonomy of dark pattern characteristics, we classify Forced Enrollment as asymmetric (it requires competing the additional, tangential tasks, creating unequal choices) and restrictive (it mandates enrolling in marketing communication or creating accounts) in nature. 5.2 Dark Patterns as A Third-Party Service: A Case Study Of Social Proof Activity Notifications In many instances, third-party entities—i.e., organizations and companies other than the shopping websites themselves—were responsible for creating and presenting dark patterns on behalf of the shopping websites. We observed this frequently to be the case for one dark pattern in particular: Social Proof Activity Notifications (Section 5.1.4). In this section, we shed light on the ecosystem of third parties that enable Social Proof Activity Notifications, using our starting point as the list of websites in our data set that displayed such Activity Notifications. 5.2.1 Detecting Third-party Entities. In order to detect third-party entities, it is sufficient to uncover scripts that are served from third-party domains and are responsible for creating Social Proof Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:23 Activity Notifications. However, automatically attributing certain interface elements and webpage modifications to third-party scripts constitutes a more challenging task because modern browsers do not expose any means to attribute DOM changes (e.g. displaying a popup dialog) to particular scripts. Further, web pages may be modified by several different first and third-party scripts in the same visit, making attribution trickier",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". displaying a popup dialog) to particular scripts. Further, web pages may be modified by several different first and third-party scripts in the same visit, making attribution trickier. To overcome this challenge, we employed a combination of automated and manual analyses. We used the following observation: when a third-party entity displays an Activity Notification on a shopping website, its content should be included in the HTTP response received from this third party’s servers on that website. For example, if the notification states ‘Jane from Washington, DC just purchased this product’, looking up the customer name and location—in this case ‘Jane’ and ‘Washington, DC’—in the HAR file for that website should reveal the end point of the server that issued the notification. Thus, for all notifications of this kind, we extracted the name and location pairs from the content, searched the HAR files for these pairs; where successful, we recorded the HTTP endpoints corresponding to the third-parties. We then manually verified these endpoints and determined the responsible entities by using the WHOIS database, visiting the script domains and using search engines to uncover the company identities and websites. Where this analysis failed to return an HTTP endpoint from the HAR files, and for all other kinds of Social Proof Activity Notification (e.g., ‘This product was added to cart 10 times in the last day’), we manually visited the websites containing the message to determine the responsible third parties. We sped up this analysis using Google Chrome Developer Tool’s ‘DOM change breakpoints’ feature [16], which helped us easily determine the responsible entities. Having determined the third-party entities, we measured their prevalence across all the shopping websites in our data set. To do so, we searched the HTTP request data from checkout crawls for the third-party domains we identified",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". To do so, we searched the HTTP request data from checkout crawls for the third-party domains we identified. Finally, as a reference point, we also determined their prevalence on the web—beyond shopping websites—using the latest publicly available crawl data (November 2018) from the Princeton Web Census Project [ 7 , 40 ]. This public project documents the prevalence of third-party scripts using periodic scans of home pages of Alexa top million sites and is available for external researchers to use. 5.2.2 The Ecosystem Of Third-party Entities. Table 2 summarizes our findings. We discovered a total of 22 third-party entities, embedded in 1,066 of the 11K shopping websites in our data set, and in 7,769 of the Alexa top million websites. We note that the prevalence figures from the Princeton Web Census Project data should be taken as a lower bound since their crawls are limited to home pages of websites. This difference in prevalence is particularly visible for certain third-party entities like Qubit and Taggstar, where their prevalence is higher in our data set compared to the Web Census data. By manually examining websites that contained these third parties, we discovered that many shopping websites only embedded them in their product—and not home—pages, presumably for functionality and performance reasons. We learned that many third-party entities offered a variety of services for shopping websites, including plugins for popular e-commerce platforms such as Shopify 10 and Woocommerce 11 . To better understand the nature and capabilities of each third-party entity, we examined any publicly available marketing materials on their websites. Broadly, we could classify the third-party entities into two groups. The first group exclusively provided Social Proof Activity Notifications integration as a service. The second group provided a wider array of marketing services that often enabled other types of dark patterns; most commonly 10 https://shopify.com 11 https://woocommerce.com Proc. ACM Hum.-Comput. Interact., Vol",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:24 Arunesh Mathur et al. Table 2. List and prevalence of Social Proof Activity Notifications enabling third-party entities in our data set of 11K shopping websites and the home pages of Alexa top million websites [ 7 ]. Where available, we list additional dark patterns the third parties claim to offer. Nice/Bizzy, Woocommerce Notification, Boost, and Amasty are Shopify, Woocommerce, Wordpress and Magento plugins respectively. Third-party Entity Prevalence Additional Dark Patterns # Shopping Websites # Alexa Top Million Beeketing 406 4,151 Pressured Selling, Urgency, Scarcity Dynamic Yield 114 416 Urgency Yieldify 111 323 Urgency, Scarcity Fomo 91 663 – Fresh Relevance 86 208 Urgency Insider 52 484 Scarcity, Urgency Bizzy 33 213 – ConvertCart 31 62 – Taggstar 27 4 Scarcity, Urgency Qubit 25 73 Pressured Selling, Scarcity, Urgency Exponea 18 180 Urgency, Scarcity Recently 14 66 – Proof 11 508 – Fera 11 132 Pressured Selling, Scarcity, Urgency Nice 10 80 – Woocommerce Notification 10 61 – Bunting 5 17 Urgency, Scarcity Credibly 4 67 – Convertize 3 58 Scarcity, Urgency LeanConvert 2 0 – Boost 1 3 – Amasty 1 0 Pressured Selling, Scarcity, Urgency these were Scarcity and Urgency dark patterns. We list all these additional dark pattern capabilities in the rightmost column of Table 2. Many of the third-parties advertised practices that appeared to be—and sometimes unambiguously were—manipulative: ‘[p]lay upon [customers’] fear of missing out by showing shoppers which products are creating a buzz on your website’ (Fresh Relevance), ‘[c]reate a sense of urgency to boost conversions and speed up sales cycles with Price Alert Web Push’ (Insider), ‘[t]ake advantage of impulse purchases or encourage visitors over shipping thresholds’ (Qubit). Further, Qubit also advertised Social Proof Activity Notifications that could be tailored to users’ preferences and backgrounds",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Further, Qubit also advertised Social Proof Activity Notifications that could be tailored to users’ preferences and backgrounds. In some instances, we found that third parties openly advertised the deceptive capabilities of their products. For example, Boost dedicated a web page—titled ‘Fake it till you make it’—to describing how it could help create fake orders [ 12 ]. Woocommerce Notification—a Woocommerce platform plugin—also advertised that it could create fake social proof messages: ‘[t]he plugin will create fake orders of the selected products’ [ 24 ]. Interestingly, certain third parties (Fomo, Proof, and Boost) used Activity Notifications on their websites to promote their own products. Finally, we also discovered that some of these deceptive practices resulted in e-commerce plaforms taking action against third-party entities. For instance, Beeketing’s—the most popular third Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:25 This is an instance of Dark Pattern called ‘Countdown Timer’. The timer might be fake. Click to learn more. Fig. 10. Mockup of a possible browser extension that can be developed using our data set. The extension flags instances of dark patterns with a red warning icon. By hovering over the icon, the user can learn more about the specific pattern. party provider in our data set—‘Sales Pop’ Shopify plugin was temporarily removed from Shopify in an effort to crack down on deceptive practices [ 67 , 76 ]. The plugin had allowed websites to create fake Activity Notifications by entering fabricated sales data. In summary, we discovered that third party entities widely enable dark patterns on shopping websites. Furthermore, some of these third-parties even advertised the deceptive use of their services. 6 DISCUSSION 6.1 Dark Patterns and Implications For Consumers Many dark patterns constitute manipulative and deceptive practices that past work has shown users are increasingly becoming aware of [ 36 ]",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Our current data set of dark patterns, comprising of screenshots and text segments, can be used to build countermeasures to help users make more informed decisions even in the presence of dark patterns. One such countermeasure could be a public-facing website that scores shopping websites based on their use of dark patterns. Our data set can also enable the development of browser extensions that automatically detect and flag dark patterns (e.g., shopping websites, as shown in Figure 10). Such a tool could be augmented to flag dark patterns on websites not in our data set through users’ submissions, through communitgenerated and maintained lists (similar to how ad blockers work [ 26 ]), or through trained machine learning classifiers. Eventually, such tools could be integrated into browsers themselves. For example, in recent years, Firefox and Safari have shown interest in integrating tools that promote consumer privacy (e.g., features to block web tracking by default [ 66 , 82 ]). However, finding the right incentives for browser vendors to implement these solutions might be challenging in the context of dark patterns, since they might be wary of policing content on the web. Finally, future studies could leverage our descriptive and comparative taxonomy of dark pattern characteristics to better understand their effects on users, as well as to ascertain which dark patterns are considered most egregious by users (e.g., by means of users studies). 6.2 Implications for Consumer Protection Policy and Retailers Our results demonstrate that a number of shopping websites use deceptive dark patterns, involving affirmative and false representations to consumers. We also found 22 different third-party entities that enable the creation of Social Proof Activity Notification dark patterns. Some of these entities promote blatantly deceptive practices and provide the infrastructure for retailers to use these Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. 81:26 Arunesh Mathur et al. practices to influence consumer behavior for profit. These practices are unambiguously unlawful in the United States (under Section 5 of the Federal Trade Commission Act and similar state laws [ 45 ]), and the European Union (under the Unfair Commercial Practices Directive and similar member state laws [42]). We also find practices that are unlawful in a smaller set of jurisdictions. In the European Union, businesses are bound by an array of affirmative disclosure and independent consent requirements in the Consumer Rights Directive [ 43 ]. Websites that use the Sneaking dark patterns (Sneak into Basket, Hidden Subscription, and Hidden Costs) on European Union consumers are likely in violation of the Directive. Furthermore, user consent obtained through Trick Questions and Visual Interference dark patterns do not constitute freely given, informed and active consent as required by the General Data Protection Regulation (GDPR) [ 44 ]. In fact, the Norwegian Consumer Council filed a GDPR complaint against Google in 2018, arguing that Google used dark patterns to manipulate users into turning on the ‘Location History’ feature on Android, and thus enabling constant location tracking [47]. In addition to demonstrating specific instances of unlawful business practices, we contribute a new approach for regulatory agencies and other consumer protection stakeholders (e.g., journalists and civil society groups) to detect dark patterns. The crawling and clustering methodology that we developed is readily generalizable, and it radically reduces the difficulty of discovering and measuring dark patterns at web scale. Furthermore, our data set of third-party entities which provide the infrastructure to enable certain deceptive dark patterns can be used by regulators as a starting point to inform policy and regulation around what kinds of practices should be allowable in the context online shopping",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". 6.3 Dark Patterns and Future Studies At Scale We created automated techniques that can be used to conduct measurements of dark patterns at web scale. Researchers can extend our tools and infrastructure to document the presence of dark patterns other types of websites (e.g., travel and ticket booking websites) by building a crawler that traverses users’ primary interaction paths on those websites. Researchers can also extend our techniques to measure dark patterns that are not inherently dark because of the text they display but because they take advantage of visual elements. For example, urgency can be created by a blinking timer; similarly, Hidden Subscriptions can make the default option (e.g., subscribing to a paid service) visually more appealing and noticeable than its alternative (e.g., not subscribing). One starting point to detect such interfaces could be to incorporate style and color as features for clustering, or even use the design mining literature [ 39 , 56 , 59 ] to analyze specific types of interfaces (e.g., page headers) in isolation. Finally, researchers can leverage our descriptive taxonomy of dark pattern characteristics to study and analyze dark patterns in other domains, such as emails and mobile applications. 6.4 Limitations Our research has several limitations. First, we only take into account text-based dark patterns and, therefore, leave out those that are inherently visual (e.g., using font size or color to emphasize one part of the text more than another). Second, many of the dark patterns we document are derived from the existing dark patterns literature. However, some of these are exist in a gray area, and in those cases determining whether a dark pattern is deliberately misleading or not can sometimes be hard to discern. Opinions of dark patterns may also vary between and among experts and users (e.g., countdown timers to indicate when to order to be eligible for free shipping)",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". Opinions of dark patterns may also vary between and among experts and users (e.g., countdown timers to indicate when to order to be eligible for free shipping). Clarifying this gray area and establishing the degree to which these patterns are perceived as manipulative by users can be further investigated by future user studies. Third, in Section 3 we drew connections between Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW, Article 81. Publication date: November 2019. Dark Patterns at Scale 81:27 each type of dark pattern and a set of cognitive biases it exploits. However, these connections may be more nuanced or complex. For example, not all individuals may be equally susceptible to these cognitive biases; some individuals may be more susceptible to one kind over another. Fourth, during our crawls we experienced a small number of Selenium crashes, which did not allow us to either retrieve product pages or complete data collection on certain websites. Fifth, while the crawler was mostly effective in simulating user actions, it failed to complete the product purchase flow on some websites (see Section 4). Sixth, and finally, we only crawled product pages and checkout pages, missing out on dark patterns commonly present in other pages, such as the home page, product search, and account creation pages. Many dark patterns also appear after purchase (e.g., upselling) which our crawler fails to capture because we do not make purchases. Future studies could consider collecting these kinds of dark patterns from users. 7 CONCLUSION In this paper, we developed automated techniques to study dark patterns on the web at scale. By simulating user actions on the ∼ 11K most popular shopping websites, we collected text and screenshots of these websites to identify their use of dark patterns. We defined and characterized these dark patterns, describing how they affect users’ decisions by linking our definitions to the cognitive biases leveraged by dark patterns",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites",
    "author": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini, Jonathan Mayer, Marshini Chetty, and Arvind Narayanan",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Mathur-2019-Dark-patterns-at-scale.pdf",
    "date_published": "2019-09-23",
    "keywords": "Dark Patterns; Consumer Protection; Deceptive Content; Nudging; Manipulation",
    "flag": "",
    "chunk_text": ". We defined and characterized these dark patterns, describing how they affect users’ decisions by linking our definitions to the cognitive biases leveraged by dark patterns. We found at least one instance of dark pattern on approximately 11.1% of the examined websites. Notably, 183 of the websites displayed deceptive messages. Furthermore, we observed that dark patterns are more likely to appear on popular websites. Finally, we discovered that dark patterns are often enabled by third-party entities, of which we identify 22; two of these advertise practices that enable deceptive patterns. Based on these findings, we suggest that future work focuses on empirically evaluating the effects of dark patterns on user behavior, developing countermeasures against dark patterns so that users have a fair and transparent experience, and extending our work to discover dark patterns in other domains. ACKNOWLEDGMENTS We are grateful to Mihir Kshirsagar, Finn Myrstad, Vincent Toubiana, and Joe Calandrino for feedback on this paper.",
    "chunk_id": "Human_computer_interaction_dark_patterns_at_scale_findings_from_a_crawl_of_11k_shopping_websites.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "A FFORDANCE , C ONVENTIONS , AND D ESIGN A FFORDANCE , C ONVENTIONS , AND D ESIGN DONALD A. NORMAN The Nielsen Norman Group Web: http://www.jnd.org E-mail: don@jnd.org I was quietly lurking in the background of a CHI-Web discussion, when I lost all reason: I just couldn’t take it anymore. “I put an affordance there,” a participant would say, “I wonder if the object affords clicking “ Affordances this, affordances that. And no data, just opinion. Yikes! What had I unleashed upon the world? “No!” I screamed, and out came this article. I don’t know if it changed anyone’s minds, but it brought the CHI-Web discussion to a halt (not what good list managers want to happen). But then, Steven Pemberton asked me to submit it here. Hope it doesn’t stop the discussion again. Mind you, this is not the exact piece I dashed off to CHI-Web: it has been polished and refined: the requirements of print are more demanding than those of e-mail discussions. a r t i c l e To my great surprise, the concept of affodance was adopted by the design community, especially graphical and industrial design. Alas, yes, the concept has caught on, but not always with complete understanding. My fault: I was really talking about perceived affordances, which are not at all the same as real ones. Perceived Affordance POET was about “perceived affordance.” When I get around to revising POET, I will make a global change, replacing all instances of the word “affordance” with the phrase “peceived affordance.” The designer cares more about what actions the user perceives to be possible than what is true. Moreover, affodances, both real and perceived, play very diferent roles in physical products than they do in the world of screen-based products. In the latter case, affordances play a relatively minor role: cultural conventions are much more important. More on that in a moment. In product design, where one deals with real, physical objects, there can be both real and perceived affordances, and the two sets need not be the same",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". More on that in a moment. In product design, where one deals with real, physical objects, there can be both real and perceived affordances, and the two sets need not be the same. In graphical, screen-based interfaces, the designer primarily can control only perceived affordances. The computer system already comes with built-in physical affordances. The computer, with its keyboard, display screen, pointing device, and selection buttons (e.g., mouse buttons) affords pointing, touching, looking, and clicking on every pixel of the screen. Most of this affordance is of little interest for the purpose of the application under design. Although all screens within reaching ditance afford touching, only some can detect the touch and respond to it. Thus, if the diplay does not have a touch-sensitive screen, the screen still affords touching, but it has no effect on the computer system. While the affordance has useful value in allowing people viewing the same screen to indicate regions of interest, this affordance mainly serves to make the screen-cleaning companies happy: they can sell lots of tissue and cleaning fluid. But this affordance is seldom useful to the inteface designer. Affordances, Constraints, and Conceptual Models The word affordance was coined by the peceptual psychologist J. J. Gibson [1, 2] to refer to the actionable properties between the world and an actor (a person or animal). To Gibson, affordances are relationships. They exist natrally: they do not have to be visible, known, or desirable. I originally hated the idea: it didn’t make sense. I cared about processing mechanisms, and Gibson waved them off as irrelevant. Then Gibson started spending considerable time in La Jolla, and so I was able to argue with him for long hours (both of us relished intellectual arguments)",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Then Gibson started spending considerable time in La Jolla, and so I was able to argue with him for long hours (both of us relished intellectual arguments). I came to appreciate the concept of affordances, even if I never understood his other concepts, such as “infomation pickup.” He and I disagreed fundmentally about how the mind actually processes perceptual information (that phrase alone would infuriate him). Turn now to the late 1980s, when I spent a sabbatical at the Applied Psychology Unit in Cambridge, England. My struggles with British water taps, light switches, and doors propelled me to write The Psychology of Everday Things (POET [5]). A major theme of POET was the attempt to understand how we managed in a world of tens of thousands of objects, many of which we would encounter only once. When you first see something you have never seen before, how do you know what to do? The answer, I decided, was that the required information was in the world: the appearance of the device could provide the critical clues required for its proper operation. In POET, I argued that understanding how to operate a novel device had three major dimensions: conceptual models, constraints, and affordances. These three concepts have had a mixed reception. To me, the most important part of a sucessful design is the underlying conceptual model. This is the hard part of design: formlating an appropriate conceptual model and then assuring that everything else be consitent with it. I see lots of token acceptance of this idea, but far too little serious work. The power of constraints has largely been ignored. When you learn not to click unless you have the proper cursor form, you are following a cultural constraint. sor. Now if we locked the mouse button when the wrong cursor appeared, that would be a real affordance, although somewhat pondeous. The cursor shape is visual information: it is a learned convention. When you learn not to click unless you have the proper cursor form, you are following a cultural constraint",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The cursor shape is visual information: it is a learned convention. When you learn not to click unless you have the proper cursor form, you are following a cultural constraint. Far too often I hear graphic designers claim that they have added an affordance to the screen design when they have done nothing of the sort. Usually they mean that some graphcal depiction suggests to the user that a certain action is possible. This is not affordance, either real or perceived. Honest, it isn’t. It is a symbolic communication, one that works only if it follows a convention understood by the user. Constraints and Conventions When designing a graphical screen layout, designers greatly rely on conventional intepretations of the symbols and placement. Much of the discussion about the use of affodances is really addressing conventions, or what I call cultural constraints. In POET, I introduced the distinctions among three kinds of behavioral constraints: physical, logical, and cultural. These are powerful design tools, so let’s be clear where each is being used. Physical constraints are closely related to real affordances: For example, it is not possible to move the cursor outside the screen: this is a physical constraint. Locking the mouse buton when clicking is not desired would be a physical constraint. Restricting the cursor to exist only in screen locations where its postion is meaningful is a physical constraint. Logical constraints use reasoning to detemine the alternatives. Thus, if we ask the user to click on five locations and only four are immediately visible, the person knows, logcally, that there is one location off the screen. Logical constraints are valuable in guiding behavior. It is how the user knows to scroll down and see the rest of the page. It is how users know when they have finished a task. By making the fundamental design model visible, users can readily (logically) deduce what actions are required. Logical constraints go hand in hand with a good conceptual model",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". By making the fundamental design model visible, users can readily (logically) deduce what actions are required. Logical constraints go hand in hand with a good conceptual model. Cultural constraints are conventions shared Now consider the traditional computer screen where the user can move the cursor to any location on the screen and click the mouse button at anytime. In this circumstance, designers sometimes will say that when they put an icon, cursor, or other target on the screen, they have added an “affordance” to the system. This is a misuse of the concept. The affordance exists independently of what is visble on the screen. Those displays are not affodances; they are visual feedback that advertise the affordances: they are the perceived affodances. The difference is important because they are independent design concepts: the affordances, the feedback, and the perceived affordances can all be manipulated indepedently of one another. Perceived affordances are sometimes useful even if the system does not support the real affordance. Real affodances do not always have to have a visible presence (and in some cases, it is best to hide the real affordance). And the presence of feeback can dramatically affect the usability and understandability of a system, but quite indpendently of the affordances or their visibility. Similarly, it is wrong to claim that the design of a graphical object on the screen “affords clicking.” Sure, you can click on the object, but you can click anywhere. Yes, the object provides a target and it helps the user know where to click and maybe even what to expect in return, but those aren’t affordances, those are conventions, and feedback, and the like",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Yes, the object provides a target and it helps the user know where to click and maybe even what to expect in return, but those aren’t affordances, those are conventions, and feedback, and the like. This is what the interface designer should care about: Does the user perceive that clicing on that object is a meaningful, useful action, with a known outcome? It is possible to change the physical affodances of the screen so that the cursor appears only at spots that are defined to be “clickable.” This would indeed allow a designer to add or subtract the affordance of clicking, much as many computer forms afford the addition of characters only in designated fields. This would be a real use of affordances. In today’s screen design sometimes the cusor shape changes to indicate the desired action (e.g., the change from arrow to hand shape in a browser), but this is a convention, not an affordance. After all, the user can still click anywhere, whatever the shape of the cu a r t i c l e My partner, Jakob Nielsen, has long argued that you can get these data at a discount: three to five people will give you enough for most purposes [3, 4] . But they need to be real peple, doing real activities. Don’t speculate. Don’t argue. Observe. Concluding Summary We have many tactics to follow to help people understand how to use our designs. It is important to be clear about the distinctions among them, for they have very different functions and implications. Sloppy thinking about the concepts and tactics often leads to sloppiness in design. And sloppiness in design translates into confusion for users. In this article I covered the following cocepts: ! The conceptual model ! Real affordances ! Perceived affordances ! Constraints ! Conventions The most important design tool is that of coherence and understandability, which comes through an explicit, perceivable coceptual model. Affordances specify the range of possible activities, but affordances are of little use if they are not visible to the users",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Affordances specify the range of possible activities, but affordances are of little use if they are not visible to the users. Hence, the art of the designer is to ensure that the desired, relevant actions are readily perceivable. Today we do much of our design on coputer screens, where the range of possible actions are limited to typing on a keyboard, pointing with a mouse, and clicking on mouse and keyboard switches. Soon we will add spken words and visual gestures to the list of interactions. All of these actions are abstract and arbitrary compared to the real, physical manipulation of objects, which is where the power of real and perceived affordances lies. Today’s design often lies in the virtual world, where depiction stands in for reality. Many aspects of physical affordances are denied the designer: the alternatives are constraints and conventions. These are powerful when used well. Personally, I believe that our reliance on abstract representations and actions is a mitake and that people would be better served if we would return to control through physical by a cultural group. The fact that the graphic on the right-hand side of a display is a “scroll bar” and that one should move the cursor to it, hold down a mouse button, and “drag” it downward in order to see objects located below the current visible set (thus causing the image itself to appear to move upwards) is a cultural, learned convention. The choice of action is arbitrary: there is nothing inherent in the devices or design that requires the system to act in this way. The word “arbitrary” does not mean that any random depiction would do equally well: the current choice is an inteligent fit to human cognition, but there are alternative methods that work equally well. A convention is a constraint in that it prhibits some activities and encourages others. Physical constraints make some actions impossible: there is no way to ignore them",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Affordance, conventions, and design",
    "author": "Hal",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\affordance_conventions_design_norman.pdf",
    "date_published": "1999-04-27",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A convention is a constraint in that it prhibits some activities and encourages others. Physical constraints make some actions impossible: there is no way to ignore them. Logical and cultural constraints are weaker in the sense that they can be violated or ignored, but they act as valuable aids to navigating the unknowns and complexities of everyday life. As a result, they are powerful tools for the designer. A convention is a cultural constraint, one that has evolved over time. Conventions are not arbitrary: they evolve, they require a community of practice. They are slow to be adopted and, once adopted, slow to go away. So although the word implies voluntary choice, the reality is that they are real costraints on our behavior. Use them with respect. Violate them only with great risk. Symbols and constraints are not affodances. They are examples of the use of a shared and visible conceptual model, approprate feedback, and shared, cultural conventions. How do you know if the user shares the conventions? Why, with data, of course. This is something that cannot be decided by argments, logic, or theory. Cultural constraints and conventions are about what people believe and do, and the only way to find out what people do is to go out and watch them—not in the laboratories, not in the usability testing rooms, but in their normal environment. I still hear far too much dogmatism about what people really “want,” what they “believe,” or how they “really” behave, but I see very little data. It doesn’t take much data. Logical and cultural constraints are powerful tools for the designer. not so readily change established social coventions. Know the difference and exploit that knowledge. Skilled design makes use of all.",
    "chunk_id": "Human_computer_interaction_affordance,_conventions,_and_design.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Postel’s Law Be conservative in what you do, be liberal in what you accept from others. Key Takeaways • Be empathetic to, flexible about, and tolerant of any of the various actions the user could take or any input they might provide. • Anticipate virtually anything in terms of input, access, and capabiity while providing a reliable and accessible interface. • The more we can anticipate and plan for in design, the more resiient the design will be. • Accept variable input from users, translating that input to meet your requirements, defining boundaries for input, and providing clear feedback to the user. Overview Designing good user experiences means designing good human experiences. People don’t behave like machines: we are sometimes inconsistent, frequently distracted, occasionally error-prone, and usually driven by emotion. We expect the products and services we interact with to intuitively understand us and be fogiving. We expect to feel in control at all times, and we’re generally annoyed when asked to provide more information than is necessary. At the same time, the devices and software we use vary greatly in terms of feature support, capabilities, and form factors. To be capable of meeting users’ expectations, the products and 43 1 Jon Postel, “RFC 793: Transmission Control Protocol,” September 1981, https://www.rfc-editor.org/rfc/ rfc793 . services designers build must be robust and adaptable. Postel’s law, also known as the robustness principle, gives us a guiding principle for designing humacentric experiences that account for both scale and complexity. The first half of Postel’s law states that you should “be conservative in what you do.” In the context of design this can be interpreted as stipulating that the output of our efforts, whether that’s an interface or a comprehensive system, should be reliable and accessible",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These are important characteristics of a digital product or service, because not only must the interface be easy to use, but it must be easy to use for the largest spectrum of users possible. This means that anyone, regardless of device size, feature support, input mechanism, assistive technology, or even connection speed, should be served something that works. The second half of the principle states that you should “be liberal in what you accept from others.” In the context of design, this can be taken to mean the acceptance of input from users via any input mechanism and in a variety of posible formats. It applies to data entered into a form via mouse and keyboard (or perhaps keyboard only), assistive technology, touch and gesture input from mobile users, and even voice input in all its variations of language, dialect, and nomenclature. It applies to screens of any size and resolution, from a watch interface all the way up to a TV. It encompasses differences in network banwidth, connection strength, and any other possible variation. In this chapter, we’ll take a close look at some examples of Postel’s law in action and how designers can leverage this principle to design products and serices that adapt to how people actually are. Origins Jon Postel was an American computer scientist who made significant contribtions to the underlying protocols that would come to form the internet. One of these contributions was an early implementation of the Transmission Control Protocol (TCP), the foundation on which data is sent and received over a nework",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". One of these contributions was an early implementation of the Transmission Control Protocol (TCP), the foundation on which data is sent and received over a nework. In this specification Postel introduced what he called the robustness princple , which stated that “TCP implementations will follow a general principle of robustness: be conservative in what you do, be liberal in what you accept from others.” 1 The idea was that programs that send data (either to other machines or to different programs on the same machine) should conform to specifications, while programs that receive data should be robust enough to accept and parse nonconformant input, as long as the meaning is clear. Postel’s principle was originally intended to be a guideline for network engneering, specifically in regard to the transfer of data across computer networks. The fault tolerance introduced by the robustness principle helped to ensure nodes on the early internet could communicate reliably, but its influence extends beyond just computer network engineering—software architecture has also been influenced by this principle. Take, for example, declarative languages such as HTML and CSS. Their loose error handling means that problems like authoring mistakes or lack of browser support for specific features are handled gracefully by the browser. If the browser doesn’t understand something, it simply ignores it and moves on. This has lent an amazing amount of flexibility to these laguages—flexibility that led to their dominance on the internet stage. The philosophy outlined in Postel’s law can also be applied to user experence design and how we deal with user input and system output. As I mentioned earlier, designing good user experiences means designing good human experieces. Since humans and computers communicate and process information in fudamentally different ways, it’s the responsibility of design to bridge the communication gap. Let’s take a look at some examples to see how this can be done",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Let’s take a look at some examples to see how this can be done. Examples Postel’s law describes an approach to design that’s more akin to the philosophy of human–computer interaction: we should anticipate virtually anything in terms of input, access, and capability while providing a reliable and accessible interface. There are countless examples that demonstrate this philosophical approach, but we’ll begin with one that is ubiquitous in the digital world: input forms. Forms have long been a primary means for people to provide information to systems in the digital space. In essence, they are the medium through which humans and systems interact: a product or service requires information, and the user provides that information by way of form elements that are submitted for processing. Using Postel’s law as a guide in regard to forms, the first consideration is to be conservative in how much information you ask people to provide. The more fields you require users to fill out, the more cognitive energy and effort you’re asking of them, which can lead to a deterioration in the quality of the decisions made (commonly referred to as decision fatigue ) and reduce the likelihood that they’ll complete the form. By asking only for what’s absolutely necessary and not 2 Ethan Marcotte, “Responsive Web Design,” A List Apart, May 25, 2010, https://alistapart.com/article/ responsive-web-design . requesting information you already have, such as an email address or a pasword, you can minimize the effort required to fill out a form. There’s also the consideration of how flexible the system is with respect to user input. Since humans and computers communicate in different ways, there’s sometimes a disconnect between the information that humans provide and the information that the computer expects. Postel’s law dictates that computers should be robust enough to accept varying types of human input and not only make sense of it but also process it into a computer-readable format",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Postel’s law dictates that computers should be robust enough to accept varying types of human input and not only make sense of it but also process it into a computer-readable format. This can be done in a variety of ways, but perhaps most exciting are the ones that require the least amount of effort. Take, for example, Apple’s Face ID ( Figure 5-1 ), a facial recognition system that enables Apple users to authenticate on their mobile devces without the need to provide a username or a password each time they attempt to unlock their devices. Figure 5-1. Face ID lets you securely unlock your iPhone or iPad, authenticate purchases, sign in to apps, and more (source: Apple, 2020) Next, let’s look at an example that has become ubiquitous in the post-desktop era of computing: responsive design . Over the past few decades, as more and more devices gained the ability to connect to the web, the need to serve content that could adapt to any screen size increased. Ethan Marcotte introduced an approach in 2010 that he called “responsive web design,” which relies on “fluid grids, flexble images, and media queries” 2 to create websites that allow content to respond in a fluid manner to different viewing contexts. It was a completely new approach to designing and building websites, at a time when the predominant strategy was to create separate websites for desktops and internet-capable mobile devices. Responsive web design pushed designers beyond creating device-specific experences and toward an approach that embraced the fluid nature of the web. The growing capability of Cascading Style Sheets (CSS) enabled designers to define how content could flexibly adapt to any viewing context, be it an internet-capable smartwatch, smartphone, gaming console, laptop, desktop computer, or TV ( Figure 5-2 )",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Today responsive web design is the de facto standard when creating web experiences, and it embodies the philosophy of accepting a broad spectrum of input while providing output that is reliably adaptable and not quarantined to specific dimensions or devices. Figure 5-2. Responsive web design embraces the fluid nature of the web Progressive enhancement , which describes a strategy for web design focusing on content and gradual layering of styling and interaction, can also be considered an example of Postel’s law. First introduced by Steve Champeon and Nick Finck at SXSW in 2003 in a presentation titled “Inclusive Web Design For the Future” , this strategy emphasizes access to the basic content and functionality for all users, regardless of browser feature support, device features and capabilities, or internet connection speed. Additional style and interaction layers are progresively added as feature support and capabilities are detected, ensuring that people with newer browsers, more advanced devices, or faster connections receive a more enhanced experience without obscuring the core content. It’s an approach that stands in contrast to a previous strategy known as “graceful degradation,” which places emphasis on fault tolerance and focuses first on the more advanced software and hardware while providing a fallback for others. Progressive enhancement’s strength centers around its ability to liberally accept any range of browser feature support, any level of device capability suport, and any connection speed and to conservatively layer enhancements while preserving the core content, thus enabling universal access for everyone. Take, for example, a simple search box, which provides the ability for anyone to select it and enter a search query but is enhanced to support voice input for devices that support voice recognition ( Figure 5-3 ). Everyone will get a default search box intially, and it will be usable by everyone, including those using assistive technoogy such as screen readers",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Everyone will get a default search box intially, and it will be usable by everyone, including those using assistive technoogy such as screen readers. If voice recognition support is detected, a layer of functional enhancement is added by allowing the user to select the microphone icon to invoke a voice assistant that will transcribe speech to text, thereby extening the input methods of the search box without taking away from its core functionality. Figure 5-3. A progressively enhanced search component that provides a search box by default, and then voice support for devices that support voice recognition Examples of Postel’s law aren’t limited to interfaces—they can also be found in our processes. Take, for example, design systems, which are collections of reusable components and patterns guided by standards that define how they are used. The goal of a design system is to enable these components and patterns to be assembled to build any number of applications and to provide a framework for ensuring the extensibility of designs. These tools have proven incredibly valuable, enabling companies to scale design in a consistent manner across the organiztion ( Figure 5-4 ). To create an effective design system, organizations must be lieral in terms of what is accepted: everything from design, content, and code to strategy, opinions, and criticism may be provided by a diverse team of contribtors. In contrast, the output of the design system is conservative: guidelines, components, patterns, and principles must all be clear and purposeful. Figure 5-4. Design systems have enabled design to scale in a manageable and consistent manner within a number of well-known companies—pictured here, from left to right, are Carbon Design System by IBM, Lightning Design System by Salesforce, and Polaris by Shopify (source: IBM, Salesforce, Shopify, 2020) KEY CONSIDERATION Design Resiliency The input that users provide to a system is variable and may span a wide spectrum",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Therefore, to ensure a better user experience, we should design systems that are liberal in their acceptance of input. However, this also means there is an increased opportunity for things to go wrong, or to at least result in a less than ideal user experience. The more we can anticipate and plan for in design, the more resilient the design will be. Take, for example, the topic of internationalization. The same text string can span different lengths, depending on the language. Many designers plan only for their native language, while not accounting for text expansion in other languages that could result in a considerable increase in length. English, a very compact language, contains words that can expand up to 300% when translated into a less compact language such as Italian ( Figure 5-5 ). Text orientation can also vary per region of the world—from left-to-right in many Western countries to right-to-left or even vertical in others. By designing with these variations in mind, we can create more robust designs that can adapt to varying text string lengths and text orientations. Figure 5-5. Text expansion from English (left) to Italian (right) (source: w3.org ) Another example is the default font size, which the user can custoize both on mobile devices and in browsers. The purpose of this feature is to give the user control over the display, typically by increasing the size of all text throughout an application or website and thereby improving its accessibility. However, this can cause problems in designs that don’t account for the possibility of the text size increasing—specifically, how it affects layout and the space available for text. Adaptable designs account for this feature and have a graceful response. Take, for example, Amazon, which does a great job of responding to font size customization in its website header navigation ( Figure 5-6 )",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Laws of UX",
    "author": "Jon  Yablonski;",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Laws of UX.pdf",
    "date_published": "2020-04-21",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Take, for example, Amazon, which does a great job of responding to font size customization in its website header navigation ( Figure 5-6 ). The design accounts for the possibility of minimum font size customization by organizing the quick links below the search bar by importance and removing the links of lesser importance as the font size increases. Figure 5-6. Amazon.com adapting to minimum font size customization (source: Amazon, 2019) Conclusion Postel’s law can help us bridge the gap between human and machine. By desiging systems that liberally accept variable human input and translate it into a structured, machine-friendly output, we transfer this burden away from users and therefore ensure a more human user experience. This allows us to build the products and services that are robust and adaptable to fit the needs of growing scale and complexity. While it also means there is an increased opportunity for things to go wrong, we can anticipate and plan for this in design and thus ensure our work is more resilient.",
    "chunk_id": "Human_computer_interaction_laws_of_ux_page-43-52.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "CHAPTER 43 Designing with the Mind in Mind. https://doi.org/10.1016/B978-0-12-818202-4.00004-0 Copyright © 2021 Elsevier Inc. All rights reserved. Human color perception has both strengths and limitations, many of which are reevant to user-interface design—for example: l Our vision is optimized to detect contrasts (edges), not absolute brightness. l Our ability to distinguish colors depends on how colors are presented. l Some people have color blindness. l The user’s display and viewing conditions affect color perception. To understand these qualities of human color vision, let’s start with a brief description of how the human visual system processes color information from the environment. HOW COLOR VISION WORKS If you took introductory psychology or neurophysiology in college, you probably learned that the retina at the back of the human eye—the surface onto which the eye focuses images—has two types of light-receptor cells: rods and cones. You probably also learned that the rods detect light levels but not colors, while the cones detect colors. Finally, you probably learned that there are three types of cones—sensitive to red, green, and blue light—suggesting that our color vision is similar to video cameras and computer displays, which detect or project a wide variety of colors through cobinations of red, green, and blue pixels. What you learned in college is only partly right. People with normal vision do in fact have rods and three types of cones 1 in their retinas. The rods are sensitive to overall brighness while the three types of cones are sensitive to different frequencies of light. But that is where the truth departs from what most people learned in college, until recently. 1 People with color blindness may have fewer than three cone types, and some women have four (Eagleman, 2012; Macdonald, 2016). Our Color Vision is Limited 4 First, those of us who live in industrialized societies hardly use our rods at all. They function only at low levels of light",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Our Color Vision is Limited 4 First, those of us who live in industrialized societies hardly use our rods at all. They function only at low levels of light. They are for getting around in poorly lighted environments—the environments our ancestors lived in until the 19th century. Today, we use our rods only when we are having dinner by candlelight, feeling our way around our dark house at night, camping outside after dark, etc. (see Chapter 5 ). In bright daylight and modern artificially lighted environments— where we spend most of our time—our rods provide little useful information. Most of the time, our vision is based entirely on input from our cones (Ware, 2008). So how do our cones work? Are the three types of cones sensitive to red, green, and blue light, respectively? In fact, each type of cone is sensitive to a wider range of light frequencies than you might expect, and the sensitivity ranges of the three types overlap considerably. In addition, the overall sensitivity of the three types of cones differs greatly (see Fig. 4.1A ): l Low frequency . These cones are sensitive to light over almost the entire range of visible light but are most sensitive to the middle (yellow) and low (red) frequencies. l Medium frequency . These cones respond to light ranging from the high-frquency blues through the lower middle-frequency yellows and oranges. Overall, they are less sensitive than the low-frequency cones. l High frequency . These cones are most sensitive to light at the upper end of the visible light spectrum—violets and blues—but also respond weakly to middle frequencies, such as green. These cones are much less sensitive overall than the other two types of cones and also less numerous. One result is that our eyes are much less sensitive to blues and violets than to other colors. Compare a graph of the light sensitivity of our retinal cone cells ( Fig",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". One result is that our eyes are much less sensitive to blues and violets than to other colors. Compare a graph of the light sensitivity of our retinal cone cells ( Fig. 4.1A ) to what the graph might look like if electrical engineers had designed our retinas as a mosaic of receptors sensitive to red, green, and blue, like a camera ( Fig. 4.1B ). 1.0 0.8 0.6 0.4 0.2 (B) (A) 400 500 600 700 Wavelength (nanometers) L M H 400 500 600 700 Wavelength (nanometers) 0.2 0.4 0.6 0.8 1.0 Relative absorbance FIGURE 4.1 Sensitivity of the three types of retinal cones (A) versus artificial red, green, and blue receptors (B). Given the odd relationships among the sensitivities of our three types of retinal cone cells, one might wonder how the brain combines the signals from the cones to allow us to see a broad range of colors. The answer is by subtraction . Neurons in the visual cortex at the back of our brain subtract the signals coming over the optic nerves from the mediuand low-frequency cones, producing a red–green difference signal channel. Other neurons in the visual cotex subtract the signals from the higand low-frequency cones, yielding a yellow–blue difference signal channel. A third group of neurons in the visual cortex adds the signals coming from the loand medium-frequency cones to produce an overall luminance (or black–white) signal channel. 2 These three channels are called color-opponent channels . The brain then applies additional subtractive processes to all three color-opponent channels: signals coming from a given area of the retina are effectively subtracted from similar signals coming from nearby areas of the retina. VISION IS OPTIMIZED FOR DETECTION OF EDGES, NOT BRIGHTNESS All this subtraction makes our visual system much more sensitive to differences in color and brightness—that is, to contrasting colors and edges—than to absolute brightness levels. To see this, look at the inner bar in Fig. 4.2 . The inner bar looks darker on the right, but in fact is one solid shade of gray",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To see this, look at the inner bar in Fig. 4.2 . The inner bar looks darker on the right, but in fact is one solid shade of gray. To our contrast-sensitive visual system, it looks lighter on the left and darker on the right because the outer rectangle is darker on the left and lighter on the right. The sensitivity of our visual system to contrast, edges, and rapid changes rather than to absolute brightness level is an advantage: it helped our distant ancestors recognize a leopard in the nearby bushes as the same dangerous animal whether they saw it in bright noon sunlight or in the early morning hours of a cloudy day. Similarly, being 2 The overall brightness sum omits the signal from the high-frequency (blue-violet) cones. Those cones are so insensitive that their contributions to the total would be negligible, so omitting them makes little difference. FIGURE 4.2 The inner gray bar looks darker on the right but in fact is all one shade of gray. sensitive to contrasting rather than absolute colors allows us to see a rose as the same red whether it is in the sun or shade. Brain researcher Edward H. Adelson at the Massachusetts Institute of Technoogy developed an outstanding illustration of our visual system’s insensitivity to absolute brightness and its sensitivity to contrast (see Fig. 4.3 ). As difficult as it may be to believe, square A on the checkerboard is exactly the same shade as square B. Square B only appears white because it is depicted as being in the cyinder’s shadow. DISCRIMINABILITY OF COLORS DEPENDS ON HOW THEY ARE PRESENTED Even our ability to detect differences between colors is limited. Because of how our visual system works, three presentation factors affect our ability to distinguish colors from each other: l Paleness . The paler (less saturated) two colors are, the harder it is to tell them apart (see Fig. 4.4A ). l Color patch size . The smaller or thinner objects are, the harder it is to distiguish their colors (see Fig. 4.4B )",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 4.4A ). l Color patch size . The smaller or thinner objects are, the harder it is to distiguish their colors (see Fig. 4.4B ). Text is often thin, so the exact color of text is often hard to determine. FIGURE 4.3 The squares marked A and B are the same gray. We see B as white because it is shaded from the cylinder’s shadow. l Separation . The more separated color patches are, the more difficult it is to ditinguish their colors, especially if the separation is great enough to require eye motion between patches (see Fig. 4.4C ). Fig. 4.5 shows an example of color that is too pale to be seen by anyone on any device. It is a simulated airline website check-in step indicator. The current step is marked only with pale green in the circle. Maybe you can distinguish the green-filled circle from the white circles, but if you have a vision impairment or you are viewing this on a grayscale screen or a digital projector with a white-balance problem, maybe you cannot. Small color patches are often seen in data charts and plots. Many business grapics packages produce legends on charts and plots but make the color patches in the legend very small (see Fig. 4.6 ). Color patches in chart legends should be large to help people distinguish the colors (see Fig. 4.7 ). On websites, a common use of color is to distinguish unfollowed links from already followed ones. On some sites, the “followed” and “unfollowed” colors are too similar. The website of the Federal Reserve Bank of Minneapolis (see Fig. 4.8 ) has this prolem. Furthermore, the two colors are shades of blue, the color range to which our eyes are least sensitive. Can you spot the two followed links? 3 3 Already followed links in Fig. 4.8: Housing Units Authorized and House Price Index. (A) (B) (C) FIGURE 4.4 Factors affecting ability to distinguish colors: (A) paleness, (B) size, and (C) separation. FIGURE 4.5 The current step is marked only with a pale color, making it hard for some users to see",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". FIGURE 4.5 The current step is marked only with a pale color, making it hard for some users to see. 1 3 5 7 9 11 13 15 17 1 0.8 0.6 0.4 0.2 0 –0.2 –0.4 –0.6 –0.8 –1 S1 S6 S16 S11 0.8–1 0.6–0.8 0.4–0.6 0.2–0.4 0–0.2 –0.4–0.2 –0.6–0.4 –0.8–0.6 –1–0.8 –0.2–0 FIGURE 4.6 The tiny color patches in this chart legend are hard to distinguish. 0 200 400 600 800 Legend Beverages Condiments Confections Dairy products Grains/Cereals Meat/Poultry Produce Seafood FIGURE 4.7 Large color patches make it easier to distinguish the colors. COLOR BLINDNESS A fourth factor of color presentation that affects design principles for interactive sytems is whether the colors can be distinguished by people who have common types of color blindness. Except in severe cases, having color blindness doesn’t mean a total inability to see colors. It just means that not all three of a normally sighted person’s types of color-detecting cone receptor cells (see “ How Color Vision Works ,” section above) function, making it difficult or impossible to distinguish certain pairs of colors. Approximately 8% of men and slightly under 0.5% of women have a color perception deficit: difficulty discriminating certain pairs of colors (Wolfmaier, 1999; Johnson and Finn, 2017). The most common type of color blindness is red–green; other types are rarer. Fig. 4.9 shows color pairs that people with red–green color blindness (deuteraopia) have trouble distinguishing. The home finance application Moneydance provides a graphical breakdown of houshold expenses using color to indicate the various expense categories (see Fig. 4.10 ). FIGURE 4.8 The difference in color between visited and unvisited links is too subtle in MinneapolisFed.org’s website. (A) (C) (B) FIGURE 4.9 Red–green color-blind people cannot distinguish (A) dark-red from black, (B) blue from purple, and (C) light-green from white. FIGURE 4.10 Moneydance’s graph uses colors some users can’t distinguish. FIGURE 4.11 Moneydance’s graph rendered in grayscale",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". FIGURE 4.10 Moneydance’s graph uses colors some users can’t distinguish. FIGURE 4.11 Moneydance’s graph rendered in grayscale. Unfortunately, many of the colors are hues that color-blind people cannot tell apart. For example, people with red–green color blindness cannot distinguish the blue from the purple or the green from the khaki. If you are not color-blind, you can get an idea of which colors in an image will be hard to distinguish by converting the image to grascale (see Fig. 4.11 ), but as described in the “ Guidelines for Using Color ” section later in this chapter, it is best to run the image through a color-blindness filter or simulator (see Fig. 4.12 ). EXTERNAL FACTORS THAT INFLUENCE OUR ABILITY TO DISTINGUISH COLORS The environment in which digital technology is used can also affect people’s ability to distinguish colors—for example: l Variations in color displays . Computer displays vary in how they display colors depending on their technologies, driver software, or color settings. Digital projectors and auxiliary screens sometimes display colors differently than on the computer sending the image. Even monitors of the same model with the same settings may display colors slightly differently. Something that looks yellow on one display may look beige on another. Colors that are clearly different on one may look the same on another. l Grayscale displays . Although most displays these days are color, some devices, especially small handheld e-readers, have black-and-white or grayscale displays (see Fig. 4.13 ). A grayscale display can make different colors look the same. l Daytime/nighttime adjustments and dark mode . Most modern smarphones, tablets, and computers can adjust their color balance, either on demand or based on the time of day. Some color adjustments are subtle, such as when devices decrease the amount of blue in the display to avoid interfering with a user’s ability to sleep after an evening of device use",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Some color adjustments are subtle, such as when devices decrease the amount of blue in the display to avoid interfering with a user’s ability to sleep after an evening of device use. Other color adjustments are dramatic, such as switching to “dark mode,” which displays light content on dark backgrounds (ironically, like most computer terminals did many decades ago). All of these affect the colors users see in a user interface. (A) (B) FIGURE 4.12 Google logo: (A) normal and (B) with deuteranopia (red–green color blindness) filter. l Display angle . Some computer displays, particularly LCDs, work much better when viewed straight-on than at an angle. When LCDs are viewed at an angle, colors—and color differences—often are altered. l Ambient illumination . Strong light on a display washes out colors before it washes out light and dark areas, reducing color displays to grayscale, as anyone who has tried to use a bank ATM in direct sunlight knows. In offices, glare and venetian blind shadows can mask color differences. Smartphones and tablets are used everywhere, in all possible lighting conditions. These external factors are usually out of the software designer’s control. Designers should therefore keep in mind that they don’t have full control of users’ color viewing experiences. Colors that seem highly distinguishable in the development facility, on the development team’s computer displays and under normal office lighting condtions, may not be as distinguishable in some environments where the software is used. FIGURE 4.13 E-reader with grayscale screen. GUIDELINES FOR USING COLOR In interactive software systems that rely on color to convey information, follow these five guidelines to assure that the users of the software receive the information: l Use distinctive colors . Recall that our visual system combines the signals from retinal cone cells to produce three color-opponent channels: red–green, yellow–blue, and black–white (luminance)",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Recall that our visual system combines the signals from retinal cone cells to produce three color-opponent channels: red–green, yellow–blue, and black–white (luminance). The colors that people can distiguish most easily are those that cause a strong signal (positive or negative) on one of the three color-perception channels and neutral signals on the other two channels. Not surprisingly, those colors are red, green, yellow, blue, black, and white (see Fig. 4.14 ). All other colors cause signals on more than one color channel, so our visual system cannot distinguish them from other colors as quickly and easily as it can distinguish those six colors (Ware, 2008). l Separate strong opponent colors . Placing opponent colors right next to or on top of each other causes a disturbing shimmering sensation, so it should be avoided (see Fig. 4.15 ). l Distinguish colors by saturation and brightness, as well as hue . To make your software’s use of colors perceptible to all sighted users, avoid subtle color differences. Make sure the contrast between colors is high (but see guidline 5). One way to test whether colors are different enough is to view them in grayscale. If you cannot distinguish the colors when they are rendered in grays, they are not different enough. l Avoid color pairs that color-blind people cannot distinguish . Such pairs include dark red versus black, dark red versus dark green, blue versus purple, light green versus white. Don’t use dark reds, blues, or violets against any dark colors. Instead, use dark reds, blues, and violets against light yellows and greens. FIGURE 4.14 The most distinctive colors: black, white, red, green, yellow, and blue. Each color causes a strong signal on only one color-opponent channel. FIGURE 4.15 Opponent colors, placed on or directly next to each other, clash. Use an online color-blindness simulator 4 to check web pages and images to see how people with various color-vision deficiencies would see them. l Use color redundantly with other cues . Don’t rely on color alone",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". l Use color redundantly with other cues . Don’t rely on color alone. If you use color to mark something, mark it another way as well. For example, if green means one thing and blue means something else, don’t show green and blue dots; show a green triangle versus a blue dot, so both shape and color indicate the difference (see Fig. 4.16 ). 4 Search the Web for “color-blindness filter” or “color-blindness simulator.” (A) (B) FIGURE 4.16 Don’t use color alone to convey meaning (A). Use it redundantly with other cues—e.g., shape (B). FIGURE 4.17 MinneapolisFed.org’s graph uses shade differences visible to all sighted people on any display. A graph from the Federal Reserve Bank follows guideline 3 above by using shades of green (see Fig. 4.17 ). This is a well-designed graph. Any sighted person could read it. Now let’s use guideline 5 (above) to fix the design problem discussed earlier (see Fig. 4.5 ), where the current step in a simulated airline check-in process was marked only with a pale green (see Fig. 4.18A ). A simple way to correct it would be to mark the current step with a bold circle, a bold step number, a bold label below the circle, and increase the saturation of the green so it contrasts more strongly with the white backgrounds of the other circles (see Fig. 4.18B ). To allow blind people using screen readers to know what step they are one, one can also set the ALT-text of the current step to “current step” to mark it. With these improvements, the current step is marked redundantly , as guideline 4 recommends. IMPORTANT TAKEAWAYS l People with normal color vision have three types of cone receptor cells in the retinas of their eyes. People with color blindness have only two functioning types—in rare cases one type—of cone receptor cells. A small percentage of women have four types. l Human color vision works mainly by subtraction: red - green and blue - yelow",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Designing with the Mind in Mind",
    "author": "Jeff Johnson",
    "source": "raw_syllabi\\master_courses\\Human_computer_interaction\\pdf_material\\Designing with the Mind in Mind.pdf",
    "date_published": "2020-08-13",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A small percentage of women have four types. l Human color vision works mainly by subtraction: red - green and blue - yelow . This makes our visual system sensitive mainly to contrast, edges, and quick changes and relatively insensitive to overall brightness levels or absolute colors. l Our ability to see differences between two patches of color depends on: l Paleness: The paler two color patches are, the harder it is to distinguish them. (A) (B) FIGURE 4.18 (A) Poor design; (B) improved, more accessible design: the current step is highlighted redundantly using boldness and a more saturated color. l Size: The larger the patches, the easier it is for us to distinguish the colors. l Separation: The closer together they are, the easier they are to distinguish. l Several external factors influence our ability to distinguish colors: l Color displays may vary in color balance. l Some displays, such as those on many e-readers, are grayscale or black and white. l Many modern displays allow users to adjust color balance. Evening/night mode (which lowers blue levels) and dark-versus-light mode. l The angle at which some screens are viewed affects how colors appear. l Ambient light. l Guidelines for using color: l Use distinctive colors. l Separate strong opponent colors. l Distinguish colors by saturation and brightness as well as hue. l Avoid color pairs that color-blind people cannot distinguish. l Use color redundantly with other cues.",
    "chunk_id": "Human_computer_interaction_designing_with_the_mind_in_mind_page-43-56.json_chunk_11"
  },
  {
    "document_type": "online_article",
    "title": "Usability (User) Testing 101",
    "author": "Kate Moran",
    "source": "https://www.nngroup.com/articles/usability-testing-101/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": "December 1, 20192019-12-01 Share Usabilitytesting (alternately calleduser testing) is a popular UX research methodology. In ausability-testingsession, a researcher (called a “facilitator” or a “moderator”) asks a participant to perform tasks, usually using one or more specific user interfaces. While the participant completes each task, the researcher observes the participant’s behavior and listens for feedback. The phrase “usability testing” is often used interchangeably with “user testing.” The goals of usability testing vary by study, but they usually include: Why do we need to do usability testing?Won’t a good professional UX designer knowhow to design a great user interface? Even the best UX designers can’t design a perfect — or even good enough — user experience withoutiterative designdriven by observations of real usersand of their interactions with the design. There are many variables in designing a modern user interface and there are even more variables in thehuman brain. The total number of combinations is huge.The only way to get UX design right is to test it. There are many different types of usability testing, but the core elements in most usability tests arethe facilitator, the tasks, and the participant. The facilitator administers tasks to the participant. As the participant performs these tasks, the facilitator observes the participant’s behavior and listens for feedback. The facilitator may also ask followup questions to elicit detail from the participant. Thefacilitatorguides the participantthrough the test process. She gives instructions, answers the participant’s questions, and asks followup questions. The facilitator works to ensure that the test results in high-quality, valid data, withoutaccidentally influencing the participant’s behavior. Achieving this balance is difficult and requires training",
    "chunk_id": "Human_computer_interaction_usability_(user)_testing_101.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Usability (User) Testing 101",
    "author": "Kate Moran",
    "source": "https://www.nngroup.com/articles/usability-testing-101/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Achieving this balance is difficult and requires training. (In one form of remote usability testing, calledremote unmoderated testing, an application may perform some of the facilitator’s roles.) Thetasksin a usability test are realistic activities that the participant might perform in real life. They can be very specific or very open-ended,depending on the research questionsand the type of usability testing. Examples of tasks from real usability studies: Your printer is showing “Error 5200”. How can you get rid of the error message? You're considering opening a new credit card with Wells Fargo. Please visit wellsfargo.com and decide which credit card you might want to open, if any. You’ve been told you need to speak to Tyler Smith from the Project Management department. Use the intranet to find out where they are located. Tell the researcher your answer. Task wordingis very important in usability testing. Small errors in the phrasing of a task can cause the participant to misunderstand what they’re asked to do or can influence how participants perform the task (a psychological phenomenon calledpriming). Task instructions can be delivered to the participant verbally (the facilitator might read them) or can be handed to a participant written on task sheets. We often ask participants to read the task instructions out loud. This helps ensure that the participant reads the instructions completely, and helps the researchers with their notetaking, because they always know which task the user is performing. Theparticipantshould be arealistic userof the product or service being studied. That might mean that the user is already using the product or service in real life. Alternatively, in some cases, the participant might just have a similar background to the target user group, or might have the same needs, even if he isn’t already a user of the product. Participants are often asked tothink out loudduring usability testing (called the “think-aloud method”)",
    "chunk_id": "Human_computer_interaction_usability_(user)_testing_101.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Usability (User) Testing 101",
    "author": "Kate Moran",
    "source": "https://www.nngroup.com/articles/usability-testing-101/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". Participants are often asked tothink out loudduring usability testing (called the “think-aloud method”). The facilitator might ask the participants to narrate their actions and thoughts as they perform tasks. The goal of this approach is to understand participants’ behaviors, goals, thoughts, and motivations. Usability testing can be eitherqualitative or quantitative. Qualitative usability testingfocuses on collecting insights, findings, and anecdotes about how people use the product or service. Qualitative usability testing is best for discovering problems in the user experience. This form of usability testing is more common than quantitative usability testing. Quantitative usability testingfocuses on collecting metrics that describe the user experience. Two of the metrics most commonly collected in quantitative usability testing are task success and time on task. Quantitative usability testing is best for collectingbenchmarks. The number of participants needed for a usability test varies depending on the type of study. For a typicalqualitative usability studyof a single user group, we recommendusing five participantsto uncover the majority of the most common problems in the product. Remote usability tests are popular because they often require less time and money than in-person studies. There are two types of remote usability testing:moderated and unmoderated. Remote moderatedusability tests work very similarly to in-person studies. The facilitator still interacts with the participant and asks her to perform tasks. However, the facilitator and participant are in different physical locations. Usually, moderated tests can be performed using screen-sharing software like Skype or GoToMeeting. Remote unmoderatedremote usability tests do not have the same facilitator–participant interaction as an in-person or moderated tests. The researcher uses a dedicatedonline remote-testing toolto set up written tasks for the participant. Then, the participant completes those tasks alone on her own time",
    "chunk_id": "Human_computer_interaction_usability_(user)_testing_101.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Usability (User) Testing 101",
    "author": "Kate Moran",
    "source": "https://www.nngroup.com/articles/usability-testing-101/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". The researcher uses a dedicatedonline remote-testing toolto set up written tasks for the participant. Then, the participant completes those tasks alone on her own time. The testing tool delivers the task instructions and any followup questions. After the participant completes her test, the researcher receives a recording of the session, along with metrics like task success. Simple,“discount” usabilitystudies can be inexpensive, though you usually must pay a few hundred dollars as incentives to participants. The testing session can take place in a conference room, and the simplest study will take 3 days of your time (assuming that you have already learned how to do it, and you have access to participants): On the other hand,more-expensive researchis sometimes required, and the cost can run into several hundred thousand dollars for the most elaborate studies. Things that add cost include: Thereturn on investment(ROI) for advanced studies can still be high, though usually not as high as that for simple studies. For hands-on training and help honing your facilitation skills, check outour full-day course on usability testing. For detailed help planning, conducting, and analyzing remote user testing, check out our full-day seminar:Remote Usability Testing",
    "chunk_id": "Human_computer_interaction_usability_(user)_testing_101.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Usability (User) Testing 101",
    "author": "Kate Moran",
    "source": "https://www.nngroup.com/articles/usability-testing-101/",
    "date_published": "Unknown",
    "flag": "",
    "chunk_text": ". For detailed help planning, conducting, and analyzing remote user testing, check out our full-day seminar:Remote Usability Testing. Plan, conduct, and analyze your own studies, whether in person or remote Research Orchestrate and optimize research to amplify its impact Research Management Use surveys to drive and evaluate UX design Research Enable cookiesto watch NN/g videos Usability Testing 101 5 Reasons to Test Even When You “Know” the Answer Hoa Loranger·4 min A Case for Returning to In-Person Usability Testing Hoa Loranger·6 min What Is User Research? Caleb Sponheim·3 min Avoid Leading Questions to Get Better Insights from Participants Amy Schade·4 min ‘But You Tested with Only 5 Users!’: Responding to Skepticism About Findings From Small Studies Kathryn Whitenton·8 min Qualitative Usability Testing: Study Guide Kate Moran·5 min Testing Visual Design: A Comprehensive Guide Megan Chan·10 min Should You Run a Survey? Maddie Brown·6 min Quantitative Research: Study Guide Kate Moran·8 min Get weekly UX articles, videos, and upcoming training events straight to your inbox. Copyright© 1998-2025 Nielsen Norman Group, All Rights Reserved.",
    "chunk_id": "Human_computer_interaction_usability_(user)_testing_101.json_chunk_5"
  }
]