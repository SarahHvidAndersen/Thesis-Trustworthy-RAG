[
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": "Efficient Estimation of Word Representations in Vector Space Tomas Mikolov Google Inc., Mountain View, CA tmikolov@google.com Kai Chen Google Inc., Mountain View, CA kaichen@google.com Greg Corrado Google Inc., Mountain View, CA gcorrado@google.com Jeffrey Dean Google Inc., Mountain View, CA jeff@google.com Abstract We propose two novel model architectures for computing continuous vector reprsentations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the prevously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art perfomance on our test set for measuring syntactic and semantic word similarities. 1 Introduction Many current NLP systems and techniques treat words as atomic units - there is no notion of similaity between words, as these are represented as indices in a vocabulary. This choice has several good reasons - simplicity, robustness and the observation that simple models trained on huge amounts of data outperform complex systems trained on less data. An example is the popular N-gram model used for statistical language modeling - today, it is possible to train N-grams on virtually all available data (trillions of words [3]). However, the simple techniques are at their limits in many tasks. For example, the amount of relevant in-domain data for automatic speech recognition is limited - the performance is usually dominated by the size of high quality transcribed speech data (often just millions of words). In machine translation, the existing corpora for many languages contain only a few billions of words or less",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In machine translation, the existing corpora for many languages contain only a few billions of words or less. Thus, there are situations where simple scaling up of the basic techniques will not result in any significant progress, and we have to focus on more advanced techniques. With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17]. 1.1 Goals of the Paper The main goal of this paper is to introduce techniques that can be used for learning high-quality word vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As far as we know, none of the previously proposed architectures has been successfully trained on more arXiv:1301.3781v3 [cs.CL] 7 Sep 2013 than a few hundred of millions of words, with a modest dimensionality of the word vectors between 50 - 100. We use recently proposed techniques for measuring the quality of the resulting vector representtions, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14]. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are peformed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vetor(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20]",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In this paper, we try to maximize accuracy of these vector operations by developing new model architectures that preserve the linear regularities among words. We design a new comprehensive test set for measuring both syntactic and semantic regularities 1 , and show that many such regularities can be learned with high accuracy. Moreover, we discuss how training time and accuracy depends on the dimensionality of the word vectors and on the amount of the training data. 1.2 Previous Work Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others. Another interesting architecture of NNLM was presented in [13, 14], where the word vectors are first learned using neural network with a single hidden layer. The word vectors are then used to train the NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this work, we directly extend this architecture, and focus just on the first step where the word vectors are learned using a simple model. It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word vectors were made available for future research and comparison 2 . However, as far as we know, these architectures were significantly more computationally expensive for training than the one proposed in [13], with the exception of certain version of log-bilinear model where diagonal weight matrices are used [23]",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2 Model Architectures Many different types of models were proposed for estimating continuous representations of words, including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). In this paper, we focus on distributed representations of words learned by neural networks, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [20, 31]; LDA moreover becomes computationally very expensive on large data sets. Similar to [18], to compare different model architectures we define first the computational compleity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize the accuracy, while minimizing the computational complexity. 1 The test set is available at www.fit.vutbr.cz/ ̃imikolov/rnnlm/word-test.v1.txt 2 http://ronan.collobert.com/senna/ http://metaoptimize.com/projects/wordreprs/ http://www.fit.vutbr.cz/ ̃imikolov/rnnlm/ http://ai.stanford.edu/ ̃ehhuang/ For all the following models, the training complexity is proportional to O = E × T × Q, (1) where E is number of the training epochs, T is the number of the words in the training set and Q is defined further for each model architecture. Common choice is E = 3 − 50 and T up to one billion. All models are trained using stochastic gradient descent and backpropagation [26]. 2.1 Feedforward Neural Net Language Model (NNLM) The probabilistic feedforward neural network language model has been proposed in [1]. It consists of input, projection, hidden and output layers. At the input layer, N previous words are encoded using 1-oV coding, where V is size of the vocabulary. The input layer is then projected to a projection layer P that has dimensionality N × D , using a shared projection matrix. As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As only N inputs are active at any given time, composition of the projection layer is a relatively cheap operation. The NNLM architecture becomes complex for computation between the projection and the hidden layer, as values in the projection layer are dense. For a common choice of N = 10 , the size of the projection layer ( P ) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000 units. Moreover, the hidden layer is used to compute probability distribution over all the words in the vocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity per each training example is Q = N × D + N × D × H + H × V, (2) where the dominating term is H × V . However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized models completely by using models that are not normalized during training [4, 9]. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log 2 ( V ) . Thus, most of the complexity is caused by the term N × D × H . In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary tree. This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [16]. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log 2 ( V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log 2 ( Unigram perplexity ( V )) . For example when the vocabulary size is one million words, this results in about two times speedup in evaluation",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example when the vocabulary size is one million words, this results in about two times speedup in evaluation. While this is not crucial speedup for neural network LMs as the computational bottleneck is in the N × D × H term, we will later propose architectures that do not have hidden layers and thus depend heavily on the efficiency of the softmax normalization. 2.2 Recurrent Neural Net Language Model (RNNLM) Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and output layer. What is special for this type of model is the recurrent matrix that connects hidden layer to itself, using time-delayed connections. This allows the recurrent model to form some kind of short term memory, as information from the past can be represented by the hidden layer state that gets updated based on the current input and the state of the hidden layer in the previous time step. The complexity per training example of the RNN model is Q = H × H + H × V, (3) where the word representations D have the same dimensionality as the hidden layer H . Again, the term H × V can be efficiently reduced to H × log 2 ( V ) by using hierarchical softmax. Most of the complexity then comes from H × H . 2.3 Parallel Training of Neural Networks To train models on huge data sets, we have implemented several models on top of a large-scale distributed framework called DistBelief [6], including the feedforward NNLM and the new models proposed in this paper. The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The framework allows us to run multiple replicas of the same model in parallel, and each replica synchronizes its gradient updates through a centralized server that keeps all the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with an adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use one hundred or more model replicas, each using many CPU cores at different machines in a data center. 3 New Log-linear Models In this section, we propose two new model architectures for learning distributed representations of words that try to minimize computational complexity. The main observation from the previous section was that most of the complexity is caused by the non-linear hidden layer in the model. While this is what makes neural networks so attractive, we decided to explore simpler models that might not be able to represent the data as precisely as neural networks, but can possibly be trained on much more data efficiently. The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words. While there has been later substantial amount of work that focuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one. Note that related models have been proposed also much earlier [26, 8]. 3.1 Continuous Bag-of-Words Model The first proposed architecture is similar to the feedforward NNLM, where the non-linear hidden layer is removed and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged). We call this archtecture a bag-of-words model as the order of words in the history does not influence the projection",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We call this archtecture a bag-of-words model as the order of words in the history does not influence the projection. Furthermore, we also use words from the future; we have obtained the best performance on the task introduced in the next section by building a log-linear classifier with four future and four history words at the input, where the training criterion is to correctly classify the current (middle) word. Training complexity is then Q = N × D + D × log 2 ( V ) . (4) We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context. The model architecture is shown at Figure 1. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM. 3.2 Continuous Skip-gram Model The second architecture is similar to CBOW, but instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer, and predict words within a certain range before and after the current word. We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. The training complexity of this architecture is proportional to Q = C × ( D + D × log 2 ( V )) , (5) where C is the maximum distance of the words. Thus, if we choose C = 5 , for each training word we will select randomly a number R in range < 1; C > , and then use R words from history and w(t-2) w(t+1) w(t-1) w(t+2) w(t) SUM INPUT PROJECTION OUTPUT w(t) INPUT PROJECTION OUTPUT w(t-2) w(t-1) w(t+1) w(t+2) CBOW Skip-gram Figure 1: New model architectures",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The CBOW architecture predicts the current word based on the context, and the Skip-gram predicts surrounding words given the current word. R words from the future of the current word as correct labels. This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output. In the following experiments, we use C = 10 . 4 Results To compare the quality of different versions of word vectors, previous papers typically use a table showing example words and their most similar words, and understand them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more challenging when subjecting those vectors in a more complex similarity task, as follows. We follow previous observation that there can be many different types of similarities between words, for example, word big is similar to bigger in the same sense that small is similar to smaller . Example of another type of relationship can be word pairs big - biggest and small - smallest [20]. We further denote two pairs of words with the same relationship as a question, as we can ask: ”What is the word that is similar to small in the same sense as biggest is similar to big ?” Somewhat surprisingly, these questions can be answered by performing simple algebraic operations with the vector representation of words. To find a word that is similar to small in the same sense as biggest is similar to big , we can simply compute vector X = vector (” biggest ”) − vector (” big ”)+ vector (” small ”) . Then, we search in the vector space for the word closest to X measured by cosine distance, and use it as the answer to the question (we discard the input question words during this search). When the word vectors are well trained, it is possible to find the correct answer (word smallest ) using this method",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". When the word vectors are well trained, it is possible to find the correct answer (word smallest ) using this method. Finally, we found that when we train high dimensional word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented. Table 1: Examples of five types of semantic and nine types of syntactic questions in the SemantiSyntactic Word Relationship test set. Type of relationship Word Pair 1 Word Pair 2 Common capital city Athens Greece Oslo Norway All capital cities Astana Kazakhstan Harare Zimbabwe Currency Angola kwanza Iran rial City-in-state Chicago Illinois Stockton California Man-Woman brother sister grandson granddaughter Adjective to adverb apparent apparently rapid rapidly Opposite possibly impossibly ethical unethical Comparative great greater tough tougher Superlative easy easiest lucky luckiest Present Participle think thinking read reading Nationality adjective Switzerland Swiss Cambodia Cambodian Past tense walking walked swimming swam Plural nouns mouse mice dollar dollars Plural verbs work works speak speaks 4.1 Task Description To measure quality of the word vectors, we define a comprehensive test set that contains five types of semantic questions, and nine types of syntactic questions. Two examples from each category are shown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The questions in each category were created in two steps: first, a list of similar word pairs was created manually. Then, a large list of questions is formed by connecting two word pairs. For example, we made a list of 68 large American cities and the states they belong to, and formed about 2.5K questions by picking two word pairs at random. We have included in our test set only single token words, thus multi-word entities are not present (such as New York ). We evaluate the overall accuracy for all question types, and for each question type separately (smantic, syntactic). Question is assumed to be correctly answered only if the closest word to the vector computed using the above method is exactly the same as the correct word in the question; synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely to be impossible, as the current models do not have any input information about word morphology. However, we believe that usefulness of the word vectors for certain applications should be positively correlated with this accuracy metric. Further progress can be achieved by incorporating information about structure of words, especially for the syntactic questions. 4.2 Maximization of Accuracy We have used a Google News corpus for training the word vectors. This corpus contains about 6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we are facing time constrained optimization problem, as it can be expected that both using more data and higher dimensional word vectors will improve the accuracy. To estimate the best choice of model architecture for obtaining as good as possible results quickly, we have first evaluated models trained on subsets of the training data, with vocabulary restricted to the most frequent 30k words. The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The results using the CBOW architecture with different choice of word vector dimensionality and increasing amount of the training data are shown in Table 2. It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using word vectors from the CBOW architecture with limited vocabulary. Only questions containing words from the most frequent 30k words are used. Dimensionality / Training words 24M 49M 98M 196M 391M 783M 50 13.4 15.7 18.6 19.1 22.5 23.2 100 19.4 23.1 27.8 28.7 33.4 32.2 300 23.2 29.2 35.3 38.6 43.7 45.9 600 24.0 30.1 36.5 40.8 46.6 50.4 Table 3: Comparison of architectures using models trained on the same data, with 640-dimensional word vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set, and on the syntactic relationship test set of [20] Model Semantic-Syntactic Word Relationship test set MSR Word Relatedness Architecture Semantic Accuracy [%] Syntactic Accuracy [ % ] Tes t S et [ 2 0 ] RNNLM 9 36 35 NNLM 23 53 47 CBOW 24 64 61 Skip-gram 55 59 5 6 (such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the same increase of computational complexity as increasing vector size twice. For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We chose starting learning rate 0.025 and decreased it linearly, so that it approaches zero at the end of the last training epoch. 4.3 Comparison of Model Architectures First we compare different model architectures for deriving the word vectors using the same training data and using the same dimensionality of 640 of the word vectors. In the further experiments, we use full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to the 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic similarity between words 3 . The training data consists of several LDC corpora and is described in detail in [18] (320M words, 82K vocabulary). We used these data to provide a comparison to a previously trained recurrent neural network language model that took about 8 weeks to train on a single CPU. We trained a feeforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6], using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the projection layer has size 640 × 8 ). In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly on the syntactic questions. The NNLM vectors perform significantly better than the RNN - this is not surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden layer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the same on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic task than the CBOW model (but still better than the NNLM), and much better on the semantic part of the test than all the other models. Next, we evaluated our models trained using one CPU only and compared the results against publicly available word vectors. The comparison is given in Table 4. The CBOW model was trained on subset 3 We thank Geoff Zweig for providing us the test set",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The comparison is given in Table 4. The CBOW model was trained on subset 3 We thank Geoff Zweig for providing us the test set. Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relatioship test set, and word vectors from our models. Full vocabularies are used. Model Vector Training Accuracy [%] Dimensionality words Seman tic S y n tactic To tal Collobert-Weston NNLM 50 660M 9.3 12.3 11.0 Turian NNLM 50 37M 1.4 2.6 2.1 Turian NNLM 200 37M 1.4 2.2 1.8 Mnih NNLM 50 37M 1.8 9.1 5.8 Mnih NNLM 100 37M 3.3 13.2 8.8 Mikolov RNNLM 80 320M 4.9 18.4 12.7 Mikolov RNNLM 640 320M 8.6 36.5 24.6 Huang NNLM 50 990M 13 .3 1 1 .6 1 2 .3 Our NNLM 20 6B 12.9 26.4 20.3 Our NNLM 50 6B 27.9 55.8 43.2 Our NNLM 100 6B 34.2 64.5 50.8 CBOW 300 783M 15.5 53.1 36.1 Skip-gram 300 783M 50 .0 5 5 .9 5 3 .3 Table 5: Comparison of models trained for three epochs on the same data and models trained for one epoch. Accuracy is reported on the full Semantic-Syntactic data set. Model Vector Training Accuracy [%] Training time Dimensional i t y words [days] Semantic Syntactic Total 3 epoch CBOW 300 783M 15.5 53.1 36.1 1 3 epoch Skip-gram 300 783M 50.0 55.9 53.3 3 1 epoch CBOW 300 783M 13.8 49.9 33.6 0.3 1 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.6 1 epoch CBOW 600 783M 15.4 53.3 36.2 0.7 1 epoch Skip-gram 300 783M 45.6 52.2 49.2 1 1 epoch Skip-gram 300 1.6B 52.2 55.1 53.8 2 1 epoch Skip-gram 600 783M 56.7 54.5 55.5 2.5 of the Google News data in about a day, while training time for the Skip-gram model was about three days. For experiments reported further, we used just one training epoch (again, we decrease the learning rate linearly so that it approaches zero at the end of training). Training a model on twice as much data using one epoch gives comparable or better results than iterating over the same data for three epochs, as is shown in Table 5, and provides additional small speedup",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4.4 Large Scale Parallel Training of Models As mentioned earlier, we have implemented various models in a distributed framework called DitBelief. Below we report the results of several models trained on the Google News 6B data set, with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Adgrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an Table 6: Comparison of models trained using the DistBelief distributed framework. Note that training of NNLM with 1000-dimensional vectors would take too long to complete. Model Vector Training Accuracy [%] Training time Dimensionality w ords [days x CPU cores] Semantic Syntactic Total NNLM 100 6B 34.2 64.5 50.8 14 x 180 CBOW 1000 6B 57.3 68.9 63.7 2 x 140 Skip-gram 1000 6B 66.1 65.1 65.6 2.5 x 125 Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge. Architecture Accuracy [%] 4-gram [32] 39 Average LSA similarity [32] 49 Log-bilinear model [24] 54.8 RNNLMs [19] 55.4 Skip-gram 48.0 Skip-gram + RNNLMs 58.9 estimate since the data center machines are shared with other production tasks, and the usage can fluctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of the CBOW model and the Skip-gram model are much closer to each other than their single-machine implementations. The result are reported in Table 6. 4.5 Microsoft Research Sentence Completion Challenge The Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing language modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one word is missing in each sentence and the goal is to select word that is the most coherent with the rest of the sentence, given a list of five reasonable choices",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Performance of several techniques has been already reported on this set, including N-gram models, LSA-based model [32], log-bilinear model [24] and a combination of recurrent neural networks that currently holds the state of the art performance of 55.4% accuracy on this benchmark [19]. We have explored the performance of Skip-gram architecture on this task. First, we train the 640- dimensional model on 50M words provided in [32]. Then, we compute score of each sentence in the test set by using the unknown word at the input, and predict all surrounding words in a sentence. The final sentence score is then the sum of these individual predictions. Using the sentence scores, we choose the most likely sentence. A short summary of some previous results together with the new results is presented in Table 7. While the Skip-gram model itself does not perform on this task better than LSA similarity, the scores from this model are complementary to scores obtained with RNNLMs, and a weighted combination leads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and 58.7% on the test part of the set). 5 Examples of the Learned Relationships Table 8 shows words that follow various relationships. We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word. Thus for example, Paris - France + Italy = Rome . As it can be seen, accuracy is quite good, although there is clearly a lot of room for further improvements (note that using our accuracy metric that Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skigram model trained on 783M words with 300 dimensionality)",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Relationship Example 1 Example 2 Example 3 France - Paris Italy: Rome Japan: Tokyo Florida: Tallahassee big - bigger small: larger cold: colder quick: quicker Miami - Florida Baltimore: Maryland Dallas: Texas Kona: Hawaii Einstein - scientist Messi: midfielder Mozart: violinist Picasso: painter Sarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japan copper - Cu zinc: Zn gold: Au uranium: plutonium Berlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: Barack Microsoft - Windows Google: Android IBM: Linux Apple: iPhone Microsoft - Ballmer Google: Yahoo IBM: McNealy Apple: Jobs Japan - sushi Germany: bratwurst France: tapas USA: pizza assumes exact match, the results in Table 8 would score only about 60%). We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. By using ten examples instead of one to form the relationship vector (we average the individual vectors together), we have observed improvement of accuracy of our best models by about 10% absolutely on the semantic-syntactic test. It is also possible to apply the vector operations to solve different tasks. For example, we have observed good accuracy for selecting out-of-the-list words, by computing average vector for a list of words, and finding the most distant word vector. This is a popular type of problems in certain human intelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques. 6 Conclusion In this paper we studied the quality of vector representations of words derived by various models on a collection of syntactic and semantic language tasks. We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent)",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We observed that it is possible to train high quality word vectors using very simple model architectures, compared to the popular neural network models (both feedforward and recurrent). Because of the much lower computational complexity, it is possible to compute very accurate high dimensional word vectors from a much larger data set. Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram models even on corpora with one trillion words, for basically unlimited size of the vocabulary. That is several orders of magnitude larger than the best previously published results for similar models. An interesting task where the word vectors have recently been shown to significantly outperform the previous state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were used together with other techniques to achieve over 50% increase in Spearman’s rank correlation over the previous best result [31]. The neural network based word vectors were previously applied to many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can be expected that these applications can benefit from the model architectures described in this paper. Our ongoing work shows that the word vectors can be successfully applied to automatic extension of facts in Knowledge Bases, and also for verification of correctness of existing facts. Results from machine translation experiments also look very promising. In the future, it would be also interesting to compare our techniques to Latent Relational Analysis [30] and others. We believe that our comprehensive test set will help the research community to improve the existing techniques for estimating the word vectors. We also expect that high quality word vectors will become an important building block for future NLP applications",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "efficient_estimation_of_word_representations",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\efficient_estimation_of_word_representations.pdf",
    "date_published": "2013-09-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We also expect that high quality word vectors will become an important building block for future NLP applications. 7 Follow-Up Work After the initial version of this paper was written, we published single-machine multi-threaded C++ code for computing the word vectors, using both the continuous bag-of-words and skip-gram archtectures 4 . The training speed is significantly higher than reported earlier in this paper, i.e. it is in the order of billions of words per hour for typical hyperparameter choices. We also published more than 1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of our follow-up work will be published in an upcoming NIPS 2013 paper [21].",
    "chunk_id": "Natural_language_processing_efficient_estimation_of_word_representations.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": "Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors Marco Baroni and Georgiana Dinu and Germ ́an Kruszewski Center for Mind/Brain Sciences (University of Trento, Italy) (marco.baroni|georgiana.dinu|german.kruszewski)@unitn.it Abstract Context-predicting models (more comonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literture is still lacking a systematic comparson of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counteparts. 1 Introduction A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning, since semantcally similar words tend to have similar contetual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g., co-occurring words) in which target terms appear in a large corpus as proxies for meaning represetations, and apply geometric techniques to these vectors to measure the similarity in meaning of the corresponding words (Clark, 2013; Erk, 2012; Turney and Pantel, 2010). It has been clear for decades now that raw coccurrence counts don’t work that well, and DSMs achieve much higher performance when various transformations are applied to the raw vectors, for example by reweighting the counts for cotext informativeness and smoothing them with dmensionality reduction techniques",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This vector optimization process is generally unsupervised, and based on independent considerations (for eample, context reweighting is often justified by information-theoretic considerations, dimensioality reduction optimizes the amount of preserved variance, etc.). Occasionally, some kind of indrect supervision is used: Several parameter setings are tried, and the best setting is chosen based on performance on a semantic task that has been selected for tuning. The last few years have seen the development of a new generation of DSMs that frame the vetor estimation problem directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2003; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013a; Turian et al., 2010). The traditional costruction of context vectors is turned on its head: Instead of first collecting context vectors and then reweighting these vectors based on various critria, the vector weights are directly set to optimally predict the contexts in which the corresponding words tend to appear. Since similar words occur in similar contexts, the system naturally learns to assign similar vectors to similar words. This new way to train DSMs is attractive bcause it replaces the essentially heuristic stacking of vector transforms in earlier models with a sigle, well-defined supervised learning step. At the same time, supervision comes at no manual anntation cost, given that the context windows used for training can be automatically extracted from an unannotated corpus (indeed, they are the very same data used to build traditional DSMs). Morover, at least some of the relevant methods can eficiently scale up to process very large amounts of input data",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Morover, at least some of the relevant methods can eficiently scale up to process very large amounts of input data. 1 1 The idea to directly learn a parameter vector based on an objective optimum function is shared by Latent Dirichlet 238 We will refer to DSMs built in the traditional way as count models (since they initialize vectors with co-occurrence counts), and to their traininbased alternative as predict(ive) models. 2 Now, the most natural question to ask, of course, is which of the two approaches is best in empirical terms. Surprisingly, despite the long tradition of extensive evaluations of alternative count DSMs on standard benchmarks (Agirre et al., 2009; Broni and Lenci, 2010; Bullinaria and Levy, 2007; Bullinaria and Levy, 2012; Sahlgren, 2006; Pad ́o and Lapata, 2007), the existing literature contains very little in terms of direct comparison of count vs. predictive DSMs. This is in part due to the fact that context-predicting vectors were first deveoped as an approach to language modeling and/or as a way to initialize feature vectors in neuranetwork-based “deep learning” NLP architectures, so their effectiveness as semantic representations was initially seen as little more than an interesing side effect. Sociological reasons might also be partly responsible for the lack of systematic coparisons: Context-predictive models were deveoped within the neural-network community, with little or no awareness of recent DSM work in coputational linguistics. Whatever the reasons, we know of just three works reporting direct comparisons, all limited in their scope. Huang et al. (2012) compare, in pasing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). In this experiment, the count model actually outperforms the best predictive aproach. Instead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Instead, in a word-similarity-in-context task (Table 5), the best predict model outperforms the count model, albeit not by a large margin. Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. Count vectors make for better inputs in a phrase similarity task, whereas the two reprsentations are comparable in a paraphrase classifcation experiment. 3 Allocation (LDA) models (Blei et al., 2003; Griffiths et al., 2007), where parameters are set to optimize the joint proability distribution of words and documents. However, the fully probabilistic LDA models have problems scaling up to large data sets. 2 We owe the first term to Hinrich Sch ̈utze (p.c.). Preditive DSMs are also called neural language models, because their supervised context prediction training is performed with neural networks, or, more cryptically, “embeddings”. 3 We refer here to the updated results reported in the erratum at http://homepages.inf.ed.ac.uk/ s1066731/pdf/emnlp2012erratum.pdf Finally, Mikolov et al. (2013d) compare their predict models to “Latent Semantic Analysis” (LSA) count vectors on syntactic and semantic analogy tasks, finding that the predict models are highly superior. However, they provide very little details about the LSA count vectors they use. 4 In this paper, we overcome the comparison scarcity problem by providing a direct evaluation of count and predict DSMs across many parameter settings and on a large variety of mostly standard lexical semantics benchmarks. Our title already gave away what we discovered. 2 Distributional semantic models Both count and predict models are extracted from a corpus of about 2.8 billion tokens constructed by concatenating ukWaC, 5 the English Wikipedia 6 and the British National Corpus. 7 For both model types, we consider the top 300K most frequent words in the corpus both as target and context elments. 2.1 Count models We prepared the count models using the DISSECT toolkit",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 7 For both model types, we consider the top 300K most frequent words in the corpus both as target and context elments. 2.1 Count models We prepared the count models using the DISSECT toolkit. 8 We extracted count vectors from symetric context windows of two and five words to either side of target. We considered two weighing schemes: positive Pointwise Mutual Informtion and Local Mutual Information (akin to the widely used Log-Likelihood Ratio scheme) (Eert, 2005). We used both full and compressed vetors. The latter were obtained by applying the Sigular Value Decomposition (Golub and Van Loan, 1996) or Non-negative Matrix Factorization (Lee and Seung, 2000), Lin (2007) algorithm, with rduced sizes ranging from 200 to 500 in steps of 100. In total, 36 count models were evaluated. Count models have such a long and rich hitory that we can only explore a small subset of the counting, weighting and compressing metods proposed in the literature. However, it is worth pointing out that the evaluated parameter subset encompasses settings (narrow context widow, positive PMI, SVD reduction) that have been 4 Chen et al. (2013) present an extended empirical evalution, that is however limited to alternative context-predictive models, and does not include the word2vec variant we use here. 5 http://wacky.sslmit.unibo.it 6 http://en.wikipedia.org 7 http://www.natcorp.ox.ac.uk 8 http://clic.cimec.unitn.it/composes/ toolkit/ 239 found to be most effective in the systematic explrations of the parameter space conducted by Bulinaria and Levy (2007; 2012). 2.2 Predict models We trained our predict models with the word2vec toolkit. 9 The toolkit implements both the skigram and CBOW approaches of Mikolov et al. (2013a; 2013c). We experimented only with the latter, which is also the more computationallefficient model of the two, following Mikolov et al. (2013b) which recommends CBOW as more suitable for larger datasets",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We experimented only with the latter, which is also the more computationallefficient model of the two, following Mikolov et al. (2013b) which recommends CBOW as more suitable for larger datasets. The CBOW model learns to predict the word in the middle of a symmetric window based on the sum of the vector representations of the words in the window. We considered context windows of 2 and 5 words to either side of the central elment. We vary vector dimensionality within the 200 to 500 range in steps of 100 . The word2vec toolkit implements two efficient alternatives to the standard computation of the output word probbility distributions by a softmax classifier. Herarchical softmax is a computationally efficient way to estimate the overall probability distribtion using an output layer that is proportional to log ( unigram.perplexity ( W )) instead of W (for W the vocabulary size). As an alternative, negtive sampling estimates the probability of an ouput word by learning to distinguish it from draws from a noise distribution. The number of these draws (number of negative samples ) is given by a parameter k . We test both hierarchical softmax and negative sampling with k values of 5 and 10 . Very frequent words such as the or a are not very informative as context features. The word2vec toolkit implements a method to downsize their efect (and simultaneously improve speed perfomance). More precisely, words in the training data are discarded with a probability that is prportional to their frequency (capturing the same intuition that motivates traditional count vector weighting measures such as PMI). This is cotrolled by a parameter t and words that occur with higher frequency than t are aggressively subsapled. We train models without subsampling and with subsampling at t = 1 e − 5 (the toolkit page suggests 1 e − 3 − 1 e − 5 as a useful range based on empirical observations). In total, we evaluate 48 predict models, a nu9 https://code.google.com/p/word2vec/ ber comparable to that of the count models we consider",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In total, we evaluate 48 predict models, a nu9 https://code.google.com/p/word2vec/ ber comparable to that of the count models we consider. 2.3 Out-of-the-box models Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. 10 This model, based on the same input corpus we use, exemplifies a “linguistically rich” count-based DSM, that relies on lemmas instead or raw word forms, and has dimensions that encode the syntactic relations and/or lexicsyntactic patterns linking targets and contexts. Broni and Lenci showed, in a large scale evaluation, that dm reaches near-state-of-the-art performance in a variety of semantic tasks. We also experiment with the popular predict vectors made available by Ronan Collobert. 11 Folowing the earlier literature, with refer to them as Collobert and Weston (cw) vectors. These are 100-dimensional vectors trained for two months (!) on the Wikipedia. In particular, the vectors were trained to optimize the task of choosing the right word over a random alternative in the middle of an 11-word context window (Collobert et al., 2011). 3 Evaluation materials We test our models on a variety of benchmarks, most of them already widely used to test and copare DSMs. The following benchmark descritions also explain the figures of merit and statof-the-art results reported in Table 2. Semantic relatedness A first set of semantic benchmarks was constructed by asking human subjects to rate the degree of semantic similarity or relatedness between two words on a numercal scale. The performance of a computational model is assessed in terms of correlation between the average scores that subjects assigned to the pairs and the cosines between the corresponding vectors in the model space (following the prevous art, we use Pearson correlation for rg, Speaman in all other cases). The classic data set of Rubenstein and Goodenough (1965) (rg) consists of 65 noun pairs",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The classic data set of Rubenstein and Goodenough (1965) (rg) consists of 65 noun pairs. State of the art performance on this set has been reported by Hassan and Mhalcea (2011) using a technique that exploits the Wikipedia linking structure and word sense diambiguation techniques. Finkelstein et al. (2002) 10 http://clic.cimec.unitn.it/dm/ 11 http://ronan.collobert.com/senna/ 240 introduced the widely used WordSim353 set (ws) that, as the name suggests, consists of 353 pairs. The current state of the art is reached by Halawi et al. (2012) with a method that is in the spirit of the predict models, but lets synonymy infomation from WordNet constrain the learning prcess (by favoring solutions in which WordNet syonyms are near in semantic space). Agirre et al. (2009) split the ws set into similarity (wss) and rlatedness (wsr) subsets. The first contains tighter taxonomic relations, such as synonymy and chyponymy ( king/queen ) whereas the second ecompasses broader, possibly topical or syntamatic relations ( family/planning ). We report statof-the-art performance on the two subsets from the work of Agirre and colleagues, who used different kinds of count vectors extracted from a very large corpus (orders of magnitude larger than ours). Fnally, we use (the test section of) MEN (men), that comprises 1,000 word pairs. Bruni et al. (2013), the developers of this benchmark, achieve state-othe-art performance by extensive tuning on ad-hoc training data, and by using both textual and imagextracted features to represent word meaning. Synonym detection The classic TOEFL (toefl) set was introduced by Landauer and Dumais (1997). It contains 80 multiple-choice questions that pair a target term with 4 synonym candidates. For example, for the target levied one must choose between imposed (correct), believed , requested and correlated . The DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer. Peformance is evaluated in terms of correct-answer accuracy",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The DSMs compute cosines of each candidate vector with the target, and pick the candidate with largest cosine as their answer. Peformance is evaluated in terms of correct-answer accuracy. Bullinaria and Levy (2012) achieved 100% accuracy by a very thorough exploration of the count model parameter space. Concept categorization Given a set of nominal concepts, the task is to group them into natural caegories (e.g., helicopters and motorcycles should go to the vehicle class, dogs and elephants into the mammal class). Following previous art, we tackle categorization as an unsupervised clustering task. The vectors produced by a model are clustered into n groups (with n determined by the gold stadard partition) using the CLUTO toolkit (Karypis, 2003), with the repeated bisections with global otimization method and CLUTO’s default settings otherwise (these are standard choices in the liteature). Performance is evaluated in terms of prity , a measure of the extent to which each cluster contains concepts from a single gold category. If the gold partition is reproduced perfectly, purity reaches 100%; it approaches 0 as cluster quality deteriorates. The Almuhareb-Poesio (ap) bencmark contains 402 concepts organized into 21 caegories (Almuhareb, 2006). State-of-the-art purity was reached by Rothenh ̈ausler and Sch ̈utze (2009) with a count model based on carefully crafted sytactic links. The ESSLLI 2008 Distributional Smantic Workshop shared-task set (esslli) contains 44 concepts to be clustered into 6 categories (Broni et al., 2008) (we ignore here the 3- and 2- way higher-level partitions coming with this set). Katrenko and Adriaans (2008) reached top peformance on this set using the full Web as a copus and manually crafted, linguistically motivated patterns. Finally, the Battig (battig) test set intrduced by Baroni et al. (2010) includes 83 concepts from 10 categories. Current state of the art was reached by the window-based count model of Broni and Lenci (2010)",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2010) includes 83 concepts from 10 categories. Current state of the art was reached by the window-based count model of Broni and Lenci (2010). Selectional preferences We experiment with two data sets that contain verb-noun pairs that were rated by subjects for the typicality of the noun as a subject or object of the verb (e.g., peple received a high average score as subject of to eat , and a low score as object of the same verb). We follow the procedure proposed by Broni and Lenci (2010) to tackle this challenge: For each verb, we use the corpus-based tuples they make available to select the 20 nouns that are most strongly associated to the verb as subjects or ojects, and we average the vectors of these nouns to obtain a “prototype” vector for the relevant agument slot. We then measure the cosine of the vector for a target noun with the relevant prottype vector (e.g., the cosine of people with the eaing subject prototype vector). Systems are evauated by Spearman correlation of these cosines with the averaged human typicality ratings. Our first data set was introduced by Ulrike Pad ́o (2007) and includes 211 pairs (up). Top-performance was reached by the supervised count vector system of Herda ̆gdelen and Baroni (2009) (supervised in the sense that they directly trained a classifier on gold data, as opposed to the 0-cost supervision of the context-learning methods). The mcrae set (McRae et al., 1998) consists of 100 noun–verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on 241 syntactic information. Analogy While all the previous data sets are reatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al. (2013a) specifically to test prdict models. The data-set contains about 9K smantic and 10.5K syntactic analogy questions",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2013a) specifically to test prdict models. The data-set contains about 9K smantic and 10.5K syntactic analogy questions. A semantic question gives an example pair ( brother - sister ), a test word ( grandson ) and asks to find another word that instantiates the relation illutrated by the example with respect to the test word ( granddaughter ). A syntactic question is similar, but in this case the relationship is of a grammatical nature ( work – works , speak speaks ). Mikolov and colleagues tackle the challenge by subtracing the second example term vector from the first, adding the test term, and looking for the nearest neighbour of the resulting vector (what is the neaest neighbour of ⃗ brother − ⃗ sister + ⃗ grandson ?). Systems are evaluated in terms of proportion of questions where the nearest neighbour from the whole semantic space is the correct answer (the given example and test vector triples are excluded from the nearest neighbour search). Mikolov et al. (2013a) reach top accuracy on the syntactic subset (ansyn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). Top acuracy on the entire data set (an) and on the smantic subset (ansem) was reached by Mikolov et al. (2013c) using a skip-gram predict model. Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al. (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al. (2013c) among 700K words, and we among 300K words. Some characteristics of the benchmarks we use are summarized in Table 1. 4 Results Table 2 summarizes the evaluation results. The first block of the table reports the maximum petask performance (across all considered parameter settings) for count and predict vectors. The latter emerge as clear winners, with a large margin over count vectors in most tasks",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The latter emerge as clear winners, with a large margin over count vectors in most tasks. Indeed, the preditive models achieve an impressive overall perfomance, beating the current state of the art in seeral cases, and approaching it in many more. It is worth stressing that, as reviewed in Section 3, the state-of-the-art results were obtained in almost all cases using specialized approaches that rely on eternal knowledge, manually-crafted rules, parsing, larger corpora and/or task-specific tuning. Our predict results were instead achieved by simply downloading the word2vec toolkit and running it with a range of parameter choices recommended by the toolkit developers. The success of the predict models cannot be blamed on poor performance of the count moels. Besides the fact that this would not explain the near-state-of-the-art performance of the prdict vectors, the count model results are actually quite good in absolute terms. Indeed, in several cases they are close, or even better than those atained by dm, a linguistically-sophisticated counbased approach that was shown to reach top peformance across a variety of tasks by Baroni and Lenci (2010). Interestingly, count vectors achieve perfomance comparable to that of predict vectors only on the selectional preference tasks. The up task in particular is also the only benchmark on which predict models are seriously lagging behind statof-the-art and dm performance. Recall from Setion 3 that we tackle selectional preference by crating average vectors representing typical verb aguments. We conjecture that this averaging aproach, that worked well for dm vectors, might be problematic for prediction-trained vectors, and we plan to explore alternative methods to build the prototypes in future research. Are our results robust to parameter choices, or are they due to very specific and brittle settings? The next few blocks of Table 2 address this quetion",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Are our results robust to parameter choices, or are they due to very specific and brittle settings? The next few blocks of Table 2 address this quetion. The second block reports results obtained with single count and predict models that are best in terms of average performance rank across tasks (these are the models on the top rows of tables 3 and 4, respectively). We see that, for both aproaches, performance is not seriously affected by using the single best setup rather than task-specific settings, except for a considerable drop in perfomance for the best predict model on esslli (due to the small size of this data set?), and an even more dramatic drop of the count model on ansem. A more cogent and interesting evaluation is reported in the third block of Table 2, where we see what happens if we use the single models with worst performance across tasks (recall from Section 2 above that, in any case, we are exploring a space of reasonable parameter settings, of the sort that an 242 name task measure source soa rg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011) (1965) ws relatedness Spearman Finkelstein et al. (2002) Halawi et al. (2012) wss relatedness Spearman Agirre et al. (2009) Agirre et al. (2009) wsr relatedness Spearman Agirre et al. (2009) Agirre et al. (2009) men relatedness Spearman Bruni et al. (2013) Bruni et al. (2013) toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012) (1997) ap categorization purity Almuhareb (2006) Rothenh ̈ausler and Sch ̈utze (2009) esslli categorization purity Baroni et al. (2008) Katrenko and Adriaans (2008) battig categorization purity Baroni et al. (2010) Baroni and Lenci (2010) up sel pref Spearman Pad ́o (2007) Herda ̆gdelen and Baroni (2009) mcrae sel pref Spearman McRae et al. (1998) Baroni and Lenci (2010) an analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013c) ansyn analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013a) ansem analogy accuracy Mikolov et al. (2013a) Mikolov et al",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2013a) Mikolov et al. (2013c) ansyn analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013a) ansem analogy accuracy Mikolov et al. (2013a) Mikolov et al. (2013c) Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference (source) and reference to current state-of-the-art system (soa). rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansem best setup on each task cnt 74 62 70 59 72 76 66 84 98 41 27 49 43 60 pre 84 75 80 70 80 91 75 86 99 41 28 68 71 66 best setup across tasks cnt 70 62 70 57 72 76 64 84 98 37 27 43 41 44 pre 83 73 78 68 80 86 71 77 98 41 26 67 69 64 worst setup across tasks cnt 11 16 23 4 21 49 24 43 38 -6 -10 1 0 1 pre 74 60 73 48 68 71 65 82 88 33 20 27 40 10 best setup on rg cnt (74) 59 66 52 71 64 64 84 98 37 20 35 42 26 pre (84) 71 76 64 79 85 72 84 98 39 25 66 70 61 other models soa 86 81 77 62 76 100 79 91 96 60 32 61 64 61 dm 82 35 60 13 42 77 76 84 94 51 29 NA NA NA cw 48 48 61 38 57 56 58 61 70 28 15 11 12 9 Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks. See Section 3 and Table 1 for figures of merit and state-of-the-art results (soa). Since dm has very low coverage of the an* data sets, we do not report its performance there. 243 experimenter might be tempted to choose without tuning). The count model performance is severely affected by this unlucky choice (2-word window, Local Mutual Information, NMF, 400 dimensions, mean performance rank: 83), whereas the predict approach is much more robust: To put its worst istantiation (2-word window, hierarchical softmax, no subsampling, 200 dimensions, mean rank: 51) into perspective, its performance is more than 10% below the best count model only for the an and ansem tasks, and actually higher than it in 3 cases (note how on esslli the worst predict models peforms much better than the best one, confirming our suspicion about the brittleness of this small data set)",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The fourth block reports performance in what might be the most realistic scenario, namely by tuning the parameters on a development task. Specifically, we pick the models that work best on the small rg set, and report their performance on all tasks (we obtained similar results by picing other tuning sets). The selected count model is the third best overall model of its class as rported in Table 3. The selected predict model is the fourth best model in Table 4. The overall count performance is not greatly affected by this choice. Again, predict models confirm their robustness, in that their rg-tuned performance is always close (and in 3 cases better) than the one achieved by the best overall setup. Tables 3 and 4 let us take a closer look at the most important count and predict paramters, by reporting the characteristics of the best models (in terms of average performance-based ranking across tasks) from both classes. For the count models, PMI is clearly the better weighing scheme, and SVD outperforms NMF as a dmensionality reduction technique. However, no compression at all (using all 300K original dimesions) works best. Compare this to the best oveall predict vectors, that have 400 dimensions only, making them much more practical to use. For the predict models, we observe in Table 4 that negtive sampling, where the task is to distinguish the target output word from samples drawn from the noise distribution, outperforms the more costly herarchical softmax method. Subsampling frequent words, which downsizes the importance of these words similarly to PMI weighting in count moels, is also bringing significant improvements. Finally, we go back to Table 2 to point out the poor performance of the out-of-the-box cw model. window weight compress dim. mean rank 2 PMI no 300K 35 5 PMI no 300K 38 2 PMI SVD 500 42 2 PMI SVD 400 46 5 PMI SVD 500 47 2 PMI SVD 300 50 5 PMI SVD 400 51 2 PMI NMF 300 52 2 PMI NMF 400 53 5 PMI SVD 300 53 Table 3: Top count models in terms of mean performance-based model ranking across all tasks",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The first row states that the window-2, PMI, 300K count model was the best count model, and, across all tasks, its average rank, when ALL models are decreasingly ordered by performance, was 35. See Section 2.1 for explanation of the parameters. We must leave the investigation of the parameters that make our predict vectors so much better than cw (more varied training corpus? window size? objective function being used? subsampling? ) to further work. Still, our results show that it’s not just training by context prediction that ensures good performance. The cw approach is very poplar (for example both Huang et al. (2012) and Blcoe and Lapata (2012) used it in the studies we dicussed in Section 1). Had we also based our sytematic comparison of count and predict vectors on the cw model, we would have reached opposite conclusions from the ones we can draw from our word2vec-trained vectors! 5 Conclusion This paper has presented the first systematic coparative evaluation of count and predict vectors. As seasoned distributional semanticists with thoough experience in developing and using count vectors, we set out to conduct this study because we were annoyed by the triumphalist overtones oten surrounding predict models, despite the almost complete lack of a proper comparison to count vectors. 12 Our secret wish was to discover that it is all hype, and count vectors are far superior to their predictive counterparts. A more realistic expe12 Here is an example, where word2vec is called the crown jewel of natural language processing: http://bit.ly/ 1ipv72M 244 win. hier. neg. subsamp. dim mean softm. samp. rank 5 no 10 yes 400 10 2 no 10 yes 300 13 5 no 5 yes 400 13 5 no 5 yes 300 13 5 no 10 yes 300 13 2 no 10 yes 400 13 2 no 5 yes 400 15 5 no 10 yes 200 15 2 no 10 yes 500 15 2 no 5 yes 300 16 Table 4: Top predict models in terms of mean performance-based model ranking across all tasks. See Section 2.2 for explanation of the parameters",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". See Section 2.2 for explanation of the parameters. tation was that a complex picture would emerge, with predict and count vectors beating each other on different tasks. Instead, we found that the prdict models are so good that, while the triumphaist overtones still sound excessive, there are very good reasons to switch to the new architecture. However, due to space limitations we have only focused here on quantitative measures: It remains to be seen whether the two types of models are complementary in the errors they make, in which case combined models could be an interesting aenue for further work. The space of possible parameters of count DSMs is very large, and it’s entirely possible that some options we did not consider would have iproved count vector performance somewhat. Still, given that the predict vectors also outperformed the syntax-based dm model, and often approxmated state-of-the-art performance, a more profiuous way forward might be to focus on parameters and extensions of the predict models instead: Ater all, we obtained our already excellent results by just trying a few variations of the word2vec dfaults. Add to this that, beyond the standard leical semantics challenges we tested here, predict models are currently been successfully applied in cutting-edge domains such as representing phrases (Mikolov et al., 2013c; Socher et al., 2012) or fuing language and vision in a common semantic space (Frome et al., 2013; Socher et al., 2013). Based on the results reported here and the cosiderations we just made, we would certainly reommend anybody interested in using DSMs for theoretical or practical applications to go for the predict models, with the important caveat that they are not all created equal (cf. the big difference btween word2vec and cw models)",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",
    "author": "Marco Baroni ; Georgiana Dinu ; German Kruszewski",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Baroni-2014-Dont-count-predict-a-systematic-com.pdf",
    "date_published": "2014-06-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". the big difference btween word2vec and cw models). At the same time, given the large amount of work that has been carried out on count DSMs, we would like to eplore, in the near future, how certain questions and methods that have been considered with rspect to traditional DSMs will transfer to predict models. For example, the developers of Latent Semantic Analysis (Landauer and Dumais, 1997), Topic Models (Griffiths et al., 2007) and related DSMs have shown that the dimensions of these models can be interpreted as general “latent” smantic domains, which gives the corresponding models some a priori cognitive plausibility while paving the way for interesting applications. Aother important line of DSM research concerns “context engineering”: There has been for exaple much work on how to encode syntactic iformation into context features (Pad ́o and Lapata, 2007), and more recent studies construct and cobine feature spaces expressing topical vs. funtional information (Turney, 2012). To give just one last example, distributional semanticists have looked at whether certain properties of vectors rflect semantic relations in the expected way: e.g., whether the vectors of hypernyms “distributioally include” the vectors of hyponyms in some mathematical precise sense. Do the dimensions of predict models also ecode latent semantic domains? Do these models afford the same flexibility of count vectors in caturing linguistically rich contexts? Does the struture of predict vectors mimic meaningful sematic relations? Does all of this even matter, or are we on the cusp of discovering radically new ways to tackle the same problems that have been aproached as we just sketched in traditional distrbutional semantics? Either way, the results of the present investigtion indicate that these are important directions for future research in computational semantics. Acknowledgments We acknowledge ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES).",
    "chunk_id": "Natural_language_processing_don't_count,_predict!_a_systematic_comparison_of_context-counting_vs._context-predicting_semantic_vectors.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": "Journal of Machine Learning Research 21 (2020) 1- 67 Submitted 1/20; Revised 6/20; Published 6/20 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Colin Raffel ∗ craffel@gmail.com Noam Shazeer ∗ noam@google.com Adam Roberts ∗ adarob@google.com Katherine Lee ∗ katherinelee@google.com Sharan Narang sharannarang@google.com Michael Matena mmatena@google.com Yanqi Zhou yanqiz@google.com Wei Li mweili@google.com Peter J. Liu peterjliu@google.com Google, Mountain View, CA 94043, USA Editor: Ivan Titov Abstract pre-trained models, and code. 1 Keywords: 1. Introduction ∗. Equal contribution. A description of each author’s contribution is available in Appendix A . Correspondence to craffel@gmail.com . 1. https://github.com/google-research/text-to-text-transfer-transformer ©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. arXiv:1910.10683v4 [cs.LG] 19 Sep 2023 Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu to the Internet—for example, the Common Crawl project 2 produces about 20TB of text 2. http://commoncrawl.org Exploring the Limits of Transfer Learning \"translate English to German: That is good.\" \"cola sentence: The course is jumping well.\" \"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi\" \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" T5 \"Das ist gut.\" \"not acceptable\" \"six people hospitalized after a storm in attala county.\" \"3.8\" “T5” refers to our model, which we dub the “ T ext T ext T ransfer T ransformer”. pre-trained models. 1 Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 2. Setup data. We refer to our model and framework as the “ T ext T ext T ransfer T ransformer” 2.1 Model the interested reader to the original paper ( Vaswani et al. , 2017 ) or follow-up tutorials 3 , 4 for 3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2017 ) or follow-up tutorials 3 , 4 for 3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/ Exploring the Limits of Transfer Learning models on “slices” of Cloud TPU Pods. 5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with 2.2 The Colossal Clean Crawled Corpus 5. https://cloud.google.com/tpu/ Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu • We only retained lines that ended in a terminal punctuation mark (i.e. a period, • We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”. 6 • Some pages had placeholder “lorem ipsum” text; we removed any page where the we used langdetect 7 to filter out any pages that were not classified as English with a 6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words 7. https://pypi.org/project/langdetect/ Exploring the Limits of Transfer Learning comprises reasonably clean and natural English text. We dub this data set the “ C olossal C lean C rawled C orpus” (or C4 for short) and release it as part of TensorFlow Datasets. 8 2.3 Downstream Tasks Datasets. 9 8. https://www.tensorflow.org/datasets/catalog/c4 9. https://www.tensorflow.org/datasets Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu et al. , 2017 ) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 data from 2015 and newstest2014 as a validation set ( Bojar et al. , 2015 ). For English to 2.4 Input and Output Format Exploring the Limits of Transfer Learning in increments of 0 . 2, so we simply rounded any score to the nearest increment of 0 . 2 and value 2 . 57 would be mapped to the string “2.6”). At test time, if the model outputs a Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu training set, but the DPR data set adds about 1 , 000 pronoun resolution examples. Examples 3",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". At test time, if the model outputs a Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu training set, but the DPR data set adds about 1 , 000 pronoun resolution examples. Examples 3. Experiments Exploring the Limits of Transfer Learning 3.1 Baseline 3.1.1 Model size and configuration to a “BERT BASE ” ( Devlin et al. , 2018 ) stack. Specifically, both the block consist of a dense layer with an output dimensionality of d ff = 3072 followed by a mechanisms have an inner dimensionality of d kv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of d model = 768. In total, of parameters of BERT BASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0 . 1 everywhere dropout is applied 3.1.2 Training We pre-train each model for 2 19 = 524 , 288 steps on C4 before fine-tuning. We use a Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu we “pack” multiple sequences into each entry of the batch 10 so that our batches contain roughly 2 16 = 65 , 536 tokens. In total, this batch size and number of steps corresponds to pre-training on 2 35 ≈ 34B tokens. This is considerably less than BERT ( Devlin et al. , 2 . 2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still effect of pre-training for more steps in Sections 3.6 and 3.7 . Note that 2 35 tokens only covers p max( n, k ) where n is the current training iteration and k is the number of warm-up steps (set to 10 4 in all of our experiments). This sets a constant learning rate of 0 . 01 for the first 10 4 steps, Our models are fine-tuned for 2 18 = 262 , 144 steps on all tasks. This value was chosen 2 16 tokens per batch). We use a constant learning rate of 0 . 001 when fine-tuning. We save a checkpoint every 5 , 000 steps and report results on the model checkpoint corresponding 3.1.3 Vocabulary ( Sennrich et al. , 2015 ; Kudo , 2018 )",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 001 when fine-tuning. We save a checkpoint every 5 , 000 steps and report results on the model checkpoint corresponding 3.1.3 Vocabulary ( Sennrich et al. , 2015 ; Kudo , 2018 ). For all experiments, we use a vocabulary of 32 , 000 3.1.4 Unsupervised Objective 10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/ index.html#data_generators.generator_utils.pack_examples Exploring the Limits of Transfer Learning <X> <Y> <X> <Y> <Z> words “for”, “inviting” and “last” (marked with an × ) are randomly chosen for token (shown as <X> and <Y> ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel <X> . The tokens used to replace them in the input plus a final sentinel token <Z> . 3.1.5 Baseline Performance Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline average 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Baseline standard deviation 0 . 235 0 . 065 0 . 343 0 . 416 0 . 112 0 . 090 0 . 108 No pre-training 66 . 22 17 . 60 50 . 31 53 . 04 25 . 86 39 . 77 24 . 04 different changes. Separately, we also measure the performance of training our model for 2 18 configuration appears, we will mark it with a ⋆ (as in the first row of Table 1 ). We also will boldface any score that is within two standard deviations of the maximum (best) in a models of similar size. For example, BERT BASE achieved an exact match score of 80 . 8 on SQuAD and an accuracy of 84 . 4 on MNLI-matched, whereas we achieve 80 . 88 and 84 . 24, respectively (see Table 16 ). Note that we cannot directly compare our baseline to BERT BASE because ours is an encoder-decoder model and was pre-trained for roughly 1 ⁄ 4 Exploring the Limits of Transfer Learning For example, on CB our baseline model had an average F1 score of 91 . 22 with a standard deviation of 3 . 237 (see Table 16 ), which may be partly due to the fact that CB’s validation 3.2 Architectures 3.2.1 Model Structures sequence",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 22 with a standard deviation of 3 . 237 (see Table 16 ), which may be partly due to the fact that CB’s validation 3.2 Architectures 3.2.1 Model Structures sequence. Specifically, let y i refer to the i th element of the output sequence and x j refer to the j th entry of the input sequence. y i is computed as P j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention shown in Figure 3 . For example, the causal mask (Figure 3 , middle) sets any w i,j to zero if j > i . Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i . A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j the i th output element from depending on any input elements from “the future”. When producing the i th entry of the output sequence, causal masking prevents the model from attending to the j th entry of the input sequence for j > i . This is used during training Exploring the Limits of Transfer Learning x 1 x 2 x 3 x 4 y 1 y 2 . Encoder Decoder x 1 x 2 x 3 y 1 y 2 x 2 x 3 y 1 y 2 . Language model x 1 x 2 x 3 y 1 y 2 x 2 x 3 y 1 y 2 . Prefix LM as x and y respectively. Left: A standard encoder-decoder architecture uses fullto-text setting is that causal masking forces the model’s representation of the i th entry of the input sequence to only depend on the entries up until i . To see why this is potentially Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 3.2.2 Comparing Different Model Structures at the same time",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To see why this is potentially Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 3.2.2 Comparing Different Model Structures at the same time. To see why, first note an encoder-decoder model with L layers in the encoder and L layers in the decoder has approximately the same number of parameters as a language model with 2 L layers. However, the same L + L encoder-decoder model will have approximately the same computational cost as a language model with only L layers. This is a consequence of the fact that the L layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence Exploring the Limits of Transfer Learning times for L -layer language models versus L + L -layer encoder-decoder models, suggesting a count, so we make the simplifying assumption that an L + L -layer encoder-decoder model has the same number of parameters as an 2 L -layer language model. BERT BASE -sized layer stack as L and P , respectively. We will use M to refer to the number of FLOPs required for an L + L -layer encoder-decoder model or L -layer decoder-only model • An encoder-decoder model with L layers in the encoder and L layers in the decoder. This model has 2 P parameters and a computation cost of M FLOPs. resulting in P parameters and an M -FLOP computational cost. • An encoder-decoder model with L/ 2 layers each in the encoder and decoder, giving P parameters and an M/ 2-FLOP cost. • A decoder-only language model with L layers and P parameters and a resulting computational cost of M FLOPs. 3.2.3 Objectives 3.2.4 Results This variant has the highest parameter count (2 P ) but the same computational cost as the P -parameter decoder-only models. Surprisingly, we found that sharing parameters across the Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Architecture Objective Params Cost GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Encoder-decoder Denoising 2 P M 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Enc-dec, shared Denoising P M 82 . 81 18 . 78 80 . 63 70 . 73 26 . 72 39 . 03 27 . 46 Enc-dec, 6 layers Denoising P M/ 2 80 . 88 18 . 97 77 . 59 68 . 42 26 . 38 38 . 40 26 . 95 Language model Denoising P M 74 . 70 17 . 93 61 . 14 55 . 02 25 . 09 35 . 28 25 . 86 Prefix LM Denoising P M 81 . 82 18 . 61 78 . 94 68 . 11 26 . 43 37 . 98 27 . 39 Encoder-decoder LM 2 P M 79 . 56 18 . 59 76 . 02 64 . 29 26 . 27 39 . 17 26 . 86 Enc-dec, shared LM P M 79 . 60 18 . 13 76 . 35 63 . 50 26 . 62 39 . 17 27 . 05 Enc-dec, 6 layers LM P M/ 2 78 . 67 18 . 26 75 . 32 64 . 06 26 . 13 38 . 42 26 . 89 Language model LM P M 73 . 78 17 . 54 53 . 81 56 . 51 25 . 23 34 . 31 25 . 38 Prefix LM LM P M 79 . 68 17 . 84 76 . 87 64 . 86 26 . 28 37 . 51 26 . 76 use P to refer to the number of parameters in a 12-layer base Transformer layer stack and M to refer to the FLOPs required to process a sequence using the encode3.3 Unsupervised Objectives Exploring the Limits of Transfer Learning Objective Inputs Targets Prefix language modeling Thank you for inviting me to your party last week . BERT-style Devlin et al. ( 2018 ) Thank you <M> <M> me to your party apple week . (original text) Deshuffling party me for your to . last fun you inviting week Thank (original text) MASS-style Song et al. ( 2019 ) Thank you <M> <M> me to your party <M> week . (original text) I.i.d. noise, replace spans Thank you <X> me to your party <Y> week . <X> for inviting <Y> last <Z> I.i.d. noise, drop tokens Thank you me to your party week . for inviting last Random spans Thank you <X> to <Y> week . <X> for inviting me <Y> your party last <Z> week .” Note that all of our objectives process tokenized text. For this particular (original text) as a target to denote that the model is tasked with reconstructing the entire input text",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For this particular (original text) as a target to denote that the model is tasked with reconstructing the entire input text. <M> denotes a shared mask token and <X> , <Y> , and <Z> denote 3.3.1 Disparate High-Level Approaches Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Objective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo Prefix language modeling 80 . 69 18 . 94 77 . 99 65 . 27 26 . 86 39 . 73 27 . 49 BERT-style ( Devlin et al. , 2018 ) 82 . 96 19 . 17 80 . 65 69 . 85 26 . 78 40 . 03 27 . 41 Deshuffling 73 . 17 18 . 59 67 . 61 58 . 47 26 . 11 39 . 30 25 . 62 Objective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo BERT-style ( Devlin et al. , 2018 ) 82 . 96 19 . 17 80 . 65 69 . 85 26 . 78 40 . 03 27 . 41 MASS-style ( Song et al. , 2019 ) 82 . 32 19 . 16 80 . 10 69 . 28 26 . 79 39 . 89 27 . 55 ⋆ Replace corrupted spans 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Drop corrupted tokens 84 . 44 19 . 31 80 . 52 68 . 67 27 . 07 39 . 76 27 . 82 3.3.2 Simplifying the BERT Objective in the GLUE score thanks to a significantly higher score on CoLA (60 . 04, compared to our Exploring the Limits of Transfer Learning Corruption rate GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo 10% 82 . 82 19 . 00 80 . 38 69 . 55 26 . 87 39 . 28 27 . 44 ⋆ 15% 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 25% 83 . 00 19 . 54 80 . 96 70 . 48 27 . 04 39 . 83 27 . 47 50% 81 . 27 19 . 32 79 . 80 70 . 33 27 . 01 39 . 90 27 . 49 baseline average of 53 . 84, see Table 16 ). This may be due to the fact that CoLA involves 3.3.3 Varying the Corruption Rate 3.3.4 Corrupting Spans Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Span length GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline (i.i.d.) 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 2 83 . 54 19 . 39 82 . 09 72 . 20 26 . 76 39 . 99 27 . 63 3 83 . 49 19 . 62 81 . 84 72 . 53 26 . 86 39 . 65 27 . 62 5 83 . 40 19 . 24 82 . 05 72 . 23 26 . 88 39 . 40 27 . 53 10 82 . 85 19 . 33 81 . 84 70 . 44 26 . 79 39 . 49 27",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 20 26 . 76 39 . 99 27 . 63 3 83 . 49 19 . 62 81 . 84 72 . 53 26 . 86 39 . 65 27 . 62 5 83 . 40 19 . 24 82 . 05 72 . 23 26 . 88 39 . 40 27 . 53 10 82 . 85 19 . 33 81 . 84 70 . 44 26 . 79 39 . 49 27 . 69 should be 25 total spans, then the total number of corrupted tokens would be 500 × 0 . 15 = 75 and the average span length would be 75 / 25 = 3. Note that given the original sequence 3.3.5 Discussion 3.4 Pre-training Data set Exploring the Limits of Transfer Learning of the C4 data set variants we consider as part of TensorFlow Datasets. 11 3.4.1 Unlabeled Data Sets C4 As a baseline, we first consider pre-training on our proposed unlabeled data set as Unfiltered C4 To measure the effect of the heuristic filtering we used in creating C4 an alternate version of C4 that forgoes this filtering. Note that we still use langdetect 11. https://www.tensorflow.org/datasets/catalog/c4 Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu because langdetect sometimes assigns a low probability to non-natural English text. RealNews-like Recent work has used text data extracted from news websites ( Zellers WebText-like Similarly, the WebText data set ( Radford et al. , 2019 ) only uses content appeared in the list prepared by the OpenWebText effort. 12 However, this resulted in Wikipedia The website Wikipedia consists of millions of encyclopedia articles written English Wikipedia text data from TensorFlow Datasets, 13 which omits any markup or Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia 12. https://github.com/jcpeterson/openwebtext 13. https://www.tensorflow.org/datasets/catalog/wikipedia Exploring the Limits of Transfer Learning Data set Size GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ C4 745GB 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 C4, unfiltered 6.1TB 81 . 46 19 . 14 78 . 78 68 . 04 26 . 55 39 . 34 27 . 21 RealNews-like 35GB 83 . 83 19 . 23 80 . 39 72 . 38 26 . 75 39 . 90 27 . 48 WebText-like 17GB 84 . 03 19 . 31 81 . 42 71 . 40 26 . 80 39 . 74 27",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 46 19 . 14 78 . 78 68 . 04 26 . 55 39 . 34 27 . 21 RealNews-like 35GB 83 . 83 19 . 23 80 . 39 72 . 38 26 . 75 39 . 90 27 . 48 WebText-like 17GB 84 . 03 19 . 31 81 . 42 71 . 40 26 . 80 39 . 74 27 . 59 Wikipedia 16GB 81 . 85 19 . 31 81 . 29 68 . 01 26 . 94 39 . 69 27 . 67 Wikipedia + TBC 20GB 83 . 65 19 . 28 82 . 08 73 . 24 26 . 77 39 . 63 27 . 57 produced a SuperGLUE score of 73 . 24, beating our baseline’s score (using C4) of 71 . 36. This is almost entirely attributable to a boost in performance from 25 . 78 (baseline, C4) to 50 . 93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16 ). MultiRC for pre-training conferred an increase from 68 . 16 to 73 . 72 on the Exact Match score for The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks . This is unsurprising but also unsatisfying if data set that was about 40 × smaller than C4 despite being based on 12 × more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on 2 35 ≈ 34B 3.4.2 Pre-training Data set Size Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Number of tokens Repeats GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Full data set 0 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 2 29 64 82 . 87 19 . 19 80 . 97 72 . 03 26 . 83 39 . 74 27 . 63 2 27 256 82 . 62 19 . 20 79 . 78 69 . 97 27 . 02 39 . 71 27 . 33 2 25 1 , 024 79 . 55 18 . 57 76 . 27 64 . 76 26 . 38 39 . 56 26 . 80 2 23 4 , 096 76 . 34 18 . 33 70 . 92 59 . 29 26 . 37 38 . 84 25 . 81 we only use the first N tokens from C4 (with varying values of N shown in the first column) but still pre-train over 2 35 tokens. This results in the data set being 2 35 ≈ 34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 2 29 , 2 27 , 2 25 and 2 23 tokens. These sizes correspond to repeating the data set 64, 256, 1 , 024, and 4 , 096 times respectively over the course of pre-training",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". These sizes correspond to repeating the data set 64, 256, 1 , 024, and 4 , 096 times respectively over the course of pre-training. 3.5 Training Strategy Exploring the Limits of Transfer Learning 0 100 200 300 400 500 Step × 1,000 0.0 0.2 0.4 0.6 0.8 1.0 Training loss Dataset size Full dataset 2 29 2 27 2 25 2 23 sizes considered correspond to repeating the data set between 64 and 4 , 096 times 3.5.1 Fine-tuning Methods hyperparameter of this approach is the inner dimensionality d of the feed-forward network, various values for d . Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Fine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ All parameters 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Adapter layers, d = 32 80 . 52 15 . 08 79 . 32 60 . 40 13 . 84 17 . 88 15 . 54 Adapter layers, d = 128 81 . 51 16 . 62 79 . 47 63 . 03 19 . 83 27 . 50 22 . 63 Adapter layers, d = 512 81 . 54 17 . 78 79 . 18 64 . 30 23 . 45 33 . 98 25 . 81 Adapter layers, d = 2048 81 . 51 16 . 62 79 . 47 63 . 03 19 . 83 27 . 50 22 . 63 Gradual unfreezing 82 . 50 18 . 95 79 . 17 70 . 79 26 . 71 39 . 02 26 . 93 of the model’s parameters. For adapter layers, d refers to the inner dimensionality 2 18 steps. As such, we subdivide the fine-tuning process into 12 episodes of 2 18 / 12 steps each and train from layers 12 − n to 12 in the n th episode. We note that Howard and Ruder strategy of fine-tuning an additional layer after every 2 18 / 12 steps. For adapter layers, we report the performance using an inner dimensionality d of 32, 128, lower-resource tasks like SQuAD work well with a small value of d whereas higher resource enough that it necessitates a large value of d",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We found that gradual unfreezing caused 3.5.2 Multi-task Learning Exploring the Limits of Transfer Learning Examples-proportional mixing A major factor in how quickly a model will overfit to our N task’s data sets is e n , n ∈{ 1 , , N } then we set probability of sampling an example from the m th task during training to r m = min ( e m , K ) / P min ( e n , K ) where K is the artificial data set size limit. Temperature-scaled mixing An alternative way of mitigating the huge disparity between resource languages. 14 To implement temperature scaling with temperature T , we raise 14. https://github.com/google-research/bert/blob/master/multilingual.md Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu each task’s mixing rate r m to the power of 1 ⁄ T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing. We retain the data set size limit K (applied to obtain r m before temperature scaling) but set it to a large value of K = 2 21 . We use a large value of K because increasing the temperature will Equal mixing In this case, we sample examples from each task with equal probability. fine-tune results, we train multi-task models for the same total number of steps: 2 19 + 2 18 = 786 , 432. The results are shown in Table 11 . for most tasks there is a “sweet spot” for K where the model obtains the best performance, and larger or smaller values of K tend to result in worse performance. The exception (for the range of K values we considered) was WMT English to French translation, which is such a from most tasks, with T = 2 performing the best in most cases. The finding that a multi-task 3.5.3 Combining Multi-Task Learning with Fine-Tuning model on an examples-proportional mixture with an artificial data set size limit of K = 2 19 Exploring the Limits of Transfer Learning Mixing strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline (pre-train/fine-tune) 83 . 28 19 . 24 80",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Equal 76 . 13 19 . 02 76 . 51 63 . 37 23 . 89 34 . 31 26 . 78 Examples-proportional, K = 2 16 80 . 45 19 . 04 77 . 25 69 . 95 24 . 35 34 . 99 27 . 10 Examples-proportional, K = 2 17 81 . 56 19 . 12 77 . 00 67 . 91 24 . 36 35 . 00 27 . 25 Examples-proportional, K = 2 18 81 . 67 19 . 07 78 . 17 67 . 94 24 . 57 35 . 19 27 . 39 Examples-proportional, K = 2 19 81 . 42 19 . 24 79 . 78 67 . 30 25 . 21 36 . 30 27 . 76 Examples-proportional, K = 2 20 80 . 80 19 . 24 80 . 36 67 . 38 25 . 66 36 . 93 27 . 68 Examples-proportional, K = 2 21 79 . 83 18 . 79 79 . 50 65 . 10 25 . 82 37 . 22 27 . 13 Temperature-scaled, T = 2 81 . 90 19 . 28 79 . 42 69 . 92 25 . 42 36 . 72 27 . 20 Temperature-scaled, T = 4 80 . 56 19 . 22 77 . 99 69 . 54 25 . 04 35 . 82 27 . 45 Temperature-scaled, T = 8 77 . 21 19 . 10 77 . 14 66 . 07 24 . 55 35 . 35 27 . 17 the total size of each data set, with an artificial limit ( K ) on the maximum data set size. Temperature-scaled mixing re-scales the sampling rates by a temperature T . For temperature-scaled mixing, we use an artificial data set size limit of K = 2 21 . examples-proportional mixture (with K = 2 19 ) except that we omit one of the downstream all of the supervised tasks we consider with K = 2 19 . In all of these variants, we follow our standard procedure of pre-training for 2 19 steps before fine-tuning for 2 18 steps. (without fine-tuning) on an examples-proportional mixture with K = 2 19 . We find that Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Training strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Unsupervised pre-training + fine-tuning 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Multi-task training 81 . 42 19 . 24 79 . 78 67 . 30 25 . 21 36 . 30 27 . 76 Multi-task pre-training + fine-tuning 83 . 11 19 . 12 80 . 26 71 . 03 27 . 08 39 . 80 28 . 07 Leave-one-out multi-task training 81 . 98 19 . 05 79 . 97 71 . 68 26 . 93 39 . 79 27 . 87 Supervised multi-task pre-training 79 . 93 18 . 96 77 . 38 65",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 12 80 . 26 71 . 03 27 . 08 39 . 80 28 . 07 Leave-one-out multi-task training 81 . 98 19 . 05 79 . 97 71 . 68 26 . 93 39 . 79 27 . 87 Supervised multi-task pre-training 79 . 93 18 . 96 77 . 38 65 . 36 26 . 81 40 . 13 28 . 04 3.6 Scaling different approaches by addressing the following premise: “You were just given 4 × more fine-tuned for 2 19 and 2 18 steps respectively. The encoder and decoder are both sized similarly to “BERT BASE ”. To experiment with increased model size, we follow the guidelines of “BERT LARGE ” Devlin et al. ( 2018 ) and use d ff = 4096, d model = 1024, d kv = 64 and the encoder and decoder, producing models with 2 × and 4 × as many parameters as our original model. These two variants also have a roughly 2 × and 4 × the computational cost. Using our baseline and these two larger models, we consider three ways of using 4 × as much computation: Training for 4 × as many steps, training for 2 × as many steps with the 2 × bigger model, and training the 4 × bigger model for the “baseline” number of training steps. the data even when training for 2 23 steps. An alternative way for the model to see 4 × as much data is to increase the batch size by a However, training with a 4 × larger batch size can yield a different outcome than training Exploring the Limits of Transfer Learning Scaling strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 1 × size, 4 × training steps 85 . 33 19 . 33 82 . 45 74 . 72 27 . 08 40 . 66 27 . 93 1 × size, 4 × batch size 84 . 60 19 . 42 82 . 52 74 . 64 27 . 07 40 . 60 27 . 84 2 × size, 2 × training steps 86 . 18 19 . 66 84 . 18 77 . 18 27 . 52 41 . 03 28 . 19 4 × size, 1 × training steps 85 . 91 19 . 73 83 . 86 78 . 04 27 . 47 40 . 71 28 . 10 4 × ensembled 84 . 77 20 . 10 83 . 09 71 . 74 28 . 05 40 . 53 28 . 57 4 × ensembled, fine-tune only 84 . 05 19 . 57 82 . 36 71 . 55 27 . 55 40 . 22 28 . 09 except ensembling fine-tuned models use 4 × the computation as the baseline. for 4 × as many steps ( Shallue et al",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 05 19 . 57 82 . 36 71 . 55 27 . 55 40 . 22 28 . 09 except ensembling fine-tuned models use 4 × the computation as the baseline. for 4 × as many steps ( Shallue et al. , 2018 ). We include an additional experiment where we train our baseline model with a 4 × larger batch size to compare these two cases. separate fine-tuned versions. While this does not use our entire 4 × computational budget, improves the baseline. There was no clear winner between training for 4 × as many steps or using a 4 × larger batch size, though both were beneficial. In general, increasing the 2 × bigger model for 2 × as long and training a 4 × bigger model on any of the tasks we Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu ensembling N separate models has a similar cost to using a model that has an N × higher 3.7 Putting It All Together Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption Longer training Our baseline model uses a relatively small amount of pre-training ( 1 ⁄ 4 as much as BERT ( Devlin et al. , 2018 ), 1 ⁄ 16 as much as XLNet ( Yang et al. , 2019 ), 1 ⁄ 64 as steps on a batch size of 2 11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens (about 32 × as many as our baseline). In Section 3.4.1 , we Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved • Base. • Small. We consider a smaller model, which scales the baseline down by using d model = 512, d ff = 2 , 048, 8-headed attention, and only 6 layers each in the Exploring the Limits of Transfer Learning • Large. Since our baseline uses a BERT BASE -sized encoder and decoder, we and structure to BERT LARGE . Specifically, this variant uses d model = 1 , 024, d ff = 4 , 096, d kv = 64, 16-headed attention, and 24 layers each in the encoder and • 3B and 11B. To further explore what kind of performance is possible when d model = 1024, a 24 layer encoder and decoder, and d kv = 128",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To further explore what kind of performance is possible when d model = 1024, a 24 layer encoder and decoder, and d kv = 128. For the “3B” variant, we use d ff = 16 , 384 with 32-headed attention, which results in around 2 . 8 billion parameters; for “11B” we use d ff = 65 , 536 with 128-headed attention producing a model with about 11 billion parameters. We chose to scale up d ff Multi-task pre-training In Section 3.5.3 , we showed that pre-training on a multi-task standard example-proportional mixing (described in Section 3.5.2 ): 710 , 000 for Small, 2 , 620 , 000 for Base, 8 , 660 , 000 for Large, 33 , 500 , 000 for 3B, and 133 , 000 , 000 for 11B. Fine-tuning on individual GLUE and SuperGLUE tasks So far, when fine-tuning For example, our large batch size of 2 11 length-512 sequences would result in the entire every 1 , 000 steps rather than every 5 , 000 steps to ensure we have access to the model’s Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Beam search All of our previous results were reported using greedy decoding. For tasks of α = 0 . 6 ( Wu et al. , 2016 ) for the WMT translation and CNN/DM summarization Test set Since this is our final set of experiments, we report results on the test set rather with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for Engliscompute official test set scores. 15 , 16 For SQuAD, evaluating on the test set requires We achieved a state-of-the-art average GLUE score of 90 . 3. Notably, our performance was has historically lagged behind human performance, which is 93 . 6 and 95 . 9 respectively ( Wang 15. http://gluebenchmark.com 16. http://super.gluebenchmark.com Exploring the Limits of Transfer Learning GLUE CoLA SST-2 MRPC MRPC STS-B STS-B Model Average Matthew’s Accuracy F1 Accuracy Pearson Spearman Previous best 89 . 4 a 69 . 2 b 97 . 1 a 93 . 6 b 91 . 5 b 92 . 7 b 92 . 3 b T5-Small 77 . 4 41 . 0 91 . 8 89 . 7 86 . 6 85 . 6 85 . 0 T5-Base 82 . 7 51 . 1 95",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 a 69 . 2 b 97 . 1 a 93 . 6 b 91 . 5 b 92 . 7 b 92 . 3 b T5-Small 77 . 4 41 . 0 91 . 8 89 . 7 86 . 6 85 . 6 85 . 0 T5-Base 82 . 7 51 . 1 95 . 2 90 . 7 87 . 5 89 . 4 88 . 6 T5-Large 86 . 4 61 . 2 96 . 3 92 . 4 89 . 9 89 . 9 89 . 2 T5-3B 88 . 5 67 . 1 97 . 4 92 . 5 90 . 0 90 . 6 89 . 8 T5-11B 90 . 3 71 . 6 97 . 5 92 . 8 90 . 4 93 . 1 92 . 8 QQP QQP MNLI-m MNLI-mm QNLI RTE WNLI Model F1 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Previous best 74 . 8 c 90 . 7 b 91 . 3 a 91 . 0 a 99 . 2 a 89 . 2 a 91 . 8 a T5-Small 70 . 0 88 . 0 82 . 4 82 . 3 90 . 3 69 . 9 69 . 2 T5-Base 72 . 6 89 . 4 87 . 1 86 . 2 93 . 7 80 . 1 78 . 8 T5-Large 73 . 9 89 . 9 89 . 9 89 . 6 94 . 8 87 . 2 85 . 6 T5-3B 74 . 4 89 . 7 91 . 4 91 . 2 96 . 3 91 . 1 89 . 7 T5-11B 75 . 1 90 . 6 92 . 2 91 . 9 96 . 9 92 . 8 94 . 5 SQuAD SQuAD SuperGLUE BoolQ CB CB COPA Model EM F1 Average Accuracy F1 Accuracy Accuracy Previous best 90 . 1 a 95 . 5 a 84 . 6 d 87 . 1 d 90 . 5 d 95 . 2 d 90 . 6 d T5-Small 79 . 10 87 . 24 63 . 3 76 . 4 56 . 9 81 . 6 46 . 0 T5-Base 85 . 44 92 . 08 76 . 2 81 . 4 86 . 2 94 . 0 71 . 2 T5-Large 86 . 66 93 . 79 82 . 3 85 . 4 91 . 6 94 . 8 83 . 4 T5-3B 88 . 53 94 . 95 86 . 4 89 . 9 90 . 3 94 . 4 92 . 0 T5-11B 91 . 26 96 . 22 88 . 9 91 . 2 93 . 9 96 . 8 94 . 8 MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC Model F1a EM F1 Accuracy Accuracy Accuracy Accuracy Previous best 84 . 4 d 52 . 5 d 90 . 6 d 90 . 0 d 88 . 2 d 69 . 9 d 89 . 0 d T5-Small 69 . 3 26 . 3 56 . 3 55 . 4 73 . 3 66 . 9 70 . 5 T5-Base 79 . 7 43 . 1 75 . 0 74 . 2 81 . 5 68 . 3 80 . 8 T5-Large 83 . 3 50 . 7 86 . 8 85 . 9 87 . 8 69 . 3 86 . 3 T5-3B 86 . 8 58 . 3 91 . 2 90 . 4 90 . 7 72 . 1 90 . 4 T5-11B 88 . 1 63 . 3 94 . 1 93 . 4 92 . 5 76 . 9 93 . 8 WMT EnDe WMT EnFr WMT EnRo CNN/DM CNN/DM CNN/DM Model BLEU BLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L Previous best 33 . 8 e 43 . 8 e 38 . 5 f 43 . 47 g 20 . 30 g 40 . 63 g T5-Small 26 . 7 36 . 0 26 . 8 41 . 12 19 . 56 38 . 35 T5-Base 30 . 9 41 . 2 28 . 0 42 . 05 20 . 34 39 . 40 T5-Large 32 . 0 41 . 5 28 . 1 42 . 50 20 . 68 39",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5 f 43 . 47 g 20 . 30 g 40 . 63 g T5-Small 26 . 7 36 . 0 26 . 8 41 . 12 19 . 56 38 . 35 T5-Base 30 . 9 41 . 2 28 . 0 42 . 05 20 . 34 39 . 40 T5-Large 32 . 0 41 . 5 28 . 1 42 . 50 20 . 68 39 . 75 T5-3B 31 . 8 42 . 6 28 . 2 42 . 72 21 . 02 39 . 94 T5-11B 32 . 1 43 . 4 28 . 1 43 . 52 21 . 55 40 . 69 a ( Lan et al. , 2019 ) b ( Wang et al. , 2019c ) c ( Zhu et al. , 2019 ) d ( Liu et al. , 2019c ) e ( Edunov et al. , 2018 ) f ( Lample and Conneau , 2019 ) g ( Dong et al. , 2019 ) Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu the small SQuAD training set. Human performance on SQuAD is estimated at 82 . 30 and 91 . 22 for the Exact Match and F1 metric respectively ( Rajpurkar et al. , 2016 ), so it is not average score of 84 . 6 ( Liu et al. , 2019c ) to 88 . 9). SuperGLUE was designed to include performance of 89 . 8 ( Wang et al. , 2019b ). Interestingly, on the reading comprehension tasks newstest2014 set use the much larger training set from WMT 2018 ( Edunov et al. , 2018 ), on 2 35 ≈ 34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. Exploring the Limits of Transfer Learning Model GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Baseline-1T 84 . 80 19 . 62 83 . 01 73 . 90 27 . 46 40 . 30 28 . 34 T5-Base 85 . 97 20 . 90 85 . 44 75 . 64 28 . 37 41 . 37 28 . 98 tokens (the same number used for the T5 model variants) instead of 2 35 ≈ 34B 4",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 80 19 . 62 83 . 01 73 . 90 27 . 46 40 . 30 28 . 34 T5-Base 85 . 97 20 . 90 85 . 44 75 . 64 28 . 37 41 . 37 28 . 98 tokens (the same number used for the T5 model variants) instead of 2 35 ≈ 34B 4. Reflection 4.1 Takeaways Text-to-text Our text-to-text framework provides a simple way to train a single model Architectures While some work on transfer learning for NLP has considered architectural Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Unsupervised objectives Overall, we found that most “denoising” objectives, which train Data sets We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises Training strategies We found that the basic approach of updating all of a pre-trained Scaling We compared various strategies for taking advantage of additional compute, incluPushing the limits We combined our above insights and trained substantially larger set, and pre-trained model weights for each T5 variant. 1 Exploring the Limits of Transfer Learning 4.2 Outlook The inconvenience of large models An unsurprising but important result from our More efficient knowledge extraction Recall that one of the goals of pre-training is Formalizing the similarity between tasks We observed that pre-training on unlabeled also help choose supervised pre-training tasks, which has been shown to be helpful for Language-agnostic models We were disappointed to find that English-only pre-training Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Acknowledgments Exploring the Limits of Transfer Learning Appendix A. Contributions Appendix B. Converting WNLI to Our Text-to-Text Format Appendix C. Example Predictions on CNN/Daily Mail Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 1. Ground-truth: leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside - watched by ms moolman . Prediction: leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . 2. Ground-truth: researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us . Prediction: researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us . 3. Ground-truth: executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn’t work . it is ’essential that executions are carried out in a constitutional manner,’ said georgia attorney general sam olens . Prediction: the execution of kelly gissendaner, the only woman on georgia’s death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country . 4. Ground-truth: dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain . Prediction: dani alves was not selected for brazil’s friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter Exploring the Limits of Transfer Learning of a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news . Appendix D. Preprocessed Examples D.1 CoLA Original input: Sentence: John made Bill master of himself. Processed input: cola sentence: John made Bill master of himself. Original target: 1 Processed target: acceptable D.2 RTE Original input: Sentence 1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). Sentence 2: Slovenia has 3,000 inhabitants. Processed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). sentence2: Slovenia has 3,000 inhabitants. Original target: 1 Processed target: not_entailment D.3 MNLI Original input: Hypothesis: The St. Louis Cardinals have always won",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". sentence2: Slovenia has 3,000 inhabitants. Original target: 1 Processed target: not_entailment D.3 MNLI Original input: Hypothesis: The St. Louis Cardinals have always won. Premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but Processed input: mnli hypothesis: The St. Louis Cardinals have always won. premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: 2 Processed target: contradiction D.4 MRPC Original input: Sentence 1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . Sentence 2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . Processed input: mrpc sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . Original target: 1 Processed target: equivalent D.5 QNLI Original input: Question: Where did Jebe die? Sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Processed input: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Processed input: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0 Processed target: entailment D.6 QQP Original input: Question 1: What attributes would have made you highly desirable in ancient Rome? Question 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? Processed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? Exploring the Limits of Transfer Learning Original target: 0 Processed target: not_duplicate D.7 SST2 Original input: Sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1 Processed target: positive D.8 STSB Original input: Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25 Processed target: 3.2 D.9 CB Original input: Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1 Processed target: contradiction Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu D.10 COPA Original input: Question: effect Premise: Political violence broke out in the nation. Choice 1: Many citizens relocated to the capitol. Choice 2: Many citizens took refuge in other territories. Processed input: copa choice1: Many citizens relocated to the capitol. choice2: Many citizens took refuge in other territories. premise: Political violence broke out in the nation",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". question: effect Original target: 1 Processed target: True D.11 MultiRC Original input: Answer: There was only pie to eat, rather than traditional breakfast foods Paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br> Question: Why was Joey surprised the morning he woke up for breakfast? Processed input: multirc question: Why was Joey surprised the morning he woke up for breakfast? answer: There was only pie to eat, rather than traditional breakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together Exploring the Limits of Transfer Learning at their Aunt Julie’s",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": "and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together Exploring the Limits of Transfer Learning at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br> Original target: 1 Processed target: True D.12 WiC Original input: POS: N Sentence 1: It was the deliberation of his act that was insulting",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Sentence 2: The deliberations of the jury . Word: deliberation Processed input: wic pos: N sentence1: It was the deliberation of his act that was insulting . sentence2: The deliberations of the jury . word: deliberation Original target: 0 Processed target: False D.13 WSC and DPR Original input: Span 2 text: it Span 1 text: stable Span 2 index: 20 Span 1 index: 1 Text: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made it pleasant and airy. Processed input: wsc: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy. Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: 1 Processed target: stable D.14 CNN/Daily Mail Original input: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game . Processed input: summarize: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom Exploring the Limits of Transfer Learning from manchester united’s dutch manager louis van gaal",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". januzaj receives some words of wisdom Exploring the Limits of Transfer Learning from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game . Original target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . Processed target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . D.15 SQuAD Original input: Question: What does increased oxygen concentrations in the patient’s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient’s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: carbon monoxide Processed target: carbon monoxide D.16 WMT English to German Original input: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote. Processed input: translate English to German: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote. Original target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie. Processed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie. D.17 WMT English to French Original input: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room. Processed input: translate English to French: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Original target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers. Processed target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers. D.18 WMT English to Romanian Original input: Taco Bell said it plans to add 2,000 locations in the US by 2022. Processed input: translate English to Romanian: Taco Bell said it plans to add 2,000 locations in the US by 2022. Original target: Taco Bell a afirmat că, până în 2022, intent , ionează să deschidă 2000 de restaurante în SUA. Exploring the Limits of Transfer Learning Processed target: Taco Bell a afirmat că, până în 2022, intent , ionează să deschidă 2000 de restaurante în SUA. Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Appendix E. Scores on Every Task for All Experiments GLUE SuperGLUE WMT Score CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLI m MNLI mm QNLI RTE CNN/DM SQuAD Score BoolQ CB CB COPA MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC EnDe EnFr EnRo Table Experiment Average MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc R-1-F R-2-F R-L-F EM F1 Average Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc BLEU BLEU BLEU 1 ⋆ Baseline average 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 1 Baseline standard deviation 0 . 235 1 . 111 0 . 569 0 . 729 1 . 019 0 . 374 0 . 418 0 . 108 0 . 070 0 . 291 0 . 231 0 . 361 1 . 393 0 . 065 0 . 065 0 . 058 0 . 343 0 . 226 0",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 98 39 . 82 27 . 65 1 Baseline standard deviation 0 . 235 1 . 111 0 . 569 0 . 729 1 . 019 0 . 374 0 . 418 0 . 108 0 . 070 0 . 291 0 . 231 0 . 361 1 . 393 0 . 065 0 . 065 0 . 058 0 . 343 0 . 226 0 . 416 0 . 365 3 . 237 2 . 560 2 . 741 0 . 716 1 . 011 0 . 370 0 . 379 1 . 228 0 . 850 2 . 029 0 . 112 0 . 090 0 . 108 1 No pre-training 66 . 22 12 . 29 80 . 62 81 . 42 73 . 04 72 . 58 72 . 97 81 . 94 86 . 62 68 . 02 67 . 98 75 . 69 58 . 84 39 . 19 17 . 60 36 . 69 50 . 31 61 . 97 53 . 04 65 . 38 71 . 61 76 . 79 62 . 00 59 . 10 0 . 84 20 . 33 17 . 95 54 . 15 54 . 08 65 . 38 25 . 86 39 . 77 24 . 04 2 ⋆ Enc/dec, denoising 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 2 Enc/dec, shared, denoising 82 . 81 55 . 24 91 . 86 91 . 58 88 . 24 87 . 43 87 . 58 88 . 69 91 . 60 83 . 88 84 . 01 90 . 23 73 . 65 41 . 11 18 . 78 38 . 48 80 . 63 88 . 49 70 . 73 77 . 13 95 . 04 96 . 43 65 . 00 66 . 16 22 . 98 68 . 95 68 . 09 70 . 76 68 . 18 75 . 96 26 . 72 39 . 03 27 . 46 2 Enc/dec, 6 layers, denoising 80 . 88 46 . 26 92 . 09 91 . 51 87 . 99 87 . 01 86 . 76 87 . 93 90 . 97 82 . 20 82 . 41 88 . 83 71 . 48 40 . 83 18 . 97 38 . 31 77 . 59 86 . 07 68 . 42 73 . 79 91 . 70 92 . 86 67 . 00 61 . 02 19 . 62 61 . 26 60 . 33 72 . 20 65 . 99 75 . 00 26 . 38 38 . 40 26 . 95 2 Language model, denoising 74 . 70 24 . 50 90 . 60 86 . 08 78 . 92 85 . 22 85 . 42 85 . 40 88 . 99 76 . 72 77 . 05 86 . 02 64 . 62 39 . 49 17 . 93 36 . 91 61 . 14 71 . 37 55 . 02 65 . 47 60 . 08 71 . 43 58 . 00 43 . 03 2 . 94 53 . 35 52 . 31 53 . 07 58 . 62 63 . 46 25 . 09 35 . 28 25 . 86 2 Prefix LM, denoising 81 . 82 49 . 99 92 . 43 91 . 43 88 . 24 87 . 20 86 . 98 88 . 41 91 . 39 82 . 32 82 . 93 88 . 71 74 . 01 40 . 46 18 . 61 37 . 90 78 . 94 87 . 31 68 . 11 75 . 50 93 . 37 91 . 07 60 . 00 63 . 43 21 . 20 65 . 03 64 . 11 71 . 48 65 . 67 73 . 08 26 . 43 37 . 98 27",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 41 91 . 39 82 . 32 82 . 93 88 . 71 74 . 01 40 . 46 18 . 61 37 . 90 78 . 94 87 . 31 68 . 11 75 . 50 93 . 37 91 . 07 60 . 00 63 . 43 21 . 20 65 . 03 64 . 11 71 . 48 65 . 67 73 . 08 26 . 43 37 . 98 27 . 39 2 Enc/dec, LM 79 . 56 42 . 03 91 . 86 91 . 64 88 . 24 87 . 13 87 . 00 88 . 21 91 . 15 81 . 68 81 . 66 88 . 54 65 . 70 40 . 67 18 . 59 38 . 13 76 . 02 84 . 85 64 . 29 72 . 23 85 . 74 89 . 29 57 . 00 60 . 53 16 . 26 59 . 28 58 . 30 65 . 34 64 . 89 70 . 19 26 . 27 39 . 17 26 . 86 2 Enc/dec, shared, LM 79 . 60 44 . 83 92 . 09 90 . 20 85 . 78 86 . 03 85 . 87 87 . 77 91 . 02 81 . 74 82 . 29 89 . 16 65 . 34 40 . 16 18 . 13 37 . 59 76 . 35 84 . 86 63 . 50 70 . 49 91 . 41 87 . 50 55 . 00 60 . 21 16 . 89 57 . 83 56 . 73 63 . 54 63 . 48 70 . 19 26 . 62 39 . 17 27 . 05 2 Enc/dec, 6 layers, LM 78 . 67 38 . 72 91 . 40 90 . 40 86 . 52 86 . 82 86 . 49 87 . 87 91 . 03 80 . 99 80 . 92 88 . 05 65 . 70 40 . 29 18 . 26 37 . 70 75 . 32 84 . 06 64 . 06 71 . 38 85 . 25 89 . 29 60 . 00 57 . 56 16 . 79 55 . 22 54 . 30 66 . 79 63 . 95 71 . 15 26 . 13 38 . 42 26 . 89 2 Language model, LM 73 . 78 28 . 53 89 . 79 85 . 23 78 . 68 84 . 22 84 . 00 84 . 88 88 . 70 74 . 94 75 . 77 84 . 84 58 . 84 38 . 97 17 . 54 36 . 37 53 . 81 64 . 55 56 . 51 64 . 22 59 . 92 71 . 43 64 . 00 53 . 04 1 . 05 46 . 81 45 . 78 58 . 84 56 . 74 69 . 23 25 . 23 34 . 31 25 . 38 2 Prefix LM, LM 79 . 68 41 . 26 92 . 09 90 . 11 86 . 27 86 . 82 86 . 32 88 . 35 91 . 35 81 . 71 82 . 02 89 . 04 68 . 59 39 . 66 17 . 84 37 . 13 76 . 87 85 . 39 64 . 86 71 . 47 93 . 37 91 . 07 57 . 00 58 . 67 16 . 89 59 . 25 58 . 16 64 . 26 66 . 30 71 . 15 26 . 28 37 . 51 26 . 76 4 Language modeling with prefix 80 . 69 44 . 22 93 . 00 91 . 68 88 . 48 87 . 20 87 . 18 88 . 39 91 . 41 82 . 66 83 . 09 89 . 29 68 . 95 40 . 71 18 . 94 38 . 15 77 . 99 86 . 43 65 . 27 73 . 55 83 . 95 87 . 50 55 . 00 59 . 65 18 . 89 61 . 76 60 . 76 68 . 59 65 . 67 73 . 08 26 . 86 39 . 73 27 . 49 4 BERT-style ( Devlin et al. , 2018 ) 82 . 96 52 . 49 92 . 55 92 . 79 89 . 95 87 . 68 87 . 66 88 . 47 91 . 44 83 . 60 84 . 05 90",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 89 61 . 76 60 . 76 68 . 59 65 . 67 73 . 08 26 . 86 39 . 73 27 . 49 4 BERT-style ( Devlin et al. , 2018 ) 82 . 96 52 . 49 92 . 55 92 . 79 89 . 95 87 . 68 87 . 66 88 . 47 91 . 44 83 . 60 84 . 05 90 . 33 75 . 45 41 . 27 19 . 17 38 . 72 80 . 65 88 . 24 69 . 85 76 . 48 94 . 37 94 . 64 61 . 00 63 . 29 25 . 08 66 . 76 65 . 85 72 . 20 69 . 12 75 . 00 26 . 78 40 . 03 27 . 41 4 Deshuffling 73 . 17 22 . 82 87 . 16 86 . 88 81 . 13 84 . 03 83 . 82 86 . 38 89 . 90 76 . 30 76 . 34 84 . 18 58 . 84 40 . 75 18 . 59 38 . 10 67 . 61 76 . 76 58 . 47 69 . 17 63 . 70 78 . 57 56 . 00 59 . 85 12 . 70 45 . 52 44 . 36 57 . 04 64 . 89 68 . 27 26 . 11 39 . 30 25 . 62 5 BERT-style ( Devlin et al. , 2018 ) 82 . 96 52 . 49 92 . 55 92 . 79 89 . 95 87 . 68 87 . 66 88 . 47 91 . 44 83 . 60 84 . 05 90 . 33 75 . 45 41 . 27 19 . 17 38 . 72 80 . 65 88 . 24 69 . 85 76 . 48 94 . 37 94 . 64 61 . 00 63 . 29 25 . 08 66 . 76 65 . 85 72 . 20 69 . 12 75 . 00 26 . 78 40 . 03 27 . 41 5 MASS-style ( Song et al. , 2019 ) 82 . 32 47 . 01 91 . 63 92 . 53 89 . 71 88 . 21 88 . 18 88 . 58 91 . 44 82 . 96 83 . 67 90 . 02 77 . 26 41 . 16 19 . 16 38 . 55 80 . 10 88 . 07 69 . 28 75 . 08 84 . 98 89 . 29 63 . 00 64 . 46 23 . 50 66 . 71 65 . 91 72 . 20 67 . 71 78 . 85 26 . 79 39 . 89 27 . 55 5 ⋆ Replace corrupted spans 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 5 Drop corrupted tokens 84 . 44 60 . 04 92 . 89 92 . 79 89 . 95 87 . 28 86 . 85 88 . 56 91 . 54 83 . 94 83 . 92 90 . 74 79 . 42 41 . 27 19 . 31 38 . 70 80 . 52 88 . 28 68 . 67 75 . 90 96 . 02 94 . 64 56 . 00 65 . 06 23 . 92 65 . 54 64 . 60 71 . 12 67 . 40 74 . 04 27 . 07 39 . 76 27 . 82 6 Corruption rate = 10% 82 . 82 52 . 71 92 . 09 91 . 55 88 . 24 88 . 19 88 . 15 88 . 47 91 . 40 83 . 50 84 . 51 90 . 33 75 . 45 41 . 05 19 . 00 38 . 53 80 . 38 88 . 36 69 . 55 74 . 98 92 . 37 92 . 86 62 . 00 66",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 82 52 . 71 92 . 09 91 . 55 88 . 24 88 . 19 88 . 15 88 . 47 91 . 40 83 . 50 84 . 51 90 . 33 75 . 45 41 . 05 19 . 00 38 . 53 80 . 38 88 . 36 69 . 55 74 . 98 92 . 37 92 . 86 62 . 00 66 . 04 24 . 66 67 . 93 67 . 09 70 . 76 67 . 24 75 . 96 26 . 87 39 . 28 27 . 44 6 ⋆ Corruption rate = 15% 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 6 Corruption rate = 25% 83 . 00 53 . 47 93 . 00 92 . 44 89 . 46 87 . 36 87 . 36 88 . 68 91 . 53 84 . 44 84 . 15 90 . 77 74 . 01 41 . 69 19 . 54 39 . 14 80 . 96 88 . 61 70 . 48 76 . 39 93 . 02 92 . 86 68 . 00 65 . 46 24 . 66 68 . 20 67 . 39 73 . 65 67 . 87 72 . 12 27 . 04 39 . 83 27 . 47 6 Corruption rate = 50% 81 . 27 46 . 26 91 . 63 91 . 11 87 . 99 87 . 87 87 . 64 88 . 70 91 . 57 83 . 64 84 . 10 90 . 24 70 . 76 41 . 51 19 . 32 38 . 89 79 . 80 87 . 76 70 . 33 75 . 02 93 . 05 92 . 86 68 . 00 62 . 97 24 . 13 64 . 94 64 . 13 72 . 20 68 . 50 77 . 88 27 . 01 39 . 90 27 . 49 7 ⋆ Baseline (i.i.d.) 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 7 Average span length = 2 83 . 54 53 . 82 92 . 20 93 . 05 90 . 44 87 . 85 87 . 71 88 . 42 91 . 40 84 . 28 84 . 46 90 . 88 77 . 62 41 . 23 19 . 39 38 . 69 82 . 09 89 . 69 72 . 20 77 . 06 90 . 43 91 . 07 70 . 00 66 . 28 26 . 13 71 . 34 70 . 61 75 . 45 68 . 34 78 . 85 26 . 76 39 . 99 27 . 63 7 Average span length = 3 83 . 49 53 . 90 92 . 43 92 . 25 89 . 46 87 . 49 87 . 53 88 . 72 91 . 51 84 . 85 84 . 84 90 . 99 77 . 26 41 . 50 19 . 62 38 . 94 81 . 84 89 . 66 72 . 53 76 . 85 94 . 37 94 . 64 70 . 00 67 . 64 28 . 75 70 . 84 69 . 90 74 . 73 67 . 71 77 . 88 26 . 86 39 . 65 27 . 62 7 Average span length = 5 83 . 40 52 . 12 93 . 12 92",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 94 81 . 84 89 . 66 72 . 53 76 . 85 94 . 37 94 . 64 70 . 00 67 . 64 28 . 75 70 . 84 69 . 90 74 . 73 67 . 71 77 . 88 26 . 86 39 . 65 27 . 62 7 Average span length = 5 83 . 40 52 . 12 93 . 12 92 . 63 89 . 71 88 . 70 88 . 47 88 . 84 91 . 64 84 . 32 84 . 29 90 . 79 76 . 90 41 . 39 19 . 24 38 . 82 82 . 05 89 . 79 72 . 23 77 . 06 83 . 06 89 . 29 69 . 00 68 . 16 30 . 12 71 . 36 70 . 53 75 . 81 69 . 91 79 . 81 26 . 88 39 . 40 27 . 53 7 Average span length = 10 82 . 85 50 . 11 92 . 09 91 . 95 88 . 97 88 . 45 88 . 22 88 . 86 91 . 63 84 . 34 84 . 28 91 . 07 76 . 17 41 . 38 19 . 33 38 . 80 81 . 84 89 . 39 70 . 44 76 . 45 87 . 40 89 . 29 65 . 00 66 . 87 29 . 59 69 . 82 68 . 94 72 . 56 67 . 55 75 . 96 26 . 79 39 . 49 27 . 69 8 ⋆ C4 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 8 C4, unfiltered 81 . 46 48 . 01 91 . 63 92 . 72 89 . 95 87 . 79 87 . 60 88 . 31 91 . 27 82 . 30 82 . 34 88 . 71 72 . 20 41 . 09 19 . 14 38 . 54 78 . 78 87 . 04 68 . 04 75 . 75 89 . 17 91 . 07 62 . 00 65 . 52 25 . 60 62 . 42 61 . 58 69 . 68 67 . 08 72 . 12 26 . 55 39 . 34 27 . 21 8 RealNews-like 83 . 83 56 . 55 92 . 66 92 . 06 88 . 97 87 . 71 87 . 37 88 . 51 91 . 49 84 . 35 84 . 46 90 . 61 78 . 34 41 . 38 19 . 23 38 . 84 80 . 39 88 . 50 72 . 38 77 . 00 93 . 09 94 . 64 66 . 00 65 . 92 23 . 82 74 . 56 73 . 72 75 . 81 66 . 61 80 . 77 26 . 75 39 . 90 27 . 48 8 WebText-like 84 . 03 56 . 38 93 . 12 92 . 31 89 . 22 88 . 69 88 . 68 88 . 65 91 . 56 84 . 70 84 . 84 90 . 83 77 . 62 41 . 23 19 . 31 38 . 70 81 . 42 89 . 15 71 . 40 76 . 88 83 . 08 89 . 29 66 . 00 64 . 10 24 . 24 72 . 24 71 . 36 75 . 45 68 . 03 82 . 69 26 . 80 39 . 74 27 . 59 8 Wikipedia 81 . 85 45 . 53 92 . 32 91 . 67 88 . 24 85 . 62 86 . 40 88 . 37 91 . 34 82 . 61 83 . 25 90 . 96 77 . 26 41 . 39 19 . 31 38 . 81 81 . 29 89 . 18 68 . 01 76 . 12 56 . 03 80 . 36 67 . 00 65 . 01 25 . 92 69",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 85 45 . 53 92 . 32 91 . 67 88 . 24 85 . 62 86 . 40 88 . 37 91 . 34 82 . 61 83 . 25 90 . 96 77 . 26 41 . 39 19 . 31 38 . 81 81 . 29 89 . 18 68 . 01 76 . 12 56 . 03 80 . 36 67 . 00 65 . 01 25 . 92 69 . 03 68 . 06 74 . 73 67 . 08 76 . 92 26 . 94 39 . 69 27 . 67 8 Wikipedia + TBC 83 . 65 55 . 53 92 . 78 92 . 41 89 . 22 86 . 67 86 . 27 89 . 47 92 . 29 84 . 38 83 . 45 91 . 94 76 . 90 41 . 22 19 . 28 38 . 67 82 . 08 89 . 70 73 . 24 76 . 22 95 . 40 92 . 86 69 . 00 51 . 59 50 . 93 69 . 53 68 . 51 77 . 62 66 . 93 81 . 73 26 . 77 39 . 63 27 . 57 9 ⋆ Full data set 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 9 2 29 (64 repeats) 82 . 87 53 . 82 92 . 78 91 . 79 88 . 73 87 . 56 87 . 58 88 . 73 91 . 54 84 . 07 84 . 21 90 . 59 73 . 65 41 . 18 19 . 19 38 . 67 80 . 97 88 . 90 72 . 03 76 . 76 92 . 96 92 . 86 66 . 00 65 . 11 26 . 76 69 . 35 68 . 49 75 . 81 67 . 24 82 . 69 26 . 83 39 . 74 27 . 63 9 2 27 (256 repeats) 82 . 62 50 . 60 92 . 32 92 . 07 88 . 73 87 . 83 87 . 60 88 . 65 91 . 54 83 . 43 84 . 37 90 . 12 75 . 81 41 . 24 19 . 20 38 . 70 79 . 78 87 . 63 69 . 97 75 . 29 93 . 42 91 . 07 63 . 00 61 . 82 23 . 61 66 . 27 65 . 39 73 . 65 66 . 30 80 . 77 27 . 02 39 . 71 27 . 33 9 2 25 (1 , 024 repeats) 79 . 55 43 . 84 91 . 28 89 . 32 85 . 05 85 . 92 85 . 74 88 . 05 91 . 09 81 . 29 81 . 72 87 . 90 69 . 31 40 . 66 18 . 57 38 . 13 76 . 27 84 . 58 64 . 76 72 . 63 83 . 97 82 . 14 64 . 00 59 . 39 17 . 94 56 . 94 56 . 04 64 . 98 65 . 20 73 . 08 26 . 38 39 . 56 26 . 80 9 2 23 (4 , 096 repeats) 76 . 34 32 . 68 89 . 45 89 . 84 86 . 03 83 . 49 83 . 42 87 . 18 90 . 61 77 . 80 78 . 69 85 . 47 64 . 62 40 . 16 18 . 33 37 . 66 70 . 92 80 . 20 59 . 29 69 . 85 73 . 48 73 . 21 56 . 00 57 . 66 14 . 38 46 . 69 45 . 79 59 . 57 65 . 05 68 . 27 26 . 37 38 . 84 25 . 81 10 ⋆ All parameters 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 20 59 . 29 69 . 85 73 . 48 73 . 21 56 . 00 57 . 66 14 . 38 46 . 69 45 . 79 59 . 57 65 . 05 68 . 27 26 . 37 38 . 84 25 . 81 10 ⋆ All parameters 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 10 Adapter layers, d = 32 80 . 52 45 . 33 91 . 63 90 . 59 86 . 76 88 . 38 88 . 06 86 . 99 90 . 26 83 . 63 83 . 94 90 . 72 67 . 15 34 . 50 15 . 08 32 . 15 79 . 32 87 . 70 60 . 40 65 . 32 50 . 87 73 . 21 52 . 00 58 . 61 19 . 41 65 . 50 64 . 58 62 . 09 64 . 58 73 . 08 13 . 84 17 . 88 15 . 54 10 Adapter layers, d = 128 81 . 51 45 . 35 92 . 89 91 . 49 88 . 24 87 . 73 87 . 65 87 . 73 90 . 93 83 . 64 84 . 09 90 . 52 72 . 56 36 . 71 16 . 62 34 . 37 79 . 47 87 . 61 63 . 03 69 . 20 52 . 21 75 . 00 56 . 00 61 . 08 18 . 05 67 . 94 66 . 97 68 . 59 66 . 77 73 . 08 19 . 83 27 . 50 22 . 63 10 Adapter layers, d = 512 81 . 54 44 . 25 93 . 35 91 . 00 87 . 25 88 . 74 88 . 44 88 . 02 91 . 15 83 . 08 83 . 80 89 . 62 74 . 37 38 . 63 17 . 78 36 . 25 79 . 18 87 . 32 64 . 30 73 . 18 59 . 86 71 . 43 56 . 00 62 . 94 18 . 57 66 . 56 65 . 74 70 . 76 67 . 87 74 . 04 23 . 45 33 . 98 25 . 81 10 Adapter layers, d = 2048 82 . 62 49 . 86 92 . 55 91 . 30 87 . 99 88 . 46 88 . 35 88 . 36 91 . 40 83 . 63 83 . 18 90 . 66 76 . 53 39 . 44 18 . 30 37 . 06 79 . 40 87 . 36 68 . 61 74 . 53 88 . 00 91 . 07 58 . 00 61 . 10 18 . 89 66 . 73 66 . 06 73 . 29 71 . 16 75 . 96 25 . 64 36 . 92 26 . 93 10 Gradual Unfreezing 82 . 50 51 . 74 91 . 97 92 . 61 89 . 71 87 . 27 86 . 90 88 . 26 91 . 35 83 . 42 83 . 49 89 . 71 75 . 09 40 . 88 18 . 95 38 . 40 79 . 17 87 . 30 70 . 79 75 . 51 93 . 09 94 . 64 70 . 00 62 . 03 21 . 51 65 . 69 64 . 79 72 . 92 69 . 12 77 . 89 26 . 71 39 . 02 26 . 93 11 ⋆ Baseline (pre-train/fine-tune) 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 02 26 . 93 11 ⋆ Baseline (pre-train/fine-tune) 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 11 Equal 76 . 13 39 . 47 90 . 94 82 . 90 75 . 74 78 . 83 78 . 44 86 . 45 89 . 71 82 . 08 82 . 92 90 . 13 59 . 93 40 . 95 19 . 02 38 . 39 76 . 51 85 . 61 63 . 37 73 . 06 82 . 37 83 . 93 65 . 00 60 . 89 17 . 52 60 . 51 59 . 70 61 . 01 60 . 03 65 . 38 23 . 89 34 . 31 26 . 78 11 Examples-proportional, K = 2 16 80 . 45 42 . 07 91 . 97 90 . 97 87 . 50 85 . 41 85 . 04 86 . 89 90 . 10 83 . 01 83 . 66 90 . 74 72 . 56 41 . 16 19 . 04 38 . 59 77 . 25 85 . 72 69 . 95 76 . 67 86 . 38 89 . 29 70 . 00 65 . 93 27 . 91 62 . 78 61 . 95 76 . 90 65 . 83 73 . 08 24 . 35 34 . 99 27 . 10 11 Examples-proportional, K = 2 17 81 . 56 47 . 35 91 . 40 91 . 55 88 . 24 86 . 15 85 . 93 86 . 94 90 . 06 82 . 76 84 . 12 90 . 79 75 . 09 41 . 06 19 . 12 38 . 47 77 . 00 85 . 87 67 . 91 77 . 89 77 . 54 85 . 71 57 . 00 67 . 78 27 . 07 61 . 51 60 . 54 79 . 06 65 . 20 74 . 04 24 . 36 35 . 00 27 . 25 11 Examples-proportional, K = 2 18 81 . 67 46 . 85 91 . 63 91 . 99 88 . 73 87 . 68 87 . 20 86 . 93 90 . 35 83 . 30 84 . 01 91 . 47 73 . 29 40 . 96 19 . 07 38 . 43 78 . 17 86 . 74 67 . 94 76 . 57 78 . 88 87 . 50 62 . 00 67 . 70 30 . 85 63 . 43 62 . 54 76 . 53 65 . 67 67 . 31 24 . 57 35 . 19 27 . 39 11 Examples-proportional, K = 2 19 81 . 42 45 . 94 91 . 63 92 . 20 89 . 22 88 . 44 88 . 32 86 . 84 90 . 10 83 . 73 84 . 29 91 . 84 70 . 40 41 . 26 19 . 24 38 . 71 79 . 78 88 . 15 67 . 30 75 . 66 75 . 59 87 . 50 59 . 00 68 . 22 30 . 64 65 . 32 64 . 29 73 . 65 65 . 05 69 . 23 25 . 21 36 . 30 27 . 76 11 Examples-proportional, K = 2 20 80 . 80 42 . 55 92 . 78 91 . 27 87 . 99 88 . 36 88 . 10 86 . 10 89 . 62 84 . 15 84 . 26 92 . 20 68 . 95 41 . 05 19 . 24 38 . 46 80 . 36 88 . 27 67 . 38 73 . 21 76 . 18 83 . 93 62 . 00 67 . 57 26 . 86 66 . 12 65 . 22 76 . 90 64",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 27 87 . 99 88 . 36 88 . 10 86 . 10 89 . 62 84 . 15 84 . 26 92 . 20 68 . 95 41 . 05 19 . 24 38 . 46 80 . 36 88 . 27 67 . 38 73 . 21 76 . 18 83 . 93 62 . 00 67 . 57 26 . 86 66 . 12 65 . 22 76 . 90 64 . 73 69 . 23 25 . 66 36 . 93 27 . 68 11 Examples-proportional, K = 2 21 79 . 83 44 . 45 91 . 28 89 . 00 84 . 31 87 . 54 87 . 40 84 . 93 88 . 53 82 . 54 84 . 16 90 . 85 67 . 87 40 . 51 18 . 79 37 . 92 79 . 50 87 . 48 65 . 10 71 . 16 68 . 88 85 . 71 57 . 00 62 . 75 23 . 40 64 . 50 63 . 65 72 . 92 64 . 11 71 . 15 25 . 82 37 . 22 27 . 13 11 Temperature-scaled, T = 2 81 . 90 54 . 00 91 . 74 90 . 56 86 . 76 85 . 11 84 . 60 86 . 40 89 . 74 83 . 47 84 . 15 91 . 51 72 . 56 41 . 09 19 . 28 38 . 54 79 . 42 87 . 77 69 . 92 76 . 73 92 . 37 92 . 86 57 . 00 69 . 80 31 . 90 66 . 65 65 . 74 72 . 92 67 . 08 75 . 96 25 . 42 36 . 72 27 . 20 11 Temperature-scaled, T = 4 80 . 56 45 . 38 91 . 97 89 . 68 85 . 78 83 . 13 82 . 76 86 . 39 90 . 00 82 . 78 84 . 19 91 . 16 73 . 65 41 . 09 19 . 22 38 . 51 77 . 99 86 . 81 69 . 54 76 . 76 97 . 36 96 . 43 59 . 00 68 . 10 31 . 48 64 . 26 63 . 27 74 . 73 64 . 26 71 . 15 25 . 04 35 . 82 27 . 45 11 Temperature-scaled, T = 8 77 . 21 40 . 07 91 . 06 88 . 11 83 . 33 79 . 20 79 . 06 86 . 60 89 . 90 83 . 05 83 . 56 90 . 21 59 . 93 41 . 01 19 . 10 38 . 40 77 . 14 85 . 99 66 . 07 73 . 94 93 . 70 94 . 64 60 . 00 66 . 36 26 . 86 63 . 46 62 . 60 62 . 09 63 . 32 65 . 38 24 . 55 35 . 35 27 . 17 12 ⋆ Unsupervised pre-training + fine-tuning 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 12 Multi-task training 81 . 42 45 . 94 91 . 63 92 . 20 89 . 22 88 . 44 88 . 32 86 . 84 90 . 10 83 . 73 84 . 29 91 . 84 70 . 40 41 . 26 19 . 24 38 . 71 79 . 78 88 . 15 67 . 30 75 . 66 75 . 59 87 . 50 59 . 00 68 . 22 30 . 64 65 . 32 64 . 29 73 . 65 65 . 05 69 . 23 25 . 21 36 . 30 27",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 84 90 . 10 83 . 73 84 . 29 91 . 84 70 . 40 41 . 26 19 . 24 38 . 71 79 . 78 88 . 15 67 . 30 75 . 66 75 . 59 87 . 50 59 . 00 68 . 22 30 . 64 65 . 32 64 . 29 73 . 65 65 . 05 69 . 23 25 . 21 36 . 30 27 . 76 12 Multi-task pre-training + fine-tuning 83 . 11 51 . 42 92 . 66 91 . 73 88 . 73 88 . 06 87 . 70 88 . 61 91 . 61 84 . 09 84 . 31 91 . 85 76 . 53 41 . 15 19 . 12 38 . 59 80 . 26 88 . 50 71 . 03 79 . 54 81 . 69 87 . 50 65 . 00 70 . 72 31 . 48 65 . 94 65 . 03 81 . 23 68 . 18 73 . 08 27 . 08 39 . 80 28 . 07 12 Leave-one-out multi-task training 81 . 98 48 . 00 93 . 23 91 . 72 88 . 24 87 . 76 87 . 32 88 . 61 91 . 44 84 . 00 84 . 11 90 . 79 72 . 20 41 . 34 19 . 05 38 . 77 79 . 97 88 . 10 71 . 68 78 . 35 86 . 76 89 . 29 66 . 00 68 . 09 29 . 49 66 . 23 65 . 27 79 . 06 68 . 65 78 . 85 26 . 93 39 . 79 27 . 87 12 Supervised multi-task pre-training 79 . 93 36 . 60 92 . 43 91 . 58 88 . 24 87 . 03 86 . 78 88 . 15 91 . 20 82 . 87 83 . 16 90 . 13 70 . 76 41 . 12 18 . 96 38 . 49 77 . 38 85 . 65 65 . 36 75 . 66 68 . 87 83 . 93 58 . 00 64 . 81 21 . 93 55 . 37 54 . 61 71 . 12 67 . 40 75 . 96 26 . 81 40 . 13 28 . 04 13 ⋆ Baseline 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 13 1 × size, 4 × training steps 85 . 33 60 . 29 93 . 81 94 . 06 91 . 67 89 . 42 89 . 25 89 . 15 91 . 87 86 . 01 85 . 70 91 . 63 78 . 34 41 . 52 19 . 33 38 . 96 82 . 45 90 . 19 74 . 72 79 . 17 94 . 75 92 . 86 71 . 00 67 . 34 29 . 70 72 . 63 71 . 59 78 . 34 72 . 10 82 . 69 27 . 08 40 . 66 27 . 93 13 1 × size, 4 × batch size 84 . 60 56 . 08 93 . 12 92 . 31 89 . 22 88 . 85 88 . 84 89 . 35 92 . 07 85 . 98 86 . 13 91 . 07 80 . 14 41 . 70 19 . 42 39 . 08 82 . 52 90 . 21 74 . 64 78 . 78 93 . 69 94 . 64 72 . 00 68 . 09 30 . 95 74 . 73 73 . 90 76 . 53 70 . 06 81 . 73 27 . 07 40 . 60 27 . 84 13 2 × size, 2 × training steps 86 . 18 62 . 04 93 . 69 93 . 36 90",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 52 90 . 21 74 . 64 78 . 78 93 . 69 94 . 64 72 . 00 68 . 09 30 . 95 74 . 73 73 . 90 76 . 53 70 . 06 81 . 73 27 . 07 40 . 60 27 . 84 13 2 × size, 2 × training steps 86 . 18 62 . 04 93 . 69 93 . 36 90 . 69 89 . 18 89 . 23 89 . 35 92 . 05 87 . 23 87 . 05 92 . 68 81 . 95 41 . 74 19 . 66 39 . 14 84 . 18 91 . 29 77 . 18 80 . 98 97 . 36 96 . 43 74 . 00 71 . 34 35 . 68 77 . 11 76 . 34 80 . 51 69 . 28 85 . 58 27 . 52 41 . 03 28 . 19 13 4 × size, 1 × training steps 85 . 91 57 . 58 94 . 38 92 . 67 89 . 95 89 . 60 89 . 60 89 . 44 92 . 14 87 . 05 87 . 12 93 . 12 83 . 39 41 . 60 19 . 73 39 . 08 83 . 86 91 . 32 78 . 04 81 . 38 89 . 09 94 . 64 73 . 00 73 . 74 40 . 40 78 . 25 77 . 40 81 . 59 70 . 22 91 . 35 27 . 47 40 . 71 28 . 10 13 4 × ensembled 84 . 77 56 . 14 93 . 46 93 . 31 90 . 67 89 . 71 89 . 60 89 . 62 92 . 24 86 . 22 86 . 53 91 . 60 77 . 98 42 . 10 20 . 10 39 . 56 83 . 09 90 . 40 71 . 74 77 . 58 89 . 85 91 . 07 66 . 00 69 . 32 29 . 49 72 . 67 71 . 94 76 . 90 69 . 12 72 . 12 28 . 05 40 . 53 28 . 09 13 4 × ensembled, fine-tune only 84 . 05 54 . 78 92 . 78 93 . 15 90 . 44 88 . 34 88 . 12 89 . 27 91 . 97 85 . 33 85 . 88 90 . 98 77 . 62 41 . 66 19 . 57 39 . 12 82 . 36 89 . 86 71 . 56 77 . 43 90 . 07 92 . 86 69 . 00 67 . 31 26 . 34 70 . 47 69 . 64 75 . 45 68 . 18 74 . 04 27 . 55 40 . 22 28 . 09 Table 16: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with ⋆ denotes our baseline model (described in Section 3.1 ). Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu",
    "chunk_id": "Natural_language_processing_exploring_the_limits_of_transfer_learning.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": "Model Cards for Model Reporting Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru {mmitchellai,simonewu,andrewzaldivar,parkerbarnes,lucyvasserman,benhutch,espitzer,tgebru}@google.com deborah.raji@mail.utoronto.ca ABSTRACT Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, edcation, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their perfomance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phnotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation prcedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supevised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation. CCS CONCEPTS • General and reference → Evaluation ; • Social and profesional topics → User characteristics ; • Software and its engneering → Use cases ; Documentation ; Software evolution ; • Humacentered computing → Walkthrough evaluations ; Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. FAT* ’19, January 29–31, 2019, Atlanta, GA, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6125-5/19/01. https://doi.org/10.1145/3287560.3287596 KEYWORDS datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations ACM Reference Format: Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. 2019. Model Cards for Model Reporting. In FAT* ’19: Conference on Fairness, Accountability, and Transparency, January 29–31, 2019, Atlanta, GA, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3287560. 3287596 1 INTRODUCTION Currently, there are no standardized documentation procedures to communicate the performance characteristics of trained machine learning (ML) and artificial intelligence (AI) models",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". This lack of documentation is especially problematic when models are used in applications that have serious impacts on people’s lives, such as in health care [14, 42, 44], employment [1, 13, 29], education [23, 45] and law enforcement [2, 7, 20, 34]. Researchers have discovered systematic biases in commercial mchine learning models used for face detection and tracking [ 4 , 9 , 49 ], attribute detection [ 5 ], criminal justice [ 10 ], toxic comment detetion [ 11 ], and other applications. However, these systematic errors were only exposed after models were put into use, and negatively affected users reported their experiences. For example, after MIT Media Lab graduate student Joy Buolamwini found that commercial face recognition systems failed to detect her face [ 4 ], she collabrated with other researchers to demonstrate the disproportionate errors of computer vision systems on historically marginalized groups in the United States, such as darker-skinned women [ 5 , 41 ]. In spite of the potential negative effects of such reported biases, documentation accompanying trained machine learning models (if supplied) provide very little information regarding model perfomance characteristics, intended use cases, potential pitfalls, or other information to help users evaluate the suitability of these systems to their context. This highlights the need to have detailed documetation accompanying trained machine learning models, including metrics that capture bias, fairness and inclusion considerations. As a step towards this goal, we propose that released machine learning models be accompanied by short (one to two page) records we call model cards. Model cards (for model reporting) are coplements to “Datasheets for Datasets” [ 21 ] and similar recently proposed documentation paradigms [ 3 , 28 ] that report details of the datasets used to train and test machine learning models. Model cards are also similar to the tripod statement proposal in medicine [ 25 ]",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Model cards are also similar to the tripod statement proposal in medicine [ 25 ]. We provide two example model cards in Section 5: A smiling detection model trained on the CelebA dataset [ 36 ] (Figure 2), and a public toxicity detection model [ 32 ] (Figure 3). Where Datasheets highlight characteristics of the data feeding into the model, we arXiv:1810.03993v2 [cs.LG] 14 Jan 2019 FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru focus on trained model characteristics such as the type of model, intended use cases, information about attributes for which model performance may vary, and measures of model performance. We advocate for measures of model performance that contain quantitative evaluation results to be broken down by individual cultural, demographic, or phenotypic groups, domain-relevant coditions, and intersectional analysis combining two (or more) groups and conditions. In addition to model evaluation results, model cards should detail the motivation behind chosen performance metrics, group definitions, and other relevant factors. Each model card could be accompanied with Datasheets [ 21 ], Nutrition Labels [ 28 ], Data Statements [ 3 ], or Factsheets [ 27 ], describing datasets that the model was trained and evaluated on. Model cards provide a way to inform users about what machine learning systems can and cannot do, the types of errors they make, and additional steps that could create more fair and inclusive outcomes with the technology. 2 BACKGROUND Many mature industries have developed standardized methods of benchmarking various systems under different conditions. For eample, as noted in [ 21 ], the electronic hardware industry provides datasheets with detailed characterizations of components’ perfomances under different test conditions",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". For eample, as noted in [ 21 ], the electronic hardware industry provides datasheets with detailed characterizations of components’ perfomances under different test conditions. By contrast, despite the broad reach and impact of machine learning models, there are no standard stress tests that are performed on machine learning based systems, nor standardized formats to report the results of these tests. Recently, researchers have proposed standardized forms of communicating characteristics of datasets used in machine learing [ 3 , 21 , 28 ] to help users understand the context in which the datasets should be used. We focus on the complementary task for machine learning models, proposing a standardized method to evauate the performance of human-centric models: Disaggregated by unitary and intersectional groups such as cultural, demographic, or phenotypic population groups. A framework that we refer to as “Model Cards” can present such evaluation supplemented with additional considerations such as intended use. Outside of machine learning, the need for population-based rporting of outcomes as suggested here has become increasingly evdent. For example, in vehicular crash tests, dummies with prototyical female characteristics were only introduced after researchers discovered that women were more likely than men to suffer serous head injuries in real-world side impacts [ 18 ]. Similarly, drugs developed based on results of clinical trials with exclusively male participants have led to overdosing in women [ 17 , 50 ]. In 1998, the U.S. Food and Drug Administration mandated that clinical trial rsults be disaggregated by groups such as age, race and gender [ 16 ]. While population-based analyses of errors and successes can be provided for unitary groups such as “men”, “women”, and “nobinary” gender groups, they should also be provided intersectioally, looking at two or more characteristics such as gender and age simultaneously",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Intersectional analyses are linked to intersectioality theory, which describes how discrete experiences associated with characteristics like race or gender in isolation do not accurately reflect their interaction [ 8 ]. Kimberlé Crenshaw, who pioneered intersectional research in critical race theory, discusses the story of Emma DeGraffenreid, who was part of a failed lawsuit against General Motors in 1976, claiming that the company’s hiring pratices discriminated against Black women. In their court opinion, the judges noted that since General Motors hired many women for secretarial positions, and many Black people for factory roles, they could not have discriminated against Black women. However, what the courts failed to see was that only White women were hired into secretarial positions and only Black men were hired into factory roles. Thus, Black women like Emma DeGraffenreid had no chance of being employed at General Motors. This example highlights the importance of intersectional analyses: empirical analyses that ephasize the interaction between various demographic categories including race, gender, and age. Before further discussing the details of the model card, it is impotant to note that at least two of the three characteristics discussed so far, race and gender, are socially sensitive. Although analyzing models by race and gender may follow from intersectionality thory, how “ground truth” race or gender categories should be labeled in a dataset, and whether or not datasets should be labeled with these categories at all, is not always clear. This issue is further confounded by the complex relationship between gender and sex. When using cultural identity categories such as race and gender to subdivide analyses, and depending on the context, we recommend either using datasets with self-identified labels or with labels clearly designated as perceived (rather than self-identified). When this is not possible, datasets of public figures with known public identity labels may be useful",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". When this is not possible, datasets of public figures with known public identity labels may be useful. Further research is necessary to expand how groups may be defined, for example, by automatically discovering groups with similarities in the evaluation datasets. 3 MOTIVATION As the use of machine learning technology has rapidly increased, so too have reports of errors and failures. Despite the potentially serious repercussions of these errors, those looking to use trained machine learning models in a particular context have no way of understanding the systematic impacts of these models before dploying them. The proposal of “Model Cards” specifically aims to standardize ethical practice and reporting - allowing stakeholders to compare candidate models for deployment across not only traditional evalation metrics but also along the axes of ethical, inclusive, and fair considerations. This goes further than current solutions to aid stakholders in different contexts. For example, to aid policy makers and regulators on questions to ask of a model, and known benchmarks around the suitability of a model in a given setting. Model reporting will hold different meaning to those involved in different aspects of model development, deployment, and use. Below, we outline a few use cases for different stakeholders: • ML and AI practitioners can better understand how well the model might work for the intended use cases and track its performance over time. • Model developers can compare the model’s results to other models in the same space, and make decisions about training their own system. • Software developers working on products that use the model’s predictions can inform their design and implemetation decisions. Model Cards for Model Reporting FAT* ’19, January 29–31, 2019, Atlanta, GA, USA • Policymakers can understand how a machine learning sytem may fail or succeed in ways that impact people. • Organizations can inform decisions about adopting tecnology that incorporates machine learning",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". • Organizations can inform decisions about adopting tecnology that incorporates machine learning. • ML-knowledgeable individuals can be informed on diffeent options for fine-tuning, model combination, or additional rules and constraints to help curate models for intended use cases without requiring technical expertise. • Impacted individuals who may experience effects from a model can better understand how it works or use information in the card to pursue remedies. Not only does this practice improve model understanding and help to standardize decision making processes for invested stakholders, but it also encourages forward-looking model analysis techniques. For example, slicing the evaluation across groups funtions to highlight errors that may fall disproportionately on some groups of people, and accords with many recent notions of matematical fairness (discussed further in the example model card in Figure 2). Including group analysis as part of the reporting prcedure prepares stakeholders to begin to gauge the fairness and inclusion of future outcomes of the machine learning system. Thus, in addition to supporting decision-making processes for determiing the suitability of a given machine learning model in a particular context, model reporting is an approach for responsible transparent and accountable practices in machine learning. People and organizations releasing models may be additionally incentivized to provide model card details because it helps potential users of the models to be better informed on which models are best for their specific purposes. If model card reporting becomes standard, potential users can compare and contrast different models in a well-informed way. Results on several different evaluation datasets will additionally aid potential users, although evaluation datasets suitable for disaggregated evaluation are not yet common",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Results on several different evaluation datasets will additionally aid potential users, although evaluation datasets suitable for disaggregated evaluation are not yet common. Future research could include creating robust evaluation datasets and protocols for the types of disaggregated evaluation we advocate for in this work, for example, by including differential privacy mechanisms [ 12 ] so that individuals in the testing set cannot be uniquely identified by their characteristics. 4 MODEL CARD SECTIONS Model cards serve to disclose information about a trained machine learning model. This includes how it was built, what assumptions were made during its development, what type of model behavior different cultural, demographic, or phenotypic population groups may experience, and an evaluation of how well the model performs with respect to those groups. Here, we propose a set of sections that a model card should have, and details that can inform the stakeholders discussed in Section 3. A summary of all suggested sections is provided in Figure 1. The proposed set of sections below are intended to provide reevant details to consider, but are not intended to be complete or exhaustive, and may be tailored depending on the model, context, and stakeholders. Additional details may include, for example, iterpretability approaches, such as saliency maps, TCAV [ 33 ], and Path-Integrated Gradients [ 38 , 43 ]); stakeholder-relevant explantions (e.g., informed by a careful consideration of philosophical, Model Card • Model Details . Basic information about the model. – Person or organization developing model – Model date – Model version – Model type – Information about training algorithms, parameters, fainess constraints or other applied approaches, and features – Paper or other resource for more information – Citation details – License – Where to send questions or comments about the model • Intended Use . Use cases that were envisioned during dvelopment",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Use cases that were envisioned during dvelopment. – Primary intended uses – Primary intended users – Out-of-scope use cases • Factors . Factors could include demographic or phenotypic groups, environmental conditions, technical attributes, or others listed in Section 4.3. – Relevant factors – Evaluation factors • Metrics . Metrics should be chosen to reflect potential reaworld impacts of the model. – Model performance measures – Decision thresholds – Variation approaches • Evaluation Data . Details on the dataset(s) used for the quantitative analyses in the card. – Datasets – Motivation – Preprocessing • Training Data . May not be possible to provide in practice. When possible, this section should mirror Evaluation Data. If such detail is not possible, minimal allowable information should be provided here, such as details of the distribution over various factors in the training datasets. • Quantitative Analyses – Unitary results – Intersectional results • Ethical Considerations • Caveats and Recommendations Figure 1: Summary of model card sections and suggested prompts for each. psychological, and other factors concerning what is as a good eplanation in different contexts [ 22 ]); and privacy approaches used in model training and serving. 4.1 Model Details This section of the model card should serve to answer basic quetions regarding the model version, type and other details. Person or organization developing model : What person or oganization developed the model? This can be used by all stakeholers to infer details pertaining to model development and potential FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru conflicts of interest. Model date : When was the model developed? This is useful for all stakeholders to become further informed on what techniques and data sources were likely to be available during model development",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Model date : When was the model developed? This is useful for all stakeholders to become further informed on what techniques and data sources were likely to be available during model development. Model version : Which version of the model is it, and how does it differ from previous versions? This is useful for all stakeholders to track whether the model is the latest version, associate known bugs to the correct model versions, and aid in model comparisons. Model type : What type of model is it? This includes basic model architecture details, such as whether it is a Naive Bayes classifier, a Convolutional Neural Network, etc. This is likely to be particularly relevant for software and model developers, as well as individuals knowledgeable about machine learning, to highlight what kinds of assumptions are encoded in the system. Paper or other resource for more information : Where can rsources for more information be found? Citation details : How should the model be cited? License : License information can be provided. Feedback on the model : E.g., what is an email address that people may write to for further information? There are cases where some of this information may be sensitive. For example, the amount of detail corporations choose to disclose might be different from academic research groups. This section should not be seen as a requirement to compromise private infomation or reveal proprietary training techniques; rather, a place to disclose basic decisions and facts about the model that the orgnization can share with the broader community in order to better inform on what the model represents. 4.2 Intended Use This section should allow readers to quickly grasp what the model should and should not be used for, and why it was created. It can also help frame the statistical analysis presented in the rest of the card, including a short description of the user(s), use-case(s), and context(s) for which the model was originally developed",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Possible information includes: Primary intended uses : This section details whether the model was developed with general or specific tasks in mind (e.g., plant recognition worldwide or in the Pacific Northwest). The use cases may be as broadly or narrowly defined as the developers intend. For example, if the model was built simply to label images, then this task should be indicated as the primary intended use case. Primary intended users : For example, was the model developed for entertainment purposes, for hobbyists, or enterprise solutions? This helps users gain insight into how robust the model may be to different kinds of inputs. Out-of-scope uses : Here, the model card should highlight tecnology that the model might easily be confused with, or related contexts that users could try to apply the model to. This section may provide an opportunity to recommend a related or similar model that was designed to better meet that particular need, where possible. This section is inspired by warning labels on food and toys, and similar disclaimers presented in electronic datasheets. Examples include “not for use on text examples shorter than 100 tokens” or “for use on black-and-white images only; please consider our research group’s full-color-image classifier for color images.” 4.3 Factors Model cards ideally provide a summary of model performance across a variety of relevant factors including groups , instrumentation , and environments . We briefly describe each of these factors and their relevance followed by the corresponding prompts in the model card. 4.3.1 Groups. “Groups” refers to distinct categories with similar characteristics that are present in the evaluation data instances. For human-centric machine learning models, “groups” are people who share one or multiple characteristics",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". For human-centric machine learning models, “groups” are people who share one or multiple characteristics. Intersectional model analysis for human-centric models is inspired by the sociological concept of intersectionality, which explores how an individual’s identity and experiences are shaped not just by unitary personal characteristics – such as race, gender, sexual orientation or health – but instead by a complex combination of many factors. These characteristics, which include but are not limited to cultural, demographic and phenotypic categories, are important to consider when evaluating machine learning models. Determining which groups to include in an intersectional analysis requires examining the intended use of the model and the context under which it may be deployed. Depending on the situation, certain groups may be more vulnerable than others to unjust or prejudicial treatment. For human-centric computer vision models, the visual presenttion of age, gender, and Fitzpatrick skin type [ 15 ] may be relevant. However, this must be balanced with the goal of preserving the privacy of individuals. As such, collaboration with policy, privacy, and legal experts is necessary in order to ascertain which groups may be responsibly inferred, and how that information should be stored and accessed (for example, using differential privacy [12]). Details pertaining to groups, including who annotated the traiing and evaluation datasets, instructions and compensation given to annotators, and inter-annotator agreement, should be provided as part of the data documentation made available with the dataset. See [3, 21, 28] for more details. 4.3.2 Instrumentation. In addition to groups, the performance of a model can vary depending on what instruments were used to capture the input to the model. For example, a face detection model may perform differently depending on the camera’s hardware and software, including lens, image stabilization, high dynamic range techniques, and background blurring for portrait mode",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Perfomance may also vary across real or simulated traditional camera settings such as aperture, shutter speed and ISO. Similarly, video and audio input will be dependent on the choice of recording istruments and their settings. 4.3.3 Environment. A further factor affecting model performance is the environment in which it is deployed. For example, face detetion systems are often less accurate under low lighting conditions or when the air is humid [ 51 ]. Specifications across different lighting and moisture conditions would help users understand the impacts of these environmental factors on model performance. 4.3.4 Card Prompts. We propose that the Factors section of model cards expands on two prompts: Model Cards for Model Reporting FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Relevant factors : What are foreseeable salient factors for which model performance may vary, and how were these determined? Evaluation factors : Which factors are being reported, and why were these chosen? If the relevant factors and evaluation factors are different, why? For example, while Fitzpatrick skin type is a relevant factor for face detection, an evaluation dataset annotated by skin type might not be available until reporting model performance across groups becomes standard practice. 4.4 Metrics The appropriate metrics to feature in a model card depend on the type of model that is being tested. For example, classification sytems in which the primary output is a class label differ significantly from systems whose primary output is a score. In all cases, the reported metrics should be determined based on the model’s struture and intended use",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". In all cases, the reported metrics should be determined based on the model’s struture and intended use. Details for this section include: Model performance measures : What measures of model perfomance are being reported, and why were they selected over other measures of model performance? Decision thresholds : If decision thresholds are used, what are they, and why were those decision thresholds chosen? When the model card is presented in a digital format, a threshold slider should ideally be available to view performance parameters across various decision thresholds. Approaches to uncertainty and variability : How are the mesurements and estimations of these metrics calculated? For eample, this may include standard deviation, variance, confidence intervals, or KL divergence. Details of how these values are aproximated should also be included (e.g., average of 5 runs, 10-fold cross-validation). 4.4.1 Classification systems. For classification systems, the error types that can be derived from a confusion matrix are false positive rate , false negative rate , false discovery rate , and false omission rate . We note that the relative importance of each of these metrics is system, product and context dependent. For example, in a surveillance scenario, surveillors may value a low false negative rate (or the rate at which the surveillance system fails to detect a person or an object when it should have). On the other hand, those being surveilled may value a low false positive rate (or the rate at which the surveillance system detects a person or an object when it should not have). We recommend listing all values and providing context about which were prioritized during development and why. Equality between some of the different confusion matrix metrics is equivalent to some definitions of fairness. For example, equal false negative rates across groups is equivalent to fulfilling Equality of Opportunity, and equal false negative and false positive rates across groups is equivalent to fulfilling Equality of Odds [26]",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". 4.4.2 Score-based analyses. For score-based systems such as priing models and risk assessment algorithms, describing differences in the distribution of measured metrics across groups may be helful. For example, reporting measures of central tendency such as the mode, median and mean, as well as measures of dispersion or variation such as the range, quartiles, absolute deviation, variance and standard deviation could facilitate the statistical commentary necessary to make more informed decisions about model deveopment. A model card could even extend beyond these summary statistics to reveal other measures of differences between distribtions such as cross entropy, perplexity, KL divergence and pinned area under the curve (pinned AUC) [11]. There are a number of applications that do not appear to be score-based at first glance, but can be considered as such for the purposes of intersectional analysis. For instance, a model card for a translation system could compare BLEU scores [ 40 ] across demgraphic groups, and a model card for a speech recognition system could compare word-error rates. Although the primary outputs of these systems are not scores, looking at the score differences btween populations may yield meaningful insights since comparing raw inputs quickly grows too complex. 4.4.3 Confidence. Performance metrics that are disaggregated by various combinations of instrumentation, environments and groups makes it especially important to understand the confidence intevals for the reported metrics. Confidence intervals for metrics drived from confusion matrices can be calculated by treating the matrices as probabilistic models of system performance [24]. 4.5 Evaluation Data All referenced datasets would ideally point to any set of documents that provide visibility into the source and composition of the dataset. Evaluation datasets should include datasets that are publicly avaiable for third-party use",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Evaluation datasets should include datasets that are publicly avaiable for third-party use. These could be existing datasets or new ones provided alongside the model card analyses to enable further benchmarking. Potential details include: Datasets : What datasets were used to evaluate the model? Motivation : Why were these datasets chosen? Preprocessing : How was the data preprocessed for evaluation (e.g., tokenization of sentences, cropping of images, any filtering such as dropping images without faces)? To ensure that model cards are statistically accurate and verfiable, the evaluation datasets should not only be representative of the model’s typical use cases but also anticipated test scenaios and challenging cases. For instance, if a model is intended for use in a workplace that is phenotypically and demographically homogeneous, and trained on a dataset that is representative of the expected use case, it may be valuable to evaluate that model on two evaluation sets: one that matches the workplace’s popultion, and another set that contains individuals that might be more challenging for the model (such as children, the elderly, and people from outside the typical workplace population). This methodology can highlight pathological issues that may not be evident in more routine testing. It is often difficult to find datasets that represent populations outside of the initial domain used in training. In some of these situtions, synthetically generated datasets may provide representation for use cases that would otherwise go unevaluated [ 35 ]. Section 5.2 provides an example of including synthetic data in the model evaluation dataset. FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru 4.6 Training Data Ideally, the model card would contain as much information about the training data as the evaluation data",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". However, there might be cases where it is not feasible to provide this level of detailed information about the training data. For example, the data may be proprietary, or require a non-disclosure agreement. In these cases, we advocate for basic details about the distributions over groups in the data, as well as any other details that could inform stakeholders on the kinds of biases the model may have encoded. 4.7 Quantitative Analyses Quantitative analyses should be disaggregated , that is, broken down by the chosen factors. Quantitative analyses should provide the results of evaluating the model according to the chosen metrics, providing confidence interval values when possible. Parity on the different metrics across disaggregated population subgroups coresponds to how fairness is often defined [ 37 , 48 ]. Quantitative analyses should demonstrate the metric variation (e.g., with error bars), as discussed in Section 4.4 and visualized in Figure 2. The disaggregated evaluation includes: Unitary results : How did the model perform with respect to each factor? Intersectional results : How did the model perform with respect to the intersection of evaluated factors? 4.8 Ethical Considerations This section is intended to demonstrate the ethical considerations that went into model development, surfacing ethical challenges and solutions to stakeholders. Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Ethical analysis does not always lead to precise solutions, but the process of ethical contemplation is worthwhile to inform on responsible practices and next steps in future work. While there are many frameworks for ethical decision-making in technology that can be adapted here [ 19 , 30 , 46 ], the following are specific questions you may want to explore in this section: Data : Does the model use any sensitive data (e.g., protected classes)? Human life : Is the model intended to inform decisions about maters central to human life or flourishing – e.g., health or safety? Or could it be used in such a way? Mitigations : What risk mitigation strategies were used during model development? Risks and harms : What risks may be present in model usage? Try to identify the potential recipients, likelihood, and magnitude of harms. If these cannot be determined, note that they were consiered but remain unknown. Use cases : Are there any known model use cases that are especially fraught? This may connect directly to the intended use section of the model card. If possible, this section should also include any additional ethical considerations that went into model development, for example, review by an external board, or testing with a specific community. 4.9 Caveats and Recommendations This section should list additional concerns that were not covered in the previous sections. For example, did the results suggest any further testing? Were there any relevant groups that were not represented in the evaluation dataset? Are there additional recomendations for model use? What are the ideal characteristics of an evaluation dataset for this model? 5 EXAMPLES We present worked examples of model cards for two models: an image-based classification system and a text-based scoring system. 5.1 Smiling Classifier To show an example of a model card for an image classification problem, we use the public CelebA dataset [ 36 ] to examine the performance of a trained “smiling” classifier across both age and gender categories",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Figure 2 shows our prototype. These results demonstrate a few potential issues. For example, the false discovery rate on older men is much higher than that for other groups. This means that many predictions incorrectly classify older men as smiling when they are not. On the other hand, men (in aggregate) have a higher false negative rate, meaning that many of the men that are in fact smiling in the photos are incorrectly classified as not smiling. The results of these analyses give insight into contexts the model might not be best suited for. For example, it may not be advisable to apply the model on a diverse group of audiences, and it may be the most useful when detecting the presence of a smile is more important than detecting its absence (for example, in an application that automatically finds ‘fun moments’ in images). Additional fintuning, for example, with images of older men, may help create a more balanced performance across groups. 5.2 Toxicity Scoring Our second example provides a model card for Perspective API’s TOXICITY classifier built to detect ‘toxicity’ in text [ 32 ], and is prsented in Figure 3. To evaluate the model, we use an intersectional version of the open source, synthetically created Identity Phrase Templates test set published in [ 11 ]. We show two versions of the quantitative analysis: one for TOXICITY v. 1, the initial version of the this model, and one for TOXICITY v. 5, the latest version. This model card highlights the drastic ways that models can change over time, and the importance of having a model card that is updated with each new model release. TOXICITY v. 1 has low performance for several terms, especially “lesbian”, “gay”, and “hmosexual”. This is consistent with what some users of the initial TOXICITY model found, as reported by the team behind Perspective API in [ 47 ]. Also in [ 47 ], the Perspective API team shares the bias mitigation techniques they applied to the TOXICITY v. 1 model, in order to create the more equitable performance in TOXICITY v. 5",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Also in [ 47 ], the Perspective API team shares the bias mitigation techniques they applied to the TOXICITY v. 1 model, in order to create the more equitable performance in TOXICITY v. 5. By making model cards a standard part of API launches, teams like the Perspective API team may be able to find and mitigate some of these biases earlier. 6 DISCUSSION & FUTURE WORK We have proposed frameworks called model cards for reporting iformation about what a trained machine learning model is and how well it works. Model cards include information about the context of the model, as well as model performance results disaggregated by different unitary and intersectional population groups. Model Model Cards for Model Reporting FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Model Card - Smiling Detection in Images Model Details • Developed by researchers at Google and the University of Toronto, 2018, v1. • Convolutional Neural Net. • Pretrained for face recognition then fine-tuned with cross-entropy loss for binary smiling classification. Intended Use • Intended to be used for fun applications, such as creating cartoon smiles on real images; augmentative applications, such as providing details for people who are blind; or assisting applications such as automatically finding smiling photos. • Particularly intended for younger audiences. • Not suitable for emotion detection or determining affect; smiles were annotated based on physical appearance, and not underlying emotions. Factors • Based on known problems with computer vision face technology, potential reevant factors include groups for gender, age, race, and Fitzpatrick skin type; hardware factors of camera type and lens type; and environmental factors of lighting and humidity. • Evaluation factors are gender and age group, as annotated in the publicly available dataset CelebA [ 36 ]. Further possible factors not currently available in a public smiling dataset",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". • Evaluation factors are gender and age group, as annotated in the publicly available dataset CelebA [ 36 ]. Further possible factors not currently available in a public smiling dataset. Gender and age determined by third-party annotators based on visual presentation, following a set of examples of male/female gender and young/old age. Further details available in [36]. Metrics • Evaluation metrics include False Positive Rate and False Negative Rate to measure disproportionate model performance errors across subgroups. False Discovery Rate and False Omission Rate , which measure the fraction of negtive (not smiling) and positive (smiling) predictions that are incorrectly predicted to be positive and negative, respectively, are also reported. [48] • Together, these four metrics provide values for different errors that can be calclated from the confusion matrix for binary classification systems. • These also correspond to metrics in recent definitions of “fairness” in machine learning (cf. [ 6 , 26 ]), where parity across subgroups for different metrics corrspond to different fairness criteria. • 95% confidence intervals calculated with bootstrap resampling. • All metrics reported at the .5 decision threshold, where all error types (FPR, FNR, FDR, FOR) are within the same range (0.04 - 0.14). Training Data • CelebA [ 36 ], training data split. Evaluation Data • CelebA [36], test data split. • Chosen as a basic proof-of-concept. Ethical Considerations • Faces and annotations based on public figures (celebrities). No new information is inferred or annotated. Quantitative Analyses Caveats and Recommendations • Does not capture race or skin type, which has been reported as a source of disproportionate errors [5]. • Given gender classes are binary (male/not male), which we include as male/female. Further work needed to evaluate across a spectrum of genders. • An ideal evaluation dataset would additionally include annotations for Fitzpatrick skin type, camera details, and environment (lighting/humidity) details",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". • An ideal evaluation dataset would additionally include annotations for Fitzpatrick skin type, camera details, and environment (lighting/humidity) details. Figure 2: Example Model Card for a smile detector trained and evaluated on the CelebA dataset. FAT* ’19, January 29–31, 2019, Atlanta, GA, USA Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru Model Card - Toxicity in Text Model Details • The TOXICITY classifier provided by Perspective API [ 32 ], trained to predict the likelihood that a comment will be perceived as toxic. • Convolutional Neural Network. • Developed by Jigsaw in 2017. Intended Use • Intended to be used for a wide range of use cases such as supporting human moderation and providing feedback to comment authors. • Not intended for fully automated moderation. • Not intended to make judgments about specific individuals. Factors • Identity terms referencing frequently attacked groups, fcusing on sexual orientation, gender identity, and race. Metrics • Pinned AUC, as presented in [ 11 ], which measures threshold-agnostic separability of toxic and non-toxic coments for each group, within the context of a background distribution of other groups. Ethical Considerations • Following [ 31 ], the Perspective API uses a set of values to guide their work. These values are Community, Tranparency, Inclusivity, Privacy, and Topic-neutrality. Because of privacy considerations, the model does not take into acount user history when making judgments about toxicity. Training Data • Proprietary from Perspective API. Following details in [ 11 ] and [ 32 ], this includes comments from a online forums such as Wikipedia and New York Times, with crowdsourced labels of whether the comment is “toxic”",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". Following details in [ 11 ] and [ 32 ], this includes comments from a online forums such as Wikipedia and New York Times, with crowdsourced labels of whether the comment is “toxic”. • “Toxic” is defined as “a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion.” Evaluation Data • A synthetic test set generated using a template-based aproach, as suggested in [ 11 ], where identity terms are swapped into a variety of template sentences. • Synthetic data is valuable here because [ 11 ] shows that real data often has disproportionate amounts of toxicity directed at specific groups. Synthetic data ensures that we evaluate on data that represents both toxic and non-toxic statements referencing a variety of groups. Caveats and Recommendations • Synthetic test data covers only a small set of very specif i c comments. While these are designed to be representative of common use cases and concerns, it is not comprehensive. Quantitative Analyses Figure 3: Example Model Card for two versions of Perspective API’s toxicity detector. Model Cards for Model Reporting FAT* ’19, January 29–31, 2019, Atlanta, GA, USA cards are intended to accompany a model after careful review has determined that the foreseeable benefits outweigh the foreseeable risks in the model’s use or release. To demonstrate the use of model cards in practice, we have prvided two examples: A model card for a smiling classifier tested on the CelebA dataset, and a model card for a public toxicity dtector tested on the Identity Phrase Templates dataset. We report confusion matrix metrics for the smile classifier and Pinned AUC for the toxicity detector, along with model details, intended use, pointers to information about training and evaluation data, ethical considerations, and further caveats and recommendations. The framework presented here is intended to be general enough to be applicable across different institutions, contexts, and stakholders",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Model Cards for Model Reporting",
    "author": "Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\model_cards_for_model.pdf",
    "date_published": "2019-01-16",
    "keywords": "datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations",
    "flag": "",
    "chunk_text": ". The framework presented here is intended to be general enough to be applicable across different institutions, contexts, and stakholders. It also is suitable for recently proposed requirements for analysis of algorithmic decision systems in critical social instittions, for example, for models used in determining government benefits, employment evaluations, criminal risk assessment, and criminal DNA analysis [39]. Model cards are just one approach to increasing transparency between developers, users, and stakeholders of machine learning models and systems. They are designed to be flexible in both scope and specificity in order to accommodate the wide variety of mchine learning model types and potential use cases. Therefore the usefulness and accuracy of a model card relies on the integrity of the creator(s) of the card itself. It seems unlikely, at least in the near term, that model cards could be standardized or formalized to a degree needed to prevent misleading representations of model rsults (whether intended or unintended). It is therefore important to consider model cards as one transparency tool among many, which could include, for example, algorithmic auditing by third-parties (both quantitative and qualitative), “adversarial testing” by techncal and non-technical analysts, and more inclusive user feedback mechanisms. Future work will aim to refine the methodology of creating model cards by studying how model information is intepreted and used by different stakeholders. Researchers should also explore how model cards can strengthen and complement other transparency methods 7 ACKNOWLEDGEMENTS Thank you to Joy Buolamwini, Shalini Ananda and Shira Mitchell for invaluable conversations and insight.",
    "chunk_id": "Natural_language_processing_model_cards_for_model_reporting.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": "A Primer in BERTology: What We Know About How BERT Works Anna Rogers Center for Social Data Science University of Copenhagen arogers@sodas.ku.dk Olga Kovaleva Dept. of Computer Science University of Massachusetts Lowell okovalev@cs.uml.edu Anna Rumshisky Dept. of Computer Science University of Massachusetts Lowell arum@cs.uml.edu Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our undestanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research. 1 Introduction Since their introduction in 2017, Transformers (Vaswani et al., 2017) have taken NLP by storm, offering enhanced parallelization and better moeling of long-range dependencies. The best known Transformer-based model is BERT (Devlin et al., 2019); it obtained state-of-the-art results in numrous benchmarks and is still a must-have baseline. Although it is clear that BERT works remarably well, it is less clear why , which limits further hypothesis-driven improvement of the architeture. Unlike CNNs, the Transformers have little cognitive motivation, and the size of these models limits our ability to experiment with pre-training and perform ablation studies. This explains a large number of studies over the past year that atempted to understand the reasons behind BERT’s performance. In this paper, we provide an overview of what has been learned to date, highlighting the questions that are still unresolved. We first consider the linguistic aspects of it, namely, the current evdence regarding the types of linguistic and world knowledge learned by BERT, as well as where and how this knowledge may be stored in the model",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We then turn to the technical aspects of the model and provide an overview of the current proposals to improve BERT’s architecture, pre-training, and fine-tuning. We conclude by discussing the issue of overparameterization, the approaches to copressing BERT, and the nascent area of pruning as a model analysis technique. 2 Overview of BERT Architecture Fundamentally, BERT is a stack of Transformer encoder layers (Vaswani et al., 2017) that consist of multiple self-attention ‘‘heads’’. For every iput token in a sequence, each head computes key, value, and query vectors, used to create a weighted representation. The outputs of all heads in the same layer are combined and run through a fully connected layer. Each layer is wrapped with a skip connection and followed by layer normalization. The conventional workflow for BERT consists of two stages: pre-training and fine-tuning. Prtraining uses two self-supervised tasks: masked language modeling (MLM, prediction of randomly masked input tokens) and next sentence predition (NSP, predicting if two input sentences are adjacent to each other). In fine-tuning for dowstream applications, one or more fully connected layers are typically added on top of the final encoder layer. The input representations are computed as follows: Each word in the input is first tokenized into wordpieces (Wu et al., 2016), and then three embedding layers (token, position, and segment) are combined to obtain a fixed-length vector. Special token [CLS] is used for classification predictions, and [SEP] separates input segments. Google 1 and HuggingFace (Wolf et al., 2020) provide many variants of BERT, including the original ‘‘base’’ and ‘‘large’’ versions. They vary in the number of heads, layers, and hidden state size. 1 https://github.com/google-research/bert . 842 3 What Knowledge Does BERT Have? A number of studies have looked at the knoledge encoded in BERT weights",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1 https://github.com/google-research/bert . 842 3 What Knowledge Does BERT Have? A number of studies have looked at the knoledge encoded in BERT weights. The popular aproaches include fill-in-the-gap probes of MLM, analysis of self-attention weights, and probing classifiers with different BERT representations as inputs. 3.1 Syntactic Knowledge Lin et al. (2019) showed that BERT representtions are hierarchical rather than linear , that is, there is something akin to syntactic tree structure in addition to the word order information. Tenney et al. (2019b) and Liu et al. (2019a) also showed that BERT embeddings encode information about parts of speech, syntactic chunks, and roles . Enough syntactic information seems to be captured in the token embeddings themselves to recover syntactic trees (Vilares et al., 2020; Kim et al., 2020; Rosa and Mareˇcek, 2019), although probing classifiers could not recover the labels of distant parent nodes in the syntactic tree (Liu et al., 2019a). Warstadt and Bowman (2020) report evidence of hierarchical structure in three out of four probing tasks. As far as how syntax is represented, it seems that syntactic structure is not directly encoded in self-attention weights . Htut et al. (2019) were unable to extract full parse trees from BERT heads even with the gold annotations for the root. Jawahar et al. (2019) include a brief illustration of a dependency tree extracted directly from selattention weights, but provide no quantitative evaluation. However, syntactic information can be recoered from BERT token representations . Hewitt and Manning (2019) were able to learn transformtion matrices that successfully recovered syntactic dependencies in PennTreebank data from BERT’s token embeddings (see also Manning et al., 2020). Jawahar et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Jawahar et al. (2019) experimented with transfomations of the [CLS] token using Tensor Product Decomposition Networks (McCoy et al., 2019a), concluding that dependency trees are the best matchamongfivedecompositionschemes (although the reported MSE differences are very small). Miaschi and Dell’Orletta (2020) perform a range of syntactic probing experiments with concatnated token representations as input. Note that all these approaches look for the evidence of gold-standard linguistic structures, Figure 1: Parameter-free probe for syntactic knoledge: words sharing syntactic subtrees have larger impact on each other in the MLM prediction (Wu et al., 2020). and add some amount of extra knowledge to the probe. Most recently, Wu et al. (2020) proposed a parameter-free approach based on measuring the impact that one word has on predicting another wordwithinasequence in the MLM task (Figure 1). They concluded that BERT ‘‘naturally’’ learns some syntactic information, although it is not very similar to linguistic annotated resources . The fill-in-the-gap probes of MLM showed that BERT takes subject-predicate agreement into account when performing the cloze task (Goldberg, 2019; van Schijndel et al., 2019), even for meaningless sentences and sentences with distractor clauses between the subject and the verb (Goldberg, 2019). A study of negative polarity items (NPIs) by Warstadt et al. (2019) showed that BERT is better able to detect the presence of NPIs (e.g., ‘‘ever’’) and the words that allow their use (e.g., ‘‘whether’’) than scope violations. The above claims of syntactic knowledge are belied by the evidence that BERT does not ‘‘understand’’ negation and is insensitive to malformed input . In particular, its predictions were not altered 2 even with shuffled word order, 2 See also the recent findings on adversarial triggers, which get the model to produce a certain output even though they 843 truncated sentences, removed subjects and objects (Ettinger, 2019)",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This could mean that either BERT’s syntactic knowledge is incomplete, or it does not need to rely on it for solving its tasks . The latter seems more likely, since Glavaˇs and Vuli ́c (2020) report that an intermediate fine-tuning step with supervised parsing does not make much difference for downstream task performance. 3.2 Semantic Knowledge To date, more studies have been devoted to BERT’s knowledge of syntactic rather than smantic phenomena. However, we do have evdence from an MLM probing study that BERT has some knowledge of semantic roles (Ettinger, 2019). BERT even displays some preference for the incorrect fillers for semantic roles that are semantically related to the correct ones, as oposed to those that are unrelated (e.g., ‘‘to tip a chef’’ is better than ‘‘to tip a robin’’, but worse than ‘‘to tip a waiter’’). Tenney et al. (2019b) showed that BERT ecodes information about entity types, relations, semantic roles, and proto-roles , since this infomation can be detected with probing classifiers. BERT struggles with representations of nubers. Addition and number decoding tasks showed that BERT does not form good representations for floating point numbers and fails to generalize away from the training data (Wallace et al., 2019b). A part of the problem is BERT’s wordpiece tokeniztion, since numbers of similar values can be dvided up into substantially different word chunks. Out-of-the-box BERT is surprisingly brittle to named entity replacements : For example, replacing names in the coreference task changes 85% of predictions (Balasubramanian et al., 2020). This suggests that the model does not actually form a generic idea of named entities, although its F1 scores on NER probing tasks are high (Tenney et al., 2019a). Broscheit (2019) finds that fine-tuning BERT on Wikipedia entity linking ‘‘teaches’’ it additional entity knowledge, which would suggest that it did not absorb all the relevant entity information during pre-training on Wikipedia",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". are not well-formed from the point of view of a human reader (Wallace et al., 2019a). Figure 2: BERT world knowledge (Petroni et al., 2019). 3.3 World Knowledge The bulk of evidence about commonsense knoledge captured in BERT comes from practitioners using it to extract such knowledge. One direct probing study of BERT reports that BERT strugles with pragmatic inference and role-based event knowledge (Ettinger, 2019). BERT also struggles with abstract attributes of objects, as well as visual and perceptual properties that are likely to be assumed rather than mentioned (Da and Kasai, 2019). The MLM component of BERT is easy to adapt for knowledge induction by filling in the blanks (e.g., ‘‘Cats like to chase [ ]’’). Petroni et al. (2019) showed that, for some relation types, vnilla BERT is competitive with methods relying on knowledge bases (Figure 2), and Roberts et al. (2020) show the same for open-domain QA using the T5 model (Raffel et al., 2019). Davison et al. (2019) suggest that it generalizes better to unseen data. In order to retrieve BERT’s knowledge, we need good template sentences, and there is work on their automatic extraction and augmentation (Bouraoui et al., 2019; Jiang et al., 2019b). However, BERT cannot reason based on its world knowledge . Forbes et al. (2019) show that BERT can ‘‘guess’’ the affordances and properties of many objects, but cannot reason about the relationship between properties and affordances. For example, it ‘‘knows’’ that people can walk into houses, and that houses are big, but it cannot infer that houses are bigger than people. Zhou et al. (2020) and Richardson and Sabharwal (2019) also show that the performance drops with the number of necessary inference steps. Some of BERT’s world knowledge success comes from learning stereotypical associations (Poerner et al., 2019), for example, a person with an Italian-sounding name is predicted to be Italian, even when it is incorrect",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 844 3.4 Limitations Multiple probing studies in section 3 and section 4 report that BERT possesses a surprising amount of syntactic, semantic, and world knowledge. Hoever, Tenney et al. (2019a) remark, ‘‘the fact that a linguistic pattern is not observed by our probing classifier does not guarantee that it is not there, and the observation of a pattern does not tell us how it is used.’’ There is also the issue of how complex a probe should be allowed to be (Liu et al., 2019a). If a more complex probe recovers more infomation, to what extent are we still relying on the original model? Furthermore, different probing methods may lead to complementary or even contradictory coclusions, which makes a single test (as in most studies) insufficient (Warstadt et al., 2019). A given method might also favor one model over another, for example, RoBERTa trails BERT with one tree extraction method, but leads with another (Htut et al., 2019). The choice of linguistic formaism also matters (Kuznetsov and Gurevych, 2020). In view of all that, the alternative is to focus on identifying what BERT actually relies on at inference time. This direction is currently pursued both at the level of architecture blocks (to be discussed in detail in subsection 6.3), and at the level of information encoded in model weights. Amnesic probing (Elazar et al., 2020) aims to specifically remove certain information from the model and see how it changes performance, finding, for example, that language modeling does rely on part-of-speech information. Another direction is information-theoretic proing. Pimentel et al. (2020) operationalize probing as estimating mutual information between the learned representation and a given linguistic proerty, which highlights that the focus should be not on the amount of information contained in a representation, but rather on how easily it can be extracted from it",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Voita and Titov (2020) quatify the amount of effort needed to extract infomation from a given representation as minimum description length needed to communicate both the probe size and the amount of data required for it to do well on a task. 4 Localizing Linguistic Knowledge 4.1 BERT Embeddings In studies of BERT, the term ‘‘embedding’’ refers to the output of a Transformer layer (typically, the final one). Both conventional static embedings (Mikolov et al., 2013) and BERT-style embeddings can be viewed in terms of mutual information maximization (Kong et al., 2019), but the latter are contextualized . Every token is represented by a vector dependent on the paticular context of occurrence, and contains at least some information about that context (Miaschi and Dell’Orletta, 2020). Several studies reported that distilled contexualized embeddings better encode lexical sematic information (i.e., they are better at traditional word-level tasks such as word similarity). The methods to distill a contextualized representation into static include aggregating the information across multiple contexts (Akbik et al., 2019; Bommasani et al., 2020), encoding ‘‘semantically bleached’’ sentences that rely almost exclusively on the meaning of a given word (e.g., \"This is <>\") (May et al., 2019), and even using contextualized embeddings to train static embeddings (Wang et al., 2020d). But this is not to say that there is no room for improvement. Ethayarajh (2019) measure how similar the embeddings for identical words are in every layer, reporting that later BERT layers produce more context-specific representations. 3 They also find that BERT embeddings occupy a narrow cone in the vector space, and this effect increases from the earlier to later layers. That is, two random words will on average have a much higher cosine similarity than expected if ebeddings were directionally uniform (isotrpic)",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". That is, two random words will on average have a much higher cosine similarity than expected if ebeddings were directionally uniform (isotrpic) . Because isotropy was shown to be beneficial for static word embeddings (Mu and Viswanath, 2018), this might be a fruitful direction to explore for BERT. Because BERT embeddings are contextualized, an interesting question is to what extent they capture phenomena like polysemy and hoonymy. There is indeed evidence that BERT’s contextualized embeddings form distinct cluters corresponding to word senses (Wiedemann et al., 2019; Schmidt and Hofmann, 2020), making BERT successful at word sense disambiguation task. However, Mickus et al. (2019) note that the representations of the same word depend 3 Voita et al. (2019a) look at the evolution of token embeddings, showing that in the earlier Transformer layers, MLM forces the acquisition of contextual information at the expense of the token identity, which gets recreated in later layers. 845 Figure 3: Attention patterns in BERT (Kovaleva et al., 2019). on the position of the sentence in which it occurs , likely due to the NSP objective. This is not desirable from the linguistic point of view, and could be a promising avenue for future work. The above discussion concerns token embedings, but BERT is typically used as a sentence or text encoder. The standard way to generate sentence or text representations for classification is to use the [CLS] token, but alternatives are also being discussed, including concatenation of token representations (Tanaka et al., 2020), normalized mean (Tanaka et al., 2020), and layer activations (Ma et al., 2019). See Toshniwal et al. (2020) for a systematic comparison of several methods across tasks and sentence encoders. 4.2 Self-attention Heads Several studies proposed classification of attention head types. Raganato and Tiedemann (2018) dicuss attending to the token itself, previous/next tokens, and the sentence end. Clark et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Raganato and Tiedemann (2018) dicuss attending to the token itself, previous/next tokens, and the sentence end. Clark et al. (2019) distinguish between attending to previous/next tokens, [CLS] , [SEP] , punctuation, and ‘‘atending broadly’’ over the sequence. Kovaleva et al. (2019) propose five patterns, shown in Figure 3. 4.2.1 Heads With Linguistic Functions The ‘‘heterogeneous’’ attention pattern shown in Figure 3 could potentially be linguistically interpretable, and a number of studies focused on identifying the functions of self-attention heads. In particular, some BERT heads seem to specialize in certain types of syntactic relations. Htut et al. (2019) and Clark et al. (2019) report that there are BERT heads that attended significantly more than a random baseline to words in certain syntactic positions. The datasets and methods used in these studies differ, but they both find that there are heads that attend to words in obj role more than the positional baseline. The evidence for nsubj , advmod , and amod varies between these two studies. The overall conclusion is also supported by Voita et al.’s (2019b) study of the base Transformer in machine translation context. Hoover et al. (2019) hypothesize that even complex dependencies like dobj are encoded by a combination of heads rather than a single head, but this work is limited to qualitative analysis. Zhao and Bethard (2020) looked specifically for the heads encoding negation scope. Both Clark et al. (2019) and Htut et al. (2019) conclude that no single head has the complete syntactic tree information , in line with evidence of partial knowledge of syntax (cf. subsection 3.1). However, Clark et al. (2019) identify a BERT head that can be directly used as a classifier to perform coreference resolution on par with a rule-based system, which by itself would seem to require quite a lot of syntactic knowledge. Lin et al. (2019) present evidence that attention weights are weak indicators of subject-verb agreement and reflexive anaphora",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Lin et al. (2019) present evidence that attention weights are weak indicators of subject-verb agreement and reflexive anaphora. Instead of serving as strong pointers between tokens that should be related, BERT’s self-attention weights were close to a uniform attention baseline, but there was some sensitivity to different types of distractors coherent with psycholinguistic data. This is consistent with conclusions by Ettinger (2019). To our knowledge, morphological information in BERT heads has not been addressed, but with the sparse attention variant by Correia et al. (2019) in the base Transformer, some attention heads appear to merge BPE-tokenized words. For semantic relations, there are reports of selattention heads encoding core frame-semantic relations (Kovaleva et al., 2019), as well as lexcographic and commonsense relations (Cui et al., 2020). The overall popularity of self-attention as an interpretability mechanism is due to the idea that ‘‘attention weight has a clear meaning: how much 846 a particular word will be weighted when compuing the next representation for the current word’’ (Clark et al., 2019). This view is currently debated (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Brunner et al., 2020), and in a multilayer model where attention is followed by nonlinear transformations, the patterns in individual heads do not provide a full picture. Also, although many current papers are accompanied by attention visualizations, and there is a growing number of visualization tools (Vig, 2019; Hoover et al., 2019), the visualization is typically limited to qualitative analysis (often with cherry-picked examples) (Belinkov and Glass, 2019), and should not be interpreted as definitive evidence. 4.2.2 Attention to Special Tokens Kovaleva et al. (2019) show that most selattention heads do not directly encode any non-trivial linguistic information , at least when fine-tuned on GLUE (Wang et al., 2018), since only fewer than 50% of heads exhibit the ‘‘heterogeneous’’ pattern",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Much of the model prduced the vertical pattern (attention to [CLS] , [SEP] , and punctuation tokens), consistent with the observations by Clark et al. (2019). This rdundancy is likely related to the overparameterzation issue (see section 6). More recently, Kobayashi et al. (2020) showed that the norms of attention-weighted input vectors, which yield a more intuitive interpretation of selattention, reduce the attention to special tokens. However, even when the attention weights are normed, it is still not the case that most heads that do the ‘‘heavy lifting’’ are even potentially interpretable (Prasanna et al., 2020). One methodological choice in in many studies of attention is to focus on inter-word attention and simply exclude special tokens (e.g., Lin et al. [2019] and Htut et al. [2019]). However, if attetion to special tokens actually matters at inference time, drawing conclusions purely from inter-word attention patterns does not seem warranted. The functions of special tokens are not yet well understood. [CLS] is typically viewed as an agregated sentence-level representation (although all token representations also contain at least some sentence-level information, as discussed in subsection 4.1); in that case, we may not see, for example, full syntactic trees in inter-word attetion because part of that information is actually packed in [CLS] . Clark et al. (2019) experiment with encoding Wikipedia paragraphs with base BERT to consider specifically the attention to special tokens, noting that heads in early layers attend more to [CLS] , in middle layers to [SEP] , and in final layers to periods and commas. They hypothesize that its function might be one of ‘‘no-op’’, a signal to ignore the head if its pattern is not applicable to the current case. As a result, for example, [SEP] gets increased attention starting in layer 5, but its importance for prediction drops. However, after fine-tuning both [SEP] and [CLS] get a lot of attention, depending on the task (Kovaleva et al., 2019)",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, after fine-tuning both [SEP] and [CLS] get a lot of attention, depending on the task (Kovaleva et al., 2019). Interestingly, BERT also pays a lot of attention to punctuation, which Clark et al. (2019) explain by the fact that periods and commas are simply almost as frequent as the special tokens, and so the model might learn to rely on them for the same reasons. 4.3 BERT Layers The first layer of BERT receives as input a combination of token, segment, and positional embeddings. It stands to reason that the lower layers have the most information about linear word order. Lin et al. (2019) report a decrease in the knowledge of linear word order around layer 4 in BERT-base. This is accompanied by an increased knowledge of hierarchical sentence structure, as detected by the probing tasks of predicting the token index, the main auxiliary verb and the sentence subject. There is a wide consensus in studies with different tasks, datasets, and methodologies that syntactic information is most prominent in the middle layers of BERT. 4 Hewitt and Manning (2019) had the most success reconstructing sytactic tree depth from the middle BERT layers (6-9 for base-BERT, 14-19 for BERT-large). Goldberg (2019) reports the best subject-verb agreement around layers 8-9, and the performance on syntatic probing tasks used by Jawahar et al. (2019) also seems to peak around the middle of the model. The prominence of syntactic information in the middle BERT layers is related to Liu et al.’s 4 These BERT results are also compatible with findings by Vig and Belinkov (2019), who report the highest attention to tokens in dependency relations in the middle layers of GPT-2. 847 Figure 4: BERT layer transferability (columns correspond to probing tasks, Liu et al. (2019a). (2019a) observation that the middle layers of Transformers are best-performing overall and the most transferable across tasks (see Figure 4). There is conflicting evidence about syntactic chunks . Tenney et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". There is conflicting evidence about syntactic chunks . Tenney et al. (2019a) conclude that ‘‘the basic syntactic information appears earlier in the network while high-level semantic features appear at the higher layers’’, drawing parallels between this order and the order of components in a typical NLP pipeline—from POS-tagging to dependency parsing to semantic role labeling. Jawahar et al. (2019) also report that the lower layers were more useful for chunking, while middle layers were more useful for parsing. At the same time, the probing experiments by Liu et al. (2019a) find the opposite: Both POS-tagging and chunking were performed best at the middle layers, in both BERT-base and BERT-large. However, all three studies use different suites of probing tasks. The final layers of BERT are the most tasspecific . In pre-training, this means specificity to the MLM task, which explains why the middle layers are more transferable (Liu et al., 2019a). In fine-tuning, it explains why the final layers change the most (Kovaleva et al., 2019), and why restoring the weights of lower layers of fine-tuned BERT to their original values does not dramatically hurt the model performance (Hao et al., 2019). Tenney et al. (2019a) suggest that whereas syntactic information appears early in the model and can be localized, semantics is spread across the entire model , which explains why certain non-trivial examples get solved incorrectly at first but correctly at the later layers. This is rather to be expected: Semantics permeates all language, and linguists debate whether meaningless structures can exist at all (Goldberg, 2006, p.166–182). But this raises the question of what stacking more Transformer layers in BERT actually achieves in terms of the spread of semantic knowledge, and whether that is beneficial. Tenney et al. compared BERT-base and BERT-large, and found that the overall pattern of cumulative score gains is the same, only more spread out in the larger model",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Tenney et al. compared BERT-base and BERT-large, and found that the overall pattern of cumulative score gains is the same, only more spread out in the larger model. Note that Tenney et al.’s (2019a) experiments concern sentence-level semantic relations; Cui et al. (2020) report that the encoding of ConceptNet semantic relations is the worst in the early layers and increases towards the top. Jawahar et al. (2019) place ‘‘surface features in lower layers, syntactic features in middle layers and semantic features in higher layers’’, but their conclusion is surprising, given that only one semantic task in this study actually topped at the last layer, and three others peaked around the middle and then considerably degraded by the final layers. 5 Training BERT This section reviews the proposals to optimize the training and architecture of the original BERT. 5.1 Model Architecture Choices To date, the most systematic study of BERT achitecture was performed by Wang et al. (2019b), who experimented with the number of layers, heads, and model parameters, varying one option and freezing the others. They concluded that the number of heads was not as significant as the number of layers . That is consistent with the finings of Voita et al. (2019b) and Michel et al. (2019) (section 6), and also the observation by Liu et al. (2019a) that the middle layers were the most transferable. Larger hidden representation size was consistently better, but the gains varied by setting. All in all, changes in the number of heads and layers appear to perform different functions . The issue of model depth must be related to the information flow from the most task-specific layers closer to the classifier (Liu et al., 2019a), to the initial layers which appear to be the most tasinvariant (Hao et al., 2019), and where the tokens resemble the input tokens the most (Brunner et al., 2020) (see subsection 4.3). If that is the case, a deeper model has more capacity to encode information that is not task-specific",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". If that is the case, a deeper model has more capacity to encode information that is not task-specific. On the other hand, many self-attention heads in vanilla BERT seem to naturally learn the same patterns (Kovaleva et al., 2019). This explains 848 why pruning them does not have too much impact. The question that arises from this is how far we could get with intentionally encouraging diverse self-attention patterns: Theoretically, this would mean increasing the amount of information in the model with the same number of weights. Raganato et al. (2020) show for Transformer-based machine translation we can simply pre-set the patterns that we already know the model would learn, instead of learning them from scratch. Vanilla BERT is symmetric and balanced in terms of self-attention and feed-forward layers, but it may not have to be. For the base Transformer, Press et al. (2020) report benefits from more self-attention sublayers at the bottom and more feedforward sublayers at the top. 5.2 Improvements to the Training Regime Liu et al. (2019b) demonstrate the benefits of large-batch training : With 8k examples, both the language model perplexity and downstream task performance are improved. They also publish their recommendations for other parameters. You et al. (2019) report that with a batch size of 32k BERT’s training time can be significantly reduced with no degradation in performance. Zhou et al. (2019) observe that the normalization of the trained [CLS] token stabilizes the training and slightly improves performance on text classification tasks. Gong et al. (2019) note that, because selattention patterns in higher and lower layers are similar, the model training can be done in a recursive manner , where the shallower version is trained first and then the trained parameters are copied to deeper layers. Such a ‘‘warm-start’’ can lead to a 25% faster training without sacrificing performance. 5.3 Pre-training BERT The original BERT is a bidirectional Transformer pre-trained on two tasks: NSP and MLM (section 2)",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5.3 Pre-training BERT The original BERT is a bidirectional Transformer pre-trained on two tasks: NSP and MLM (section 2). Multiple studies have come up with alternative training objectives to improve on BERT, and these could be categorized as follows: • How to mask. Raffel et al. (2019) systeatically experiment with corruption rate and corrupted span length. Liu et al. (2019b) propose diverse masks for training examples within an epoch, while Baevski et al. (2019) mask every token in a sequence instead of a random selection. Clinchant et al. (2019) replace the MASK token with [UNK] token, to help the model learn a representation for unknowns that could be useful for transltion. Song et al. (2020) maximize the amount of information available to the model by conditioning on both masked and unmasked tokens, and letting the model see how many tokens are missing. • What to mask. Masks can be applied to full words instead of word-pieces (Devlin et al., 2019; Cui et al., 2019). Similarly, we can mask spans rather than single tokens (Joshi et al., 2020), predicting how many are missing (Lewis et al., 2019). Masking phrases and named entities (Sun et al., 2019b) improves representation of structured knowledge. • Where to mask. Lample and Conneau (2019) use arbitrary text streams instead of sentence pairs and subsample frequent ouputs similar to Mikolov et al. (2013). Bao et al. (2020) combine the standard autoencoing MLM with partially autoregressive LM objective using special pseudo mask tokens. • Alternatives to masking. Raffel et al. (2019) experiment with replacing and dropping spans; Lewis et al. (2019) explore deletion, infilling, sentence permutation and docment rotation; and Sun et al. (2019c) predict whether a token is capitalized and whether it occurs in other segments of the same document. Yang et al. (2019) train on diferent permutations of word order in the input sequence, maximizing the probability of the original word order (cf. the n -gram word oder reconstruction task (Wang et al., 2019a)). Clark et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". the n -gram word oder reconstruction task (Wang et al., 2019a)). Clark et al. (2020) detects tokens that were replaced by a generator network rather than masked. • NSP alternatives. Removing NSP does not hurt or slightly improves performance (Liu et al., 2019b; Joshi et al., 2020; Clinchant et al., 2019). Wang et al. (2019a) and Cheng et al. (2019) replace NSP with the task of predicting both the next and the previous sentences. Lan et al. (2020) replace the negative NSP examples by swapped 849 sentences from positive examples, rather than sentences from different documents. ERNIE 2.0 includes sentence reordering and sentence distance prediction. Bai et al. (2020) replace both NSP and token position embeddings by a combination of paragraph, sentence, and token index embeddings. Li and Choi (2020) experiment with utterance order prediction task for multiparty dialogue (and also MLM at the level of utterances and the whole dialogue). • Other tasks. Sun et al. (2019c) propose simultaneous learning of seven tasks, icluding discourse relation classification and predicting whether a segment is relevant for IR. Guu et al. (2020) include a latent knowedge retriever in language model pretraiing. Wang et al. (2020c) combine MLM with a knowledge base completion objective. Glass et al. (2020) replace MLM with span predition task (as in extractive question answeing), where the model is expected to provide the answer not from its own weights, but from a different passage containing the corect answer (a relevant search engine query snippet). Another obvious source of improvement is prtraining data. Several studies explored the benefits of increasing the corpus volume (Liu et al., 2019b; Conneau et al., 2019; Baevski et al., 2019) and longer training (Liu et al., 2019b). The data also does not have to be raw text: There is a number efforts to incorporate explicit linguistic information , both syntactic (Sundararaman et al., 2019) and semantic (Zhang et al., 2020). Wu et al. (2019b) and Kumar et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Wu et al. (2019b) and Kumar et al. (2020) include the label for a given sequence from an annotated task dataset. Schick and Sch ̈utze (2020) separately learn representations for rare words. Although BERT is already actively used as a source of world knowledge (see subsection 3.3), there is also work on explicitly supplying structured knowledge . One approach is entitenhanced models. For example, Peters et al. (2019a); Zhang et al. (2019) include entity embeddings as input for training BERT, while Poerner et al. (2019) adapt entity vectors to BERT representations. As mentioned above, Wang et al. (2020c) integrate knowledge not through entity Figure 5: Pre-trained weights help BERT find wider optima in fine-tuning on MRPC (right) than training from scratch (left) (Hao et al., 2019). embeddings, but through the additional prtraining objective of knowledge base completion. Sun et al. (2019b,c) modify the standard MLM task to mask named entities rather than random words, and Yin et al. (2020) train with MLM objective over both text and linearized table data. Wang et al. (2020a) enhance RoBERTa with both linguistic and factual knowledge with task-specific adapters. Pre-training is the most expensive part of traiing BERT, and it would be informative to know how much benefit it provides. On some tasks, a randomly initialized and fine-tuned BERT obtains competitive or higher results than the pre-trained BERT with the task classifier and frozen weights (Kovaleva et al., 2019). The consensus in the community is that pre-training does help in most situations, but the degree and its exact contribution requires further investigation. Prasanna et al. (2020) found that most weights of pre-trained BERT are useful in fine-tuning, although there are ‘‘better’’ and ‘‘worse’’ subnetworks. One eplanation is that pre-trained weights help the fintuned BERT find wider and flatter areas with smaller generalization error, which makes the model more robust to overfitting (see Figure 5 from Hao et al. [2019])",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". [2019]). Given the large number and variety of prposed modifications, one would wish to know how much impact each of them has. However, due to the overall trend towards large model sizes, systmatic ablations have become expensive. Most new models claim superiority on standard bencmarks, but gains are often marginal, and estimates of model stability and significance testing are very rare. 850 5.4 Fine-tuning BERT Pre-training + fine-tuning workflow is a crucial part of BERT. The former is supposed to provide task-independent knowledge, and the latter would presumably teach the model to rely more on the representations useful for the task at hand. Kovaleva et al. (2019) did not find that to be the case for BERT fine-tuned on GLUE tasks: 5 during fine-tuning, the most changes for three epochs occurred in the last two layers of the models, but those changes caused self-attention to focus on [SEP] rather than on linguistically interpretable patterns. It is understandable why fine-tuning would increase the attention to [CLS] , but not [SEP] . If Clark et al. (2019) are correct that [SEP] serves as ‘‘no-op’’ indicator, fine-tuning basically tells BERT what to ignore. Several studies explored the possibilities of improving the fine-tuning of BERT: • Taking more layers into account : learning a complementary representation of the infomation in deep and output layers (Yang and Zhao, 2019), using a weighted combination of all layers instead of the final one (Su and Cheng, 2019; Kondratyuk and Straka, 2019), and layer dropout (Kondratyuk and Straka, 2019). • Two-stage fine-tuning introduces an intemediate supervised training stage between pre-training and fine-tuning (Phang et al., 2019; Garg et al., 2020; Arase and Tsujii, 2019; Pruksachatkun et al., 2020; Glavaˇs and Vuli ́c, 2020). Ben-David et al. (2020) propose a pivot-based variant of MLM to fine-tune BERT for domain adaptation. • Adversarial token perturbations improve the robustness of the model (Zhu et al., 2019)",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Ben-David et al. (2020) propose a pivot-based variant of MLM to fine-tune BERT for domain adaptation. • Adversarial token perturbations improve the robustness of the model (Zhu et al., 2019). • Adversarial regularization in combination with Bregman Proximal Point Optimization helps alleviate pre-trained knowledge forgeting and therefore prevents BERT from overfitting to downstream tasks (Jiang et al., 2019a). • Mixout regularization improves the staility of BERT fine-tuning even for a small 5 Kondratyuk and Straka (2019) suggest that fine-tuning on Universal Dependencies does result in syntactically meaningful attention patterns, but there was no quantitative evaluation. number of training examples (Lee et al., 2019). With large models, even fine-tuning becomes expensive, but Houlsby et al. (2019) show that it can be successfully approximated with adapter modules. They achieve competitive performance on 26 classification tasks at a fraction of the coputational cost. Adapters in BERT were also used for multitask learning (Stickland and Murray, 2019) and cross-lingual transfer (Artetxe et al., 2019). An alternative to fine-tuning is extracting features from frozen representations, but fintuning works better for BERT (Peters et al., 2019b). A big methodological challenge in the current NLP is that the reported performance improvements of new models may well be within variation induced by environment factors (Crane, 2018). BERT is not an exception. Dodge et al. (2020) report significant variation for BERT fine-tuned on GLUE tasks due to both weight initialization and training data order. They also propose early stopping on the less-promising seeds. Although we hope that the above observations may be useful for the practitioners, this section does not exhaust the current research on fintuning and its alternatives. For example, we do not cover such topics as Siamese architectures, policy gradient training, automated curriculum learning, and others",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example, we do not cover such topics as Siamese architectures, policy gradient training, automated curriculum learning, and others. 6 How Big Should BERT Be? 6.1 Overparameterization Transformer-based models keep growing by oders of magnitude: The 110M parameters of base BERT are now dwarfed by 17B parameters of Turing-NLG (Microsoft, 2020), which is dwarfed by 175B of GPT-3 (Brown et al., 2020). This trend raises concerns about computational complexity of self-attention (Wu et al., 2019a), environmental issues (Strubell et al., 2019; Schwartz et al., 2019), fair comparison of architectures (Aßenmacher and Heumann, 2020), and reproducibility. Human language is incredibly complex, and would perhaps take many more parameters to describe fully, but the current models do not make good use of the parameters they already have. Voita et al. (2019b) showed that all but a few Transformer heads could be pruned without 851 Compression Performance Speedup Model Evaluation BERT-base (Devlin et al., 2019) × 1 100% × 1 BERT 12 All GLUE tasks, SQuAD BERT-small × 3.8 91% − BERT 4 † All GLUE tasks Distillation DistilBERT (Sanh et al., 2019) × 1.5 90% × 1.6 BERT 6 All GLUE tasks, SQuAD BERT 6 -PKD (Sun et al., 2019a) × 1.6 98% × 1.9 BERT 6 No WNLI, CoLA, STS-B; RACE BERT 3 -PKD (Sun et al., 2019a) × 2.4 92% × 3.7 BERT 3 No WNLI, CoLA, STS-B; RACE Aguilar et al. (2019), Exp. 3 × 1.6 93% − BERT 6 CoLA, MRPC, QQP, RTE BERT-48 (Zhao et al., 2019) × 62 87% × 77 BERT 12 ∗† MNLI, MRPC, SST-2 BERT-192 (Zhao et al., 2019) × 5.7 93% × 22 BERT 12 ∗† MNLI, MRPC, SST-2 TinyBERT (Jiao et al., 2019) × 7.5 96% × 9.4 BERT 4 † No WNLI; SQuAD MobileBERT (Sun et al., 2020) × 4.3 100% × 4 BERT 24 † No WNLI; SQuAD PD (Turc et al., 2019) × 1.6 98% × 2",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5 BERT 6 † No WNLI, CoLA and STS-B WaLDORf (Tian et al., 2019) × 4.4 93% × 9 BERT 8 †∥ SQuAD MiniLM (Wang et al., 2020b) × 1.65 99% × 2 BERT 6 No WNLI, STS-B, MNLI mm ; SQuAD MiniBERT(Tsai et al., 2019) × 6 ∗∗ 98% × 27 ∗∗ mBERT 3 † CoNLL-18 POS and morphology BiLSTM-soft (Tang et al., 2019) × 110 91% × 434 BiLSTM 1 MNLI, QQP, SST-2 Quantzation Q-BERT-MP (Shen et al., 2019) × 13 98% − BERT 12 MNLI, SST-2, CoNLL-03, SQuAD BERT-QAT (Zafrir et al., 2019) × 4 99% − BERT 12 No WNLI, MNLI; SQuAD GOBO (Zadeh and Moshovos, 2020) × 9 . 8 99% − BERT 12 MNLI Pruning McCarley et al. (2020), ff2 × 2 . 2 98% × 1 . 9 BERT 24 SQuAD, Natural Questions RPP (Guo et al., 2019) × 1 . 7 99% − BERT 24 No WNLI, STS-B; SQuAD Soft MvP (Sanh et al., 2020) × 33 94% − BERT 12 MNLI, QQP, SQuAD IMP (Chen et al., 2020), rewind 50% × 1.4–2.5 94–100% − BERT 12 No MNLI-mm; SQuAD Other ALBERT-base (Lan et al., 2020) × 9 97% − BERT 12 † MNLI, SST-2 ALBERT-xxlarge (Lan et al., 2020) × 0.47 107% − BERT 12 † MNLI, SST-2 BERT-of-Theseus (Xu et al., 2020) × 1.6 98% × 1.9 BERT 6 No WNLI PoWER-BERT (Goyal et al., 2020) N/A 99% × 2–4.5 BERT 12 No WNLI; RACE Table 1: Comparison of BERT compression studies. Compression, performance retention, and inference time speedup figures are given with respect to BERT base , unless indicated otherwise. Performance retention is measured as a ratio of average scores achieved by a given model and by BERT base . The subscript in the model description reflects the number of layers used. ∗ Smaller vocabulary used. † The dimensionality of the hidden layers is reduced. ∥ Convolutional layers used. Compared to BERT large . ∗∗ Compared to mBERT. As reported in Jiao et al. (2019) . In comparison to the dev set. significant losses in performance . For BERT, Clark et al. (2019) observe that most heads in the same layer show similar self-attention patterns (perhaps related to the fact that the output of all self-attention heads in a layer is passed through the same MLP), which explains why Michel et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2019) were able to reduce most layers to a single head. Depending on the task, some BERT heads/ layers are not only redundant (Kao et al., 2020), but also harmful to the downstream task performance. Positive effect from head disabling was reported for machine translation (Michel et al., 2019), abstractive summarization (Baan et al., 2019), and GLUE tasks (Kovaleva et al., 2019). Additionally, Tenney et al. (2019a) examine the cumulative gains of their structural probing classifier, observing that in 5 out of 8 probing tasks some layers cause a drop in scores (typically in the final layers). Gordon et al. (2020) find that 30%–40% of the weights can be pruned without impact on downstream tasks. In general, larger BERT models perform better (Liu et al., 2019a; Roberts et al., 2020), but not always: BERT-base outperformed BERT-large on subject-verb agreement (Goldberg, 2019) and sentence subject detection (Lin et al., 2019). Given the complexity of language, and amounts of prtraining data, it is not clear why BERT ends up with redundant heads and layers. Clark et al. (2019) suggest that one possible reason is the use of attention dropouts, which causes some attention weights to be zeroed-out during training. 6.2 Compression Techniques Given the above evidence of overparameteriztion, it does not come as a surprise that BERT can be efficiently compressed with minimal accracy loss , which would be highly desirable for real-world applications. Such efforts to date are summarized in Table 1. The main approaches are knowledge distillation, quantization, and pruning. The studies in the knowledge distillation framework (Hinton et al., 2014) use a smaller 852 student-network trained to mimic the behavior of a larger teacher-network",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The studies in the knowledge distillation framework (Hinton et al., 2014) use a smaller 852 student-network trained to mimic the behavior of a larger teacher-network. For BERT, this has been achieved through experiments with loss functions (Sanh et al., 2019; Jiao et al., 2019), mimicing the activation patterns of individual portions of the teacher network (Sun et al., 2019a), and knowledge transfer at the pre-training (Turc et al., 2019; Jiao et al., 2019; Sun et al., 2020) or fintuning stage (Jiao et al., 2019). McCarley et al. (2020) suggest that distillation has so far worked better for GLUE than for reading comprehesion, and report good results for QA from a cobination of structured pruning and task-specific distillation. Quantization decreases BERT’s memory footprint through lowering the precision of its weights (Shen et al., 2019; Zafrir et al., 2019). Note that this strategy often requires compatible hardware. As discussed in section 6, individual selattention heads and BERT layers can be disabled without significant drop in performance (Michel et al., 2019; Kovaleva et al., 2019; Baan et al., 2019). Pruning is a compression technique that takes advantage of that fact, typically reducing the amount of computation via zeroing out of certain parts of the large model. In structured pruning, architecture blocks are dropped, as in LayerDrop (Fan et al., 2019). In unstructured, the weights in the entire model are pruned irrespective of their location, as in magnitude pruning (Chen et al., 2020) or movement pruning (Sanh et al., 2020). Prasanna et al. (2020) and Chen et al. (2020) explore BERT from the perspective of the lotery ticket hypothesis (Frankle and Carbin, 2019), looking specifically at the ‘‘winning’’ subneworks in pre-trained BERT. They independently find that such subnetworks do exist, and that tranferability between subnetworks for different tasks varies. If the ultimate goal of training BERT is copression, Li et al",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". They independently find that such subnetworks do exist, and that tranferability between subnetworks for different tasks varies. If the ultimate goal of training BERT is copression, Li et al. (2020) recommend training larger models and compressing them heavily rather than compressing smaller models lightly. Other techniques include decomposing BERT’s embedding matrix into smaller matrices (Lan et al., 2020), progressive module replacing (Xu et al., 2020), and dynamic elimination of intermediate encoder outputs (Goyal et al., 2020). See Ganesh et al. (2020) for a more detailed discussion of compression methods. 6.3 Pruning and Model Analysis There is a nascent discussion around pruning as a model analysis technique. The basic idea is that a compressed model a priori consists of elements that are useful for prediction; therefore by finding out what they do we may find out what the whole network does. For instance, BERT has heads that seem to encode frame-semantic relations, but disabling them might not hurt downstream task performance (Kovaleva et al., 2019); this suggests that this knowledge is not actually used. For the base Transformer, Voita et al. (2019b) identify the functions of self-attention heads and then check which of them survive the pruning, finding that the syntactic and positional heads are the last ones to go. For BERT, Prasanna et al. (2020) go in the opposite direction: pruning on the basis of importance scores, and interpreting the remaining ‘‘good’’ subnetwork. With respect to self-attention heads specifically, it does not seem to be the case that only the heads that potentially encode non-trivial linguistic patterns survive the pruning. The models and methodology in these stuies differ, so the evidence is inconclusive. In particular, Voita et al. (2019b) find that before pruning the majority of heads are syntactic, and Prasanna et al. (2020) find that the majority of heads do not have potentially non-trivial attention patterns",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2019b) find that before pruning the majority of heads are syntactic, and Prasanna et al. (2020) find that the majority of heads do not have potentially non-trivial attention patterns. An important limitation of the current head and layer ablation studies (Michel et al., 2019; Kovaleva et al., 2019) is that they inherently assume that certain knowledge is contained in heads/layers. However, there is evidence of more diffuse representations spread across the full network, such as the gradual increase in accuracy on difficult semantic parsing tasks (Tenney et al., 2019a) or the absence of heads that would perform parsing ‘‘in general’’ (Clark et al., 2019; Htut et al., 2019). If so, ablating individual components harms the weight-sharing mechanism. Conclusions from component ablations are also problematic if the same information is duplicated elsewhere in the network. 7 Directions for Further Research BERTology has clearly come a long way, but it is fair to say we still have more questions than answers about how BERT works. In this section, 853 we list what we believe to be the most promising directions for further research. Benchmarks that require verbal reasoning. Although BERT enabled breakthroughs on many NLP benchmarks, a growing list of analysis papers are showing that its language skills are not as impressive as they seem. In particular, they were shown to rely on shallow heuristics in natural laguage inference (McCoy et al., 2019b; Zellers et al., 2019; Jin et al., 2020), reading comprhension (Si et al., 2019; Rogers et al., 2020; Sugawara et al., 2020; Yogatama et al., 2019), argument reasoning comprehension (Niven and Kao, 2019), and text classification (Jin et al., 2020). Such heuristics can even be used to recostruct a non–publicly available model (Krishna et al., 2020). As with any optimization method, if there is a shortcut in the data, we have no reason to expect BERT to not learn it",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As with any optimization method, if there is a shortcut in the data, we have no reason to expect BERT to not learn it. But harder datasets that cannot be resolved with shallow heuristics are unlikely to emerge if their development is not as valued as modeling work. Benchmarks for the full range of linguistic competence. Although the language models seem to acquire a great deal of knowledge about language, we do not currently have comprehensive stress tests for different aspects of linguistic knowledge. A step in this direction is the ‘‘Checklist’’ behavioral testing (Ribeiro et al., 2020), the best paper at ACL 2020. Ideally, such tests would measure not only errors, but also sensitivity (Ettinger, 2019). Developing methods to ‘‘teach’’ reasoning. While large pre-trained models have a lot of knoledge, they often fail if any reasoning needs to be performed on top of the facts they possess (Talmor et al., 2019, see also subsection 3.3). For instance, Richardson et al. (2020) propose a method to ‘‘teach’’ BERT quantification, conditionals, coparatives, and Boolean coordination. Learning what happens at inference time. Most BERT analysis papers focus on different probes of the model, with the goal to find what the language model ‘‘knows’’. However, probing studies have limitations (subsection 3.4), and to this point, far fewer papers have focused on discovering what knowledge actually gets used. Several promising directions are the ‘‘amnesic probing’’ (Elazar et al., 2020), identifying features important for prediction for a given task (Arkhangelskaia and Dutta, 2019), and pruning the model to remove the non-important components (Voita et al., 2019b; Michel et al., 2019; Prasanna et al., 2020). 8 Conclusion In a little over a year, BERT has become a ubiquitous baseline in NLP experiments and inspired numerous studies analyzing the model and proposing various improvements",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Rogers-2020-A-primer-in-bertology-what-we-know-",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Rogers-2020-A-primer-in-bertology-what-we-know-.pdf",
    "date_published": "2020-12-23",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 8 Conclusion In a little over a year, BERT has become a ubiquitous baseline in NLP experiments and inspired numerous studies analyzing the model and proposing various improvements. The stream of papers seems to be accelerating rather than slowing down, and we hope that this survey helps the community to focus on the biggest unresolved questions. 9 Acknowledgments We thank the anonymous reviewers for their valuable feedback. This work is funded in part by NSF award number IIS-1844740 to Anna Rumshisky.",
    "chunk_id": "Natural_language_processing_rogers-2020-a-primer-in-bertology-what-we-know-.json_chunk_29"
  },
  {
    "document_type": "AU_course_page",
    "title": "Natural Language Processing",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/123489/Natural-Language-Processing",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": "Natural Language Processing Autumn semester 2024 Course Catalogue Save course ECTS 10 Form of instruction Classroom instruction Form of examination Take-home assignment (Assign) Language of instruction English Level Master Location Aarhus Use arrow keys on your keyboard to explore Course content Read more see description of qualifications. Description of qualifications Read more Purpose: The purpose of the course is to introduce students to advanced statistical methods used in the analysis of text and speech data. The course also introduces students to computational models used in speech and text recognition and prediction, and to models used to generate text and speech outputs in artificial intelligence systems, such as digital assistants and chat bots. The course addresses how we can approach theoretical and applied topics in the cognitive sciences using computational linguistics and natural language processing tools. Examples may include probabilistic topic modelling, sentiment analysis, and word2vec semantic analysis. The course also addresses key ethical topics that arise from the analysis of freely available natural language data, and in the development of natural language processing software and technologies. This course builds on students’ background knowledge in statistics and statistical programming, and introduces students to working with large data sets. The course introduces students to ethical and philosophical topics. Academic objectives: In the evaluation of the student’s performance, emphasis is placed on the extent to which the student is able to: Knowledge: - contrast different natural language processing methods in terms of their strengths and weaknesses in different use contexts - explain how formal and computational analysis of natural language can provide insights into human cognition and behaviour - discuss ethical and philosophical issues connected to natural language processing technology applications",
    "chunk_id": "Natural_language_processing_natural_language_processing.json_chunk_1"
  },
  {
    "document_type": "AU_course_page",
    "title": "Natural Language Processing",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/123489/Natural-Language-Processing",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": ". Skills: - identify relevant data sources for specific research and applied questions - correctly choose and apply tools for analysing natural language data. Competences: - critically reflect on and discuss theoretical and empirical implications of using natural language processing techniques - justify the choice between relevant methods and analyses used for specific research questions within the field of natural language processing - critically evaluate the appropriateness of a given method for a given natural language data set . See all ECTS 10 Level Master Semester MA, 1st semester Language of instruction English Hours - week - period Time and place will be announced no later than at the start of the semester here: https://timetable.au.dk/schedule Type of course Ordinary Primary programme Master's Degree Programme in Cognitive Science Department School of Communication and Culture Faculty Arts Location Aarhus STADS UVA code 147222U005 Copy UVA code Teaching Form of instruction Classroom instruction Instructor See all Kenneth Christian Enevoldsen Institut for Kultur og Samfund - Center for Humanities Computing Comments on the form of instruction Read more Combination of classroom instruction, individual practical exercises and study group projects. Language of instruction: The rules governing language of exam and teaching are stated in section 2.1 of the academic regulations. Literature Read more Will be announced on Brightspace at the start of the semester. Examination Form of examination Take-home assignment on topic of student's choice Form of co-examination Internal co-examination Assessment 7-point grading scale Permitted exam aids Not specified Comments Read more Ordinary exam and re-examination: The examination consists of a take-home assignment on a topic of the student’s choice and a related practical product. The assignment can be written individually or in groups of up to 4 students",
    "chunk_id": "Natural_language_processing_natural_language_processing.json_chunk_2"
  },
  {
    "document_type": "AU_course_page",
    "title": "Natural Language Processing",
    "author": "Aarhus University",
    "source": "https://kursuskatalog.au.dk/en/course/123489/Natural-Language-Processing",
    "flag": "",
    "date_published": "Unknown",
    "chunk_text": ". The assignment can be written individually or in groups of up to 4 students. Group assignments must be written in such a way that the contribution of each student, except for the introduction, thesis statement and conclusion, can form the basis of individual assessment. The assignment should clearly state which student is responsible for which section. Length for one student: 10-12 standard pages Length for two students: 17-22 standard pages Length for three students: 24-32 standard pages Length for four students: 31-42 standard pages The scope and nature of the product must be relevant in relation to the content of the course and is subject to the approval of the teacher. It must be possible to submit the product digitally in a documented form which can be accessed by the examiner and co-examiner. The product must be accompanied by a take-home assignment on a topic of the student’s choice, in which the student explains the relevance and methodological and theoretical basis of the product. The assignment and the product must be submitted for assessment in WISEflow before the deadline set in the examination plan. Assessment is based on an overall assessment of the take-home assignment and the practical product.",
    "chunk_id": "Natural_language_processing_natural_language_processing.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena Lianmin Zheng 1 ∗ Wei-Lin Chiang 1 ∗ Ying Sheng 4 ∗ Siyuan Zhuang 1 Zhanghao Wu 1 Yonghao Zhuang 3 Zi Lin 2 Zhuohan Li 1 Dacheng Li 13 Eric P. Xing 35 Hao Zhang 12 Joseph E. Gonzalez 1 Ion Stoica 1 1 UC Berkeley 2 UC San Diego 3 Carnegie Mellon University 4 Stanford 5 MBZUAI Abstract Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https: //github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge . 1 Introduction There has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised instruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new instruction following and conversational abilities [ 31 , 2 , 30 , 8 , 52 , 48 , 14 ]",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Once aligned with humans, these chat models are strongly preferred by human users over the original, unaligned models on which they are built. However, the heightened user preference does not always correspond to improved scores on traditional LLM benchmarks – benchmarks like MMLU [ 19 ] and HELM [ 24 ] cannot effectively tell the difference between these aligned models and the base models. This phenomenon suggests that there is a fundamental discrepancy between user perceptions of the usefulness of chatbots and the criteria adopted by conventional benchmarks. We argue that this discrepancy primarily arises due to existing evaluation that only measures LLMs’ core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues. As a demonstration, we show ∗ Joint first authors. This paper is an extended version of our earlier blog post [8]. 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks. arXiv:2306.05685v4 [cs.CL] 24 Dec 2023 Question: If the FED buys bonds in the secondary market (A) the money supply will increase. (B) the money supply will decrease. (C) the money supply will not be affected. (D) the discount rate would be affected. Output the answer only. Assistant B: (A) the money supply will increase. Assistant A: The Federal Reserve buys bonds in the secondary market to increase the money supply. Follow-up Question: How does it affect my daily life? Give 3 examples. Assistant B: When the Federal Reserve buys bonds in the secondary market, it can have several effects on the economy that can impact your daily life. Here are three examples: 1. Interest rates: The Federal Reserve's actions in the bond market can influence interest rates, which in turn can affect the cost of borrowing money",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here are three examples: 1. Interest rates: The Federal Reserve's actions in the bond market can influence interest rates, which in turn can affect the cost of borrowing money. For example, if the Federal Reserve buys bonds and drives up demand for them, interest rates on things like mortgages, car loans, and credit cards may go up, making it more expensive to borrow money. 2. Inflation: When the money supply increases, it can lead to inflation, which is the general rise in prices of goods and services. This can impact your daily life by making it more expensive to buy things like groceries, gas, and other necessities. 3. Employment: The Federal Reserve's actions in the bond market can also have an impact on employment. For example, if the Federal Reserve's actions lead to economic growth and increased demand for goods and services, it can create more job opportunities and boost employment. Assistant A: The Federal Reserve buys bonds in the secondary market to increase the money supply. This affects your daily life by: 1. Increasing the money supply. 2. Increasing the money supply and lowering the discount rate. 3. Increasing the money supply and lowering the discount rate and increasing the money supply. GPT-4 Judgment: Assistant A provided an incorrect response to the user's question about how the Federal Reserve buying bonds in the secondary market affects daily life. The answer given is repetitive and lacks clear examples of how the action impacts daily life. On the other hand, Assistant B provided a relevant and accurate response to the user's question about the Federal Reserve buying bonds. The answer includes three clear examples of how the action impacts daily life, such as interest rates, inflation, and employment. Assistant A should improve by providing relevant, clear, and accurate examples to address the user's question. Assistant B 's response is more helpful, relevant, and detailed",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Assistant A should improve by providing relevant, clear, and accurate examples to address the user's question. Assistant B 's response is more helpful, relevant, and detailed. Figure 1: Multi-turn dialogues between a user and two AI assistants—LLaMA-13B (Assistant A) and Vicuna-13B (Assistant B)—initiated by a question from the MMLU benchmark and a follow-up instruction. GPT-4 is then presented with the context to determine which assistant answers better. conversation histories with two models on an MMLU question in Figure 1. The two models are LLaMA-13B [ 39 ], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are in Appendix E). Despite the base LLaMA models showing competitive performance on conventional benchmarks (Table 8), its answers to open-ended questions are often not preferred by humans. This misalignment of conventional benchmarks underscores the core problem driving this paper: the need for a robust and scalable automated method to evaluate LLM alignment with human preferences. To study this, we introduce two benchmarks with human ratings as the primary evaluation metric: MT-bench and Chatbot Arena. MT-bench is a series of open-ended questions that evaluate a chatbot’s multi-turn conversational and instruction-following ability – two critical elements for human prefeence. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities, such as reasoning and math. In addition, we develop Chatbot Arena, a crowdsourced platform feturing anonymous battles between chatbots in real-world scenarios – Users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly. To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans. Because these models are often trained with RLHF, they already exhibit strong human alignment. We call this approach “LLM-as-a-judge” . This approach has been tried in our earlier blog post [ 8 ] and other concurrent or follow-up work [ 5 , 29 , 14 , 12 , 52 , 18 , 33 , 40 , 7 , 43 ]. However, there has not been a systematic study of this approach. In this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of human evaluation. We examine several potential limitations of the LLM-as-a-judge approach including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability. We show that some of the biases are minor or can be mitigated. Once addressed, our results from 3K controlled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match 2 human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human agreement (4.2, Table 4). Consequently, this suggests LLM-as-a-judge is a scalable method to swiftly evaluate human preference, serving as a promising alternative to traditional human evaluations. This paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human preference datasets with high-quality questions and diverse user interactions from MT-bench and Chatbot Arena. In addition, we argue for the adoption of a hybrid evaluation framework for future LLM benchmarks: by combining the existing capability-based benchmarks and the new preferencbased benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models. We publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study. Table 1: Sample multi-turn questions in MT-bench",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study. Table 1: Sample multi-turn questions in MT-bench. Category Sample Questions Writing 1st Turn Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions. 2nd Turn Rewrite your previous response. Start every sentence with the letter A. Math 1st Turn Given that f ( x ) = 4 x 3 − 9 x − 14 , find the value of f (2) . 2nd Turn Find x such that f ( x ) = 0 . Knowledge 1st Turn Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates. Explain how fiscal and monetary policies 2nd Turn Now, explain them again like I’m five. 2 MT-Bench and Chatbot Arena 2.1 Motivation With the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence across diverse tasks, from writing and chatting to coding [ 5 , 30 , 1 , 37 ]. However, evaluating their broad capabilities also becomes more challenging. Despite the availability of numerous benchmarks for language models, they primarily focus on evaluating models on closed-ended questions with short responses. Given that these chat assistants can now precisely follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate for assessing such capabilities. Existing benchmarks mostly fall into the following three categories. • Core-knowledge benchmarks , including MMLU [ 19 ], HellaSwag [ 50 ], ARC [ 9 ], WinGrande [ 36 ], HumanEval [ 6 ], GSM-8K [ 10 ], and AGIEval [ 51 ], evaluate the core capabilities of pre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to generate a short, specific answer to benchmark questions that can be automatically validated",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". They typically require LLMs to generate a short, specific answer to benchmark questions that can be automatically validated. • Instruction-following benchmarks , such as Flan [ 27 , 46 ], Self-instruct [ 44 ], NaturalInstrutions [ 28 ], Super-NaturalInstructions [ 45 ], expand to slightly more open-ended questions and more diverse tasks and are used to evaluate LLMs after instruction fine-tuning. • Conversational benchmarks , like CoQA [ 35 ], MMDialog [ 15 ] and OpenAssistant [ 23 ], are closest to our intended use cases. However, the diversity and complexity of their questions often fall short in challenging the capabilities of the latest chatbots. While largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure of a chatbot’s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we introduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously, these benchmarks are designed to distinguish the core capabilities of state-of-the-art models. 2.2 MT-Bench We create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions. MT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models. We identify 8 common categories of user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding, 3 knowledge I (STEM), and knowledge II (humanities/social science). For each category, we then manually designed 10 multi-turn questions. Table 1 lists several sample questions. 2.3 Chatbot Arena Our second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous battles. On this platform, users can interact with two anonymous models simultaneously, posing the same question to both. They vote for which model provides the preferred response, with the identities of the models disclosed post-voting",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". They vote for which model provides the preferred response, with the identities of the models disclosed post-voting. After running Chatbot Arena for one month, we have collected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering a wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A screenshot of the platform can be found at Appendix C.2. 3 LLM as a Judge While our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting human preferences can be costly and laborious [ 44 , 38 , 31 , 2 , 13 ]. To overcome this, we aim to develop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot Arena are open-ended without reference answers, devising a rule-based program to assess the outputs is extremely challenging. Traditional evaluation metrics based on the similarity between outputs and reference answers (e.g., ROUGE [25], BLEU [32]) are also ineffective for these questions. As LLMs continue to improve, they show potential in replacing human annotators in many tasks [ 17 , 20 ]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat assistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge. 3.1 Types of LLM-as-a-Judge We propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination: • Pairwise comparison . An LLM judge is presented with a question and two answers, and tasked to determine which one is better or declare a tie. The prompt used is given in Figure 5 (Appendix). • Single answer grading . Alternatively, an LLM judge is asked to directly assign a score to a single answer. The prompt used for this scenario is in Figure 6 (Appendix). • Reference-guided grading . In certain cases, it may be beneficial to provide a reference solution if applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix)",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". • Reference-guided grading . In certain cases, it may be beneficial to provide a reference solution if applicable. An example prompt we use for grading math problems is in Figure 8 (Appendix). These methods have different pros and cons. For example, the pairwise comparison may lack scalability when the number of players increases, given that the number of possible pairs grows quadratically; single answer grading may be unable to discern subtle differences between specific pairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative pairwise results if the judge model changes. 3.2 Advantages of LLM-as-a-Judge LLM-as-a-judge offers two key benefits: scalability and explainability . It reduces the need for human involvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide not only scores but also explanations, making their outputs interpretable, as shown in Figure 1. 3.3 Limitations of LLM-as-a-Judge We identify certain biases and limitations of LLM judges. However, we will also present solutions later and show the agreement between LLM judges and humans is high despite these limitations. Position bias is when an LLM exhibits a propensity to favor certain positions over others. This bias is not unique to our context and has been seen in human decision-making [ 3 , 34 ] and other ML domains [22, 41]. Figure 11 (Appendix) shows an example of position bias. GPT-4 is tasked to evaluate two responses from GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5’s answer is positioned 4 Table 2: Position bias of different LLM judges. Consistency is the percentage of cases where a judge gives consistent results when swapping the order of two assistants. “Biased toward first” is the percentage of cases when a judge favors the first answer. “Error” indicates wrong output formats. The tw o largest numbers in each column are in bold",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". “Biased toward first” is the percentage of cases when a judge favors the first answer. “Error” indicates wrong output formats. The tw o largest numbers in each column are in bold. Judge Prompt Consistency Biased toward first Biased toward second Error Claude-v1 default 23.8% 75.0% 0.0% 1.2% rename 56.2% 11.2% 28.7% 3.8% GPT-3.5 default 46.2% 50.0% 1.2% 2.5% rename 51.2% 38.8% 6.2% 3.8% GPT-4 default 65.0% 30.0% 5.0% 0.0% rename 66.2% 28.7% 5.0% 0.0% Table 3: Failure rate under “repetitive list” atack for different LLM judges on 23 answers. Judge Claude-v1 GPT-3.5 GPT-4 Failure rate 91.3% 91.3% 8.7% Table 4: Judge failure rate on 10 math questions with different prompts. We test LLaMA-13B vs. Vicuna-13B and swap positions. A failure means wh en GPT-4 says an incorrect answer is correc t. Default CoT Reference Failure rate 14/20 6/20 3/20 first, GPT-4 considers GPT-3.5’s answer more detailed and superior. However, upon switching the positions of the two responses, GPT-4’s judgement flips, favoring Vicuna’s answer. To analyze the position bias, we construct two similar answers to each first-turn question in MT-bench by calling GPT-3.5 twice with a temperature of 0.7. We then try three LLMs with two different prompts: “default” is our default prompt in Figure 5 (Appendix). “rename” renames the assistants in our default prompt to see whether the bias is on positions or names. As in Table 2, we found all of them exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a name bias which makes it favors \"Assistant A\", as illustrated by the \"rename\" prompt. The position bias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases. Note that this test is challenging because the answers are very similar and occasionally indistinguisable even to humans. We will show that position bias is less prominent in some cases in Appendix D.1",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We will show that position bias is less prominent in some cases in Appendix D.1. As for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the left-to-right architecture of causal transformers, but leave a deeper study as future work. Verbosity bias is when an LLM judge favors longer, verbose responses, even if they are not as clear, high-quality, or accurate as shorter alternatives. To examine this bias, we design a “repetitive list” attack with model answers from MT-bench. We first select 23 model answers from MT-bench that contain a numbered list. We then make them unnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and insert the rephrased new list to the beginning of the original list. For example, if the original response contains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from the original 5 items. An example is shown in Figure 12 (Appendix). We define the attack is successful if an LLM judge thinks the new response is better than the old response. Table 3 shows the failure rate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias though GPT-4 defends significantly better than others. As a calibration, we find LLM judges are able to correctly judge identical answers (i.e., they always return a tie for two identical answers) but cannot pass the more advanced “repetitive list” attack. Self-enhancement bias. We adopt the term “self-enhancement bias” from social cognition literture [4] to describe the effect that LLM judges may favor the answers generated by themselves. We examine this effect statistically. Figure 3(b) shows the win rate (w/o tie) of six models under different LLM judges and humans. Compared to humans, we do observe that some judges favor certain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself with a 25% higher win rate",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Compared to humans, we do observe that some judges favor certain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself with a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself. Due to limited data and small differences, our study cannot determine whether the models exhibit a self-enhancement bias. Conducting a controlled study is challenging because we cannot easily rephrase a response to fit the style of another model without changing the quality. 5 Limited capability in grading math and reasoning questions. LLMs are known to have limited math and reasoning capability [ 10 ], which results in its failure of grading such questions because they do not know the correct answers. However, what is more intriguing is that it also shows limitations in grading basic math problems which it is capable of solving. For instance, in Figure 13 (Appendix), we present an example of an elementary math question in which GPT-4 makes an incorrect judgment. It’s worth noting that although GPT-4 can solve the problem (when asked separately), it was misled by the provided answers, ultimately resulting in incorrect judgment. This pattern can also be seen in a reasoning question example in Figure 14 (Appendix). Both GPT-3.5 and Claude-v1 show a similar weakness. In Section 3.4, we will introduce a reference-guided method to mitigate such issues. 3.4 Addressing limitations We present a few methods to address position bias and the limited grading ability for math questions. Swapping positions. The position bias can be addressed by simple solutions. A conservative approach is to call a judge twice by swapping the order of two answers and only declare a win when an answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a tie. Another more aggressive approach is to assign positions randomly, which can be effective at a large scale with the correct expectations",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Another more aggressive approach is to assign positions randomly, which can be effective at a large scale with the correct expectations. In the following experiments, we use the conservative one. Few-shot judge. We assess whether few-shot examples can improve consistency in the position bias benchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and Vicuna for generating answers, and GPT-4 for generating judgments. The examples cover three cases: A is better, B is better, and tie. As shown in Table 12 (Appendix), the few-shot judge can significantly increase the consistency of GPT-4 from 65.0% to 77.5%. However, high consistency may not imply high accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides, the longer prompts make API calls 4 × more expensive. We use the zero-shot prompt by default in our following experiments but leave an additional study in Appendix D.2. Chain-of-thought and reference-guided judge. In Section 3.3, we have shown LLM’s limited capability in grading math and reasoning questions. We propose two simple methods to mitigate this issue: chain-of-thought judge and reference-guided judge. Chain-of-thought is a widely used technique to improve LLM’s reasoning capability [ 47 ]. We propose a similar technique to prompt an LLM judge to begin with answering the question independently and then start grading. Detailed prompt in Figure 7 (Appendix). However, even with the CoT prompt, we find that in many cases LLM makes exactly the same mistake as the given answers in its problem-solving process (See example in Figure 15 (Appendix), suggesting that LLM judge may still be misled by the context. Hence, we propose a reference-guided method, in which we first generate LLM judge’s answer independently, and then display it as a reference answer in the judge prompt. In Table 4, we see a significant improvement in failure rate (from 70% to 15%) over the default prompt. Fine-tuning a judge model",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In Table 4, we see a significant improvement in failure rate (from 70% to 15%) over the default prompt. Fine-tuning a judge model. We try fine-tuning a Vicuna-13B on arena data to act as a judge and show some promising preliminary results in Appendix F. 3.5 Multi-turn judge In MT-bench, every question involves two turns to evaluate conversational abilities. Therefore, when comparing two assistants, it becomes necessary to present a total of two questions and four responses, complicating the prompt design. We explore two possible designs, (1) breaking the two turns into two prompts or (2) displaying complete conversations in a single prompt. Our finding is the former one can cause the LLM judge struggling to locate the assistant’s previous response precisely. We illustrate a case in Figure 16 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference. This suggests the necessity of displaying a complete conversation to enable the LLM judge to better grasp the context. We then consider the alternative design that presents two full conversations in a single prompt in which we ask the LLM judge to focus on the second question (Figure 9 (Appendix)). This approach has been found to significantly alleviate the aforementioned referencing issue. 6 4 Agreement Evaluation We study the agreement between different LLM judges and humans on MT-bench and Chatbot Arena datasets. On MT-bench, we also study the agreement among humans. MT-bench represents a small-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale study with crowdsourced human evaluation in the wild. 4.1 Setup MT-bench. We generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1, Vicuna-13B, Alpaca-13B [ 38 ], and LLaMA-13B [ 39 ]. We then use 2 kinds of judges: LLM judges and 58 expert-level human labelers. The labelers are mostly graduate students so they are considered experts and more skilled than average crowd workers",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We then use 2 kinds of judges: LLM judges and 58 expert-level human labelers. The labelers are mostly graduate students so they are considered experts and more skilled than average crowd workers. We let LLM judges evaluate all pairs and let each human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all questions. The detailed data collection process is in Appendix C. Chatbot Arena. We randomly sample 3K single-turn votes from 30K arena data, which covers models including GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B [ 16 ], Alpaca-13B, LLaM13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114 unique IPs). Metrics. We define the agreement between two types of judges as the probability of randomly selected individuals (but not identical) of each type agreeing on a randomly selected question. See more explanation in Appendix D.3. Average win rate is the average of win rates against all other players. These metrics can be computed with or without including tie votes. 4.2 High agreement between GPT-4 and humans We compute agreement on MT-bench data. In Table 5, GPT-4 with both pairwise comparison and single answer grading show very high agreements with human experts. The agreement under setup S2 (w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among humans (81%). This means GPT-4’s judgments closely align with the majority of humans. We also show that GPT-4’s judgments may help humans make better judgments. During our data collection, when a human’s choice deviated from GPT-4, we presented GPT-4’s judgments to humans and ask if they are reasonable (details in Appendix C.1). Despite different views, humans deemed GPT-4’s judgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases. The data from Arena shows a similar trend, as illustrated by Table 6",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The data from Arena shows a similar trend, as illustrated by Table 6. Comparing GPT-4 and other LLM judges, we find they reach a similar non-tie agreement ratio between humans but the number of non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less suffered from position bias but other models also perform well when they give an affirmative answer. In both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human prefeences very well. This means GPT-4 has a relatively stable internal rubric. Although it may sometimes perform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method. We then perform a breakdown analysis by computing agreement on different model pairs and categories. We only include non-tied votes. In Figure 2, we observe the agreement between GPT-4 and human progressively increases in line with the performance disparity of the model pairs (i.e., larger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans better when significant performance differences exist between the models. GPT-4 Claude GPT-3.5 Vicuna-13B Alpaca-13B LLaMA-13B (a) All votes, first turn 0.0 0.2 0.4 0.6 0.8 1.0 GPT-4 GPT-3.5 Claude Vicuna-13B Alpaca-13B LLaMA-13B (b) Non-tied votes, first turn 0.0 0.2 0.4 0.6 0.8 1.0 GPT-4 Claude GPT-3.5 Vicuna-13B Alpaca-13B LLaMA-13B (c) All votes, second turn 0.0 0.2 0.4 0.6 0.8 1.0 GPT-4 Claude GPT-3.5 Vicuna-13B Alpaca-13B LLaMA-13B (d) Non-tied votes, second turn 0.0 0.2 0.4 0.6 0.8 1.0 Win rate GPT-4 Judge GPT-3.5 Judge Claude Judge Human Human (first turn) Figure 3: Average win rate of six models under different judges on MT-bench. 7 Table 5: Agreement between two types of judges on MT-bench. “G4-Pair” and “G4-Single” denote GPT-4 with pairwise comparison and single-answer grading respectively. The single-answer grading can be converted into pairwise comparison results for calculating the agreement",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The single-answer grading can be converted into pairwise comparison results for calculating the agreement. We report two setups: “S1” includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent as tie; “S2” only includes non-tie votes. The agreement between two random judges under each setup is denoted as “R=”. The top value in each cell is the agreement, and the bottom gray value is #votes . Setup S1 (R = 33%) S2 (R = 50%) Judge G4-Single Human G4-Single Human G4-Pair 70% 1138 66% 1343 97% 662 85% 859 G4-Single - 60% 1280 - 85% 739 Human - 63% 721 - 81% 479 (a) First Turn Setup S1 (R = 33%) S2 (R = 50%) Judge G4-Single Human G4-Single Human G4-Pair 70% 1161 66% 1325 95% 727 85% 864 G4-Single - 59% 1285 - 84% 776 Human - 67% 707 - 82% 474 (b) Second Turn Table 6: Agreement between two types of judges on Chatbot Arena. “G4-S” denotes GPT-4 with single-answer grading. “G4”, “G3.5” and “C” denote GPT-4, GPT-3.5, and Claude with pairwise comparison, respectively. “H” denotes human. The remaining of table follows the sam e format as Table 5. Setup S1 (Random = 33%) S2 (Random = 50%) Judge G4-S G3.5 C H G4-S G3.5 C H G4 72% 2968 66% 3061 66% 3062 64% 3066 95% 1967 94% 1788 95% 1712 87% 1944 G4-S - 60% 2964 62% 2964 60% 2968 - 89% 1593 91% 1538 85% 1761 G3.5 - - 68% 3057 54% 3061 - - 96% 1497 83% 1567 C - - - 53% 3062 - - - 84% 1475 0.0 0.5 1.0 Win rate difference 0.7 0.8 0.9 1.0 Agreement Figure 2: Agreement and win rate diference. Each point corresponds to a model pair and counts only the non-tie votes between the two models. The axis value is the win rate difference btween the two models. The y-axis value is the GPT-4 and human agreement",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The axis value is the win rate difference btween the two models. The y-axis value is the GPT-4 and human agreement. GPT-4 Claude GPT-3.5 Vicuna-13B Vicuna-7B Koala-13B Alpaca-13B Dolly-12B LLaMA-13B (a) All votes 0.0 0.2 0.4 0.6 0.8 1.0 GPT-4 Claude GPT-3.5 Vicuna-13B Vicuna-7B Koala-13B Alpaca-13B Dolly-12B LLaMA-13B (b) Non-tied votes 0.0 0.2 0.4 0.6 0.8 1.0 Win rate GPT-4 Judge GPT-3.5 Judge Human GPT-4-Single Judge Figure 4: Average win rate of nine models under different judges on Chatbot Arena. Table 7: Category-wise win rate of models. Model Writing Roleplay Reasoning Math Coding Extraction STEM Humanities GPT-4 61.2% 67.9% 49.3% 66.1% 56.3% 66.2% 76.6% 72.2% GPT-3.5 50.9% 60.6% 32.6% 63.8% 55.0% 48.8% 52.8% 53.8% Vicuna-13B 39.7% 39.2% 20.1% 18.0% 36.9% 29.2% 47.0% 47.5% LLaMA-13B 15.1% 15.1% 7.8% 7.5% 2.1% 9.3% 6.8% 10.1% 4.3 Win rates under different judges We plot the average win rate of models under different judges on MT-bench and Chatbot Arena in Figure 3 and Figure 4, respectively. The win rate curves from LLM judges closely match the curves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are more preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can better differentiate some advanced abilities of models. We also list the per-category win rate of 8 Table 8: Evaluation results of several model variants. Model #Training Token MMLU (5-shot) TruthfulQA (0-shot) MT-Bench Score (GPT-4) LLaMA-7B 1T 35.2 0.22 2.74 LLaMA-13B 1T 47.0 0.26 2.61 Alpaca-7B 4.4M 40.1 0.26 4.54 Alpaca-13B 4.4M 48.1 0.30 4.53 Vicuna-7B (selected) 4.8M 37.3 0.32 5.95 Vicuna-7B (single) 184M 44.1 0.30 6.04 Vicuna-7B (all) 370M 47.1 0.32 6.00 Vicuna-13B (all) 370M 52.1 0.35 6.39 GPT-3.5 - 70.0 - 7.94 GPT-4 - 86.4 - 8.99 representative models in Table 7 to show how MT-bench differentiates models, in which we see GPT-4 is significantly better than others. Vicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math, and coding categories",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Vicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math, and coding categories. Note that in math/coding category, GPT-3.5 and GPT-4 have similar overall win-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better than GPT-3 in the direct pairwise comparison or single-answer grading. Please see a performance breakdown of MT-bench score for each category in Appendix D.4. 5 Human Preference Benchmark and Standardized Benchmark Human preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions to the current standardized LLM benchmarks. They focus on different aspects of a model and the recommended way is to comprehensively evaluate models with both kinds of benchmarks. We evaluate several model variants derived from LLaMA on MMLU [ 19 ], Truthful QA [ 26 ] (MC1), and MT-bench (GPT-4 judge). The training details are in Appendix E. Since we have shown that GPT-4 single-answer grading also performs well in Section 4.2, we use GPT-4 single-answer grading for MT-bench in favor of its scalability and simplicity. We ask GPT-4 to give a score for each turn on a scale of 10 by using our prompt templates (Figure 6, Figure 10) and report an average score of 160 = 80 × 2 turns. Table 8 shows the results. We find that fine-tuning on high-quality dialog datasets (i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement scales with fine-tuning data size. On the other hand, a small high-quality conversation dataset can quickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve MMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens or 3K conversations. In Table 8, no single benchmark can determine model quality, meaning that a comprehensive evaluation is needed. Our results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible and could become a new standard in future benchmarks",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Our results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible and could become a new standard in future benchmarks. We are also hosting a regularly updated leaderboard with more models 2 . Notably, DynaBench [ 21 ], a research platform dedicated to dynamic data collection and benchmarking, aligns with our spirit. DynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and overfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach can automate and scale platforms of this nature. 6 Discussion Limitations. This paper emphasizes helpfulness but largely neglects safety. Honesty and harlessness are crucial for a chat assistant as well [ 2 ]. We anticipate similar methods can be used to evaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there are multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a single metric in this study. A more comprehensive evaluation can be developed by analyzing and separating these dimensions. We propose preliminary solutions to address the limitations and biases of LLM-as-a-judge in Section 3.4, but we anticipate more advanced methods can be developed. Data collection and release. Appendix C describes the detailed data collection and release processes, which include the instructions we give to users, the screenshots of the data collection interface, the information about participated users, and the content of the released data. 2 https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard 9 Societal impacts. The societal impact of this study is multi-faceted. Our evaluation methods can help enhance chatbot quality and user experiences. However, addressing biases in these methods is crucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat assistants may replace certain human tasks, resulting in job displacements and new opportunities. Future directions",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2023-judging-llm-as-a-judge-with-mt-bench.pdf",
    "date_published": "2023-12-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Advanced chat assistants may replace certain human tasks, resulting in job displacements and new opportunities. Future directions. 1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source LLM judge aligned with human preference 3) Enhancing open models’ math/reasoning capability. 7 Conclusion In this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its efficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowusers on Chatbot Arena. Our results reveal that strong LLMs can achieve an agreement rate of over 80%, on par with the level of agreement among human experts, establishing a foundation for an LLM-based evaluation framework. Acknowledgement This project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship. We extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li, Tianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.",
    "chunk_id": "Natural_language_processing_neurips-2023-judging-llm-as-a-judge-with-mt-bench.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": "F INETUNED L ANGUAGE M ODELS A RE Z ERO -S HOT L EARNERS Jason Wei ∗ , Maarten Bosma ∗ , Vincent Y. Zhao ∗ , Kelvin Guu ∗ , Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le Google Research A BSTRACT This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning —finetuning language models on a collection of datasets described via instructions—substantially improves zershot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. Target Input (Commonsense Reasoning) keep stack of pillow cases in fridge Inference on unseen task type Finetune on many tasks (“instruction-tuning”) Translate this sentence to Spanish: The new office building was built in less than three months. Input (Translation) El nuevo edificio de oficinas se construyó en tres meses. Target Input (Natural Language Inference) It is not possible to tell FLAN Response Coreference resolution tasks Sentiment analysis tasks GPT-3 175B zero shot GPT-3 175B few-shot FLAN 137B zero-shot Performance on unseen task types Natural language inference 42.9 53.2 56.2 Reading Comprehension 63.7 72.6 77.4 Closed-Book QA 49.8 55.7 56.6 Here is a goal: Get a cool sleep on summer days. How would you accomplish this goal? OPTIONS: -Keep stack of pillow cases in fridge. -Keep stack of pillow cases in oven. Premise: At my age you will probably have learnt one lesson",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". How would you accomplish this goal? OPTIONS: -Keep stack of pillow cases in fridge. -Keep stack of pillow cases in oven. Premise: At my age you will probably have learnt one lesson. Hypothesis: It's not certain how many lessons you'll learn by your thirties. Does the premise entail the hypothesis? OPTIONS: -yes -it is not possible to tell -no Figure 1: Top: overview of instruction tuning and FLAN. Instruction tuning finetunes a pretrained language model on a mixture of tasks phrased as instructions. At inference time, we evaluate on an unseen task type; for instance, we could evaluate the model on natural language inference (NLI) when no NLI tasks were seen during instruction tuning. Bottom: performance of zero-shot FLAN, compared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning improved performance substantially out of ten we evaluate. NLI datasets: ANLI R1–R3, CB, RTE. Reading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy, ARC-challenge, NQ, TriviaQA. ∗ Lead contributors. Author contributions listed at end of paper . arXiv:2109.01652v5 [cs.CL] 8 Feb 2022 1 I NTRODUCTION Language models (LMs) at scale, such as GPT-3 (Brown et al., 2020), have been shown to perform few-shot learning remarkably well. They are less successful at zero-shot learning, however. For example, GPT-3’s zero-shot performance is much worse than few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. One potential reason is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data. In this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience. We leverage the intuition that NLP tasks can be described via natural language instructions, such as “ Is the sentiment of this movie review positive or negative? ” or “ Translate ‘how are you’ into Chinese. ” We take a pretrained language model of 137B parameters and perform instruction tuning —finetuning the model on a mixture of more than 60 NLP datasets expressed via natural language instructions. We refer to this resulting model as FLAN, for F inetuned La nguage N et. To evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters based on their task types and hold out each cluster for evaluation while instruction tuning FLAN on all other clusters. For example, as shown in Figure 1, to evaluate FLAN’s ability to perform natural language inference, we instruction tune the model on a range of other NLP tasks such as commonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has not seen any natural language inference tasks in instruction tuning, we then evaluate its ability to perform zero-shot natural language inference. Our evaluations show that FLAN substantially improves the zero-shot performance of the base 137B-parameter model. FLAN’s zero-shot also outperforms 175B-parameter GPT-3’s zero-shot on 20 of 25 datasets that we evaluate, and even outperforms GPT-3’s few-shot by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we find that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the benefits of instruction tuning emerge only with sufficient model scale",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Instruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects of both the pretrain–finetune and prompting paradigms by using supervision via finetuning to improve language model’s responses to inference-time text interactions. Our empirical results demonstrate promising abilities of language models to perform tasks described purely via instructions. Source code for loading the instruction tuning dataset used for FLAN is publicly available at https://github.com/google-research/flan . (A) Pretrain–finetune (BERT, T5) Finetune on task A Inference on task A Pretrained LM • Typically requires many task-specific examples • One specialized model for each task (B) Prompting (GPT-3) Inference on task A Pretrained LM Improve performance via few-shot prompting or prompt engineering Pretrained LM (C) Instruction tuning (FLAN) Instruction-tune on many tasks: B, C, D, Inference on task A Inference on unseen task Model learns to perform many tasks via natural language instructions Figure 2: Comparing instruction tuning with pretrain–finetune and prompting. 2 FLAN: I NSTRUCTION T UNING I MPROVES Z ERO -S HOT L EARNING The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate performance on unseen tasks, we group datasets into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters. 2.1 T ASKS & T EMPLATES As creating an instruction tuning dataset with many tasks from scratch would be resource-intensive, we transform existing datasets from the research community into an instructional format. We aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language understanding and language generation tasks, into a single mixture",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We aggregate 62 text datasets that are publicly available on Tensorflow Datasets, including both language understanding and language generation tasks, into a single mixture. Figure 3 shows these datasets— each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are of the same task type. Descriptions, sizes, and examples of each dataset are shown in Appendix G. Natural language inference (7 datasets) CB ANLI (R1-R3) MNLI QNLI RTE SNLI WNLI Commonsense (4 datasets) HellaSwag CoPA PiQA StoryCloze Sentiment (4 datasets) Sent140 IMDB SST-2 Yelp Struct to text (4 datasets) DART CommonGen E2ENLG WEBNLG Closed-book QA (3 datasets) NQ ARC (easy/chal.) TQA Coreference (3 datasets) Winogrande DPR WSC273 Translation (8 datasets) ParaCrawl EN/ES ParaCrawl EN/DE ParaCrawl EN/FR WMT-16 EN/CS WMT-16 EN/DE WMT-16 EN/FI WMT-16 EN/RO WMT-16 EN/RU WMT-16 EN/TR Summarization (11 datasets) AG News AESLC CNN-DM Gigaword Multi-News Newsroom Opin-Abs: iDebate Opin-Abs: Movie SamSum Wiki Lingua EN XSum Reading comp. (5 datasets) DROP BoolQ MultiRC OBQA SQuAD Paraphrase (4 datasets) QQP MRPC PAWS STS-B Read. comp. w/ commonsense (2 datasets) CosmosQA ReCoRD Misc. (7 datasets) QuAC CoQA WIC TREC CoLA Math Fix Punctuation (NLG) Figure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal). For each dataset, we manually compose ten unique templates that use natural language instructions to describe the task for that dataset. While most of the ten templates describe the original task, to increase diversity, for each dataset we also include up to three templates that “turned the task around,” (e.g., for sentiment classification we include templates asking to generate a movie review). We then instruction tune a pretrained language model on the mixture of all datasets, with examples in each dataset formatted via a randomly selected instruction template for that dataset. Figure 4 shows multiple instruction templates for a natural language inference dataset",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Figure 4 shows multiple instruction templates for a natural language inference dataset. Entailment Not entailment Russian cosmonaut Valery Polyakov set the record for the longest continuous amount of time spent in space, a staggering 438 days, between 1994 and 1995. Premise Russians hold the record for the longest stay in space. Hypothesis Target Options: - yes - no <premise> Can we infer the following? <hypothesis> <options> Template 2 Template 1 <premise> Based on the paragraph above, can we conclude that <hypothesis> ? <options> Read the following and determine if the hypothesis can be inferred from the premise: Premise: <premise> Hypothesis: <hypothesis> <options> Template 3 Template 4, Figure 4: Multiple instruction templates describing a natural language inference task. 2.2 E VALUATION S PLITS We are interested in how FLAN performs on tasks not seen in instruction tuning, and so it is crucial to define what counts as an unseen task. Whereas some prior work defines unseen tasks by disallowing the same dataset to appear in training, we use a more conservative definition that leverages the task clusters from Figure 3. In this work, we only consider dataset D unseen at evaluation time if no datasets from any task clusters that D belongs to were seen during instruction tuning. For instance, if D is an entailment task, then no entailment datasets appeared in instruction tuning, and we instruction-tuned on all other clusters. 1 Hence, to evaluate zero-shot FLAN on c task clusters, we instruction tune c models, where each model holds out a different task cluster for evaluation. 1 When evaluating on the read. comp. with commonsense cluster, both read. comp. and commonsense reasoning were dropped from instruction tuning. Conversely, the read. comp. with commonsense cluster was not used for instruction tuning when evaluating on read. comp. or commonsense reasoning. We also drop the paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". comp. or commonsense reasoning. We also drop the paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa. 2.3 C LASSIFICATION WITH O PTIONS The output space for a given task is either one of several classes (classification) or free text (generation). As FLAN is an instruction-tuned version of a decoder-only language model, it naturally responds in free text, and so no further modifications are needed for generation tasks. For classification tasks, prior work (Brown et al., 2020) used a rank classification approach where, for example, only two outputs (“ yes ” and “ no ”) are considered and the higher probability one is taken as the model’s prediction. Though this procedure is logically sound, it is imperfect in that the probability mass for answers may have an undesired distribution among ways of saying each answer (e.g., a large number of alternative ways of saying “ yes ” may lower the probability mass assigned to “ yes ”). Therefore, we include an options suffix, in which we append the token OPTIONS to the end of a classification task along with a list of the output classes for that task. This makes the model aware of which choices are desired when responding to classification tasks. Example use of options is shown in the NLI and commonsense examples in Figure 1. 2.4 T RAINING D ETAILS Model architecture and pretraining. In our experiments, we use LaMDA-PT, a dense left-to-right, decoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece library (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog). Instruction tuning procedure. FLAN is the instruction-tuned version of LaMDA-PT",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Note that LaMDA-PT only has language model pretraining (c.f. LaMDA, which was finetuned for dialog). Instruction tuning procedure. FLAN is the instruction-tuned version of LaMDA-PT. Our instrution tuning pipeline mixes all datasets and randomly samples from each dataset. To balance the different sizes of datasets, we limit the number of training examples per dataset to 30k and follow the examples-proportional mixing scheme (Raffel et al., 2020) with a mixing rate maximum of 3k. 2 We finetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor Optimizer (Shazeer & Stern, 2018) with a learning rate of 3e-5. The input and target sequence lengths used in finetuning are 1024 and 256, respectively. We use packing (Raffel et al., 2020) to combine multiple training examples into a single sequence, separating inputs from targets using a special EOS token. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations, we report results on the final checkpoint trained for 30k steps. 3 R ESULTS We evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transltion, commonsense reasoning, coreference resolution, and struct-to-text. As described in 2.2, we evaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for evaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses a different checkpoint). For each dataset, we evaluate the mean of performance on all templates, which proxies the expected performance given a typical natural language instruction. As a dev set is sometimes available for manual prompt engineering (Brown et al., 2020), for each dataset we also obtain the test set performance using the template with the best dev set performance. For comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as GPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as GPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning). This baseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning significantly improves LaMDA-PT on most datasets. We also show the zero-shot performances of GPT-3 175B (Brown et al., 2020) and GLaM 64B/64E (Du et al., 2021), as reported in their respective papers. With the best dev template, zero-shot FLAN outperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3’s few-shot performance on 10 datasets. With the best dev-template, zero-shot FLAN outperforms zero-shot GLaM on 13 of 19 available datasets and one-shot GLaM on 11 of 19 datasets. 2 In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional sampling weight for examples in excess of 3,000. Overall, we observe that instruction tuning is very effective on tasks naturally verbalized as instrutions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as language modeling, where instructions would be largely redundant (e.g., commonsense reasoning and coreference resolution tasks that are formatted as finishing an incomplete sentence or paragraph). Results on natural language inference, reading comprehension, closed-book QA, and translation are summarized in Figure 5 and described below. Natural language inference Reading comprehension Closed-book QA 0 100 80 60 40 20 Zero-shot performance TQA NQ ARC-c ARC-e OBQA RTE CB ANLI R3 ANLI R2 ANLI R1 MultiRC BoolQ Translation EN to FR EN to DE FR to EN DE to EN EN to RO RO to EN LaMDA-PT137B FLAN 137B GPT-3 175B GLaM 64B/64E Supervised model Figure 5: Zero-shot performance of FLAN compared to LaMDA-PT 137B, GPT-3 175B, and GLaM 64B/64E on natural language inference, reading comprehension, closed-book QA, and translation. Performance of FLAN is the mean of up to 10 instructional templates per task",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Performance of FLAN is the mean of up to 10 instructional templates per task. Supervised models were either T5, BERT, or translation models (specified in Table 2 and Table 1 in the Appendix). Natural language inference (NLI). On five NLI datasets, where a model must determine whether a hypothesis is true given some premise, FLAN outperforms all baselines by a large margin. As noted by Brown et al. (2020), perhaps one reason why GPT-3 struggles with NLI is that NLI examples are unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased as a continuation of a sentence. For FLAN, we phrase NLI as the more natural question “ Does <premise> mean that <hypothesis>? ”, achieving much higher performance. Reading comprehension. On reading comprehension, where models are asked to answer a question about a provided passage, FLAN outperforms baselines for MultiRC (Khashabi et al., 2018) and OBQA (Mihaylov et al., 2018). On BoolQ (Clark et al., 2019a), FLAN outperforms GPT-3 by a large margin, though LaMDA-PT already achieves high performance on BoolQ. Closed-book QA. For closed-book QA, which asks models to answer questions about the world without access to specific information containing the answer, FLAN outperforms GPT-3 on all four datasets. Compared to GLaM, FLAN has better performance on ARC-e and ARC-c (Clark et al., 2018), and slightly lower performance on NQ (Lee et al., 2019; Kwiatkowski et al., 2019) and TQA (Joshi et al., 2017). Translation. Similar to GPT-3, the training data for LaMDA-PT is around 90% English and includes some text in other languages that was not specifically used to train the model to perform machine translation. We also evaluate FLAN’s performance on machine translation for the three datasets evaluated in the GPT-3 paper: French–English from WMT’14 (Bojar et al., 2014), and German– English and Romanian–English from WMT’16 (Bojar et al., 2016)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Compared with GPT-3, FLAN outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in most cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares favorably against supervised translation baselines. Translating from English into other languages, however, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece tokenizer and that the majority of pretraining data is English. Additional tasks. Although we see strong results for the above task clusters, one limitation with instruction tuning is that it does not improve performance for many language modeling tasks (e.g., commonsense reasoning or coreference resolution tasks formulated as sentence completions). For seven commonsense reasoning and coreference resolution tasks (see Table 2 in the Appendix), FLAN only outperforms LaMDA-PT on three of the seven tasks. This negative result indicates that when the downstream task is the same as the original language modeling pre-training objective (i.e., in cases where instructions are largely redundant), instruction tuning is not useful. Finally, we report results for sentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which GPT-3 results are not available, in Table 2 and Table 1 in the Appendix. Generally, zero-shot FLAN outperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT. 4 A BLATION S TUDIES & F URTHER A NALYSIS 4.1 N UMBER OF INSTRUCTION TUNING CLUSTERS As the core question of our paper asks how instruction tuning improves a model’s zero-shot perfomance on unseen tasks, in this first ablation we examine how performance is affected by the number of clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For this setup, we hold out NLI, closed-book QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning. 3 We show results for one to seven instruction tuning clusters, where clusters are added in decreasing order of number of tasks per cluster. Figure 6 shows these results. As expected, we observe that average performance across the three held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction tuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for the seven clusters we test, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning. Of note, this ablation does not allow us to draw conclusions about which instruction tuning cluster contributes the most to each evaluation cluster, although we see minimal added value from the sentiment analysis cluster. Performance (%) on held-out cluster Clusters used for instruction tuning 50 70 90 – Average Held-out clusters 30 # clusters: 1 2 3 4 5 6 7 + summarization + translation + read. comp. (# datasets): (11) (20) (26) (30) (34) (37) (39) + sentiment + data to text + coreference + conv. QA – Commonsense NLI Closed-book QA 49.9 55.0 59.3 59.2 60.8 61.9 63.5 Base LM: Commonsense Base LM: NLI Base LM: Closed-book QA Figure 6: Adding additional task clusters to instruction tuning improves zero-shot performance on held-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag, PiQA, and StoryCloze. NLI: ANLI R1–R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARC easy, ARC challenge, Natural Questions, and TriviaQA. 3 We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning in this ablation because they are too similar to NLI and commmonsense reasoning, respectively",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 3 We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning in this ablation because they are too similar to NLI and commmonsense reasoning, respectively. 4.2 S CALING LAWS 0.4B 2B 8B 68B 137B Model Size (# parameters) 30 40 50 60 70 Instruction tuning Untuned model Average zero-shot accuracy on 13 held-out tasks (%) Performance on held-out tasks Figure 7: Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, potetially because all model capacity is used to learn the mixture of instruction tuning tasks. As Brown et al. (2020) shows that zero and few-shot capabilities of language models sustantially improve for larger models, we next explore how the benefits of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters. Figure 7 shows these results. We see that for the two models on the order of 100B prameters, instruction tuning substantially iproves performance on held-out tasks, as is expected given the prior results in our pper. The behavior on held-out tasks for the 8B and smaller models, however, is thoughprovoking—instruction tuning actually hurts performance on held-out tasks. One potential explantion for this result could be that for small-scale models, learning the ∼ 40 tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse on new tasks. Under this potential explanation, for the larger scale models, instruction tuning fills up some model capacity but also teaches these models how to follow instructions, allowing them to generalize to new tasks with the remaining capacity",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4.3 R OLE OF INSTRUCTIONS 20 30 40 50 60 FT: no instruction Eval: instruction FT: dataset name Eval: instruction FT: dataset name Eval: dataset name FT: instruction Eval: instruction (FLAN) 37.3 46.6 47.0 55.2 Zero-shot performance (4 task cluster avg.) Figure 8: Ablation study result using moels with instructions removed from finetuing (FT). In a final ablation study, we explore the role of istructions during finetuning, as one possibility is that performance gains come entirely from multi-task fintuning and the model could perform just as well without instructions. We hence consider two finetuning setups without instructions. In a no template setup, only inputs and outputs were given to the model (e.g., for transltion the input would be “ The dog runs. ” and the output would be “ Le chien court. ”). In a dataset name setup, each input is prepended with the name of the task and dataset (e.g., for translation to French, the input would be “ [Translation: WMT’14 to French] The dog runs. ”). We compare these two ablations to FLAN’s finetuing procedure, which used natural instructions (e.g., “ Please translate this sentence to French: ‘The dog runs.’ ”). We perform evaluations for four held-out cluters from Figure 5. For the no template setup, we used the FLAN instructions during zero-shot inference (because if we used no template, the model would not know what task to perform). For models finetuned on dataset name only, we report zero-shot performance for FLAN instructions as well as using the dataset name. Figure 8 shows the results—both ablation configurations performed substantially worse than FLAN, indicating that training with instructions is crucial for zero-shot performance on unseen tasks. 4.4 I NSTRUCTIONS WITH F EW -S HOT E XEMPLARS So far, we have focused on instruction tuning in the zero-shot setting. Here, we study how instruction tuning can be used when few-shot exemplars are available at inference time. The format for the few-shot setting builds on the zero-shot format",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here, we study how instruction tuning can be used when few-shot exemplars are available at inference time. The format for the few-shot setting builds on the zero-shot format. For some input x and output y , let instruct ( x ) denote the zero-shot instructions. Then, given k few-shot exemplars ( x i , y i ) k i =1 and a new input x , the instruction format for the few-shot setting is “ instruct ( x 1 ) ⊕ y 1 ⊕ instruct ( x 2 ) ⊕ y 2 ⊕ ⊕ instruct ( x k ) ⊕ y k ⊕ instruct ( x ) ”, where ⊕ denotes string concatenation with a delimiter token inserted in between. At both training and inference time, exemplars are randomly drawn from the training set, and the number of exemplars is capped at 16 and such that the total sequence length is less than 960 tokens. Our experiment uses the same task splits and evaluation procedure as 3, such that few-shot exemplars for an unseen task are only used at inference time. As shown in Figure 9, few-shot exemplars improve the performance on all task clusters, compared with zero-shot FLAN. Exemplars are especially effective for tasks with large/complex output spaces, such as struct to text, translation, and closed-book QA, potentially because exemplars help the model better understand the output format. In addition, for all task clusters, standard deviation among templates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering. NLI Read. Comp. Closed-Book QA Commonsense Coreference Translation Zero-shot FLAN Few-shot FLAN Performance 20 40 60 80 54.7 59.3 59.6 60.0 53.7 Struct to text 57.2 31.0 33.0 80.0 80.8 63.8 67.4 39.2 49.4 Task Cluster: # datasets: 7 5 3 4 2 3 4 Figure 9: Adding few-shot exemplars to FLAN is a complementary method for improving the performance of instruction-tuned models. The orange bars indicate standard deviation among templates, averaged at the dataset level for each task cluster",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The orange bars indicate standard deviation among templates, averaged at the dataset level for each task cluster. 4.5 I NSTRUCTION T UNING F ACILITATES P ROMPT T UNING 32 training examples Full training set 100 0 50 75 25 Performance after prompt tuning Instruction-tuned model Untuned model 63.8 78.1 79.1 87.4 Figure 10: Instruction-tuned models respond better to contiuous inputs from prompt tuning. When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning. Perfomance shown is the average on the SuperGLUE dev set. As we’ve seen that instruction tuning improves the ability of a model to respond to instructions, it follows that, if FLAN is indeed more amenable to performing NLP tasks, then it should also achieve better performance when performing inference using soft prompts, represented by prepended continuous variables optimized via prompt tuning (Li & Liang, 2021; Lester et al., 2021). As further analysis, we train continuous prompts for each of the SuperGLUE (Wang et al., 2019a) tasks in accordance with the cluster splits from 2.2 such that when prompt-tuning on task T , no tasks in the same cluster as T were seen during instruction tuning. Our prompt tuning setup follows the procedure of Lester et al. (2021) except that we use a prompt length of 10, weight decay of 1e-4, and did not use dropout on the attention scores; we found in preliminary experiments that these changes improved the performance of LaMDA-PT. Figure 10 shows the results of these prompt tuning experiments for both using a fully-supervised training set and in a low-resource setting with only 32 training examples. We see that in all scnarios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the low-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over prompt tuning on the LaMDA-PT. This result exemplifies in another way how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This result exemplifies in another way how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks. 5 R ELATED W ORK Our work relates to several broad research areas including zero-shot learning, prompting, multi-task learning, and language models for NLP applications (Radford et al., 2019; Raffel et al., 2020; Brown et al., 2020; Efrat & Levy, 2020; Aghajanyan et al., 2021; Li & Liang, 2021, inter alia ). We describe prior work for these broad areas in an extended related work section (Appendix D), and here we describe two subareas narrower in scope that perhaps relate most closely to our work. The way we ask a model to respond to instructions is similar to QA-based task formulation (Kumar et al., 2016; McCann et al., 2018), which aims to unify NLP tasks by casting them as QA over a context. Though these methods are very similar to ours, they mostly focus on multi-task learning instead of zero-shot learning, and—as noted by Liu et al. (2021)—they are generally not motivated by using existing knowledge in pretrained LMs. Moreover, our work supercedes recent work such as Chai et al. (2020) and Zhong et al. (2021) in terms of both model scale and scope of tasks. The success of language models has led to nascent research on the ability of models to follow instructions. Most recently, Mishra et al. (2021) finetune 140M parameter BART on instructions with few-shot exemplars, and evaluate its few-shot abilities on unseen tasks—this is similar to our few-shot instruction tuning result from 4.4. This promising result (as well as one from Ye et al. (2021), which does not emphasize instructions as much) suggests that finetuning on a collection of tasks improves few-shot performance on unseen tasks, even at a smaller model scale. Sanh et al. (2021) finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a model of 11B parameters",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Sanh et al. (2021) finetune T5 in a setup similar to ours, finding that zero-shot learning can be improved in a model of 11B parameters. At a model scale similar to ours, OpenAI’s InstructGPT models are trained via both finetuning and reinforcement learning to produce outputs that are more preferred by human raters (Ouyang et al., 2022). 6 D ISCUSSION Our paper has explored a simple question in zero-shot prompting: does finetuning a model on a collection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize this question via instruction tuning, a simple method that combines appealing aspects of both the pretrain–finetune and prompting paradigms. Our instruction-tuned model, FLAN, improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number of instruction tuning task clusters, and, interestingly, that performance improvements from instruction tuning emerge only with sufficient model scale. Moreover, instruction tuning can be combined with other prompting methods such as few-shot prompting and prompt tuning. The diverse capabilities of language models at scale have drawn attention to the tradeoffs between specialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan et al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might expect labeled data to have the most natural role in improving specialist models, instruction tuning demonstrates how labeled data can be used to help large language models perform many, unseen tasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that task-specific training is complementary to general language modeling and motivates further research on generalist models",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though we try to use accepted categorizations in the literature), and we only explore the use of relatively short instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers). A limitation for our evaluation is that individual examples might have appeared in the models’ pretraining data, which includes web documents, though in post-hoc analysis (Appendix C) we do not find any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN 137B makes it costly to serve. Future work on instruction tuning could include gathering/generating even more task clusters for finetuning, cross-lingual experiments, using FLAN to generate data for training downstream classifiers, and using finetuning to improve model behavior with respect to bias and fairness (Solaiman & Dennison, 2021). 7 C ONCLUSIONS This paper has explored a simple method for improving the ability of language models at scale to perform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares favorably against GPT-3 and signals the potential ability for language models at scale to follow instructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot learning, and using labeled data to improve large language models. E THICAL C ONSIDERATIONS This work uses language models, for which the risks and potential harms are discussed in Bender & Koller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight two relevant ethical considerations",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". First, labeled datasets such as those we use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks. And second, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the benefits and associated risks of such models. E NVIRONMENTAL C ONSIDERATIONS We use the same pretrained language models as Austin et al. (2021). The energy cost and carbon footprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional instruction tuning gradient-steps for finetuning FLAN is less than 2% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller. A UTHOR C ONTRIBUTIONS Maarten Bosma conceived the original idea and implemented the first version of FLAN. Vincent Zhao prototyped the training and evaluation pipelines, as well as rank classification. Kelvin Guu proposed and implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao, and Adams Wei Yu conducted and managed most of the experiments. Jason Wei designed and ran the ablation studies. Jason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. Jason Wei, Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. Vincent Zhao and Kelvin Guu designed, implemented, and conducted the few-shot FLAN experiments. Maarten Bosma and Jason Wei ran the data contamination analysis. Brian Lester ran the prompt tuning experiments. Quoc V. Le and Andrew M. Dai advised, provided high-level guidance, and helped edit the paper. A CKNOWLEDGEMENTS We thank Ed Chi, Slav Petrov, Dan Garrette, Ruibo Liu, and Clara Meister for providing feedback on our manuscript. We thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer for helping debug some of our models",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer for helping debug some of our models. We thank Ellie Pavlick for feedback on the study design during the middle stages of the project. We thank Daniel De Freitas Adiwardana for helping initiate the project, large language model advising, and giving us access to some computational resources. Finally, we thank the team involved in pretraining LaMDA-PT: Daniel De Freitas Adiwardana, Noam Shazeer, Yanping Huang, Dmitry Lepikhin, Dehao Chen, Yuanzhong Xu and Zhifeng Chen. R EFERENCES Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. arXiv preprint arXiv:2101.11038 , 2021. URL https://arxiv.org/abs/2101.11038 . Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019 , 2019. URL https://arxiv.org/abs/1907.05019 . Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 , 2021. URL https://arxiv.org/abs/ 2108.07732 . Amittai Axelrod, Xiaodong He, and Jianfeng Gao. Domain adaptation via pseudo in-domain data selection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing , pp. 355–362, 2011. URL https://aclanthology.org/D11-1033 . Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl: Web-scale acquisition of parallel corpora",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 4555–4567, 2020. URL https://aclanthology.org/2020.acl-main.417 . Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and undestanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 5185–5198, 2020. URL https://aclanthology.org/ 2020.acl-main.463 . Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency , FAccT ’21, pp. 610–623. Association for Computing Machinery, 2021. URL https://doi.org/10.1145/3442188. 3445922 . Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The Fifth PASCAL Recognizing Textual Entailment Challenge. In TAC , 2009. URL https://citeseerx.ist.psu.edu/ viewdoc/download?doi=10.1.1.232.1231&rep=rep1&type=pdf . Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence , 2020. URL https://arxiv.org/abs/1911.11641 . Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, and Lucia Specia (eds.). Proceedings of the Ninth Workshop on Statistical Machine Translation , 2014. URL https://aclanthology.org/W14-3300 . Ondˇrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Pavel Pecina, Martin Popel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, Jörg Tiedemann, and Marco Turchi (eds.). Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers , 2016. URL https://aclanthology.org/W16-2200",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers , 2016. URL https://aclanthology.org/W16-2200 . Rishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, E. Brynjolfsson, S. Buch, D. Card, Rodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S. Ermon, J. Etchemendy, Kawin Ethayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Gooman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Iabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P. Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, D. Narayanan, Ben Newman, Allen Nie, J. C. Niebles, H. Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher R’e, D. Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021. URL https://arxiv.org/abs/2108.07258 . Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". arXiv preprint arXiv:2108.07258 , 2021. URL https://arxiv.org/abs/2108.07258 . Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 632–642, 2015. URL https: //aclanthology.org/D15-1075 . Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Prcessing Systems , volume 33, pp. 1877–1901, 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf . Duo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei Li. Description based text classification with reinforcement learning. In Proceedings of the International Conference on Machine Learning , pp. 1371–1382. PMLR, 2020. URL http://proceedings.mlr.press/v119/chai20a/ chai20a.pdf . Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021. URL https://arxiv.org/abs/2107. 03374 . Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pp. 2174–2184, 2018. URL https: //aclanthology.org/D18-1241 . Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2174–2184, 2018. URL https: //aclanthology.org/D18-1241 . Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 2924–2936, 2019a. URL https://aclanthology.org/N19-1300 . Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V. Le. BAM! born-again multi-task networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 5931–5937, 2019b. URL https://aclanthology.org/P19-1595 . Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457 , 2018. URL https://arxiv.org/abs/1803.05457 . Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research , 12:2493–2537, 2011. URL https://www.jmlr.org/papers/volume12/ collobert11a/collobert11a.pdf . Michele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli, and Serena Villata. Hybrid emojbased masked language models for zero-shot abusive language detection. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 943–949, 2020. URL https: //aclanthology.org/2020.findings-emnlp.84 . Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entailment challenge. In Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entaiment , MLCW’05, pp. 177–190, 2005. URL https://doi.org/10.1007/11736790_9 . Andrew M Dai and Quoc V Le",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 177–190, 2005. URL https://doi.org/10.1007/11736790_9 . Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Proceedings of the Confeence on Neural Information Processing Systems , 2015. URL https://papers.nips.cc/ paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf . Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigating projection in naturally occurring discourse. In Proceedings of Sinn und Bedeutung , pp. 107–124, 2019. URL https://ojs.ub.uni-konstanz.de/sub/index.php/sub/ article/view/601 . Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Coference of the North American Chapter of the Association for Computational Linguistics: Hman Language Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186, 2019. URL https://aclanthology.org/N19-1423 . William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL https://aclanthology.org/I05-5002 . Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905 , 2021. URL https:// arxiv.org/pdf/2112.06905 . Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Coputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 2368–2378, 2019. URL https://aclanthology.org/N19-1246 . Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine translation systems for WMT-14",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2368–2378, 2019. URL https://aclanthology.org/N19-1246 . Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. Edinburgh’s phrase-based machine translation systems for WMT-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation , pp. 97–104, 2014. URL https://aclanthology.org/W14-3309 . Ondˇrej Dušek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural language generation. In Proceedings of the 12th International Conference on Natural Language Generation , pp. 421–426, 2019. URL https://aclanthology.org/W19-8652 . Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pp. 489–500, 2018. URL https://aclanthology.org/D18-1045 . Avia Efrat and Omer Levy. The Turking Test: Can language models understand instructions? arXiv preprint arXiv:2010.11982 , 2020. URL https://arxiv.org/abs/2010.11982 . Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 1074–1084, 2019. URL https://aclanthology.org/P19-1102 . Fast.AI. Yelp Sentiment Classification Dataset. https://course.fast.ai/datasets . William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 , 2021. URL https: //arxiv.org/abs/2101.03961 . Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the International Conference on Machine Learning (ICML) , pp. 1126–1135, 2017. URL https://arxiv.org/abs/1703.03400 . Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1126–1135, 2017. URL https://arxiv.org/abs/1703.03400 . Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguitics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 3816–3830, 2021. URL https://aclanthology.org/2021.acl-long. 295 . Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation , pp. 124–133, 2017. URL https://aclanthology.org/ W17-3518 . Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Auoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Sathanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021) , pp. 96–120, 2021. URL https://aclanthology.org/2021.gem-1.10 . Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 96–120, 2021. URL https://aclanthology.org/2021.gem-1.10 . Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing , pp. 1–9, 2007. URL https://aclanthology.org/W07-1401 . Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A humaannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization , pp. 70–79, 2019. URL https://aclanthology.org/ D19-5409 . Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N project report, Stanford , 1(12):2009, 2009. URL https://www-cs.stanford. edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf . Dan Goldwasser and Dan Roth. Learning from natural instructions. Machine learing , 94(2):205–232, 2014. URL https://link.springer.com/article/10.1007/ s10994-013-5407-y . Max Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 708–719, 2018. URL https://aclanthology.org/N18-1065 . R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment , 2006. URL http://www.cs.biu.ac.il/~szpekti/papers/RTE2-organizers.pdf . Luheng He, Mike Lewis, and Luke Zettlemoyer. Question-answer driven semantic role labeling: Using natural language to annotate natural language. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 643–653, 2015. URL https: //aclanthology.org/D15-1076",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pp. 643–653, 2015. URL https: //aclanthology.org/D15-1076 . Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn’t always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , 2021. URL https: //aclanthology.org/2021.emnlp-main.564 . Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. Tward semantics-based answer pinpointing. In Proceedings of the First International Confeence on Human Language Technology Research , 2001. URL https://www.aclweb.org/ anthology/H01-1069 . Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 328–339, 2018. URL https://aclanthology.org/P18-1031 . Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 2391–2401, 2019. URL https:// aclanthology.org/D19-1243 . Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’s multilingual neural machine translation system: Enabling zero-shot transltion. Transactions of the Association for Computational Linguistics , 5:339–351, 2017. URL https://aclanthology.org/Q17-1024 . Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". URL https://aclanthology.org/Q17-1024 . Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601– 1611, 2017. URL https://aclanthology.org/P17-1147 . Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Comptational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 252–262, 2018. URL https://aclanthology.org/N18-1023 . Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 1896–1907, 2020. URL https://aclanthology.org/2020.findings-emnlp.171 . Dimitrios Kotzias, Misha Denil, Nando de Freitas, and Padhraic Smyth. From group to individual labels using deep features. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2015. URL https://dl.acm.org/doi/10. 1145/2783258.2783380 . Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018 , pp. 66–71. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-2012. URL https://doi.org/10.18653/v1/d18-2012 . Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". URL https://doi.org/10.18653/v1/d18-2012 . Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In Proceedings of the International Conference on Machine Learning , pp. 1378–1387. PMLR, 2016. URL https://arxiv.org/abs/1506.07285 . Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics , 7:452–466, 2019. URL https://aclanthology. org/Q19-1026 . Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Assocation for Computational Linguistics: EMNLP 2020 , pp. 4034–4048, 2020. URL https: //aclanthology.org/2020.findings-emnlp.360 . Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pp. 951–958. IEEE, 2009. URL https://ieeexplore.ieee.org/ document/5206594 . Anne Lauscher, Vinit Ravishankar, Ivan Vuli ́c, and Goran Glavaš. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 4483–4499, 2020. URL https://aclanthology.org/2020.emnlp-main.363 . Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 6086–6096, 2019",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 6086–6096, 2019. URL https://aclanthology.org/ P19-1612 . Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=qrwe7XHTmYb . Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing , 2021. URL https://arxiv.org/abs/2104.08691 . Hector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning , 2012. URL https://dl.acm.org/doi/10.5555/3031843.3031909 . Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017) , pp. 333–342, 2017. URL https://aclanthology. org/K17-1034 . Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 7871–7880, 2020. URL https://aclanthology.org/2020.acl-main.703 . Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pp. 4582–4597, 2021",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4582–4597, 2021. URL https://aclanthology.org/2021.acl-long.353 . Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. A unified MRC framework for named entity recognition. In Proceedings of the 58th Annual Meeing of the Association for Computational Linguistics , pp. 5849–5859, 2020. URL https: //aclanthology.org/2020.acl-main.519 . Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Confeence on Computational Linguistics , 2002. URL https://www.aclweb.org/anthology/ C02-1150 . Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp. 1823–1840, 2020. URL https://aclanthology.org/2020.findings-emnlp.165 . Han Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li, Xiao-Ming Wu, and Albert Y.S. Lam. Reconstructing capsule networks for zero-shot intent classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 4799–4809, 2019a. URL https://aclanthology.org/D19-1486 . Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhenbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586 , 2021. URL https://arxiv.org/abs/2107.13586 . Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4487–4496, 2019b. URL https://aclanthology.org/ P19-1441 . Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics , 8:726–742, 2020. URL https: //aclanthology.org/2020.tacl-1.47 . Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. Proceedings of ICLR , 2016. URL https://arxiv.org/ abs/1511.06114 . Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http: //www.aclweb.org/anthology/P11-1015 . Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730 , 2018. URL https://arxiv.org/abs/1806.08730 . John McCarthy. Programs with common sense . RLE and MIT computation center, 1960. URL http://jmc.stanford.edu/articles/mcc59/mcc59.pdf . Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pp. 2381–2391, 2018. URL https://aclanthology.org/D18-1260 . Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943 , 2021. URL https://arxiv.org/abs/2110. 15943 . Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural Instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773 , 2021. URL https://arxiv.org/abs/2104.08773",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Natural Instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773 , 2021. URL https://arxiv.org/abs/2104.08773 . Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vandewende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 839–849, 2016. URL https://aclanthology.org/N16-1098 . Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: Open-domain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 432–447, 2021. URL https://aclanthology.org/ 2021.naacl-main.37 . Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated Gigaword. In Prceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX) , pp. 95–100, 2012. URL https://aclanthology. org/W12-3018 . Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pp. 1797–1807, 2018. URL https://aclanthology.org/D18-1206 . Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". URL https://aclanthology.org/D18-1206 . Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 4885–4901, 2020. URL https: //aclanthology.org/2020.acl-main.441 . Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Preprint , 2022. URL https://cdn.openai.com/papers/Training_language_models_ to_follow_instructions_with_human_feedback.pdf . Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 2227–2237, 2018. URL https: //aclanthology.org/N18-1202 . Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and Alexander Waibel. Improving zero-shot translation with language-independent constraints. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers) , pp. 13–23, 2019. URL https://aclanthology. org/W19-5202 . Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Confeence of the North American Chapter of the Association for Computational Linguistics: Hman Language Technologies, Volume 1 (Long and Short Papers) , pp. 1267–1273, 2019. URL https://aclanthology.org/N19-1128 . Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1267–1273, 2019. URL https://aclanthology.org/N19-1128 . Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert. Massively multilingual ASR: 50 languages, 1 model, 1 billion parameters. arXiv preprint arXiv:2007.03001 , 2020. URL https://arxiv.org/abs/2007. 03001 . Guanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT) , pp. 5203–5212, 2021. URL http://cs.jhu.edu/~jason/papers/#qin-eisner-2021 . Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. https://blog.openai.com/ language-unsupervised , 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/ language_models_are_unsupervised_multitask_learners.pdf . Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020. URL http://jmlr.org/papers/v21/20-074.html . Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: The Winograd schema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp. 777–789, 2012. URL https://aclanthology.org/D12-1071 . Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383–2392, 2016. URL https://aclanthology.org/ D16-1264",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp. 2383–2392, 2016. URL https://aclanthology.org/ D16-1264 . Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pp. 784–789, 2018. URL https://aclanthology. org/P18-2124 . Siva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering challenge. Transactions of the Association for Computational Linguistics , 7:249–266, 2019. URL https://aclanthology.org/Q19-1016 . Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. A recipe for arbitrary text style transfer with large language models. arXiv preprint arXiv:2109.03910 , 2021. URL https://arxiv.org/abs/2109.03910 . Melissa Roemmele, Cosmin Bejan, and Andrew Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium Series , 2011. URL https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418 . Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot learning. In Proceedings of the International Conference on Machine Learning , pp. 2152–2161, 2015. URL https://proceedings.mlr.press/v37/romera-paredes15.pdf . Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098 , 2017. URL https://arxiv.org/abs/1706.05098 . Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An advesarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artificial Intelligence , pp. 8732–8740, 2020. URL https://arxiv.org/abs/1907.10641 . Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Multitask prompted training enables zero-shot task generalization. Proceedings of the International Conference on Learning Representations , 2021. URL https://arxiv.org/abs/2110.08207 . David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. Proceedings of the International Conference on Learning Representations , 2019. URL https://arxiv.org/pdf/1904.01557 . Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , pp. 255–269, 2021. URL https://aclanthology.org/2021.eacl-main.20 . Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1073–1083, 2017. URL https:// aclanthology.org/P17-1099 . Rico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems for WMT 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers , pp. 371–376, 2016. URL https://aclanthology.org/W16-2323 . Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning , pp. 4596–4604. PMLR, 2018. URL https://arxiv.org/abs/1804.04235 . Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp. 1631–1642, 2013. URL https://aclanthology.org/D13-1170 . Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. arXiv preprint arXiv:2106.10328 , 2021",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Irene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with values-targeted datasets. arXiv preprint arXiv:2106.10328 , 2021. URL https://arxiv.org/ abs/2106.10328 . Shashank Srivastava, Igor Labutov, and Tom Mitchell. Zero-shot learning of classifiers from natural language quantification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 306–316, 2018. URL https:// aclanthology.org/P18-1029 . Derek Tam, Menton Rakesh R., Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving and simplifying pattern exploiting training. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2021. URL https://arxiv.org/pdf/ 2103.11955 . Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 , 2022. URL https://arxiv.org/pdf/ 2201.08239 . Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548 , 2018. URL https://arxiv.org/abs/1810.03548 . Marc Velay and Fabrice Daniel. Seq2seq and multi-task learning for joint intent and content extraction for domain specific interpreters. arXiv preprint arXiv:1808.00423 , 2018. URL https: //arxiv.org/abs/1808.00423 . Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353–355, 2018. URL https://aclanthology.org/W18-5446 . Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Conference on Neural Information Processing Systems (NeurIPS) , 2019a",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Superglue: A stickier benchmark for general-purpose language understanding systems. Conference on Neural Information Processing Systems (NeurIPS) , 2019a. URL https://arxiv.org/abs/1905.00537 . Lu Wang and Wang Ling. Neural network-based abstract generation for opinions and arguments. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp. 47–57, 2016. URL https: //www.aclweb.org/anthology/N16-1007 . Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multagent dual learning. In Proceedings of the International Conference on Learning Representations (ICLR) 2019 , 2019b. URL https://openreview.net/forum?id=HyGhN2A5tm . Zirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. Towards zero-label language learning. arXiv preprint arXiv:2109.09193 , 2021. URL https://arxiv.org/abs/2109.09193 . Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics , 7:625–641, 2019. doi: 10.1162/ tacl_a_00290. URL https://aclanthology.org/Q19-1040 . Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 , 2022. URL https://arxiv.org/pdf/2201.11903 . Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for setence understanding through inference. In Proceedings of the 2018 Conference of the North Amercan Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 1112–1122, 2018. URL http://aclweb.org/anthology/ N18-1101 . Joseph Worsham and J. Kalita. Multi-task learning for natural language processing in the 2020s: where are we going? arXiv preprint arXiv:2007.16008 , 2020. URL https://arxiv.org/ abs/2007.16008",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Joseph Worsham and J. Kalita. Multi-task learning for natural language processing in the 2020s: where are we going? arXiv preprint arXiv:2007.16008 , 2020. URL https://arxiv.org/ abs/2007.16008 . Jeff Wu, Long Ouyang, Daniel M Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, and Paul Chritiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862 , 2021. URL https://arxiv.org/abs/2109.10862 . Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li. CorefQA: Coreference resolution as query-based span prediction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp. 6953–6963, 2020. URL https://aclanthology.org/ 2020.acl-main.622 . Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossfit: A few-shot learning challenge for cross-task generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP) , 2021. URL https://arxiv.org/abs/2104.08835 . Wenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pp. 3914–3923, 2019. URL https://aclanthology.org/ D19-1404 . Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp. 4791–4800, 2019. URL https://aclanthology.org/ P19-1472 . Rui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 2019. URL https://aclanthology.org/P19-1043 . Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". URL https://aclanthology.org/P19-1043 . Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR , abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885 . Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clasification. In NIPS , 2015. URL https://proceedings.neurips.cc/paper/2015/ file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf . Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 1298–1308, 2019. URL https://aclanthology.org/N19-1131 . Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Meta-tuning language models to answer prompts better. arXiv preprint arXiv:2104.04670 , 2021. URL https://arxiv.org/abs/ 2104.04670 . A A DDITIONAL R ESULTS This section shows the full results for all datasets we evaluate. Results for translation and struct to text are shown in Table 1, and the results for eight NLU task clusters are shown in Table 2. We show FLAN’s performance using the best of up to ten instruction templates as well as the template with the best performance on the dev set. For LaMDA-PT, we use the templates from Brown et al. (2020), which were optimized for GPT-3, without performing any prompt engineering to optimize them on our model. For simplicity, we use greedy search for all generative tasks (compared with beam search used in Brown et al. (2020)). Unlike GPT-3, which chooses the number of few-shot exemplars k via best dev set performance, for few-shot LaMDA-PT we choose the highest k that fits in the context length of 1024 tokens, from k ∈{ 1 , 3 , 5 , 10 } . For DROP (Dua et al., 2019) and SQuADv2 (Rajpurkar et al., 2018), based on email correspondence with Brown et al",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For DROP (Dua et al., 2019) and SQuADv2 (Rajpurkar et al., 2018), based on email correspondence with Brown et al. (2020), their definition of zero-shot differs from ours in that they actually use exemplars, but only from the same passage as the inference question (each passage has more than one question). Hence, GPT-3 zero-shot results are not directly comparable with ours for DROP and SQuADv2. We mark these results using the † symbol. Moreover, it is unclear how to parse the end of an answer for these two datasets, and so we use curly bracket delimiters { and } , where we expect } to indicate the end of the answer. For struct to text, reported T5/mT5 results are from the GEM benchmark paper (Gehrmann et al., 2021), though we do not report their results for DART (through correspondence with authors, we confirmed that their results for DART were incorrect). Though we use a summarization task cluster during instruction tuning, we leave evaluation of summarization for future work, as the mean input of most summarization datasets exceeds FLAN’s input length of 1024 tokens",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". FLAN 137B LaMDA-PT GPT-3 175B zero-shot few-shot Metric Supervised Model zershot feshot [ k ] zershot feshot [ k ] average template best dev template average template best dev template [ k ] # t T RANSLATION WMT ’14 En → Fr BLEU 35.0 d 11.2 31.5 [5] 25.2 32.6 [64] 32.9 ± 1.1 33.9 33.9 ± 0.2 33.8 [9] 5 WMT ’14 Fr → En BLEU 45.6 c 7.2 34.7 [5] 21.2 39.2 [64] 35.5 ± 1.3 35.9 38.0 ± 0.1 37.9 [9] 3 WMT ’16 En → De BLEU 38.6 f 7.7 26.7 [5] 24.6 29.7 [64] 25.4 ± 1.8 27.0 26.8 ± 0.4 26.1 [11] 5 WMT ’16 De → En BLEU 41.2 e 20.8 36.8 [5] 27.2 40.6 [64] 38.9 ± 0.3 38.9 40.6 ± 0.1 40.7 [11] 3 WMT ’16 En → Ro BLEU 39.9 g 3.5 22.9 [5] 14.1 21.0 [64] 16.7 ± 1.6 18.9 20.5 ± 0.1 20.5 [9] 5 WMT ’16 Ro → En BLEU 38.5 g 9.7 37.5 [5] 19.9 39.5 [64] 36.8 ± 0.5 37.3 38.2 ± 0.1 38.1 [9] 3 S TRUCT TO T EXT CommonGen Rouge-1 64.0 a 3.9 56.7 [3] – – 54.6 ± 2.3 56.3 56.6 ± 0.3 56.4 [16] 6 Rouge-2 29.4 a 1.5 29.6 [3] – – 28.8 ± 2.4 27.6 30.9 ± 0.7 29.9 [16] 6 Rouge-L 54.5 a 3.2 48.5 [3] – – 48.4 ± 1.9 48.7 50.7 ± 0.2 51.0 [16] 6 DART Rouge-1 – 11.3 56.0 [3] – – 45.5 ± 4.2 48.9 57.9 ± 1.6 59.2 [11] 7 Rouge-2 – 1.5 29.6 [3] – – 25.0 ± 3.7 30.0 35.8 ± 1.0 36.2 [11] 7 Rouge-L – 3.2 48.5 [3] – – 38.4 ± 3.8 43.4 48.5 ± 0.9 48.2 [11] 7 E2ENLG Rouge-1 72.6 a 6.2 56.7 [3] – – 44.8 ± 3.9 51.4 59.1 ± 1.3 59.7 [12] 9 Rouge-2 47.5 a 2.5 31.4 [3] – – 24.2 ± 3.6 30.1 33.2 ± 1.1 33.6 [12] 9 Rouge-L 56.4 a 4.9 41.1 [3] – – 37.0 ± 3.5 42.4 44.9 ± 0.8 45.1 [12] 9 WebNLG Rouge-1 83.5 a 13.9 68.3 [3] – – 50.6 ± 4.7 57.7 68.5 ± 2.2 71.2 [10] 8 Rouge-2 63.6 a 6.9 46.0 [3] – – 29.8 ± 4.2 35.4 48.0 ± 1.5 49.8 [10] 8 Rouge-L 71.0 a 11.8 56.5 [3] – – 43.4 ± 4.5 49.7 58.8 ± 1.1 60.2 [10] 8 Table 1: Results for translation and struct-to-text tasks. [ k ] indicates the number of few-shot exemplars. # t indicates the number of templates that FLAN is evaluated on. a T5-11B, c Edunov et al. (2018), d Durrani et al. (2014), e Wang et al. (2019b), f Sennrich et al. (2016), g Liu et al. (2020)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". # t indicates the number of templates that FLAN is evaluated on. a T5-11B, c Edunov et al. (2018), d Durrani et al. (2014), e Wang et al. (2019b), f Sennrich et al. (2016), g Liu et al. (2020). FLAN 137B GLaM LaMDA-PT GPT-3 175B zero-shot few-shot Random Guess Supervised Model zershot onshot zershot feshot [ k ] zershot feshot [ k ] average template best dev template average template best dev template [ k ] # t NLI ANLI R1 33.3 57.4 b 40.9 42.4 39.6 39.0 [5] 34.6 36.8 [50] 47.7 ± 1.4 46.4 44.2 ± 2.3 47.9 [6] 8 ANLI R2 33.3 48.3 b 38.2 40.0 39.9 37.5 [5] 35.4 34.0 [50] 43.9 ± 1.3 44.0 41.6 ± 1.4 41.1 [6] 8 ANLI R3 33.3 43.5 b 40.9 40.8 39.3 40.7 [5] 34.5 40.2 [50] 47.0 ± 1.3 48.5 42.8 ± 2.2 46.8 [6] 8 CB 33.3 93.6 a 33.9 73.2 42.9 34.4 [5] 46.4 82.1 [32] 64.1 ± 14.7 83.9 82.6 ± 4.4 82.1 [7] 10 MNLI-m 33.3 92.2 a – – 35.7 43.7 [5] – – 51.1 ± 6.2 61.2 60.8 ± 3.7 63.5 [10] 10 MNLI-mm 33.3 91.9 a – – 37.0 43.8 [5] – – 51.0 ± 6.5 62.4 61.0 ± 3.5 63.5 [10] 10 QNLI 50.0 96.9 a – – 50.6 55.7 [5] – – 59.6 ± 4.9 66.4 62.0 ± 1.7 63.3 [12] 9 RTE 50.0 92.5 a 68.8 71.5 73.3 70.8 [5] 63.5 72.9 [32] 78.3 ± 7.9 84.1 79.9 ± 6.9 84.5 [8] 10 SNLI 33.3 91.3 b – – 33.3 54.7 [5] – – 43.0 ± 7.4 53.4 62.3 ± 2.4 65.6 [15] 9 WNLI 50.0 94.5 a – – 56.3 64.8 [5] – – 61.0 ± 10.6 74.6 55.4 ± 11.0 70.4 [14] 10 R EADING C OMP",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". BoolQ 50.0 91.2 a 83.0 82.8 81.0 80.0 [1] 60.5 77.5 [32] 80.2 ± 3.1 82.9 83.6 ± 0.8 84.6 [4] 9 DROP – 80.5 b 54.9 55.2 3.8 10.3 [1] 23.6 † 36.5 [20] 21.9 ± 0.9 22.7 22.3 ± 1.1 23.9 [2] 7 MultiRC – 88.1 a 45.1 62.0 60.0 59.6 [5] 72.9 74.8 [32] 74.5 ± 3.7 77.5 69.2 ± 3.2 72.1 [1] 8 OBQA 25.0 85.4 a 53.0 55.2 41.8 50.6 [10] 57.6 65.4 [100] 77.4 ± 1.3 78.4 77.2 ± 1.3 78.2 [16] 7 SQuADv1 – 96.2 a – – 22.7 50.2 [3] – – 79.5 ± 1.6 80.1 82.1 ± 0.5 82.7 [4] 8 SQuADv2 – 83.4 b 68.3 70.0 11.1 34.9 [3] 59.5 † 69.8 [16] 40.9 ± 1.8 44.2 40.8 ± 0.9 43.1 [3] 10 C LOSED -B OOK QA ARC-c 25.0 81.1 a 48.2 50.3 42.0 49.4 [10] 51.4 51.5 [50] 61.7 ± 1.4 63.1 63.7 ± 0.6 63.8 [13] 7 ARC-e 25.0 92.6 a 71.9 76.6 76.4 80.9 [10] 68.8 70.1 [50] 79.5 ± 0.8 79.6 80.5 ± 0.5 80.7 [14] 7 NQ – 36.6 a 21.5 23.9 3.2 22.1 [5] 14.6 29.9 [64] 18.6 ± 2.7 20.7 27.2 ± 0.5 27.6 [16] 10 TQA (wiki) – 60.5 a 68.8 71.5 21.9 63.3 [10] 64.3 71.2 [64] 66.5 ± 2.6 68.1 66.5 ± 1.0 67.3 [16] 10 TQA (tfds-dev) – 51.0 a – – 18.4 55.1 [10] – – – 55.0 ± 2.3 56.7 57.2 ± 0.6 57.8 [16] 10 C OMMONSENSE COPA 50.0 94.8 a 90.0 92.0 90.0 89.0 [10] 91.0 92.0 [32] 90.6 ± 2.0 91.0 88.5 ± 3.8 87.0 [16] 8 HellaSwag 25.0 47.3 b 77.1 76.8 57.0 58.8 [10] 78.9 79.3 [20] 56.4 ± 0.5 56.7 59.4 ± 0.2 59.2 [3] 8 PIQA 50.0 66.8 b 80.4 81.4 80.3 ∗ 80.2 ∗ [10] 81.0 82.3 [50] 80.9 ∗ ± 0.8 80.5 ∗ 82.1 ∗ ± 0.3 81.7 ∗ [10] 8 StoryCloze 50.0 89.2 b 82.5 84.0 79.5 83.7 [10] 83.2 87.7 [70] 92.2 ± 1.3 93.4 93.3 ± 0.9 94.7 [10] 8 S ENTIMENT IMDB 50.0 95.5 b – – 76.9 83.3 [1] – – 94.1 ± 0.4 94.3 94.8 ± 0.3 95.0 [2] 7 Sent140 50.0 87.0 b – – 41.4 63.3 [5] – – 69.9 ± 2.5 73.5 68.7 ± 1.2 69.3 [16] 6 SST-2 50.0 97.5 a – – 51.0 92.3 [5] 71.6 95.6 [8] 92.6 ± 1.7 94.6 94.4 ± 0.8 94.6 [16] 8 Yelp 50.0 98.1 b – – 84.7 89.6 [3] – – 97.8 ± 0.2 98.1 97.9 ± 0.1 98.0 [4] 7 P ARAPHRASE MRPC 50.0 90.4 a – – 53.7 64.0 [5] – – 69.1 ± 1.3 69.1 67.5 ± 1.7 67.2 [10] 10 QQP 50.0 90.6 a – – 34.9 58.9 [3] – – 72.1 ± 6.8 75.9 73.5 ± 2.9 75.9 [16] 7 PAWS Wiki 50.0 91.9 a – – 45.5 53.5 [5] – – 61.5 ± 6.5 69.4 66.2 ± 0.9 70.2 [10]",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": "– 69.1 ± 1.3 69.1 67.5 ± 1.7 67.2 [10] 10 QQP 50.0 90.6 a – – 34.9 58.9 [3] – – 72.1 ± 6.8 75.9 73.5 ± 2.9 75.9 [16] 7 PAWS Wiki 50.0 91.9 a – – 45.5 53.5 [5] – – 61.5 ± 6.5 69.4 66.2 ± 0.9 70.2 [10] 10 C OREFERENCE DPR 50.0 84.8 b – – 54.6 57.3 [5] – – 60.3 ± 3.5 66.8 62.4 ± 1.6 63.3 [16] 10 Winogrande 50.0 65.8 b 73.4 73.0 68.3 68.4 [10] 70.2 77.7 [50] 67.3 ± 2.5 71.2 72.3 ± 0.9 72.8 [16] 10 WSC273 50.0 70.0 b 86.8 83.9 81.0 61.5 [5] 88.3 88.5 [32] 80.8 ± 3.7 – – ± – – [ – ] 10 R EAD",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". C OMP . W / C OMMONSENS E CosmosQA 25.0 67.1 b – – 34.1 33.8 [5] – – 58.4 ± 1.3 60.6 56.7 ± 1.3 56.0 [5] 8 ReCoRD – 93.4 a 90.3 90.3 87.8 ∗ 87.6 ∗ [1] 90.2 89.0 [32] 67.8 ∗ ± 3.0 72.5 ∗ 77.0 ∗ ± 2.0 79.0 ∗ [1] 10 Table 2: Results for eight NLU task clusters. All values shown are for accuracy (or exact match) except DROP, MultiRC, and SQuAD v1 and v2, which are F1. [ k ] indicates the number of few-shot exemplars. # t indicates the number of templates that FLAN is evaluated on. a T5-11B, b BERT-large. ∗ see data contamination (Appendix C). WSC273 (Levesque et al., 2012) does not have training or validation sets, and so we do not compute few-shot results for FLAN. For Trivia QA (TQA), we report exact match (EM) on both the wikipedia subset of the dev set to compare with GPT-3, as well as the full TFDS dev set. B F URTHER A BLATION S TUDIES AND A NALYSIS B.1 D ATASETS PER T ASK C LUSTER & T EMPLATES PER D ATASET Our primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on unseen tasks. 4.1 showed that adding more task clusters improves performance; here, we further explore whether adding additional datasets improves performance when the number of task clusters is held constant. We use the same split as in 4.1, where the NLI, commonsense reasoning, and closed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning. For these seven task clusters, we instruction tune models using just one dataset per task cluster and using four datasets per task cluster (for task clusters that did not have four tasks, we just used all available tasks). In addition, we simultaneously explore the role of the number of instruction templates per dataset; as mentioned in 2.1, for each dataset we manually composed ten instructional templates for instruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset. Figure 11 shows these results",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here, we instruction tune models using 1, 4, and 10 templates per dataset. Figure 11 shows these results. Using more datasets per cluster improved performance by almost 10% on average across the three held-out clusters. Using more templates per dataset, however, had a comparatively negligible effect on performance when there was one task per cluster, which disappeared when there were four tasks per cluster. The small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overfitting to any particular template. This results serves to underscore, however, the unpredictability of finetuning large language models, as one hypothesis is that models at such scale do not easily overfit to a finetuning single task. Figure 11: Effect of datasets per task cluster and templates per dataset on performance on three held-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task cluster substantially improves performance. Using more templates per dataset, however, only had a very small effect on performance, which disappeared when there were sufficient dataset per task cluster. B.2 R OLE OF INSTRUCTIONS DURING FINETUNING The per-cluster results for the ablation study from 4.3 are shown in Table 3. B.3 F URTHER A NALYSIS : I NSTRUCTION T UNING F ACILITATES P ROMPT T UNING The per-dataset results for the analysis in 4.5 are given in Table 4. As the above tasks are all classification, further work in this direction might include tasks such as summarization or question answering, or try to finetune the model using the supervised datasets. C D ATA C ONTAMINATION A NALYSIS One reasonable concern is that since the pretraining corpus of FLAN has more than 2 trillion tokens, it is possible that examples from a given evaluation dataset may have already been seen verbatim by the model during pre-training, hence inflating the performance of our purported zero-shot model",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To this end, like GPT-3 (Brown et al., 2020), we perform post-hoc data contamination analysis to Zero-shot performance on unseen task cluster Finetuning prompt Inference prompt NLI Read. Comp. CloseBook QA Translation Four-Task Average Natural instructions (= FLAN) Natural instructions 56.2 77.4 56.6 30.7 55.2 No template Natural instructions 50.5 58.2 25.5 15.0 37.3 Task/dataset name Natural instructions 52.8 63.0 44.8 25.9 46.6 Task/dataset name Task/dataset name 60.2 64.9 40.8 21.9 47.0 Table 3: Ablation study result using models where instructions are removed from the finetuning process. In “no template,” only inputs and outputs are given, which does not distinguish among tasks during multi-task finetuning. In “task/dataset name”, inputs during multi-task finetuning are prepended with the name of the task and dataset (e.g., “[Translation: WMT’14 to French] The dog runs” ) NLI datasets: ANLI R1–R3, CB, and RTE; reading comprehension datasets: BoolQ, MultiRC, and OpenbookQA; closed-book QA datasets: ARC-c, ARC-e, NQ, and TQA; translation datasets: WMT’14 Fr ↔ En, WMT’16 De ↔ En, and WMT’16 Ro ↔ En. Notably, training with task/dataset name achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for which the validation set only has 56 examples (FLAN also gets 83.9 with the best dev template, but the average template was only 64.1). P ROMPT T UNING A NALYSIS Prompt tuning train. examples BoolQ acc. CB acc. CoPA acc. MultiRC F1 ReCoRD acc. RTE acc. WiC acc. WSC acc. LaMDA-PT 32 55.5 55.4 87.0 65.4 78.0 52.4 51.6 65.4 FLAN 77.5 87.5 91.0 76.8 80.8 83.0 57.8 70.2 LaMDA-PT full dataset 82.8 87.5 90.0 78.6 84.8 82.0 54.9 72.7 FLAN 86.3 98.2 94.0 83.4 85.1 91.7 74.0 86.5 Table 4: FLAN (instruction tuning) responds better to continuous inputs attained via prompt tuning than LaMDA-PT (no instruction tuning). When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning. investigate whether the performance of the model is in fact inflated by evaluating on examples that occurred in the pretraining dataset. Our data contamination procedure follows the setup of Brown et al. (2020), which, for each bencmark, produces a “clean” version that removes all potentially leaked examples, defined as examples for which any n -gram ( n varies per dataset but is roughly 13) overlapped with anything in the pretraining corpus. We use the same n per dataset as Brown et al. (2020) and also split on spaces. We then evaluate our model on this clean subset, comparing against model performance on the original dataset (clean + dirty). Lower performance on the clean subset would suggest that data contamination leads to inflated results. Figure 12 summarizes these results, with the exact numbers given in Table 5. We see several trends very similar to those in the GPT-3 paper: (1) many datasets had a substantial number of examples that overlapped with the pretraining data, (2) across all datasets, we do not see a correlation that evaluating on clean data does worse than evaluating on the total dataset, and (3) as datasets had fewer clean examples, there was higher variance in the percent change in performance (likely due to a smaller number of clean examples). Like GPT-3, we also found that DROP and SQuADv2 had almost total overlap with the pretraining data. We follow their procedure of manually inspecting the data, and find that most overlapping n - grams were only in the contexts of examples (99.6% for DROP and 97.2% for SQuADv2). Overlaps never occurred in both the question and answer for DROP, and only occurred for both the question and answer for SQuADv2 in 5 of the 11,153 evaluation examples",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_53"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Overlaps never occurred in both the question and answer for DROP, and only occurred for both the question and answer for SQuADv2 in 5 of the 11,153 evaluation examples. Hence, for these two datasets, the 0 -20 20 40 60 Percent of Data Clean in Dataset Percent change in performance of FLAN (accuracy, F1, or BLEU) eval on only clean data did better eval on all data (including dirty) did better ANLI R2 SQuAD v2 ANLI R1 DROP ReCoRD PIQA 25 50 0 75 100 Figure 12: Like GPT-3, we also measured performance on cleaned versions of our datasets, which had high confidence to be unseen in the pretraining data of FLAN. We do not see a correlation that FLAN performed better on evaluation sets for which examples occurred more often in the pretraining data. When the percent of clean data is very small, there are fewer examples for computing the clean performance, which leads to high variance. model gains only background information and cannot memorize the answer to any specific questions (aside from the five examples in SQuADv2). ANLI R1 and R2 (Nie et al., 2020) also had almost complete data contamination, to a much higher degree than GPT-3. Upon further inspection, we see that most overlaps occur in example contexts and not hypotheses (97.3% for ANLI R1 and 98.2% for ANLI R2). As ANLI R1 and R2 are based entirely from Wikipedia examples (R3 is not), we posit that this higher degree of contamination in our pretraining dataset compared with GPT-3’s is potentially due to using a more-recent version of Wikipedia that includes the contexts used in ANLI R1 and R2 (which were collected in 2019). Because seeing a particular context in pretraining does not help with the NLI task given a new, unseen sentence, we think it is unlikely that these overlaps affected performance on the two datasets. Of the remaining datasets, only ReCoRD and PIQA had a clean subset performance that was lower than the overall evaluation set performance by more than 1%",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_54"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Of the remaining datasets, only ReCoRD and PIQA had a clean subset performance that was lower than the overall evaluation set performance by more than 1%. These two datasets are language modeling (i.e., “what’s the best continuation of this sentence?”), and so it is more likely compared with previous tasks that seeing a complete sentence in the pretraining data could help the model predict the right answer in downstream evaluations. For PIQA, both the goal and solution had overlaps in 93 of the 1,838 evaluation examples, and for ReCoRD, the query had overlaps in 2,320 of the 10,000 training examples. We hence mark these results with an asterisk ∗ in Table 2. Brown et al. (2020) also reported substantial contamination rates for these two datasets (61% dirty for ReCoRD and 29% for PIQA), and also mark PIQA with an asterisk. As this overlap analysis follows that performed in Brown et al. (2020), we reiterate the same caveats: the conservative nature of our n -gram matching procedure likely introduces additional false positives; there are no guarantees that the clean subset is drawn from the same distribution as the overall subset; and, accurately detecting test contamination is a relatively new research area without established best practices. Moreover, as our pretraining corpus is almost five times larger than that used for GPT-3 (which was 500B tokens), it is possible that there are more false positives in detecting dirty data",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_55"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Moreover, as our pretraining corpus is almost five times larger than that used for GPT-3 (which was 500B tokens), it is possible that there are more false positives in detecting dirty data. Dataset Metric Total count Total acc/F1/BLEU Clean count Clean acc/F1/BLEU % clean % Diff (clean − overall) DROP F1 9,536 22.4 61 33.0 0.6 47.4 SQuADv2 F1 11,873 41.3 106 38.7 0.9 -6.2 ANLI R1 acc 1,000 48.1 14 57.1 1.4 18.8 ANLI R2 acc 1,000 42.9 21 38.1 2.1 -11.2 ReCoRD acc 10,000 4.6 3,203 4.5 32.0 -2.7 MultiRC acc 4,848 75.4 1,972 75.7 40.7 0.5 PIQA acc 1,838 23.7 896 23.3 48.7 -1.7 ANLI R3 acc 1,200 44.2 718 45.3 59.8 2.5 HellaSwag acc 10,042 28.5 6,578 28.7 65.5 0.7 RTE acc 2,77 84.1 183 84.2 66.1 0.0 WMT’14 En → Fr BLEU 3,003 31.3 2,243 31.5 74.7 0.9 WMT’14 Fr → En BLEU 3,003 34.0 2,243 34.1 74.7 0.2 BoolQ acc 3,270 76.5 2,515 76.3 76.9 -0.4 TQA (tfds-dev) F1 11,313 62.2 8,731 62.0 77.2 -0.2 ARC Easy acc 2,365 79.5 1,888 79.0 79.8 -0.6 ARC Challenge acc 1,165 63.1 983 64.2 84.4 1.7 OpenbookQA acc 500 74.6 425 74.8 85.0 0.3 WMT’16 En → De BLEU 2,999 22.7 2,569 23.0 85.7 1.4 WMT’16 De → En BLEU 2,999 38.6 2,569 38.7 85.7 0.2 WMT’16 En → Ro BLEU 1,999 15.5 1,752 15.4 87.6 -0.7 WMT’16 Ro → En BLEU 1,999 36.7 1,752 36.8 87.6 0.1 COPA acc 100 88.0 91 87.9 91.0 -0.1 CB acc 56 41.1 53 41.5 94.6 1.1 NQ F1 3,610 24.5 3,495 24.3 96.8 -0.5 StoryCloze acc 1,871 92.1 1,864 92.1 99.6 0.0 Winogrande acc 1,267 39.4 1,265 39.4 99.8 0.2 Table 5: Overlap statistics for the subset of datasets that are also used in GPT-3, sorted from dirtiest to cleanest. An evaluation example was dirty if it had any n -gram collision with the pretraining corpus. We computed these results for FLAN’s performance using only a single template for each dataset, so they differ slightly compared with the results for average performance over all templates",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_56"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We computed these results for FLAN’s performance using only a single template for each dataset, so they differ slightly compared with the results for average performance over all templates. D E XTENDED R ELATED W ORK D.1 L ANGUAGE M ODELS AND M ULTI - TASK L EARNING Our work is broadly inspired by a long line of prior work on language models for NLP applications (Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; 2019, inter alia ). Instruction tuning can be seen as a formulation of multitask learning (MTL), which is an established area within deep learning (Collobert et al., 2011; Luong et al., 2016; Ruder, 2017; Velay & Daniel, 2018; Clark et al., 2019b; Liu et al., 2019b, inter alia )—see Worsham & Kalita (2020) for a recent survey on MTL for NLP. Differing from prior MTL work which focuses on performance improvements across training tasks (Raffel et al., 2020; Aghajanyan et al., 2021) or to new domains (Axelrod et al., 2011), our work is motivated by improving zero-shot generalization to tasks that were not seen in training. D.2 Z ERO -S HOT L EARNING AND M ETA -L EARNING Our work also falls in the well-established category of zero-shot learning, which has historically been used to refer to classifying instances among a set of unseen categories (Lampert et al., 2009; Romera-Paredes & Torr, 2015; Srivastava et al., 2018; Yin et al., 2019, inter alia ). In NLP, zero-shot learning work also includes translating between unseen language pairs (Johnson et al., 2017; Pham et al., 2019), language modeling on unseen languages (Lauscher et al., 2020), as well as various NLP applications (Liu et al., 2019a; Corazza et al., 2020; Wang et al., 2021). Most recently, the emergent ability of language models (Brown et al., 2020) has led to increased interest in how models generalize to unseen tasks, the definition of zero-shot learning used in our paper",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_57"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In addition, meta-learning (Finn et al., 2017; Vanschoren, 2018, inter alia ) also broadly tries to train models that adapt quickly to unseen tasks, typically based on a few examples. D.3 P ROMPTING Instruction tuning leverages the intuition that language models at scale contain substantial world knowledge and can perform a range of NLP tasks (Brown et al., 2020, see also Bommasani et al. (2021)). Another line of work that shares this goal prompts models with continuous inputs optimized via backpropagation to substantially improve performance (Li & Liang, 2021; Lester et al., 2021; Qin & Eisner, 2021), as well as work that prompts models to produce specialized outputs (Wei et al., 2022). Although the success of these approaches depends heavily on model scale (Lester et al., 2021), for which large models can be costly to serve, the ability of a single large model to perform many tasks slightly eases this burden. As shown by our experiments in 4.5, prompt tuning is an orthogonal method for which instruction tuning can additionally improve performance. Reif et al. (2021) is similar to our work in that they also use related tasks to improve zero-shot learning, though they differ by only using related tasks in the context (and not finetuning), and focus on the application of text style transfer. Our work shares similar motivations with prompting in that we use inference-time text interactions to prompt a single model, without creating separate checkpoints for each task. Whereas prompting work such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is likely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with an answer key), we hope that finetuning models to respond to natural language instructions instead of completing a sentence will make such large models more accessible to non-technical users",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_58"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". D.4 F INETUNING L ARGE L ANGUAGE M ODELS Finetuning pretrained language models is a well-established method in NLP, with much of the work so far occurring on models in the range of 100M to 10B parameters (Dai & Le, 2015; Devlin et al., 2019; Raffel et al., 2020; Lewis et al., 2020, inter alia ). For models of O(100B) parameters, recent work has finetuned task-specific models for program synthesis (Austin et al., 2021; Chen et al., 2021), summarization (Wu et al., 2021), as well as improved bias and fairness behavior (Solaiman & Dennison, 2021). In addition to the traditional “dense” models, sparse mixture of experts (MoE) models of up to more than 1T parameters have been trained and finetuned (Lepikhin et al., 2020; Fedus et al., 2021). Compared with this prior work that finetunes and evaluates on the same downstream task, our setup studies the effect of instruction tuning on ability to perform unseen tasks. D.5 M ULTI - TASK Q UESTION A NSWERING The instructions we use for instruction tuning are similar to QA-based task formulation research, which aims to unify NLP tasks by casting them as question-answering over a context. For instance, McCann et al. (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated with natural language prompts; they report transfer learning gains on finetuning tasks as well as zero-shot domain adaptation results on SNLI (Bowman et al., 2015) and Amazon/Yelp Reviews (Kotzias et al., 2015). While McCann et al. (2018) does not leverage unsupervised pre-training and only reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on zero-shot performance on unseen task clusters. UnifiedQA (Khashabi et al., 2020) shows similar transfer learning gains as McCann et al. (2018) across 20 datasets and reports good generalization to unseen tasks across four types of QA. Focusing on binary text classification, Zhong et al. (2021) finetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on unseen tasks",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_59"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Focusing on binary text classification, Zhong et al. (2021) finetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on unseen tasks. In comparison, our paper is much larger in scope, empirically demonstrating the idea on a wide range of tasks with a much larger model. Other work has used QA-based task formulation for more-targeted applications including semantic role labeling (He et al., 2015), relation extraction (Levy et al., 2017), coreference resolution (Wu et al., 2020) and named entity recognition (Li et al., 2020) as question answering. D.6 I NSTRUCTIONS -B ASED NLP Recent improvements in the capabilities of language models have led to increased interest in a nascent area of instructions-based NLP (Goldwasser & Roth, 2014, and see McCarthy (1960)). Schick & Schütze (2021) (also see Gao et al., 2021; Tam et al., 2021) use task descriptions in cloze-style phrases to help language models assign soft labels for few-shot and semi-supervised learning, though this line of work finetunes new checkpoints for each downstream task. Efrat & Levy (2020) evaluated GPT-2 (Radford et al., 2019) on simple tasks ranging from retrieving the n th word of a sentence to generating examples for SQuAD, concluding that GPT-2 performs poorly across all tasks. In terms of the setup of finetuning on a large number of tasks and evaluating on unseen tasks, two recent papers are similar to ours. Mishra et al. (2021) finetune BART (Lewis et al., 2020) using instructions and few-shot examples for tasks such as question answering, text classification, and text modification, and find that this few-shot finetuning with instructions improves performance on unseen tasks. Ye et al. (2021) introduce a setup for cross-task few-shot learning, finding that multi-task meta-learning using MAML (Finn et al., 2017) improves the few-shot capabilities of BART on unseen downstream tasks",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_60"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2021) introduce a setup for cross-task few-shot learning, finding that multi-task meta-learning using MAML (Finn et al., 2017) improves the few-shot capabilities of BART on unseen downstream tasks. Our work differs from these two papers in that we focus on zero-shot learning, for which we observe the crucial importance of model scale (FLAN is 1,000x larger than BART-base). Perhaps the papers most related to ours are the recent Sanh et al. (2021) and Min et al. (2021), which were released after our initial preprint. Min et al. (2021) finetunes GPT-2 Large (770M parameters) to be a few-shot learner, which is the same approach as our experiment in Section 4.3. Similar to our conclusions, they also observe that including few-shot exemplars and instruction tuning are complementary ways to improve performance. Sanh et al. (2021) propose to finetune T5-11B to respond to prompts, and they also report performance improvements on zero-shot learning. These two papers and our work all study finetuning with instructions, but, as noted by Min et al. (2021), it is hard to directly compare results, due to differing model sizes, model types (decoder-only vs encoder-decoder), pretraining data, task mixtures, and type of instructions (Sanh et al. (2021) say that their instructions are more diverse). Finally, OpenAI has a model called InstructGPT (Ouyang et al., 2022). InstructGPT uses human anntations to guide desired model behavior, both via finetuning and reinforcement learning, finding that InstructGPT is preferred by human rathers compared with unmodified GPT-3. E F REQUENTLY A SKED Q UESTIONS How do the FLAN instructions differ from GPT-3 or T5 prompts? GPT-3 prompting is done in a way such that the prompt looks like data that the model has been pretrained on, and the model finishes the continuation. T5 prompts are mostly just a tag for the dataset, which would not work in the zero-shot setting. In contrast, the prompts that we use for FLAN are similar to what would be used to ask a human to perform the task",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_61"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In contrast, the prompts that we use for FLAN are similar to what would be used to ask a human to perform the task. For instance, given an input for an NLI task, these would be the prompts. T5 prompt: cb hypothesis: At my age you will probably have learnt one lesson. premise: It’s not certain how many lessons you’ll learn by your thirties. GPT-3 prompt: At my age you will probably have learnt one lesson. question: It’s not certain how many lessons you’ll learn by your thirties. true, false, or neither? answer: FLAN prompt: Premise: At my age you will probably have learnt one lesson. Hypothesis: It’s not certain how many lessons you’ll learn by your thirties. Does the premise entail the hypothesis? So because FLAN prompts are formulated as responding to an instruction, they do not work well for pretrained language models without finetuning. Performance was near zero for most generation tasks. For instance, given the input “‘The dog runs.’ Translate this sentence to French.” , LaMDA-PT continues with ”The dog runs after the cat” instead of actually translating the sentence. Hence, we used the established GPT-3 prompts for our LaMDA-PT baselines. What are some limitations/failure cases of FLAN? While we qualitatively find that FLAN responds well to most tasks, it does fail on some simple tasks. For instance, as shown in Figure 22, FLAN fails at the very simple task of returning the second word in a sentence, and also incorrectly translates a question to Danish when asked to answer the question in Danish. Additional limitations include a context length of only 1024 tokens (which is not enough for most summarization tasks), and that the model was mostly trained on English data. Can FLAN be used when large amounts of training data are available? In this work, we focus on cross-task generalization to zero-shot tasks, but we also believe that instruction tuning could result in positive task transfer among seen tasks, depending on the mixture of tasks (though we leave this for future work)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_62"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In 4.5, where we apply prompt tuning to the FLAN checkpoint, we see promising results that indicate positive task transfer in a supervised setting. Are the ten unique templates per dataset or per task cluster? The ten unique templates are for each dataset and not for a task cluster. This is because datasets in the same task cluster often differed slightly (e.g., “is this movie review positive” vs “is this yelp review positive” ). In Figure 7A, why does the untuned LaMDA-PT model see worse performance with more parameters for reading comprehension and sentiment analysis? For context, Figure 7A is a check of correctness for Figure 7B. Figure 7A confirms that scale improves performance for tasks that were seen during instruction tuning, as expected. The untuned LaMDA-PT model performance in Figure 7A is shown just for completeness. Nonetheless, the fact that scale does not always improve zero-shot performance of untuned LaMDPT is an interesting artifact. Initially, we were surprised, because Brown et al. (2020) shows that scale improves performance across a large number of tasks in aggregate. It turns out that scale does not improve performance for certain tasks. This is especially true for zero-shot learning, and we think that this happens to be the case for the reading comprehension and sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot performance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does not show results on sentiment analysis, but Holtzman et al. (2021) find that zero-shot performance on SST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both GPT-3 and the models we use. This artifact is certainly worth further study, but is outside the scope of instruction tuning. Ideally, we would have performed the Figure 7 ablation with cross-validation instead of a single split, which likely would have smoothed out that artifact",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_63"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Ideally, we would have performed the Figure 7 ablation with cross-validation instead of a single split, which likely would have smoothed out that artifact. F Q UALITATIVE E XAMPLES This section shows qualitative examples of FLAN responding to various prompts. Movie review: This movie is the best RomCom since Pretty Woman. Did this critic dislike the movie? OPTIONS -yes -no no Input: FLAN output: Movie review: This movie is the best RomCom since Pretty Woman. Did this critic like the movie? OPTIONS -yes -no yes Input: FLAN output: Figure 13: For sentiment analysis, FLAN changes the answer appropriately when the question is flipped. What is Barack Obama’s profession? Answer in Chinese ● 总统 ● 美国总统 Input: FLAN output: What is Barack Obama’s profession? Input: ● politician ● lawyer ● senator ● us president FLAN output: Figure 14: For question answering, FLAN can answer a question in another language when instructed to do so. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. John is reading the paper “Finetuned Language Models are Zero-Shot Learners.” Change to passive voice. The paper “Finetuned Language Models are Zero-Shot Learners” is being read by John. Input: FLAN output: John is reading the paper “Finetuned Language Models are Zero-Shot Learners.” Change to past tense. Input: John was reading the paper “Finetuned Language Models are Zero-Shot Learners.” FLAN output: John is reading the paper “Finetuned Language Models are Zero-Shot Learners.” Change the verb to eat and the subject to Noam Chomsky. Noam Chomsky is eating the paper “Finetuned Language Models are Zero-Shot Learners.” Input: FLAN output: John is reading the paper “Finetuned Language Models are Zero-Shot Learners.” Change to the verb to eat. Input: John is eating the paper “Finetuned Language Models are Zero-Shot Learners.” FLAN output: Figure 15: FLAN can perform zero-shot rule-based manipulations. Recommend activities to do on a sunny weekend in Copenhagen",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_64"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Recommend activities to do on a sunny weekend in Copenhagen. ● Visit Christiania ● Copenhagen Zoo ● Visit the Tivoli Gardens ● Take a canal tour ● Museums ● Visit The Little Mermaid Statue ● Visit Copenhagen Zoo ● Visit the Danish Maritime Museum Input: FLAN output: Recommend fun activities to do on a rainy night in Seattle. Input: ● Go to the pacific science center ● Go out to eat ● Go to a comedy club ● Build a blanket fort ● Go to the Space Needle ● Go to underground tours ● Find a karaoke bar FLAN output: Figure 16: FLAN can make zero-shot recommendations. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. Generate utterances with the intent “where to vote in the state Senate election\" ● Tell me where to vote in the state senate election ● Find out where to vote in the state senate election ● Where am i supposed to vote in the state senate election ● Where can i get information on where to vote in the state Senate election? Input: FLAN output: Generate utterances with the intent “get COVID vaccine\" Input: ● I would like to get the covid vaccine. ● I would like a COVID vaccine as soon as possible ● Can I get a COVID vaccine as a preventative measure? ● Get COVID vaccine for a five year old ● Where is a center for COVID vaccine testing? FLAN output: Figure 17: FLAN can be used in a zero-shot fashion to generate data, such as utterances that are consistent with a given intent. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. Suggest a related search term to “Novak Djokovic\" ● novak djokovic girlfriend ● serena williams ● rafael nadal ● novak djokovic wife ● roger federer ● andy murray ● novak djokovic news ● novak djokovic serbia ● novak djokovic wimbledon Input: FLAN output: Suggest a related search term to “watermelon” Input: ● Cantaloupe ● Cucumber ● Melon ● Fruit ● Watermelon juice ● Watermelons ● Lemon ● Apple FLAN output: Figure 18: FLAN can be used for zero-shot query expansion",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_65"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. \"I guess I need to do laundry.” Rewrite the above sentence in a Shakespearean style. ● To my mind, verily, I suppose 'tis time, I wash my clothes. ● Mine eyes and ears and tongue declare me idle; and yet it is true I guess, By my garments, that I need to do laundry ● Lo my duty now calls To thee, to wash this body's clothes. Input: FLAN output: On the way to Yosemite, our front tire blew. Finish writing the story, and include a happy ending. Input: ● We got our tire replaced. The mechanic was honest, kind and helpful. Then we had a smooth ride for the rest of the trip. Our trip was filled with fun and excitement. ● We waited three hours for the tow truck, but had a great picnic. The tow truck driver helped us put the spare on, and we arrived in Yosemite for our reservations dinner, with time to spare. FLAN output (cherrypicked): Figure 19: FLAN can perform zero-shot tasks relevant to assisted-writing applications. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. Make up a word that means \"like a pipedream\" ● Pipedreamic ● Pipedreamish ● Pipedreamlike ● Phantasmagoric Input: FLAN output (cherrypicked): Make up a new word that means “when bananas shower from the sky\" Input: ● Bananation ● Banana shower ● Banania ● Bananafall ● Bananada FLAN output: Figure 20: FLAN can be used for zero-shot word formation. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. Write a sad story about carrots. The sun had barely risen when the carrots were taken from their beds and brought to market. They'd had no time to gather their strength. They smelled of earth, their greens still wet from the dew. At first no one was interested in buying them. But one man finally reached out and picked up one of the carrots. \"I'll take this one!” he cried out",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_66"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". At first no one was interested in buying them. But one man finally reached out and picked up one of the carrots. \"I'll take this one!” he cried out. \"It's for my mother! I'll pay you well for it.” The man took the carrot to the cashier, eager to get it home to his mother. He was careful to wrap it carefully so that it wouldn't get bruised or dirty. He hurried home, and when he saw his mother, he gave her the carrot. “Is this one of the ones I grew?\" she asked, her voice wavering with emotion. \"Yes, mother,\" he replied. \"It is, dear.\" The man left the carrot with his mother and hurried back to market. His mother was very sick, and he was afraid she wouldn't last much longer. He needed to buy all the carrots in market so that none would be left when she was gone. Input: FLAN output: John moved to mountain view. He spent friday night in San Francisco. He got a COVID vaccine. He voted in the state senate election. Write a story that follows the above plot. Input: John arrived at the city with his vaccine, feeling prepared enough for the state senate election of San Francisco. He had voted for the first time last night and was feeling good about it. He felt more prepared than ever to win, and win big, because he was getting a COVID vaccine. He knew that his opponent had not gotten a COVID vaccine, and he knew he had already defeated him before the election began. FLAN output (cherrypicked): Figure 21: Open-ended generation tasks by FLAN. The carrot story was from sampling sixteen outputs with a minimum length of 150 and choosing the highest probability output. What is Barack Obama’s profession? Answer in Danish ● Hvad er Barack Obamas beskæftigelse? ● Hvad er Barack Obamas erhverv? Input: FLAN output: Conan went to the store to buy some pencils. What is the second word in this sentence? Input: ● pencils ● Conan ● store ● buy FLAN output: Figure 22: Example failure cases for FLAN. Left: FLAN fails to perform a simple task of returning the n th word. Right: FLAN translates a question instead of answering it",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_67"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Left: FLAN fails to perform a simple task of returning the n th word. Right: FLAN translates a question instead of answering it. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40. C HANGES FROM V4 TO V5 • Replaced the tables in the main figure with a figure, which takes up less space and focuses on zero-shot performance. • Added GLaM 64B/64E as a baseline. • Moved the ablation about the role of instructions, as well as prompt tuning, into the main paper (and condensed the figures). C HANGES TO V4 FROM V3 • We added a Frequently Asked Questions section (Appendix E). • We added a section with qualitative examples (Appendix F). • We added an additional ablation study on the role of instructions during finetuning (Apendix B.2). • We updated the related work (Appendix D) with manuscripts posted on arxiv since our initial preprint. C HANGES TO V3 FROM V2 • The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens. C HANGES TO V2 FROM V1 • We updated the terminology to “datasets” and “task clusters.” • We renamed the previous “open-domain QA” task cluster to “closed-book QA.” • We extended the related work section and moved it to the Appendix D, using a shorter version in the main body. • We added FLAN and LaMDA-PT results for additional datasets for which GPT-3 results were not reported. • For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually evaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation on that dev set in order to compare with GPT-3’s performance. Zero-shot FLAN now beats zero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original result in Table 2, though there is no GPT-3 result to compare with. • We moved commonsense reasoning and coreference resolution from the main body to the Appendix. • We moved prompt tuning from the main body to 4.5. • We added data contamination analysis (Appendix C). • We added few-shot instruction tuning (4.4)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_68"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". • We moved prompt tuning from the main body to 4.5. • We added data contamination analysis (Appendix C). • We added few-shot instruction tuning (4.4). • We cited additional datasets in Appendix G. • The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens. G T ASKS AND D ATASETS This appendix further details the datasets that we use in this paper. We group datasets into one of the following task clusters: • Natural language inference concerns how two sentences relate, typically asking, given a first sentence, whether a second sentence is true, false, or possibly true. We use the following datasets: 1. ANLI (Nie et al., 2020) 2. CB (De Marneffe et al., 2019) 3. MNLI (Williams et al., 2018) 4. QNLI (Rajpurkar et al., 2018) 5. SNLI (Bowman et al., 2015) 6. WNLI (Levesque et al., 2012) 7. RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) • Reading comprehension tests the ability to answer a question when given a passage that contains the answer. We use the following datasets: 1. BoolQ Clark et al. (2019a) 2. DROP (Dua et al., 2019) 3. MultiRC (Khashabi et al., 2018) 4. OBQA (Mihaylov et al., 2018) 5. SQuADv1 (Rajpurkar et al., 2016) 6. SQuADv2 (Rajpurkar et al., 2018) • Commonsense reasoning evaluates the ability to perform physical or scientific reasoning with an element of common sense. We use the following datasets: 1. COPA (Roemmele et al., 2011) 2. HellaSwag (Zellers et al., 2019) 3. PiQA (Bisk et al., 2020) 4. StoryCloze (Mostafazadeh et al., 2016) • Sentiment analysis is a classic NLP task aims to understand whether a piece of text is positive or negative. We use the following datasets: 1. IMDB (Maas et al., 2011) 2. Sentiment140 (Go et al., 2009) 3. SST-2 (Socher et al., 2013) 4. Yelp (Fast.AI) • Closed-book QA asks models to answer questions about the world without specific access to information that contains the answer. We use the following datasets: 1. ARC (Clark et al., 2018) 2. NQ (Lee et al., 2019; Kwiatkowski et al., 2019) 3",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_69"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We use the following datasets: 1. ARC (Clark et al., 2018) 2. NQ (Lee et al., 2019; Kwiatkowski et al., 2019) 3. TriviaQA Joshi et al. (2017) • Paraphrase detection asks a model to determine whether two sentences are semantically equivlent. 4 We use the following datasets: 1. MRPC (Dolan & Brockett, 2005) 2. QQP (Wang et al., 2018, see) 3. Paws Wiki (Zhang et al., 2019) • Coreference resolution tests the ability to identify expressions of the same entity in some given text. We use the following datasets: 1. DPR (Rahman & Ng, 2012) 2. Winogrande (Sakaguchi et al., 2020) 4 Although paraphrasing can be seen as positive entailment in both directions, it has been distinct from NLI in the academic literature. 3. WSC273 (Levesque et al., 2012) • Reading comprehension with commonsense combines elements of both reading comprehension with commonsense. We use the following datasets: 1. CosmosQA (Huang et al., 2019) 2. ReCoRD (Zhang et al., 2018) • Struct to text tests the ability to describe some structured data using natural language. We use the following datasets: 1. CommonGen (Lin et al., 2020) 2. DART (Nan et al., 2021) 3. E2ENLG (Dušek et al., 2019) 4. WebNLG (Gardent et al., 2017) • Translation is the task of translating text from one language into a different language. We use the following datasets: 1. En–Fr from WMT’14 (Bojar et al., 2014) 2. En–De, En–Tr, En–Cs, En–Fi, En–Ro, and En–Ru from WMT’16 (Bojar et al., 2016) 3. En–Es from Paracrawl (Bañón et al., 2020) • Summarization asks models to read a piece of text and generate an abbreviated summary of it. We use the following datasets: 1. AESLC (Zhang & Tetreault, 2019) 2. CNN-DM (See et al., 2017) 3. Gigaword (Napoles et al., 2012) 4. MultiNews (Fabbri et al., 2019) 5. Newsroom (Grusky et al., 2018) 6. Samsum (Gliwa et al., 2019) 7. XSum (Narayan et al., 2018) 8. AG News (Zhang et al., 2015) 9. Opinion Abstracts - Rotten Tomatoes (Wang & Ling, 2016) 10. Opinion Abstracts - iDebate (Wang & Ling, 2016) 11",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_70"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". XSum (Narayan et al., 2018) 8. AG News (Zhang et al., 2015) 9. Opinion Abstracts - Rotten Tomatoes (Wang & Ling, 2016) 10. Opinion Abstracts - iDebate (Wang & Ling, 2016) 11. Wiki Lingua English (Ladhak et al., 2020) • Additional datasets that we assign to a miscellaneous task cluster include: 1. Conversational question-answering: QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019) 2. Evaluating context-sentence word meanings: WiC (Pilehvar & Camacho-Collados, 2019) 3. Question classification: TREC (Li & Roth, 2002; Hovy et al., 2001) 4. Linguistic acceptability: CoLA (Warstadt et al., 2019) 5. Math questions (Saxton et al., 2019) For all tasks, our finetuning and evaluation code uses tensorflow datasets (TFDS) to load and process datasets. Regarding the number of training examples per dataset, we limited the training set size per dataset to 30,000 so that no dataset dominated the finetuning distribution. When a test set with labels was available in TFDS, we used it; otherwise, we used the TFDS validation set as our test set, splitting the training set into a train and dev set. On the following pages, we show inputs and outputs for evaluation tasks where we compared with GPT-3. See the attached supplementary material for the templates for all other datasets. G.1 N ATURAL L ANGUAGE I NFERENCE I NPUT Joey Heindle (born 14 May 1993 in Munich) is a German singer. He is best known for winning the seventh season of the game show Ich bin ein Star – Holt mich hier raus! and finishing in 5th place in season 9 of Deutschland sucht den Superstar, despite universally negative reviews from the jury each week. Based on the paragraph above can we conclude that \"Joey Heindle was highly disliked by people on television.\"? OPTIONS: - Yes - It’s impossible to say - No T ARGET Yes Table 6: Example input and target for Adversarial NLI (ANLI). ANLI (Nie et al., 2020) is a large-scale NLI benchmark with adversarial examples collected iteratively with a human and model in the loop",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_71"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ANLI (Nie et al., 2020) is a large-scale NLI benchmark with adversarial examples collected iteratively with a human and model in the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, not entailment, or impossible to say). There are three rounds, R1–R3. Of the three training sets with 16,946, 45,460, and 100,459 examples, we use 16,946, 30,000, and 30,000 for train and 200 from each of the three TFDS validation sets for dev. We use the TFDS “test” sets of 1,000, 1,000, and 1,200 examples as our test set for reporting numbers. I NPUT A: so I watch the fish, you know. Whatever I can do to keep myself occupied. I like to have the TV on, because that usually keeps me, um, more occupied. It kind of takes the time away and I don’t realize, that’s really the only time I ever watch TV, is when I’m on the bike. and then usually after I’m done riding the bike, just to cool myself down, I usually take a walk, you know, and that just kind of uh, gets me, you know, to where I’m not quite as tired I guess. But it’s definitely a task. B: You think so? A: I can’t say that I really enjoy it. Based on the paragraph above can we conclude that \"she really enjoys it\"? OPTIONS: - Yes - No - It’s impossible to say T ARGET No Table 7: Example input and target for Commitment Bank (CB). CB (De Marneffe et al., 2019) is a corpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whether the hypothesis is entailed by the premise (entailment, not entailment, or impossible to say). Of the training set with 250 examples, we use 200 for train and 50 for dev. We use the TFDS validation set of 56 examples as our test set for reporting numbers. I NPUT After years of study, the Vatican’s doctrinal congregation has sent church leaders a confidential document concluding that \"sex-change\" procedures do not change a person’s gender in the eyes of the church",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_72"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Based on the paragraph above can we conclude that \"Sex-change operations become more common.\"? OPTIONS: - yes - no T ARGET no Table 8: Example input and target for Recognizing Textual Entailment (RTE). RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) asks whether a second sentence is entailed by a first (binary, either entailed or not entailed). Of the training set with 2490 examples, we use 2,290 for train and 200 for dev. We use the TFDS validation set of 277 examples as our test set for reporting numbers. G.2 R EADING C OMPREHENSION I NPUT There are four ways an individual can acquire Canadian citizenship: by birth on Canadian soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption. Among them, only citizenship by birth is granted automatically with limited exceptions, while citizenship by descent or adoption is acquired automatically if the specified conditions have been met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration, Refugees and Citizenship. Can we conclude that can i get canadian citizenship if my grandfather was canadian? OPTIONS: - no - yes T ARGET no Table 9: Example input and target for Boolean Questions (BoolQ). BoolQ Clark et al. (2019a) asks a yes/no question based on a passage and a question. Of the training set with 9,427 examples, we use 9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set for reporting numbers. I NPUT Imagine you are standing in a farm field in central Illinois. The land is so flat you can see for miles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to yourself, it sure is flat around here. If you drive one hundred miles to the south, the landscape changes. In southern Illinois, there are rolling hills. Why do you think this is? What could have caused these features? There are no big rivers that may have eroded and deposited this material",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_73"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In southern Illinois, there are rolling hills. Why do you think this is? What could have caused these features? There are no big rivers that may have eroded and deposited this material. The ground is capable of supporting grass and trees, so wind erosion would not explain it. To answer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice sheet covered much of the Midwest United States. Springfield, Illinois, was covered by over a mile of ice. Its hard to imagine a mile thick sheet of ice. The massive ice sheet, called a glacier, caused the features on the land you see today. Where did glaciers go? Where can you see them today? Glaciers are masses of flowing ice. Question: \"How big were the glaciers?\" Response: \"One mile\" Does the response correctly answer the question? OPTIONS: - no - yes T ARGET yes Table 10: Example input and target for Multi-Sentence Reading Comprehension (MultiRC). MultiRC Khashabi et al. (2018) asks an open-ended question given a paragraph that contains the answer. Of the training set with 27,243 examples, we use 27,043 for train and 200 for dev. We use the TFDS validation set of 4,848 examples as our test set for reporting numbers. I NPUT soil is a renewable resource for growing plants A plant that needs to expand will be able to have an endless resource in OPTIONS: - dirt - pesticides - pay - beans T ARGET dirt Table 11: Example input and target for Openbook Question Answering (OBQA). OBQA (Mihaylov et al., 2018) asks 4-way multiple choice questions based facts. Of the training set with 4,957 examples, we use all for train and 200 in the TFDS validation set of 500 examples for dev. We use the TFDS test set of 500 examples as our test set for reporting numbers. G.3 C OMMONSENSE R EASONING I NPUT I packed up my belongings. What is the cause? OPTIONS: - I was hunting for a new apartment. - I was moving out of my apartm en t. T ARGET I was moving out of my apartment . Table 12: Example input and target for Choice of Plausible Alternatives (COPA)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_74"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". - I was moving out of my apartm en t. T ARGET I was moving out of my apartment . Table 12: Example input and target for Choice of Plausible Alternatives (COPA). COPA (Roemmele et al., 2011) is a causal reasoning task that asks to infer either a cause of effect of a premise from two choices. Of the training set with 400 examples, we use 350 for train and 50 for dev. We use the TFDS validation set of 100 examples as our test set for reporting numbers. I NPUT What happens next in this paragraph? Once the rope is inside the hook, he begins moving up the wall but shortly after he stops and begins talking. The male then begins talking about the clip again and goes back up the wall. as he OPTIONS: - progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safety. - changes time, an instant replay of his initial move is shown a second time. - continues to talk, another male speaks about the move and shows another closeup of the plex by the male. - continues, other people start to arrive and begin to hang out with him as he makes a few parts of the rope. T ARGET progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safet y . Table 13: Example input and target for Commonsense Sentence Completion (HellaSwag). HellaSwag (Zellers et al., 2019) tests for sentence completion that requires common sense, asking for the most probable ending given four contexts. Of the training set with 39,905 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 10,042 examples as our test set for reporting numbers. I NPUT Here is a goal: Remove smell from garbage disposal. How would you accomplish this goal? OPTIONS: - Create soda ice cubes and grind through disposal. - Create vinegar ice cubes and gr i n d through disposal. T ARGET Create vinegar ice cubes and g r i nd through disposal. Table 14: Example input and target for Physical Question Answering (PiQA)",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_75"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". - Create vinegar ice cubes and gr i n d through disposal. T ARGET Create vinegar ice cubes and g r i nd through disposal. Table 14: Example input and target for Physical Question Answering (PiQA). PiQA (Bisk et al., 2020) is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal must be selected from two choices. Of the training set with 16,113 examples, we use 16,013 for train and 100 for dev. We use the TFDS validation set of 1,838 examples as our test set for reporting numbers. I NPUT Caroline never drinks carbonated beverages. Her friends pick on her because of it. One day they challenged her to drink a soda. Caroline wanted to win the challenge. Predict the next sentence. OPTIONS: - Caroline refused to open the soda. - Caroline opened the soda and dr ank it all in one gulp! T ARGET Caroline opened the soda and d r ank it all in one gulp! Table 15: Example input and target for The Story Cloze Test (StoryCloze). StoryCloze (Mostafazadeh et al., 2016) is a commonsense reasoning framework for story generation, where a system chooses the correct ending to a four-sentence story. We use the 2016 version on TFDS. Of the validation set with 1,871 examples (no training set is available), we use 1,671 for train and 200 for dev. We use the TFDS test set of 1,871 examples as our test set for reporting numbers. G.4 C LOSED -B OOK QA I NPUT What season is the Northern Hemisphere experiencing when it is tilted directly toward the Sun? OPTIONS: - fall - winter - spring - summer T ARGET summer Table 16: Example input and target for The AI2 Reasoning Challenge (ARC). ARC (Clark et al., 2018) asks grade-school level 4-way multiple choice science questions. There is a challenge set and an easy set, where the challenge set questions were answered incorrectly by both a retrieval-based algorithm and a co-occurrence algorithm. Of the training sets with 1,119 examples (challenge) and 2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_76"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Of the training sets with 1,119 examples (challenge) and 2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev. We use the TFDS test sets of 1,172 and 2,376 examples respectively as our test set for reporting numbers. I NPUT Question: who is the girl in more than you know?? Answer: T ARGET Romi Van Renterghem. Table 17: Example input and target for Natural Questions (Open) (NQ). NQ (Lee et al., 2019; Kwiatkowski et al., 2019) asks for an open-ended answer given a question, where all questions can be answered using the contents of Wikipedia. Of the training set of 87,925 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 3,610 examples as our test set for reporting numbers. I NPUT Please answer this question: Henry Croft, an orphan street sweeper who collected money for charity, is associated with what organised charitable tradition of working class culture in London, England? T ARGET pearly kings and queens Table 18: Example input and target for Trivia Question Answering (TriviaQA). TriviaQA Joshi et al. (2017) includes question-answer pairs authored by trivia enthusiasts. Of the training set of 87,622 examples, we use 30,000 for train and 200 for dev. We use 7,993 examples from Wikipedia of the 11,313 examples in the TFDS validation set, which is the same validation set used in (Brown et al., 2020). as our test set for reporting numbers. G.5 C OREFERENCE R ESOLUTION I NPUT How does the sentence end? Elena wanted to move out of her parents fast but Victoria wanted to stay for a while, OPTIONS: - Elena went to school. - Victoria went to school. T ARGET Victoria went to school. Table 19: Example input and target for Adversarial Winograd Schema Challenge (Winogrande). Winogrande (Sakaguchi et al., 2020) tests for coreference resolution by asking a model to fill in a masked token in a sentence by choosing an entity from two options. Of the 40.4k examples in the XL training set, we use 30,000 for train and 200 for dev",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_77"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Of the 40.4k examples in the XL training set, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 1,267 as our test set for reporting numbers. I NPUT Jane knocked on Susan’s door, but there was no answer. OPTIONS: - Jane was out. - Susan was out. T ARGET Susan was out. Table 20: Example input and target for Winograd Schema Challenge (WSC273). WSC273 (Levesque et al., 2012) tests for coreference resolution by asking a model to complete the sentence in a fashion that requires understanding the entities in the sentence. Of the 0 examples in the training set (WSC273 is test-set only), we use none for train and none for dev. We use the TFDS test set as our test set for reporting numbers. G.6 R EADING C OMPREHENSION WITH C OMMONSENSE I NPUT Complete the passage. (CNN) – At first glance, \"The Flat\" might seem like an episode of \"Hoarders,\" Israeli-style. The documentary film opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to clean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy gloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets full of knickknacks. But buried among the detritus they chance upon something remarkable – mysterious papers linking the grandparents to an important Nazi figure. How could such ardent Zionists, who left their native Germany in the early 1930s, have been involved with an SS official like Leopold von Mildenstein? What I found out was this journey, the Nazi ( OPTIONS: - Arnon Goldfinger) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - CNN) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Germany) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Israeli) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told CNN",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_78"
  },
  {
    "document_type": "research_paper",
    "title": "Wei_finetuned_language_models_are_zero_shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wei_finetuned_language_models_are_zero_shot_learners.pdf",
    "date_published": "2022-02-10",
    "keywords": "",
    "flag": "",
    "chunk_text": ". - Israeli) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Nazi) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - SS) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Tel Aviv) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - The Flat) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. - Zionists) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. T ARGET Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldfinger told CNN. Table 21: Example input and target for Reading Comprehension with Commonsense Reasoning (ReCoRD). ReCoRD (Zhang et al., 2018) asks for the answer to a cloze-style question where an entity is masked out. Of the the training set of 100,730 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 10,000 examples as our test set for reporting numbers. G.7 T RANSLATION (7 LANGUAGES ) I NPUT Here the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at the Weser mouth. Translate to German T ARGET An der B 211 befindet sich in Loyermoor der so genannte “Geest-Abbruch”, der eine Höhendiferenz von gut 30 Meter überbr ück t. Table 22: Example input and output for translation. This example is from WMT’16 English–German; all languages use the same translation templates.",
    "chunk_id": "Natural_language_processing_wei_finetuned_language_models_are_zero_shot_learners.json_chunk_79"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language { jacobdevlin,mingweichang,kentonl,kristout } @google.com Abstract We introduce a new language representtion model called BERT , which stands for B idirectional E ncoder R epresentations from T ransformers. Unlike recent language reprsentation models ( Peters et al. , 2018a ; Raford et al. , 2018 ), BERT is designed to prtrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a rsult, the pre-trained BERT model can be fintuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial tasspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art rsults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answeing Test F1 to 93.2 (1.5 point absolute iprovement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). 1 Introduction Language model pre-training has been shown to be effective for improving many natural language processing tasks ( Dai and Le , 2015 ; Peters et al. , 2018a ; Radford et al. , 2018 ; Howard and Ruder , 2018 ). These include sentence-level tasks such as natural language inference ( Bowman et al. , 2015 ; Williams et al. , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ), which aim to predict the rlationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce fine-grained output at the token level ( Tjong Kim Sang and De Meulder , 2003 ; Rajpurkar et al. , 2016 )",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2016 ). There are two existing strategies for appling pre-trained language representations to dowstream tasks: feature-based and fine-tuning . The feature-based approach, such as ELMo ( Peters et al. , 2018a ), uses task-specific architectures that include the pre-trained representations as addtional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) ( Radford et al. , 2018 ), introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all prtrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, espcially for the fine-tuning approaches. The mjor limitation is that standard language models are unidirectional, and this limits the choice of archtectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-tright architecture, where every token can only atend to previous tokens in the self-attention layers of the Transformer ( Vaswani et al. , 2017 ). Such rstrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying fintuning based approaches to token-level tasks such as question answering, where it is crucial to incoporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: B idirectional E ncoder R epresentations from T ransformers. BERT alleviates the previously mentioned unidrectionality constraint by using a “masked laguage model” (MLM) pre-training objective, ispired by the Cloze task ( Taylor , 1953 ). The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked arXiv:1810.04805v2 [cs.CL] 24 May 2019 word based only on its context",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Unlike left-tright language model pre-training, the MLM ojective enables the representation to fuse the left and the right context, which allows us to prtrain a deep bidirectional Transformer. In addtion to the masked language model, we also use a “next sentence prediction” task that jointly prtrains text-pair representations. The contributions of our paper are as follows: • We demonstrate the importance of bidirectional pre-training for language representations. Ulike Radford et al. ( 2018 ), which uses unidiretional language models for pre-training, BERT uses masked language models to enable prtrained deep bidirectional representations. This is also in contrast to Peters et al. ( 2018a ), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce the need for many heavily-engineered tasspecific architectures. BERT is the first fintuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outpeforming many task-specific architectures. • BERT advances the state of the art for eleven NLP tasks. The code and pre-trained moels are available at https://github.com/ google-research/bert . 2 Related Work There is a long history of pre-training general laguage representations, and we briefly review the most widely-used approaches in this section. 2.1 Unsupervised Feature-based Approaches Learning widely applicable representations of words has been an active area of research for decades, including non-neural ( Brown et al. , 1992 ; Ando and Zhang , 2005 ; Blitzer et al. , 2006 ) and neural ( Mikolov et al. , 2013 ; Pennington et al. , 2014 ) methods. Pre-trained word embeddings are an integral part of modern NLP systems, ofering significant improvements over embeddings learned from scratch ( Turian et al. , 2010 )",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2014 ) methods. Pre-trained word embeddings are an integral part of modern NLP systems, ofering significant improvements over embeddings learned from scratch ( Turian et al. , 2010 ). To prtrain word embedding vectors, left-to-right laguage modeling objectives have been used ( Mnih and Hinton , 2009 ), as well as objectives to dicriminate correct from incorrect words in left and right context ( Mikolov et al. , 2013 ). These approaches have been generalized to coarser granularities, such as sentence embedings ( Kiros et al. , 2015 ; Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ). To train sentence representations, prior work has used objectives to rank candidate next sentences ( Jernite et al. , 2017 ; Logeswaran and Lee , 2018 ), left-to-right generation of next setence words given a representation of the previous sentence ( Kiros et al. , 2015 ), or denoising autencoder derived objectives ( Hill et al. , 2016 ). ELMo and its predecessor ( Peters et al. , 2017 , 2018a ) generalize traditional word embedding rsearch along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model. The contextual reresentation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-specific architectures, ELMo advances the state of the art for several major NLP benchmarks ( Peters et al. , 2018a ) including quetion answering ( Rajpurkar et al. , 2016 ), sentiment analysis ( Socher et al. , 2013 ), and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ). Melamud et al. ( 2016 ) proposed learning contextual representations through a task to prdict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. ( 2018 ) shows that the cloze task can be used to improve the robustness of text generation moels",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. ( 2018 ) shows that the cloze task can be used to improve the robustness of text generation moels. 2.2 Unsupervised Fine-tuning Approaches As with the feature-based approaches, the first works in this direction only pre-trained word ebedding parameters from unlabeled text ( Colobert and Weston , 2008 ). More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and fine-tuned for a supervised downstream task ( Dai and Le , 2015 ; Howard and Ruder , 2018 ; Radford et al. , 2018 ). The advantage of these approaches is that few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT ( Radford et al. , 2018 ) achieved prviously state-of-the-art results on many sentenclevel tasks from the GLUE benchmark ( Wang et al. , 2018a ). Left-to-right language mode BERT BERT E [CLS] E 1 E [SEP] E N E 1 ’ E M ’ C T 1 T [SEP] T N T 1 ’ T M ’ [CLS] Tok 1 [SEP] Tok N Tok 1 TokM Question Paragraph Start/End Span BERT E [CLS] E 1 E [SEP] E N E 1 ’ E M ’ C T 1 T [SEP] T N T 1 ’ T M ’ [CLS] Tok 1 [SEP] Tok N Tok 1 TokM Masked Sentence A Masked Sentence B Pre-training Fine-Tuning NSP Mask LM Mask LM Unlabeled Sentence A and B Pair SQuAD Question Answer Pair NER MNLI Figure 1: Overall pre-training and fine-tuning procedures for BERT. Apart from output layers, the same architetures are used in both pre-training and fine-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating quetions/answers). ing and auto-encoder objectives have been used for pre-training such models ( Howard and Ruder , 2018 ; Radford et al. , 2018 ; Dai and Le , 2015 )",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". separating quetions/answers). ing and auto-encoder objectives have been used for pre-training such models ( Howard and Ruder , 2018 ; Radford et al. , 2018 ; Dai and Le , 2015 ). 2.3 Transfer Learning from Supervised Data There has also been work showing effective tranfer from supervised tasks with large datasets, such as natural language inference ( Conneau et al. , 2017 ) and machine translation ( McCann et al. , 2017 ). Computer vision research has also demostrated the importance of transfer learning from large pre-trained models, where an effective recipe is to fine-tune models pre-trained with ImgeNet ( Deng et al. , 2009 ; Yosinski et al. , 2014 ). 3 BERT We introduce BERT and its detailed implementtion in this section. There are two steps in our framework: pre-training and fine-tuning . Duing pre-training, the model is trained on unlabeled data over different pre-training tasks. For fintuning, the BERT model is first initialized with the pre-trained parameters, and all of the paraeters are fine-tuned using labeled data from the downstream tasks. Each downstream task has searate fine-tuned models, even though they are intialized with the same pre-trained parameters. The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its unified achitecture across different tasks. There is minmal difference between the pre-trained architeture and the final downstream architecture. Model Architecture BERT’s model architeture is a multi-layer bidirectional Transformer ecoder based on the original implementation dscribed in Vaswani et al. ( 2017 ) and released in the tensor2tensor library. 1 Because the use of Transformers has become common and our iplementation is almost identical to the original, we will omit an exhaustive background descrition of the model architecture and refer readers to Vaswani et al",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ( 2017 ) as well as excellent guides such as “The Annotated Transformer.” 2 In this work, we denote the number of layers (i.e., Transformer blocks) as L , the hidden size as H , and the number of self-attention heads as A . 3 We primarily report results on two model sizes: BERT BASE (L=12, H=768, A=12, Total Paraeters=110M) and BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M). BERT BASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Tranformer uses constrained self-attention where every token can only attend to context to its left. 4 1 https://github.com/tensorflow/tensor2tensor 2 http://nlp.seas.harvard.edu/2018/04/03/attention.html 3 In all cases we set the feed-forward/filter size to be 4 H , i.e., 3072 for the H = 768 and 4096 for the H = 1024 . 4 We note that in the literature the bidirectional Tran Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., ⟨ Question, Answer ⟩ ) in one token sequence. Throughout this work, a “sentence” can be an arbtrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the iput token sequence to BERT, which may be a sigle sentence or two sentences packed together. We use WordPiece embeddings ( Wu et al. , 2016 ) with a 30,000 token vocabulary. The first token of every sequence is always a special clasification token ( [CLS] ). The final hidden state corresponding to this token is used as the agregate sequence representation for classification tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token ( [SEP] ). Second, we add a learned embeding to every token indicating whether it belongs to sentence A or sentence B",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". First, we separate them with a special token ( [SEP] ). Second, we add a learned embeding to every token indicating whether it belongs to sentence A or sentence B . As shown in Figure 1 , we denote input embedding as E , the final hidden vector of the special [CLS] token as C ∈ R H , and the final hidden vector for the i th input token as T i ∈ R H . For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualiztion of this construction can be seen in Figure 2 . 3.1 Pre-training BERT Unlike Peters et al. ( 2018a ) and Radford et al. ( 2018 ), we do not use traditional left-to-right or right-to-left language models to pre-train BERT. Instead, we pre-train BERT using two unsupevised tasks, described in this section. This step is presented in the left part of Figure 1 . Task #1: Masked LM Intuitively, it is reasoable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-tright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidiretional conditioning would allow each word to idirectly “see itself”, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a “Transformer encoder” while the left-context-only version is referred to as a “Transformer decoder” since it can be used for text generation. In order to train a deep bidirectional representtion, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature ( Taylor , 1953 ). In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tkens in each sequence at random",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In all of our experiments, we mask 15% of all WordPiece tkens in each sequence at random. In contrast to denoising auto-encoders ( Vincent et al. , 2008 ), we only predict the masked words rather than recostructing the entire input. Although this allows us to obtain a bidiretional pre-trained model, a downside is that we are creating a mismatch between pre-training and fine-tuning, since the [MASK] token does not apear during fine-tuning. To mitigate this, we do not always replace “masked” words with the atual [MASK] token. The training data generator chooses 15% of the token positions at random for prediction. If the i -th token is chosen, we replace the i -th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3) the unchanged i -th token 10% of the time. Then, T i will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2 . Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Quetion Answering (QA) and Natural Language Infeence (NLI) are based on understanding the reltionship between two sentences, which is not drectly captured by language modeling. In order to train a model that understands sentence reltionships, we pre-train for a binarized next setence prediction task that can be trivially geneated from any monolingual corpus. Specifically, when choosing the sentences A and B for each prtraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext ), and 50% of the time it is a random sentence from the corpus (labeled as NotNext ). As we show in Figure 1 , C is used for next sentence predition (NSP). 5 Despite its simplicity, we demostrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6 5 The final model achieves 97%-98% accuracy on NSP. 6 The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6 5 The final model achieves 97%-98% accuracy on NSP. 6 The vector C is not a meaningful sentence representation without fine-tuning, since it was trained with NSP. [CLS] he likes play ## ing [SEP] my dog is cute [SEP] Input E [CLS] E he E likes E play E ## ing E [SEP] E my E dog E is E cute E [SEP] Token Embeddings E A E B E B E B E B E B E A E A E A E A E A Segment Embeddings E 0 E 6 E 7 E 8 E 9 E 10 E 1 E 2 E 3 E 4 E 5 Position Embeddings Figure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenttion embeddings and the position embeddings. The NSP task is closely related to representatiolearning objectives used in Jernite et al. ( 2017 ) and Logeswaran and Lee ( 2018 ). However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all prameters to initialize end-task model parameters. Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words) ( Zhu et al. , 2015 ) and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critcal to use a document-level corpus rather than a shuffled sentence-level corpus such as the Billion Word Benchmark ( Chelba et al. , 2013 ) in order to extract long contiguous sequences. 3.2 Fine-tuning BERT Fine-tuning is straightforward since the selattention mechanism in the Transformer alows BERT to model many downstream tasks— whether they involve single text or text pairs—by swapping out the appropriate inputs and outputs. For applications involving text pairs, a common pattern is to independently encode text pairs bfore applying bidirectional cross attention, such as Parikh et al. ( 2016 ); Seo et al. ( 2017 )",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For applications involving text pairs, a common pattern is to independently encode text pairs bfore applying bidirectional cross attention, such as Parikh et al. ( 2016 ); Seo et al. ( 2017 ). BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidrectional cross attention between two sentences. For each task, we simply plug in the tasspecific inputs and outputs into BERT and fintune all the parameters end-to-end. At the iput, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphraing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate tex∅ pair in text classification or sequence tagging. At the output, the token reresentations are fed into an output layer for tokelevel tasks, such as sequence tagging or question answering, and the [CLS] representation is fed into an output layer for classification, such as etailment or sentiment analysis. Compared to pre-training, fine-tuning is reltively inexpensive. All of the results in the pper can be replicated in at most 1 hour on a sigle Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. 7 We dscribe the task-specific details in the corresponing subsections of Section 4 . More details can be found in Appendix A.5 . 4 Experiments In this section, we present BERT fine-tuning rsults on 11 NLP tasks. 4.1 GLUE The General Language Understanding Evaluation (GLUE) benchmark ( Wang et al. , 2018a ) is a colection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1 . To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3 , and use the final hiden vector C ∈ R H corresponding to the first input token ( [CLS] ) as the aggregate representtion",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The only new parameters introduced during fine-tuning are classification layer weights W ∈ R K × H , where K is the number of labels. We copute a standard classification loss with C and W , i.e., log(softmax( CW T )) . 7 For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8 See (10) in https://gluebenchmark.com/faq . System MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average 392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k - Pre-OpenAI SOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0 BiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0 OpenAI GPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1 BERT BASE 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6 BERT LARGE 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1 Table 1: GLUE Test results, scored by the evaluation server ( https://gluebenchmark.com/leaderboard ). The number below each task denotes the number of training examples. The “Average” column is slightly different than the official GLUE score, since we exclude the problematic WNLI set. 8 BERT and OpenAI GPT are singlmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components. We use a batch size of 32 and fine-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best fine-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERT LARGE we found that fintuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but peform different fine-tuning data shuffling and clasifier layer initialization. 9 Results are presented in Table 1",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". With random restarts, we use the same pre-trained checkpoint but peform different fine-tuning data shuffling and clasifier layer initialization. 9 Results are presented in Table 1 . Both BERT BASE and BERT LARGE outperform all sytems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy iprovement over the prior state of the art. Note that BERT BASE and OpenAI GPT are nearly identical in terms of model architecture apart from the atention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the official GLUE leaderboard 10 , BERT LARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing. We find that BERT LARGE significantly outpeforms BERT BASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2 . 4.2 SQuAD v1.1 The Stanford Question Answering Dataset (SQuAD v1.1) is a collection of 100k crowsourced question/answer pairs ( Rajpurkar et al. , 2016 ). Given a question and a passage from 9 The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERT BASE and BERT LARGE . 10 https://gluebenchmark.com/leaderboard Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1 , in the question answeing task, we represent the input question and pasage as a single packed sequence, with the quetion using the A embedding and the passage using the B embedding. We only introduce a start vetor S ∈ R H and an end vector E ∈ R H during fine-tuning. The probability of word i being the start of the answer span is computed as a dot prouct between T i and S followed by a softmax over all of the words in the paragraph: P i = e S · T i P j e S · Tj . The analogous formula is used for the end of the answer span",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is defined as S · T i + E · T j , and the maximum scoring span where j ≥ i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems ( Seo et al. , 2017 ; Clark and Gardner , 2018 ; Peters et al. , 2018a ; Hu et al. , 2018 ). The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available, 11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by first fine-tuning on TriviaQA ( Joshi et al. , 2017 ) befor fine-tuning on SQuAD. Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble sytem in terms of F1 score. Without TriviaQA fin11 QANet is described in Yu et al. ( 2018 ), but the system has improved substantially after publication. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human - - 82.3 91.2 #1 Ensemble - nlnet - - 86.0 91.7 #2 Ensemble - QANet - - 84.5 90.5 Published BiDAF+ELMo (Single) - 85.6 - 85.8 R.M. Reader (Ensemble) 81.2 87.9 82.3 88.5 Ours BERT BASE (Single) 80.8 88.5 - - BERT LARGE (Single) 84.1 90.9 - - BERT LARGE (Ensemble) 85.8 91.8 - - BERT LARGE (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8 BERT LARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2 Table 2: SQuAD 1.1 results. The BERT ensemble is 7x systems which use different pre-training checpoints and fine-tuning seeds",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The BERT ensemble is 7x systems which use different pre-training checpoints and fine-tuning seeds. System Dev Test EM F1 EM F1 Top Leaderboard Systems (Dec 10th, 2018) Human 86.3 89.0 86.9 89.5 #1 Single - MIR-MRC (F-Net) - - 74.8 78.0 #2 Single - nlnet - - 74.2 77.1 Published unet (Ensemble) - - 71.4 74.9 SLQA+ (Single) - 71.4 74.4 Ours BERT LARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outpeforming all existing systems by a wide margin. 12 4.3 SQuAD v2.0 The SQuAD 2.0 task extends the SQuAD 1.1 problem definition by allowing for the possibility that no short answer exists in the provided pargraph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat quetions that do not have an answer as having an aswer span with start and end at the [CLS] tken. The probability space for the start and end answer span positions is extended to include the position of the [CLS] token. For prediction, we compare the score of the no-answer span: s null = S · C + E · C to the score of the best non-null span 12 The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the first 400 tokens in documents, that contain at least one of the provided possible answers. System Dev Test ESIM+GloVe 51.9 52.7 ESIM+ELMo 59.1 59.2 OpenAI GPT - 78.0 BERT BASE 81.6 - BERT LARGE 86.6 86.3 Human (expert) † - 85.0 Human (5 annotations) † - 88.0 Table 4: SWAG Dev and Test accuracies. † Human peformance is measured with 100 samples, as reported in the SWAG paper. ˆ s i,j = max j ≥ i S · T i + E · T j . We predict a non-null answer when ˆ s i,j > s null + τ , where the thresold τ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard etries and top published work ( Sun et al. , 2018 ; Wang et al",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard etries and top published work ( Sun et al. , 2018 ; Wang et al. , 2018b ) are shown in Table 3 , excluing systems that use BERT as one of their coponents. We observe a +5.1 F1 improvement over the previous best system. 4.4 SWAG The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair copletion examples that evaluate grounded commosense inference ( Zellers et al. , 2018 ). Given a setence, the task is to choose the most plausible cotinuation among four choices. When fine-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A ) and a possible continuation (sentence B ). The only task-specific parameters introduced is a vetor whose dot product with the [CLS] token reresentation C denotes a score for each choice which is normalized with a softmax layer. We fine-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Rsults are presented in Table 4 . BERT LARGE ouperforms the authors’ baseline ESIM+ELMo sytem by +27.1% and OpenAI GPT by 8.3%. 5 Ablation Studies In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERT BASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 75.7 91.6 84.9 Table 5: Ablation over the pre-training tasks using the BERT BASE architecture. “No NSP” is trained without the next sentence prediction task. “LTR & No NSP” is trained as a left-to-right LM without the next sentence prediction, like OpenAI GPT. “+ BiLSTM” adds a radomly initialized BiLSTM on top of the “LTR + No NSP” model during fine-tuning. ablation studies can be found in Appendix C",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". “+ BiLSTM” adds a radomly initialized BiLSTM on top of the “LTR + No NSP” model during fine-tuning. ablation studies can be found in Appendix C . 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidrectionality of BERT by evaluating two prtraining objectives using exactly the same prtraining data, fine-tuning scheme, and hyperprameters as BERT BASE : No NSP : A bidirectional model which is trained using the “masked LM” (MLM) but without the “next sentence prediction” (NSP) task. LTR & No NSP : A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM. The left-only constraint was also applied at fine-tuning, because removing it introduced a pre-train/fine-tune mismatch that degraded downstream performance. Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input reprsentation, and our fine-tuning scheme. We first examine the impact brought by the NSP task. In Table 5 , we show that removing NSP hurts performance significantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by coparing “No NSP” to “LTR & No NSP”. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no righside context. In order to make a good faith atempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does significantly improve results on SQuAD, but the results are still far worse than those of the prtrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two moels, as ELMo does",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two moels, as ELMo does. However: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. 5.2 Effect of Model Size In this section, we explore the effect of model size on fine-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training prcedure as described previously. Results on selected GLUE tasks are shown in Table 6 . In this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning. We can see that larger models lead to a strict acuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled traiing examples, and is substantially different from the pre-training tasks. It is also perhaps surpriing that we are able to achieve such significant improvements on top of models which are aready quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. ( 2017 ) is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters ( Al-Rfou et al. , 2018 ). By contrast, BERT BASE contains 110M parameters and BERT LARGE cotains 340M parameters. It has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, we believe that this is the first work to demonstrate conviningly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suffciently pre-trained. Peters et al. ( 2018b ) presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. ( 2016 ) metioned in passing that increasing hidden dimesion size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvments. Both of these prior works used a featurbased approach — we hypothesize that when the model is fine-tuned directly on the downstream tasks and uses only a very small number of radomly initialized additional parameters, the tasspecific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small. 5.3 Feature-based Approach with BERT All of the BERT results presented so far have used the fine-tuning approach, where a simple classifcation layer is added to the pre-trained model, and all parameters are jointly fine-tuned on a dowstream task. However, the feature-based approach, where fixed features are extracted from the prtrained model, has certain advantages. First, not all tasks can be easily represented by a Tranformer encoder architecture, and therefore require a task-specific model architecture to be added. Second, there are major computational benefits to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task ( Tjong Kim Sang and De Meulder , 2003 ). In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data. Following standard practice, we fomulate this as a tagging task but do not use a CRF Hyperparams Dev Set Accuracy #L #H #A LM (ppl) MNLI-m MRPC SST-2 3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation over BERT model size. #L = the number of layers; #H = hidden size; #A = number of atention heads. “LM (ppl)” is the masked LM perplexity of held-out training data. System Dev F1 Test F1 ELMo ( Peters et al. , 2018a ) 95.7 92.2 CVT ( Clark et al. , 2018 ) - 92.6 CSE ( Akbik et al. , 2018 ) - 93.1 Fine-tuning approach BERT LARGE 96.6 92.8 BERT BASE 96.4 92.4 Feature-based approach (BERT BASE ) Embeddings 91.0 - Second-to-Last Hidden 95.6 - Last Hidden 94.9 - Weighted Sum Last Four Hidden 95.9 - Concat Last Four Hidden 96.1 - Weighted Sum All 12 Layers 95.5 - Table 7: CoNLL-2003 Named Entity Recognition rsults. Hyperparameters were selected using the Dev set. The reported Dev and Test scores are averaged over 5 random restarts using those hyperparameters. layer in the output. We use the representation of the first sub-token as the input to the token-level classifier over the NER label set. To ablate the fine-tuning approach, we apply the feature-based approach by extracting the activtions from one or more layers without fine-tuning any parameters of BERT. These contextual ebeddings are used as input to a randomly initiaized two-layer 768-dimensional BiLSTM before the classification layer. Results are presented in Table 7 . BERT LARGE performs competitively with state-of-the-art metods. The best performing method concatenates the token representations from the top four hidden laers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "BERT_pre-training_of_deep_bidirectional_transformers",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\BERT_pre-training_of_deep_bidirectional_transformers.pdf",
    "date_published": "2019-05-28",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The best performing method concatenates the token representations from the top four hidden laers of the pre-trained Transformer, which is only 0.3 F1 behind fine-tuning the entire model. This demonstrates that BERT is effective for both fintuning and feature-based approaches. 6 Conclusion Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to benefit from deep unidirectional architetures. Our major contribution is further generaizing these findings to deep bidirectional architetures, allowing the same pre-trained model to sucessfully tackle a broad set of NLP tasks.",
    "chunk_id": "Natural_language_processing_bert_pre-training_of_deep_bidirectional_transformers.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Psychological Methods Manuscript version of Deep Learning: A Primer for Psychologists Christopher J. Urban, Kathleen M. Gates Funded by: • National Science Foundation © 2021, American Psychological Association. This manuscript is not the copy of record and may not exactly replicate the final, authoritative version of the article. Please do not copy or cite without authors’ permission. The final version of record is available via its DOI: https://dx.doi.org/10.1037/met0000374 This article is intended solely for the personal use of the individual user and is not to be disseminated broadly. Abstract Deep learning has revolutionized predictive modeling in topics such as computer vision and natural language processing but is not commonly applied to psychological data. In an effort to bring the benefits of deep learning to psychologists, we provide an overview of deep learning for researchers who have a working knowledge of linear regression. We first discuss several benefits of the deep learning approach to predictive modeling. We then present three basic deep learning models that generalize linear regression: The feedforward neural network (FNN), the recurrent neural network (RNN), and the convolutional neural network (CNN). We include concrete toy examples with R code to demonstrate how each model may be applied to answer prediction-focused research questions using common data types collected by psychologists. Keywords: Psychology, deep learning, artificial neural networks, machine learning, predictive modeling Translational Abstract Deep learning has been successfully used to solve complex problems in computer vision and in natural language processing but is rarely used in psychology. In this primer, we provide an overview of deep learning in an effort to bring the benefits of deep learning to psychologists. We first discuss several benefits of using deep learning algorithms to predict important outcomes",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We first discuss several benefits of using deep learning algorithms to predict important outcomes. We then present three basic deep learning models: The feedforward neural network (FNN), the recurrent neural network (RNN), and the convolutional neural network (CNN). We use toy examples with R code to demonstrate how these models may be applied to predict important outcomes using the kinds of data sets typically collected by psychologists. Deep Learning: A Primer for Psychologists The amount of data available to psychologists in recent years has exploded. The rise of the internet has enabled researchers to recruit large, diverse, and cheap web-based samples (e.g., Buhrmester et al., 2011; Gosling et al., 2004) as well as to utilize the vast quantities of existing web-based behavioral data (e.g., Golder & Macy, 2011; Gosling et al., 2011; Landers et al., 2016; Yarkoni, 2010). Smartphones and wearable sensors give researchers unprecedented opportunities to study experiential, behavioral, and physiological processes at the individual level (e.g., Hamaker & Wichers, 2017; Miller, 2012; Trull & Ebner-Priemer, 2014). Large, publicly available data sets (e.g., Institute for Quantitative Social Science Dataverse Network, dvn.iq.harvard.edu; OpenfMRI project, www.openfmri.org) let researchers answer questions that cannot be addressed in conventional lab-based settings (Yarkoni, 2012). Statistics and artificial intelligence researchers have developed powerful, flexible machine learning algorithms that can be applied to these big data sets. Oftentimes these algorithms are applied to predict some outcome, such as to classify an individual into some category based on the data or to predict a future event. In these cases, machine learning algorithms are typically used in conjunction with techniques to prevent overfitting , or finding spurious patterns in a data set that do not generalize to other data sets (Hastie et al., 2009)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Researchers have employed machine learning algorithms to answer a variety of prediction-focused psychology research questions using the kinds of data sets described above. Social media data have been leveraged to identify important predictors of mental health issues including depression (Schwartz et al., 2014), post-traumatic stress disorder (Coppersmith et al., 2014), suicidal ideation (Coppersmith et al., 2016; De Choudhury et al., 2016), and schizophrenia (Mitchell et al., 2015). Analyses of smartphone usage and wearable sensor data have pinpointed predictors of current and future cognitive states such as moods and emotions (e.g., LiKamWa, 2012; Mehrotra et al., 2017; Rachuri et al., 2010) with a particular focus on depressive states (e.g., Canzian & Musolesi, 2015; Mehrotra et al., 2016; Saeb et al., 2015). Large, publicly available clinical data sets have been used to make predictions associated with the diagnosis, prognosis, and treatment of mental illness in clinical psychology, psychiatry, and neuroscience (e.g., Dwyer et al., 2018). Prediction-focused studies like those just described can help inform intervention, prevention, and treatment endeavors in real-life scenarios. Focusing on prediction can also inform psychological theory by identifying important variables and relationships that can subsequently be investigated as potential causal factors underlying the phenomena being studied. However, psychologists often need to carefully select meaningful variables to include in machine learning models to get accurate predictions; this procedure is called feature engineering in machine learning (Zheng & Casari, 2018). The need for feature engineering makes it difficult to build accurate predictive models without a priori expertise in the phenomena being studied, which is often unavailable in complicated data sets with many variables",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Indeed, the difficulty of feature engineering may be a strong contributor to the fact that no single method obtained consistently high predictive accuracy across the studies described above. In this primer, we describe a potential solution to the feature engineering problem. Specifically, we introduce concepts integral for understanding deep learning , a machine learning paradigm that can be applied to the problems of prediction, forecasting, and classification. Deep learning is the subfield of machine learning concerned with extracting information from data in hierarchies of concepts where more abstract concepts are built out of less abstract concepts. We visualize this information extraction process in Figure 1 (Goodfellow et al., 2016), which contains a popular deep learning model schematic where each row of circles represents a model layer and arrows represent information flow through the model. The pictured model can classify objects in images. Each individual image pixel first enters the model’s visible layer . The model’s hidden layers extract successively more and more abstract concepts, like edges or parts of objects. Finally, the model predicts the identity of the object in the image. For those familiar with structural equation modeling (SEM), hidden layers in deep learning models actually extract latent variables. However, unlike in SEM, these extracted latent variables may not be directly interpretable, may be related to each other and to the observed variables via nonlinear functional forms that are not known a priori , and are not subject to many of the assumptions and constraints required for most SEM estimators. Additionally, unlike traditional SEM and many classical statistical methods, deep learning algorithms automatically extract their own hidden layer/latent variable representations of the data to use for making predictions, thereby potentially avoiding the need for extensive feature engineering. Each layer in a deep learning model is just a simpler machine learning model",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Each layer in a deep learning model is just a simpler machine learning model. The most popular foundational models used to build deep learning models are called artificial neural networks (ANNs; LeCun et al., 2015). In fact, ANNs and deep learning are so interlinked that some machine learning researchers consider ANNs to be another name for deep learning (e.g., Goodfellow et al., 2016). ANNs have produced major breakthroughs in computer vision and natural language processing (LeCun et al., 2015) as well as in accurate time series forecasting (e.g., Karlsson et al., 2014; Sezer et al., 2020). Computer scientists have even employed ANNs to accurately predict psychological constructs at unprecedented rates. For example, Suhara et al. (2017), Mikelsons et al. (2017), and Taylor et al. (2017) applied ANNs to forecast individuals’ future moods using daily diary, wearable sensor, smartphone log, and weather data. Huang et al. (2018) combined two kinds of ANNs to predict bipolar individuals’ mood disturbances using keystroke and circadian rhythm data. Aghaei et al. (2018) asked participants to wear cameras, then used ANNs on the resulting image data to classify individuals’ social interactions. In each case, ANNs outperformed the best statistical methods available for prediction, such as logistic regression and support vector machines. Despite their state-of-the-art prediction accuracy, deep learning approaches have not been widely adopted for predictive modeling in psychology. (One notable exception is in neuroscience, where ANNs are often used to identify brain-based disorders using brain imaging data; see Vieira et al., 2017, for a review.) The benefits that deep learning approaches might confer to psychology beyond those offered by simpler machine learning models are therefore undetermined",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Some barriers to applying deep learning for predictive modeling in psychology might include: (1) A lack of clarity about common terms like deep learning and fundamental concepts such as artificial neural networks (i.e., a knowledge barrier); (2) the complicated nature of building deep learning models in practice (i.e., a complexity barrier); and (3) a lack of standard, easy-to-use deep learning software (i.e., an implementation barrier). Due to space concerns, we primarily address the knowledge barrier in this primer and intend to more fully address the complexity and implementation barriers in future work. The objective of this primer is to familiarize psychologists with deep learning concepts so that they can be better prepared to learn emerging methods (which are largely developed in different disciplines) and to be critical consumers of articles in the psychological sciences that use deep learning methods. This primer aims to serve as an accessible reference for psychologists as they increasingly become exposed to studies that use deep learning methods. We accomplish this goal via a two-pronged approach. First, we introduce psychologists to some of the benefits of using deep learning for predictive modeling. Second, we de-mystify deep learning by explaining common terms using both equations and words. ANN models, the most successful models under the deep learning paradigm, are presented as a generalization of the linear regression model. Specialized ANNs that work well with with sequential data (e.g., daily diary data) and with image data (e.g., fMRI data) are explained. Specifically, we describe feedforward neural network (FNN), recurrent neural network (RNN), and convolutional neural network (CNN) models. Detailed toy examples and R code are presented for each model. Benefits of Deep Learning for Predictive Modeling Benefits Shared with Other Machine Learning Algorithms Deep learning shares a number of benefits with machine learning algorithms in general",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Benefits of Deep Learning for Predictive Modeling Benefits Shared with Other Machine Learning Algorithms Deep learning shares a number of benefits with machine learning algorithms in general. First, much like machine learning algorithms such as random forests (RFs; Breiman, 2001) and support vector machines (SVMs; Boser et al., 1992), deep learning algorithms implicitly model interactions and other nonlinearities that need to be explicitly specified in classical statistical methods such as linear regression (see Appendix A for a review of linear regression). This enables deep learning algorithms to obtain high predictive accuracy when the true causal relationships underlying the data are nonlinear (e.g., Yarkoni & Westfall, 2017). Second, deep learning effectively models multicollinear and high-dimensional data. Many classical statistical methods such as linear regression are unstable when the independent variables are multicollinear, often leading to inaccurate model predictions. Deep learning algorithms, on the other hand, may perform well with multicollinear and high-dimensional data sets by constructing highly predictive latent variable representations of the independent variables (De Veaux & Ungar, 1994). In this sense, deep learning algorithms are related to algorithms such as principal components regression (Hotelling, 1957; Kendall, 1957), partial least squares (Wold, 1966), and penalized partial least squares (Krämer et al., 2008), all of which aim to extract the most important information from the independent variables to make predictions. A third benefit of the deep learning learning approach is that many strategies can be employed to prevent models from overfitting. A core objective of machine learning is to build predictive models that perform well on new, previously unseen data sets (e.g., Goodfellow et al., 2016; Hastie et al., 2009)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A core objective of machine learning is to build predictive models that perform well on new, previously unseen data sets (e.g., Goodfellow et al., 2016; Hastie et al., 2009). Deep learning benefits from a number of techniques specifically developed to prevent deep learning models from overfitting (see Goodfellow et al., 2016, for a thorough, albeit technical, review). In Appendix B, we provide an overview of fundamental, general-purpose machine learning strategies for preventing overfitting. In particular, we explain the important concepts of changing a model’s representational capacity, regularizing a model, and tuning a model’s hyperparameters via validation set and k-fold cross-validation approaches. Familiarity with these fundamental concepts is essential for fitting deep learning models that perform well using previously unseen data sets. However, even if researchers never choose to fit a deep learning model, understanding and applying machine learning techniques such as regularization and cross-validation can increase the efficiency and reproducibility of many analysis pipelines without necessarily altering the end results (Yarkoni & Westfall, 2017). Unique Benefits of Deep Learning Algorithms Deep learning has some unique benefits in addition to those shared with general machine learning techniques. First, deep learning is capable of leveraging big data sets to obtain highly accurate predictions. Although all machine learning algorithms tend to benefit from increasing sample sizes, deep learning algorithms have been empirically found to leverage big data sets to increase their predictive accuracy beyond the accuracy attainable with other machine learning algorithms (e.g., RFs and SVMs; Goodfellow et al., 2016; LeCun et al., 2015). We anticipate that as society becomes increasingly digitized, psychologists will have greater access to large data sets that will be difficult to effectively analyze without using deep learning",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We anticipate that as society becomes increasingly digitized, psychologists will have greater access to large data sets that will be difficult to effectively analyze without using deep learning. Deep learning algorithms may be able to capitalize on huge data sets such as observations from internet sources (e.g., websites, blogs, and social media; Landers et al., 2016), cell phone behaviors (e.g., Hamaker & Wichers, 2017), and genomic data sets (e.g., Plomin & Davis, 2009) to predict psychological outcomes with unprecedented accuracy. We note, however, that “big data” is not required for deep learning models to obtain high predictive accuracy. Many tools exist for fitting accurate deep learning models using small data sets. For example, transfer learning (Pan & Yang, 2010; Torrey & Shavlik, 2009) is a machine learning technique wherein a model fitted to perform one task is re-purposed to perform another task, typically to predict a different outcome on a new data set. Transfer learning often produces models with higher predictive accuracy than models fitted using only a single data set (e.g., Tan et al., 2018). Transfer learning may be useful for psychologists with small data sets, who could fit a deep learning model using a large, publicly available data set (e.g., a large-scale mental health survey data set) that is related to the small data set of interest (e.g., a small clinical sample), then “fine-tune” the model using the small data set. Recent work suggests that even without specialized approaches like transfer learning, deep learning models fitted using small data sets are capable of obtaining high predictive accuracy on new data (Olson et al., 2018). Second, deep learning leverages small correlations for accurate predictions. Deep learning algorithms excel at discovering intricate relationships between large numbers of variables (LeCun et al., 2015)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Deep learning algorithms excel at discovering intricate relationships between large numbers of variables (LeCun et al., 2015). Many phenomena studied by psychologists are likely influenced by a large number of weak causal factors that interact in complicated ways – that is, “everything correlates with everything else” (Meehl, 1990). Deep learning models, which are less readily interpretable than linear regression models, may perform well in such cases (although see Montavon et al. (2017) for a tutorial overview of methods for interpreting deep learning models). Third, deep learning reduces the need for feature engineering. Psychologists usually need to think carefully about including meaningful, theoretically-relevant variables in models to obtain accurate, generalizable predictions – that is, predictive modeling in psychology often requires careful feature engineering. In some application domains, deep learning algorithms can reduce the need for feature engineering by extracting their own (though usually not directly interpretable) representations of the independent variables to make predictions (LeCun et al., 2015). For example, the psychological analysis of text data (Iliev et al., 2015), image data, and video data (Barto et al., 2017) is usually labor-intensive, requiring human coders to look through the data and manually identify important information in each observation to use for making predictions. It might not even be clear what information will lead to the most accurate predictions. In psychological text analysis, for example, it is not always clear which features in a body of text will be the most useful for predicting outcomes like the author’s personality characteristics (Iliev et al., 2015). A deep learning algorithm might prove useful in this case by extracting representations of the raw text data in an automated fashion and using these representations to accurately predict personality characteristics (Mehta et al., 2020)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In a similar vein, deep learning may circumvent the need for measurement models. Measurement models refer to models that relate latent psychological constructs to measures or indicators of those constructs. Building measurement models is an essential but challenging part of conducting psychological research that typically requires careful consideration of many factors including exact specification of the relationships between constructs and indicators (e.g., Bollen, 2001). Oftentimes, psychologists construct measurement models to explain variability in some outcome variable of interest using latent predictor variables. In the case of item response theory (IRT), psychologists often build measurement models to obtain a score for an individual which can subsequently be used to make inferences pertaining to that individual (e.g., diagnostic status, performance ability). If, however, psychologists wish to accurately predict some outcome of interest rather than to make substantive inferences, deep learning may circumvent the need for measurement models by automatically extracting the latent variable representation of the input measures that is most predictive of the outcome variable 1 . Fourth, deep learning algorithms often obtain higher predictive accuracy than other machine learning algorithms when the observations are sequences or images. We note that this does not imply that deep learning algorithms are not useful for research questions 1 Those familiar with principal components regression and its variant partial least squares may note that these methods provide a similar approach to deep learning by using causal indicators where the latent variables (or components) are predicted by the indicators. This contrasts with typical measurement models in SEM and IRT where reflective indicators are used – that is, where the indicators are predicted by the latent variables",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This contrasts with typical measurement models in SEM and IRT where reflective indicators are used – that is, where the indicators are predicted by the latent variables. While deep learning does closely correspond to traditional methods based on causal indicators (for example, deep learning can be used to perform a nonlinear version of principal component analysis; Gorban et al., 2008), deep learning diverges from these methods by applying multiple nonlinear transformations to the extracted latent variables. based in cross-sectional data (e.g., web-based behavioral measurements, cross-sectional survey data). Rather, deep learning algorithms outperform simpler machine learning algorithms more consistently and by a wider margin in sequence or image data sets than in cross-sectional data sets (e.g., Arik & Pfister, 2019; Goodfellow et al., 2016). Specifically, models called recurrent neural networks (RNNs) led to early advances in predictive modeling of text and speech data, while models called convolutional neural networks (CNNs) led to breakthroughs in processing image, video, and speech data (LeCun et al., 2015). Modern ANNs based on residual neural networks (ResNets; He et al., 2015), temporal convolutional networks (TCNs; Bai et al., 2018), and Transformers (Vaswani et al., 2017) now achieve state-of-the-art performance on a variety of sequence and image modeling problems. Fifth, deep learning can account for betweeand within-group effects. Hierarchical (or nested) data structures in which multiple observations are taken from each of many clusters are common in psychology (e.g., students within schools, patients within clinics). Traditionally, hierarchical data have been analyzed using multilevel modeling (Raudenbush & Bryk, 2002). In deep learning, a similar approach called multitask learning fits models with parameters that are shared across groups as well as group-specific parameters (Caruana, 1997)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In deep learning, a similar approach called multitask learning fits models with parameters that are shared across groups as well as group-specific parameters (Caruana, 1997). Similar to how multilevel modeling improves on non-multilevel statistical methods by permitting unbiased estimation with hierarchical data, multitask deep learning models may improve on non-multitask machine learning and deep learning approaches by predicting psychological outcomes more accurately with hierarchical data (Zhang & Yang, 2017). For example, Taylor et al. (2017) found that a multitask deep learning approach outperformed both multitask SVMs and a hierarchical logistic regression approach at predicting mood, stress, and health status using self-reported behaviors as well as wearable sensor, smartphone log, weather, and GPS data. See Bakker and Heskes (2004) for a discussion of the similarities between multitask learning and multilevel modeling. Deep Learning Models, Terminology, and Notational Conventions What exactly makes a machine learning model a deep learning model ? A defining feature of most deep learning models is that they map the input observations through a sequence of functions where each function in the sequence is called a layer . In more technical terms, nearly all deep learning models are compositions of functions. Function composition is the application of one function to the output of another function. Formally, function composition is written as g f ( x ) = g 1 f ( x ) 2 , (1) where g f is read as “ g composed with f ” or as “ g of f ”. Intuitively, when we compose g with f ( x ), we are feeding some input value x to the function f , which spits out a value f ( x ). In turn, f ( x ) is fed to the function g , which spits out a final value g 1 f ( x ) 2 . In Figure 2a, we visualize this process with a schematic in which circles represent values and arrows represent functions",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Figure 2a, we visualize this process with a schematic in which circles represent values and arrows represent functions. The functions g and f in equation 1 are univariate and scalar-valued , which means they take a single number as input and produce a single number as output, respectively. In general, functions may be multivariate and vector-valued , which means they take vectors as inputs and may produce vectors as outputs, respectively. Compositions of two multivariate vector-valued functions are written g f ( x ) = g 1 f ( x ) 2 (2) = C g 1 3Ë f 1 ( x ) , , f p 1 ( x ) È € 4 , , g p 2 3Ë f 1 ( x ) , , f p 1 ( x ) È € 4D € , (3) where x is a p ◊ 1 vector, f ( x ) is a p 1 ◊ 1 vector, and g 1 f ( x ) 2 is a p 2 ◊ 1 vector. Equation 3 demonstrates that any vector-valued function actually consists of many scalar-valued functions. For example, the vector-valued function f actually consists of p 1 different scalar-valued functions f 1 , , f p 1 that each take in the input vector x and spit out the single numbers f 1 ( x ) , , f p 1 ( x ). We visualize a composition of two multivariate vector-valued functions in Figure 2b. Notice that p , p 1 , and p 2 (i.e., the number of elements in x , f ( x ), and g 1 f ( x ) 2 , respectively) are not necessarily equal: In Figure 2b, x is three-dimensional (i.e., p = 3), f ( x ) is four-dimensional (i.e., p 1 = 4), and g 1 f ( x ) 2 is two-dimensional (i.e., p 2 = 2). We omit function arguments in schematic diagrams for multivariate vector-valued functions to avoid clutter. Compositions may include more than just two functions. For example, we write a composition of three multivariate vector-valued functions as g f (2) f (1) ( x ) = g 3 f (2) 1 f (1) ( x ) 2 4 , (4) where x is a p ◊ 1 vector, f (1) ( x ) is a p 1 ◊ 1 vector, f (2) 1 f (1) ( x ) 2 is a p 2 ◊ 1 vector, and g 3 f (2) 1 f (1) ( x ) 2 4 is a p 3 ◊ 1 vector. In this primer, we use parenthesized numbers in superscripts to denote ordered sequences of objects where object i ≠ 1 comes before object i",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In this primer, we use parenthesized numbers in superscripts to denote ordered sequences of objects where object i ≠ 1 comes before object i . We may in fact compose as many functions as we wish. Compositions of q + 1 multivariate vector-valued functions are written g f ( q ) · · · f (2) f (1) ( x ) = g A f ( q ) 3 f (2) 1 f (1) ( x ) 2 4B , (5) where x is a p ◊ 1 vector, f ( i ) 1 f (1) ( x ) 2 is a p i ◊ 1 vector for i = 1 , , q , and g 1 f (1) ( x ) 2 is a p q +1 ◊ 1 vector. Nearly all deep learning models are compositions of many multivariate vector-valued functions as in equation 5, where each function has parameters we can optimize to help the model accurately predict the outcome variable. Equation 5 helps us define important terminology for describing deep learning models. In deep learning models, each function composed together to build the model is called a layer . f (1) is called the first layer , f (2) is called the second layer , and so on. x is called the input layer . Functions f (1) through f ( q ) are collectively called hidden layers . The final function g is called the output layer . The elements of each vector-valued layer, denoted f ( i ) 1 , , f ( i ) p i for the i th layer, are called nodes , artificial neurons , or units . The total number of hidden layers q is called the model depth . Models with one hidden layer are called shallow . Models with more than one hidden layer are called deep . The number of elements in the i th layer, denoted p i , is called the width of layer i . The process in equation 5 is visualized with annotated layers in Figure 2c. The intuition behind choosing a model with many hidden layers is that the deep learning algorithm may decide how to use each hidden layer to best approximate the relationship between the input and output variables (Goodfellow et al., 2016). Overview of Artificial Neural Network Models The definition of a deep learning model as a composition of functions is quite broad",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Overview of Artificial Neural Network Models The definition of a deep learning model as a composition of functions is quite broad. In principle, we could compose together any arbitrary functions and call the resulting composition a deep learning model. However, choosing completely arbitrary functions to build deep learning models would likely produce models that are very hard to fit or models that do not perform well. Artificial neural networks (ANNs) are one popular solution to this problem. ANNs are a broad class of nonlinear statistical models that can be composed together in many layers. They were initially inspired by biological neural mechanisms (McClelland et al., 1986; McCulloch & Pitts, 1943; Rosenblatt, 1958) and aim to consolidate and transfer information much like in biological learning. Specifically, the ANN modeling framework mimics animal (and human) neural processes in which neurons transfer information via synapses in a feedforward fashion. The earliest ANNs only had one hidden layer, but deep learning implementations of ANNs have multiple hidden layers. ANNs have been extremely successful under the deep learning paradigm because they can be efficiently fitted (using the back-propagation algorithm; Werbos, 1974) and because of their potentially high predictive accuracy. In the following sections, we present some ANNs that may be useful for predictive modeling in psychology. We first describe feedforward neural networks (FNNs). Like linear regression, FNNs are useful for predicting outcomes of interest using tabular data sets (i.e., data sets which can be formatted as the standard N ◊ p design matrix X used in linear regression; see Appendix A). Next, we discuss recurrent neural networks (RNNs), which were developed to predict outcomes of interest using sequential data (e.g., daily diary data; physiological trace data)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Next, we discuss recurrent neural networks (RNNs), which were developed to predict outcomes of interest using sequential data (e.g., daily diary data; physiological trace data). Although RNNs are often outperformed in practice by convolutional neural networks and other modern ANNs (Bai et al., 2018; Vaswani et al., 2017), familiarity with RNN basics is an excellent starting point for exploring more recent deep learning approaches to sequence modeling. Finally, we discuss convolutional neural networks (CNNs), which are powerful tools for predicting outcomes with both image data (e.g., fMRI data) and with sequential data. In this primer, we focus on applications of CNNs to image data due to space concerns but note that applications of CNNs to sequential data are conceptually similar. We compare linear regression, FNNs, RNNs, and CNNs in Table 1. Single-Layer Feedforward Neural Networks The single-layer feedforward neural network (FNN), also called the single-layer perceptron , is the simplest ANN model. The single-layer FNN only has one hidden layer and is therefore a shallow model of the form g 1 f ( x ) 2 as in equation 2. It can be used either for regression (i.e., predicting continuous outcomes) or for classification (i.e., predicting categorical outcomes). Just like in linear regression, the single-layer FNN starts with N observations of p predictor variables collected in an N ◊ p design matrix X as well as N observations of one outcome variable collected in an N ◊ 1 vector y . In linear regression, we typically denote the i th observed vector of predictor variables (i.e., a single row of X ) as x i and the i th observed value of the outcome variable as y i for i = 1 , , N . In this primer, we simplify notation by dropping all i subscripts so that x i is written as x and y i is written as y . This simplification is justified because single-layer FNNs and other ANNs are usually fitted by feeding one randomly selected observation to the model at a time",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This simplification is justified because single-layer FNNs and other ANNs are usually fitted by feeding one randomly selected observation to the model at a time. The particular observation we choose has no impact on how the model is specified. The single-layer FNN aims to to produce a p 1 ◊ 1 hidden layer representation h of each p ◊ 1 observation x that can subsequently be used to predict the corresponding outcome y . To produce this hidden layer representation, the single-layer FNN first computes p 1 different weighted sums of the observed predictor values x j plus an intercept: s k = b hx k, 0 + p ÿ j =1 b hx k,j x j , k = 1 , , p 1 , (6) where s k are weighted sums called activations , b hx k,j are weight parameters, b hx k, 0 are intercept parameters, and p 1 is the number of hidden layer nodes. Equation 6 is simply a linear regression model (see equation A.1) repeated once for each hidden layer node (i.e., p 1 times) with different weight parameters for each node. Note that the weight parameters have “hx” superscripts to indicate that multiplying by these weights gets us “to” the hidden layer h “from” the input layer x . We use this “to-from” notation throughout this primer to unambiguously specify the weight parameters associated with each ANN layer, which is especially helpful when describing ANNs with many layers (Lipton et al., 2015). Next, the single-layer FNN applies a nonlinear function f to each activation s k to compute the hidden layer or the derived predictor nodes: h k = f ( s k ) , k = 1 , , p 1 . (7) These hidden layer nodes are elements of the p 1 ◊ 1 hidden layer representation h , which corresponds to f ( x ) from the g 1 f ( x ) 2 model described in equation 2. The nonlinear function f is called an activation function and enables the single-layer FNN to model outcomes that vary nonlinearly with the input variables. Note that the same activation function is applied to all of the hidden layer nodes",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Note that the same activation function is applied to all of the hidden layer nodes. The hidden layer activation function for single-layer FNNs was traditionally the sigmoid function f ( z ) = 1 1 + e ≠ z , (8) although modern single-layer FNNs mostly use the rectified linear unit or ReLU (Glorot et al., 2011; Jarrett et al., 2009; Nair & Hinton, 2010) fa ( z ) = max(0 , z ) . (9) The sigmoid activation function is visualized in Figure 3a and the ReLU activation function is visualized in Figure 3b. Activation functions are discussed further later in this section. The single-layer FNN can now use the hidden layer representation h to predict the outcome y . To do so, it computes a weighted sum of the hidden layer values plus an intercept term, then applies another nonlinear activation function g to this sum: y = g ( b yh 0 + p 1 ÿ k =1 b yh k h k ) + Á, (10) where “yh” superscripts indicate weight parameters to the output node from the hidden layer nodes and Á is a random error term. The single-layer FNN is visualized in Figure 4. ˆ y , the predicted output value in the final node, is plugged in to an objective function (e.g., mean squared error) to evaluate the model’s predictive accuracy. Note that intercepts are included in the schematic by multiplying the intercepts by new nodes x 0 and h 0 that are both equal to one. Psychologists may be familiar with this procedure from SEM, where it is used to include intercepts in path diagrams, often as triangles (Bollen, 1989). Choosing the final activation function g determines whether our single-layer FNN will perform regression or classification. In regression, we wish to predict an outcome y that may be any real number. In this case, we choose g to be the identity function g ( z ) = z. (11) Figure 3c shows that the identity function simply takes in any real number z and outputs the same real number z",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In this case, we choose g to be the identity function g ( z ) = z. (11) Figure 3c shows that the identity function simply takes in any real number z and outputs the same real number z . In binary classification, we wish to predict an outcome y that may be either zero (corresponding to the first output category) or one (corresponding to the second output category). Here, we choose g to be the sigmoid function (equation 8). Figure 3a demonstrates that the sigmoid function takes in any real number z and outputs a number between zero and one. The number output by the sigmoid activation function represents the probability that the input observation belongs to the second output category (i.e., the category represented by y = 1). Finally, in classification with k categories, we wish to predict a k ◊ 1 outcome vector y that is dummy coded such that the i th dummy variable is equal to 1 and all other dummy variables are equal to 0 (i.e., y = [0 , , 0 , 1 , 0 , , 0] € ). In this case, we choose g to be the softmax function g ( z ) i = e z i q k j =1 e z j , i = 1 , , k, (12) where z is a k ◊ 1 vector. The softmax activation function takes in a k ◊ 1 vector of real values z and outputs a k ◊ 1 vector of probabilities whose i th element represents the probability that the input observation belongs to category i . Similarly to linear regression, the single-layer FNN can be expressed concisely using matrices: y = g 1 ( b yh ) € h + b yh 0 2 + Á, (13) h = f ( B hx x + b hx 0 ) , (13a) where b yh p 1 ◊ 1 = S WWWWWWWWWWU b yh 1 b yh 2 b yh p 1 T XXXXXXXXXXV , h p 1 ◊ 1 = S WWWWWWWWWWU h 1 h 2 h p 1 T XXXXXXXXXXV , (13b–c) B hx p 1 ◊ p = S WWWWWWWWWWU b hx 1 , 1 b hx 1 , 2 · · · b hx 1 ,p b hx 2 , 1 b hx 2 , 2 · · · b hx 2 ,p b hx p 1 , 1 b hx p 1 , 2 · · · b hx p 1 ,p T XXXXXXXXXXV , x p ◊ 1 = S WWWWWWWWWWU x 1 x 2 x p T XXXXXXXXXXV , b hx 0 p 1 ◊ 1 = S WWWWWWWWWWU b hx 1 , 0 b hx 2 , 0 b hx p 1 , 0 T XXXXXXXXXXV , (13d–f) and the activation functions f and g are applied to vectors element-wise",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The p 1 ◊ 1 hidden layer vector h can be thought of as a representation of the original input observation x that the single-layer FNN has learned to help it predict y . As noted in the introduction, h is in fact a vector of p 1 latent variables , or unobserved variables that summarize the information contained in the original observation. Latent variables are likely familiar to psychologists from from SEM, which aims to explain the relationships between a set of observed variables in terms of a number of unobserved latent constructs. In single-layer FNNs, however, each latent variable in h depends on every observed variable in x . This may make the latent variables in single-layer FNNs more difficult to interpret than those in SEM, where each latent variable is intepretable in part because it is only related to a subgroup of conceptually similar observed variables. Single-layer feedforward neural networks generalize linear regression. Previously, we mentioned that ANNs can be thought of as generalizations of linear regression. We now use basic algebra to show that the single-layer FNN can be reduced to the linear regression model. We first choose the single-layer FNN’s activation functions g and f to be identity functions: g ( z ) = z, f ( z ) = z. (14) Our single-layer FNN can then be written as y = b yh 0 + p 1 ÿ k =1 b yh k h k + Á, (15) h k = b hx k, 0 + p ÿ j =1 b hx k,j x j , k = 1 , , p 1 . (15a) We can substitute equation 15a into equation 15 and rearrange to obtain y = b yh 0 + p 1 ÿ k =1 b yh k h k + Á (15) = b yh 0 + p 1 ÿ k =1 b yh k ( b hx k, 0 + p ÿ j =1 b hx k,j x j ) + Á (16) = b Õ 0 + p ÿ j =1 b Õ j x j + Á, (17) where b Õ 0 = b yh 0 + p 1 ÿ k =1 b yh k b hx k, 0 , b Õ j = p 1 ÿ k =1 b yh k b hx k,j , j = 1 , , p. (17a) Equation 17 clearly has the same form as the linear regression model in equation A.1. 2 We have therefore demonstrated that the single-layer FNN with identity activation functions reduces to the linear regression model",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2 We have therefore demonstrated that the single-layer FNN with identity activation functions reduces to the linear regression model. We can directly interpret the b Õ 0 , b Õ 1 , , b Õ p parameters just like we can directly interpret the b 0 , b 1 , , b p parameters in linear regression. However, the single-layer FNN parameters (i.e., the parameters with “yh” and “hx” superscripts) cannot be interpreted. This is because the single-layer FNN is overparameterized – that is, the model has more parameters than equations. Specifically, the single-layer FNN has ( p + 2) ◊ p 1 + 1 parameters total versus p + 1 equations total (see equations 13b, 13d, and 13f to count parameters and equations 17a to count equations). Infinitely many sets of single-layer FNN parameters will satisfy equations 17a and produce a valid model, so any particular set of 2 When we use a suitable optimization procedure, our estimates for the b Õ 0 , b Õ 1 , , b Õ p parameters will converge to the ˆ b 0 , ˆ b 1 , , ˆ b p parameter estimates produced by the usual linear regresssion algorithm (see equation A.6; e.g., Baldi & Hornik, 1995). single-layer FNN parameters we choose will have no intrinsic meaning (Kutner et al., 2004). We showed that linear regression is a special case of the single-layer FNN to help readers better understand the relationship between these models. In practice, using a single-layer FNN to perform linear regression is overly complicated. Our toy example did, however, highlight a very real, practical issue: Overparameterization. Nearly all ANNs are overparameterized and therefore have uninterpretable parameters. The uninterpretability of ANN parameters is sometimes seen as a major shortcoming. However, ANNs often achieve much higher predictive accuracy than simpler, directly interpretable models like linear regression, especially when the true causal structure underlying the data set contains weakly correlated interactions between large numbers of variables",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Additionally, recent work has explored methods for interpreting ANNs that avoid the overparameterization problem (e.g., Montavon et al., 2017). These methods aim to interpret the concepts learned by the ANN’s hidden layers and to identify the most important predictor variables. We now turn to a detailed toy example to illustrate how single-layer FNNs may be useful for psychological research despite being challenging to interpret. Single-layer FNN toy example: Predicting alcohol use disorder using cross-sectional survey data. Imagine that we have access to data from a large, nationally representative, cross-sectional survey that was designed to measure mental health and substance use. Such data are often publicly available (e.g., the National Survey on Drug Use and Health; https://nsduhweb.rti.org/respweb/homepage.cfm) and may contain a wealth of information about various psychological phenomena in the population at large. Our survey includes the demographic variables gender and age, diagnostic criteria for major depressive disorder (MDD), and diagnostic criteria for nicotine, marijuana, and alcohol use disorders (NUDs, MUDs, and AUDs, respectively). We are interested in finding out whether meeting diagnostic criteria for AUD can be predicted using the other demographic, mental health, and substance use variables available in our data set. This question may be interesting in clinical psychology, for example, where researchers and clinicians may wish to provide targeted interventions for individuals who are identified as being at risk for problematic outcomes such as AUD. Assume that our data set includes N = 5 , 000 individuals. Our AUD outcome is a binary variable that is equal to one when an individual meets the relevant diagnostic criteria and is equal to zero otherwise. Our gender, MDD, NUD, and MUD predictor variables are also binary",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Our gender, MDD, NUD, and MUD predictor variables are also binary. Our initial age predictor is a number greater than or equal to 18 (i.e., all individuals are adults); we normalize age by subtracting this predictor’s minimum value and dividing by its range before model fitting. Our goal is to use each individual’s predictors x to predict whether they meet diagnostic criteria for AUD (i.e., their outcome value is one) or not (i.e., their outcome value is zero). Prediction problems like this are called classification problems in machine learning because the goal is to assign each individual to the correct class (i.e., meeting diagnostic criteria for AUD or not). We will develop a predictive model using a training-test split approach (see Appendix B) wherein models are fitted using a training set of 4 , 500 observations and model predictive accuracy is evaluated using a test set consisting of the remaining 500 observations. We used R (R Core Team, 2020) to simulate predictors based on the description in the previous paragraph. Specifically, predictors were sampled for each individual as follows: gender ≥ Bernoulli(0 . 5) , (18) age ≥U (18 , 85) , (19) MUD ≥ Bernoulli 1 f ( ≠ 1 ≠ age + 2 · gender) 2 , (20) NUD ≥ Bernoulli 1 f ( ≠ 1 ≠ age + 2 · gender + 0 . 5 · MUD) 2 , (21) MDD ≥ Bernoulli 1 f ( ≠ 1 ≠ age + 2 · gender + 0 . 5 · MUD + 0 . 5 · NUD) 2 , (22) where U ( a, b ) denotes a uniform distribution with lower bound a and upper bound b and f denotes the sigmoid function (equation 8)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5 · MUD + 0 . 5 · NUD) 2 , (22) where U ( a, b ) denotes a uniform distribution with lower bound a and upper bound b and f denotes the sigmoid function (equation 8). This sampling scheme was motivated as follows: (1) Approximately half of the sampled individuals should be male (gender = 1; equation 18); (2) individuals should all be adults (i.e., over age 18) and all age groups should be evenly represented in the sample (equation 19); (3) holding other variables constant, individuals should be at relatively low risk of MDD and substance use disorders (SUDs; intercept of ≠ 1 in equations 20-22); (4) increasing age should protect against risk of MDD and SUDs (negative coefficient on age in equations 20-22); (5) being male should increase risk of MDD and SUDs (positive coefficient on gender in equations 20-22); and (6) having any of MDD, MUD, or NUD should increase risk for the other variables (positive coefficients on MDD, MUD, and NUD in equations 20-22). AUD outcome values were then generated nonlinearly from the predictor values where all predictors related to the outcome to varying degrees: AUD ≥ Bernoulli 3 f 1 ≠ 1 ≠ (age + age 2 + age 3 + age 4 + age 5 )+ 2 · gender + 0 . 5 · MDD + 0 . 5 · NUD + 0 . 5 · MUD ≠ 2 · (age + age 2 + age 3 + age 4 + age 5 ) · gender 2 4 , (23) where the age predictor was normalized prior to generation. Equation 23 is the same as equations 20-22 except age now enters the model nonlinearly and interacts with gender such that older males have decreased risk of AUD. Approximately 26 percent of simulated individuals met AUD criteria (i.e., 26 percent of the generated AUD outcome values were equal to one). Once the data were simulated, we used the R interface to Keras (a Python package for fitting ANNs; Falbel et al., 2019) to classify individuals using the single-layer FNN given in equations 13 and 13a",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We first constructed the model as follows: fnn = keras_model_sequential () fnn %>% layer_dense (units = 5, activation = 'relu ', input_shape = c (5)) %>% layer_dense (units = 1, activation = 'sigmoid ') In Keras, ANNs are constructed one layer at a time. The above code first creates a single hidden layer (i.e., a layer_dense () ) with five nodes and a ReLU activation function, then connects these hidden nodes to the outcome node and applies a sigmoid activation function. Next, we specified some parameters for model fitting: fnn %>% compile ( loss = 'binary_crossentropy ', optimizer = optimizer_adam (), metrics = c ('accuracy ') ) Here, we specify the objective function or loss function to be the log-likelihood of the Bernoulli-distributed AUD outcome summed over all individuals. This log-likelihood is called the binary cross-entropy in machine learning and is the same objective function used in logistic regression (Kutner et al., 2004). By setting optimizer = optimizer_adam () , we fit our model using an optimization procedure called Adam (Kingma & Ba, 2015). Adam demonstrates good performance in practice with little to no hyperparameter tuning (Goodfellow et al., 2016). We also choose to assess model performance using predictive accuracy (i.e., the proportion of outcome values correctly predicted by the model). Finally, we fitted the model as follows: fnn %>% fit ( as.matrix (train_data$X), as.matrix (train_data$y), epochs = 70, batch_size = 128 ) fnn %>% evaluate ( as.matrix (test_data$X), as.matrix (test_data$y)) The first two inputs to the fit () function are a test set matrix of predictors and the associated vector of outcomes, respectively. ANNs are fitted iteratively using small random samples of observations at each iteration. We randomly sample 128 observations at each fitting iteration using batch_size = 128 . If we sample observations without replacement, we will eventually have sampled all the observations in our data set and must replace all the observations before fitting can proceed",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If we sample observations without replacement, we will eventually have sampled all the observations in our data set and must replace all the observations before fitting can proceed. A single full pass through every observation in the data set is called an epoch in deep learning. Our code continues fitting until we have sampled all the observations in the data set 70 times by setting epochs = 70 . We compute the fitted model’s predictive accuracy using the evaluate () function; our fitted model obtained a predictive accuracy of 0.86 the test set. An important note regarding these analyses is that we did not tune our model hyperparameters. A complete R script for our simulation and analysis is available as online supplemental material. Our model is visualized in Figure 5. We now step through how our model computes a predicted value for a single observation from our simulated data set using our fitted model parameters. This observation’s normalized age was 0.22, and all of the other (binary) predictors were 1. We first compute the vector of activations s by multiplying our vector of predictors by a weight matrix B hx and adding an intecept vector b 0 hx just like in linear regression: s = B hx x + b hx 0 (24) = S WWWWWWWWWWWWWWU ≠ 0 . 03 0 . 93 0 . 47 0 . 07 0 . 50 ≠ 0 . 29 ≠ 2 . 00 1 . 59 0 . 16 ≠ 1 . 79 ≠ 0 . 01 0 . 26 0 . 81 ≠ 0 . 13 0 . 57 0 . 48 0 . 31 0 . 57 ≠ 0 . 52 0 . 44 ≠ 0 . 03 0 . 02 ≠ 0 . 07 0 . 05 0 . 42 T XXXXXXXXXXXXXXV S WWWWWWWWWWWWWWU 1 0 . 22 1 1 1 T XXXXXXXXXXXXXXV + S WWWWWWWWWWWWWWU ≠ 0 . 17 0 . 13 0 . 52 ≠ 0 . 28 0 . 43 T XXXXXXXXXXXXXXV (25) = 5 1 . 05 ≠ 0 . 65 1 . 82 0 . 76 0 . 80 6 € . (26) We then apply the ReLU activation function f (equation 9) to our vector of activations s to compute our hidden layer vector of latent variables: h = f ( s ) = 5 1 . 05 0 . 00 1 . 82 0 . 76 0 . 80 6 € , (27) which simply sets any negative values in s to zero",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 05 0 . 00 1 . 82 0 . 76 0 . 80 6 € , (27) which simply sets any negative values in s to zero. We then repeat the process with h : We multiply it by a weight vector ( b yh ) € , add an intercept b yh 0 , then apply the sigmoid activation function g (equation 8): y = g 1 ( b yh ) € h + b yh 0 2 (28) = g A 5 0 . 48 1 . 31 ≠ 0 . 84 0 . 62 1 . 57 6 S WWWWWWWWWWWWWWU 1 . 05 0 . 00 1 . 82 0 . 76 0 . 80 T XXXXXXXXXXXXXXV ≠ 1 B = g ( ≠ 0 . 29) = 0 . 43 . (29) The outputted value of 0 . 43 corresponds to the predicted probability that the simulated individual meets AUD criteria. Recall that our single-layer FNN’s objective function (Figure 5) is the same objective function used in logistic regression. To illustrate the difference between these methods, we compared our model’s performance to that of logistic regression with a lasso penalty fitted using the R package glmnet (Friedman et al. (2010); see McNeish (2015) for an overview of lasso penalized regression). Predictive accuracy as well as sensitivity (i.e., the proportion of individuals meeting AUD criteria who were correctly predicted by the model) and specificity (i.e., i.e., the proportion of individuals not meeting AUD criteria who were correctly predicted by the model) were computed on the test set for both models. Our single-layer FNN (accuracy = 0 . 86, sensitivity = 0 . 62, specificity = 0 . 94) mostly outperformed penalized logistic regression (accuracy = 0 . 80, sensitivity = 0 . 30, specificity = 0 . 97), suggesting that the FNN better captured the underlying nonlinear relationship between the predictors and the outcomes. Additionally, the FNN obtained this superior performance with no hyperparameter tuning. Although ANNs often obtain reasonable performance with little to no hyperparameter tuning, conducting extensive hyperparameter tuning may produce astonishing performance gains. See Bengio (2012), Heaton (2008), or Smith (2018) for discussions of hyperparameter tuning for ANNs",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". See Bengio (2012), Heaton (2008), or Smith (2018) for discussions of hyperparameter tuning for ANNs. We note that complex survey designs typically include sampling weights to ensure that population subgroups are adequately represented in subsequent analyses (Lavalleé & Beaumont, 2015). We chose not to simulate sampling weights to facilitate our comparison of single-layer FNNs with lasso penalized logistic regression. However, if we had simulated sampling weights, they could have been included in our single-layer FNN objective function simply by multiplying each individual’s objective function value by their sampling weight. This is easy to do using Keras – a vector of sampling weights can be included in fit () using the sample_weight argument. Deep Feedforward Neural Networks Single-layer FNNs are not deep learning models because they only have one hidden layer. Deep feedforward neural networks , also called multi-layer perceptrons , extend single-layer FNNs by including more than one hidden layer. Using many hidden layers may allow deep FNNs to model complicated, nonlinear relationships between the input and output variables more efficiently than single-layer FNNs, often making deep FNNs a better modeling choice than single-layer FNNs for large, complicated data sets. The deep FNN equations are very similar to the single-layer FNN equations. To produce its first p 1 hidden layer nodes, the deep FNN computes p 1 weighted sums of the predictor values x j plus an intercept, then applies an activation function f (1) to each sum: h (1) k = f (1) ( b h 1 x k, 0 + p ÿ j =1 b h 1 x k,j x j ) , k = 1 , , p 1 (30) where “ h 1 x ” superscripts indicate to weight parameters to the first hidden layer nodes from the predictor nodes. Equation 30 is clearly the same as equation 7 substituted into equation 6 – that is, the deep FNN’s first hidden layer is clearly computed the same way as the single-layer FNN’s single hidden layer",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Equation 30 is clearly the same as equation 7 substituted into equation 6 – that is, the deep FNN’s first hidden layer is clearly computed the same way as the single-layer FNN’s single hidden layer. The deep FNN then uses the hidden layer nodes at layer ̧ ≠ 1 to produce the hidden layer nodes at layer ̧ . Specifically, successive hidden layer nodes are produced by computing weighted sums over the previous hidden layer nodes plus intercept terms, then applying activation functions to these sums: h ( ̧ ) k = f ( ̧ ) ( b h ̧ h ̧ ≠ 1 k, 0 + p ̧ ≠ 1 ÿ j =1 b h ̧ h ̧ ≠ 1 k,j h ( ̧ ≠ 1) j ) , k = 1 , , p ̧ , ̧ = 2 , , q, (31) where q denotes the model depth, p ̧ denotes the number of nodes at hidden layer ̧ , and “h ̧ h ̧ ≠ 1 ” superscripts indicate weight parameters to hidden layer ̧ nodes from hidden layer ̧ ≠ 1 nodes. The outcome node is predicted by taking a weighted sum of the final hidden layer nodes plus an intercept term, then applying an activation function g to this sum: y = g ( b yh q 0 + p q ÿ k =1 b yh q k h ( q ) k ) + Á, (32) where “yh q ” superscripts indicate weight parameters to the outcome node from the final hidden layer nodes and Á is a random error term. As with the single-layer FNN, choice of the activation function g determines whether the deep FNN will perform regression or classification",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As with the single-layer FNN, choice of the activation function g determines whether the deep FNN will perform regression or classification. Like single-layer FNNs, deep FNNs can be represented concisely using matrices: y = g 1 ( b yh q ) € h ( q ) + b yh q 0 2 + Á, (33) h ( ̧ ) = f ( ̧ ) ( B h ̧ h ̧ ≠ 1 h ( ̧ ≠ 1) + b h ̧ h ̧ ≠ 1 0 ) , ̧ = 2 , , q, (33a) h (1) = f ( B h 1 x x + b h 1 x 0 ) , (33b) where x is the p ◊ 1 vector of predictors, B h 1 x is the p 1 ◊ p weight matrix from the predictors to the first hidden layer, b h 1 x 0 is the p 1 ◊ 1 intercept vector from the predictors to the first hidden layer, h ( ̧ ) is the p ̧ ◊ 1 hidden layer representation at layer ̧ , B h ̧ h ̧ ≠ 1 is the p ̧ ◊ p ̧ ≠ 1 weight matrix from hidden layer ̧ ≠ 1 to hidden layer ̧ , b h ̧ h ̧ ≠ 1 0 is the p ̧ ◊ 1 intercept vector from hidden layer ̧ ≠ 1 to hidden layer ̧ , b yh q is the p q ◊ 1 weight vector from the final hidden layer to the outcome, and b yh q 0 is the intercept from the final hidden layer to the outcome. We visualize the deep FNN in Figure 6. Notice that intercept terms are omitted from the schematic and that layer-wise summation and applying an activation function are represented using a single arrow. ANNs are typically represented this way in the deep learning literature. It is also common to omit intercept terms in path diagrams for structural equation models (Bollen, 1989). It is not always clear when to use deep FNNs instead of single-layer FNNs in practice. In theory, both single-layer and deep FNNs can model very complicated relationships between the input and output variables. This is stated formally in the universal approximation theorem , which holds that both single-layer and deep FNNs are capable of approximating a wide variety of continuous functions, given some mild assumptions 3 about the hidden layer activation functions (e.g., Csáji, 2001)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This capability may be useful for problems in psychology, where the true functional relationship f between the input x and the output y may be very complicated. In practice, however, single-layer FNNs may require a huge numbers of hidden layer nodes to learn the true f . Deep FNNs with many hidden layers may need fewer nodes at each layer to learn the true f and may therefore take less time to train than single-layer FNNs (Goodfellow et al., 2016). We recommend treating the number of hidden layers as a hyperparameter: Start with one hidden layer, then increase the number of hidden layers a few times and check whether predictive accuracy improves on a validation set. 3 There are a several proofs of the universal approximation theorem for both single-layer and deep FNNs. Most of these proofs assume that the hidden layer activation functions are non-constant (i.e., they are not always equal to a single, constant value), bounded (i.e., they don’t shoot offto positive or negative infinity), and continuous (i.e., their graphs do not have any gaps). More recent proofs (e.g., Lu et al., 2017; Sonoda & Murata, 2017) do not require the activation functions to be bounded. Recurrent Neural Networks Recurrent neural networks (RNNs) are specialized ANNs for processing sequential data where the order of the observations is meaningful. Like FNNs, RNNs can be used for regression or classification. Although RNNs are outperformed by CNNs and more modern ANNs in some practical settings, this section illustrates fundamental concepts that are applicable to a broad range of ANN approaches to modeling sequential data. Before discussing RNNs, we discuss how to describe a data set where each observation is a sequence . A sequence of length T is an ordered set of T observations of p different variables. Data sets, which we denote as X , typically contain many sequences. For example, daily diary data sets typically include N different individuals, each of whom is asked p questions each day for T i days, i = 1 , , N",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, daily diary data sets typically include N different individuals, each of whom is asked p questions each day for T i days, i = 1 , , N . Note that the lengths of the sequences in our data set may be different for each individual, but the number of variables is the same for all individuals. We write length T i sequences as x (1) i , x (2) i , , x ( T i ) i , where x ( t ) i is the p ◊ 1 vector of predictor values observed for individual i at time point t . In practical applications of RNNs, it is common to first ensure that all sequences are the same length (e.g., by appending vectors of zeros to short sequences), say length T , then to collect each individual’s sequences into a T ◊ p matrix X i . Our data set may then be thought of as a collection of N T ◊ p matrices or, equivalently, as an N ◊ T ◊ p array called a tensor . Tensors, which are multidimensional arrays that generalize matrices to higher-dimensions, are an important concept because deep learning software typically requires data to be input in tensor form. Tensors are briefly discussed and visualized in the Convolutional Neural Networks section. We can now formulate the simple RNN model. As before, we drop all i subscripts when specifying models. Consider an input sequence x (1) , x (2) , , x ( T ) and a corresponding output sequence y (1) , y (2) , , y ( T ) . At each time point t , the simple RNN aims to produce a p 1 ◊ 1 hidden layer representation h ( t ) of the p ◊ 1 input vector x ( t ) that can then be used to predict the outcome y ( t ) . Each hidden layer should also utilize information from the previous time step when making predictions. Intuitively, the simple RNN is just a single-layer FNN with loops carrying information forward through time",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Intuitively, the simple RNN is just a single-layer FNN with loops carrying information forward through time. The simple RNN models the current outcome node as a function of the current predictor nodes as well as the previous hidden layer nodes: y ( t ) = g ( b yh 0 + p 1 ÿ k =1 b yh k h ( t ) k ) + Á ( t ) , t = 1 , , T, (34) h ( t ) k = f ( b hx k, 0 + p ÿ j =1 b hx k,j x ( t ) j + p 1 ÿ k =1 b hh k h ( t ≠ 1) k ) , k = 1 , , p 1 , t = 1 , , T, (34a) where “hh” superscripts indicate recurrent weight parameters connecting hidden layer nodes at subsequent time steps, Á ( t ) is the error at time point t , and the initial hidden layer nodes h (0) k are all defined to be 0. Equations 34 and 34a are called update equations because they describe how to update the hidden state h ( t ) and the predicted output y ( t ) at each time point t given the current input x ( t ) and the previous hidden state h ( t ≠ 1) . Any mathematical equation that starts with initial values and defines all future values as functions of previous values is called a recurrence relation . Recurrent neural networks are called “recurrent” because their update equations are a recurrence relation. Notice that the model re-uses the same weight parameters at each time step. Re-using weight parameters this way is called parameter sharing and allows RNNs to be trained with and applied to sequences of varying lengths",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Re-using weight parameters this way is called parameter sharing and allows RNNs to be trained with and applied to sequences of varying lengths. The simple RNN update equations may be written concisely with matrices: y ( t ) = g 1 ( b yh ) € h ( t ) + b yh 0 2 + Á ( t ) , t = 1 , , T, (35) h ( t ) = f ( B hx x ( t ) + B hh h ( t ≠ 1) + b hx 0 ) , t = 1 , , T, (35a) where h ( t ) is the p 1 ◊ 1 hidden layer vector at time point t , B hx is the p 1 ◊ p matrix of weight parameters to the hidden layer nodes from the input nodes, b hx 0 is a p 1 ◊ 1 intercept vector applied to the hidden layer nodes, B hh is an p 1 ◊ p 1 matrix of recurrent weight parameters, b yh is an p 1 ◊ 1 vector of weight parameters to the output node from the hidden layer nodes, b yh 0 is an intercept to the output nodes from the hidden layer nodes, and the initial hidden state nodes h (0) are defined to be 0 (a p 1 ◊ 1 vector of zeros). Just like equations 34 and 34a, equations 35 and 35a are a recurrence relation – that is, all output values are functions of previous values. To better understand the simple RNN, it is helpful to write the network in equation form without using recurrence. This is called unfolding the network. The simple RNN is unfolded as y (1) = g (( b yh ) € f ( B hx x (1) + b hx 0 ) + b yh 0 ) + Á (1) , y (2) = g 3 ( b yh ) € f 1 B hx x (2) + B hh f ( B hx x (1) + b hx 0 ) + b hx 0 2 + b yh 0 4 + Á (2) , y ( T ) = g A ( b yh ) € f 3 B hx x ( T ) + + B hh f 1 B hx x (2) + B hh f ( B hx x (1) + b hx 0 ) + b hx 0 2 4 + b yh 0 B + Á ( T ) . (36) Equations 35 are simply equation 35a substituted into equation 35 and written down explicitly for all time steps. Schematic representations of equations 35, 35a, and 36 are presented in Figure 7. The schematic on the left hand side of Figure 7 represents equations 35 and 35a as a single-layer FNN with a loop passing information from one time step to the next",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The schematic on the left hand side of Figure 7 represents equations 35 and 35a as a single-layer FNN with a loop passing information from one time step to the next. The schematic on the right hand side of Figure 7 represents the unfolded simple RNN in equations 36 as T single-layer FNNs chained together. Both schematics are equivalent representations of the same simple RNN. In general, the input and output sequence need not have the same length. If our input sequence has length one (i.e., x (1) ) and our output sequence has length T (e.g., y (1) , y (2) , , y ( T ) ), our RNN has a one-to-many architecture (Figure 8a). An ANN’s architecture refers to the number of nodes in the network and the ways that these nodes are connected (i.e., which nodes are connected as well as which weight structures and activation functions are used; Goodfellow et al., 2016). To understand when we might use an RNN with a one-to-many architecture, consider a daily diary study in which we collect information about each individual’s moods and experiences once per day for several months. If we wish to use an individual’s mood and experiences on a given day to predict their mood each day for the next three days, we would use a one-to-many architecture. If our input sequence has length T (i.e., x (1) , x (2) , , x ( T ) ) and our output sequence has length one (i.e., y (1) ), our recurrent neural network has a many-to-one architecture (Figure 8b). If we wish to predict whether or not an individual will experience a depressive episode on a particular day using their past week of moods and experiences, we would use a many-to-one architecture. Finally, if both have length greater than one, our RNN has a many-to-many architecture (Figure 8c). If we wish to predict an individual’s moods for the next three days using their past week of moods and experiences, we would use a many-to-many architecture. RNNs are appealing in theory because they use information from the past to predict future output values",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". RNNs are appealing in theory because they use information from the past to predict future output values. In practice, however, simple RNNs struggle to use information from very far in the past (Bengio et al., 1994; Doya, 1993; Hochreiter, 1991; Pascanu et al., 2013). This may not be a problem if we only expect recent information to influence the current output value. For example, imagine collecting information about individuals’ moods and experiences twice per day. We might expect a person’s mood early in the day to strongly influence their mood later in the day, but might expect their mood yesterday or earlier to only weakly influence their nighttime mood. A simple RNN might be well-suited to to modeling this scenario. However, if an individual experienced an extremely negative life event one month ago that we expect to strongly impact their current mood, a simple RNN might struggle to use information from so far in the past to predict a current output value. To overcome this problem with simple RNNs, specialized models called gated recurrent neural networks were designed to learn long-term dependencies. Gated RNNs have been effective for some sequence modeling problems including speech recognition, machine translation, and image captioning (Goodfellow et al., 2016). Lipton et al. (2015) provide an accessible overview of gated RNNs that have demonstrated good performance in several practical applications. By now, some readers may have noticed that RNNs are quite similar to classical time series approaches such as dynamic factor analysis (DFA; e.g., Lütkepohl, 2005; Molenaar, 1985) that model the relationships between latent variables over time. For example, DFA models relations among latent variables at the current time as a function of the latent variables at previous, or lagged , time steps",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, DFA models relations among latent variables at the current time as a function of the latent variables at previous, or lagged , time steps. In a conceptually similar manner, RNNs update their hidden layer values at the current time point using hidden layer values from the previous time point and may even explicitly include higher-order lagged relations between the hidden layers (e.g., relations between h ( t ≠ 2) and h ( t ) and so on). Although they are similar, RNNs and classical approaches like DFA differ in several ways. In particular, RNN-based approaches improve on classical time series approaches in that they automatically account for nonlinear relationships between variables, they do not require conducting data reduction using latent variables, they do not require all individuals to have variability on each variable, and they do not require researchers to pre-screen the data for multicollinearity. We note, however, that the benefits of RNNs come at the cost of model interpretability. In the previous section, we discussed how making FNNs deeper by adding more hidden layers often leads to increased predictive accuracy. Building deep RNNs is less straightforward than building deep FNNs because there are multiple ways to add more hidden layers. In particular, hidden layers can be added to RNNs in three ways: (1) Between the input nodes and the hidden layer nodes at each time point, (2) between hidden layer nodes at subsequent time points, and (3) between the hidden layer nodes and the output nodes at each time point. Each kind of deep RNN may be more or less suited for different kinds of data sets (Pascanu et al., 2014). Determining which kind of deep RNN is best suited for a particular data set is often a matter of experimentation. RNN toy example: Forecasting risk of suicidal ideation using daily diary data. We now present a toy example demonstrating the possible application of RNNs to daily diary data",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". RNN toy example: Forecasting risk of suicidal ideation using daily diary data. We now present a toy example demonstrating the possible application of RNNs to daily diary data. Imagine that we collect one week of daily diary data for a clinical sample of 1 , 000 individuals who are at risk for suicidal ideation (SI; i.e., thinking about or considering suicide; Klonsky et al., 2016). Every day, our individuals receive two cell phone notifications. The first notification arrives in the morning and asks each respondent to rate their current mood using a five-category scale ranging from “Extremely good” (1) to “Extremely bad” (5). The second notification arrives at night and asks individuals to rate their mood using the same five-category scale as well as to indicate whether or not they thought about committing suicide at any time during the day (“Yes” = 1, “No” = 0). Our goal is to determine whether previous days’ moods and SI can be used to forecast whether or not respondents will engage in SI on day seven. We model this problem using a many-to-one RNN wherein respondents’ moods and SI on days one through six are the inputs (i.e., x ( t ) for t = 1 , , 6) and SI on day seven is the output (i.e., ˆ y (7) ). Figure 9 steps through how computation proceeds in our RNN approach to forecasting SI. The key idea underlying these computations is that the RNN’s hidden state evolves over time based on current and lagged information. At each time step, say time step t , we start with a vector of daily diary responses x ( t ) as well as a hidden state from the previous day h ( t ≠ 1) . To compute the current hidden state h ( t ) , we first multiply the current responses x ( t ) by the input weight matrix B hx and multiply the lagged hidden state h ( t ≠ 1) by the recurrent weight matrix B hh . After adding the resulting vectors together, we add a vector of intercepts b hx 0 and apply a ReLU activation function f to produce the current hidden state h ( t )",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". After adding the resulting vectors together, we add a vector of intercepts b hx 0 and apply a ReLU activation function f to produce the current hidden state h ( t ) . At the final time step (i.e., t = 6), we use only the final hidden state h (6) to predict whether or not the individual will engage in SI on day seven. Notice that the hidden state is updated using the same parameters at each time step – that is, RNN parameters are time-invariant . Computing model updates this way is similar to the approach used in vector autoregression (VAR; Lütkepohl, 2005), a classical time series approach to modeling the relationships between observed variables over time. 4 Due to the close relationship between RNNs and classical time series approaches such as VAR, we used R to simulate data from an ordinal VAR model of lag order six, which is essentially an ordinal regression model (McCullagh, 1980) in which the observed variables at the current time step are regressed on the observed variables at the previous six time points. Creal et al. (2013) provide a full description of generalized autoregressive score (GAS) models, of which our ordinal VAR model is a special case where the observed variables at the current time point are multinomially distributed conditional on the previous time point. Our model included three ordinal variables: Morning mood , which had 5 response categories ranging from 1 to 5; nighttime mood , which had 5 categories response categories ranging from 1 to 5; and suicidal ideation (SI), which had 2 response categories of either 0 or 1. To sample sequences from this model, we needed to specify six autoregressive (AR) weight matrices describing the relationships between current and past observed variable values as well as three vectors of strictly ordered category intercepts (i.e., one intercept vector per variable; these are equivalent to the threshold parameters used in ordinal regression). We set the first-order AR (i.e., AR(1)) weight matrix to the values given in Table 2",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We set the first-order AR (i.e., AR(1)) weight matrix to the values given in Table 2. We chose response category 1 (i.e., “Extremely good” mood) as the reference category for morning and nighttime mood and response category 1 (engaging in SI) as the SI reference category. We therefore only needed to specify AR weights for the 9 possible non-reference categories. Choosing the values in Table 2 ensured being in a very bad mood either in the morning or at night was highly predictive of SI and that the effects of previous days’ moods on individuals’ current moods tended to diminish with increasing time. More specifically, the chosen values were motivated as follows: (1) Having a good morning mood 4 Readers familiar with VAR may notice that the recurrent weight matrix B hh closely corresponds to the time-invariant weight matrix used in a VAR model of order one. The difference between these approaches is that the RNN recurrent weight matrix is used to update an unobserved hidden state, whereas the VAR weight matrix is used to predict observed variable values at each time step. or nighttime mood (response categories 2 or 3) on the previous day should not affect the chances of having a good morning or nighttime mood or engaging in SI on the next day relative to the reference category (zero coefficients on lag 1 morning and nighttime good moods); (2) having a bad morning mood or nighttime mood (response categories 4 or 5) or engaging in SI on the previous day should decrease the chances of having a good morning or nighttime mood on the next day and increase the chances of engaging in SI on the next day (positive coefficients on lag 1 morning and nighttime bad moods and SI). We constructed higher-order AR weight matrices by multiplying the AR(1) weight matrix parameters by the following constants: 0 . 5 for AR(2), 0 . 4 for AR(3), 0 . 3 for AR(4), 0 . 2 for AR(5), and 0 . 1 for AR(6)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5 for AR(2), 0 . 4 for AR(3), 0 . 3 for AR(4), 0 . 2 for AR(5), and 0 . 1 for AR(6). Constructing AR weight matrices this way was motivated by the idea that observations from more distant time points should have less influence on the current observations than more recent observations. Intercept parameters for both morning and nighttime moods were set to ≠ 0 . 8, ≠ 0 . 2, 0 . 2, and 0 . 8, corresponding to the thresholds between response categories 1 and 2, 2 and 3, 3 and 4, and 4 and 5, respectively. The intercept parameter for SI was set to 0 . 5 so that individuals from our clinical sample would be predisposed to engaging in SI. After specifying our data generating model parameters, we sampled a data set of 1 , 000 sequences of length six, discarding the first 100 observations from each sampling run for burn-in. Approximately 21 percent of simulated individuals engaged in SI on day seven (i.e., had y (7) = 1). We then used the R interface to Keras to fit a gated RNN model called a long short-term memory neural network (LSTM; Hochreiter & Schmidhuber, 1997) using 700 simulated sequences, which we evaluated using the remaining 300 sequences. A complete explanation of LSTMs is beyond the scope of this primer but is available in Lipton et al. (2015). We constructed our LSTM as follows: rnn = keras_model_sequential () rnn %>% layer_lstm(units = 32, activation = 'relu ', input_shape = c (6, 9)) %>% layer_dense (units = 32, activation = 'relu ') %>% layer_dropout (rate = 0.5) %>% layer_dense (units = 1, activation = 'sigmoid ') layer_lstm () adds an LSTM hidden layer with 32 hidden units and a ReLU activation funtion to our model. Our input consists of sequences of length six where each observation in the sequence is a nine-dimenional vector of dummy variables, so we set input_shape = c (6, 9) . We next add a single FNN layer to our LSTM, then apply a regularization technique called dropout using layer_dropout () (Srivastava et al., 2014)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We next add a single FNN layer to our LSTM, then apply a regularization technique called dropout using layer_dropout () (Srivastava et al., 2014). We complete the model specification with a single output node with a sigmoid activation function. We used the same model fitting parameters that were used for our single-layer FNN toy example: rnn %>% compile ( loss = 'binary_crossentropy ', optimizer = optimizer_adam (), metrics = c ('accuracy ') ) Specifically, we specify the binary cross-entropy objective function, the Adam optimizer, and the accuracy metric for assessing performance. Finally, we fitted and evaluated the model as follows: history = rnn %>% fit ( train_data$X, as.matrix (train_data$y), epochs = 300, batch_size = 64 ) rnn %>% evaluate (test_data$X, as.matrix (test_data$y)) The first input to fit () is a 700 ◊ 6 ◊ 9 tensor of training set sequences and the second input is a vector of outcome values. We fit the model for 300 epochs (i.e., passes through the full data set; see Single-layer FNN toy example: Predicting alcohol use disorder using cross-sectional survey data ) and randomly sample 64 sequences for each fitting iteration. Finally, we tested our LSTM on a test set of 30 percent of observations. We also calculated sensitivity and specificity using the R package caret (Kuhn, 2008). Code for the full example is available as online supplementary material. Our fitted LSTM achieved fair performance on the simulated data with relatively little hyperparameter tuning (accuracy = 0 . 80, sensitivity = 0 . 30, specificity = 0 . 87). We suspect that our LSTM struggled to model the low-order autoregressive effects underlying the data as described in Gers et al. (2001). Briefly, Gers et al. (2001) note that while classical time series models such as VAR explicitly specify lagged effects, typical RNN implementations rarely specify lagged effects and instead require lagged information to be stored in the model’s hidden layer",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It may be challenging for RNNs without lagged effects to “learn” when to store and when to overwrite lagged information, although we suspect this problem could be alleviated by greatly increasing the size of the RNN hidden layer or by fitting the RNN for many epochs using adequate regularization. If we wished to obtain better performance on our simulated data using an alternative ANN approach, we could likely instead use a CNN because CNNs are inherently well suited to leverage effects that are close together in time to obtain high predictive accuracy. We now turn to these models. Convolutional Neural Networks Just like RNNs are specialized for processing sequential data, convolutional neural networks (CNNs) are specialized for processing image data. In both cases, the ordering of the data matters. Just like FNNs and RNNs, CNNs can be used for regression or classification. CNNs process images very efficiently by assuming that local information is important in each observation. With images, this assumption is almost always reasonable. For example, consider the image in Figure 1. Imagine that a small, square-shaped patch of pixels is randomly selected from this image. We could look at this patch and state whether or not it contains the edge of an object without needing to look at any other parts of the image – that is, only local information is important for identifying edges of objects. This is how CNNs work: They break up images into overlapping, squared-shaped patches of pixels, then summarize the information contained in each patch. Deep CNNs perform this summarization step at each layer, building more and more abstract concepts (e.g., types of objects like faces or shirts) by summarizing less abstract concepts (e.g., edges of objects). We only discuss single-layer CNNs in this primer, although extending single-layer CNNs to deep CNNs is straightforward and is briefly described at the end of this section. It is helpful to think about single-layer CNNs as specialized single-layer FNNs",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It is helpful to think about single-layer CNNs as specialized single-layer FNNs. Recall that single-layer FNNs aim to produce a hidden layer representation h from each input observation x that can subsequently be used to predict the corresponding output observation y . Equation 13a describes how to produce h in a single-layer FNN: Multiply x by a weight matrix B hx , add an intercept vector b hx , then apply an element-wise activation function f . Single-layer CNNs also aim to produce a hidden layer representation of each input image that can then be used to predict the corresponding output observation. However, single-layer CNNs replace multiplying the input image by a weight matrix B hx with an operation called two-dimensional discrete convolution . Intuitively, two-dimensional discrete convolution breaks up the input image into overlapping square-shaped patches, then summarizes the information contained in each patch using a single number. We describe this process with equations in the following paragraphs. Before explaining the single-layer CNN, we explain our notation for describing image data sets. In this primer, we focus on data sets containing N different grayscale (i.e., “black-and-white”) images. Such data sets may contain fascinating psychological insights (e.g., see CNN toy example: Predicting emotions from facial expressions ). In digital form, all grayscale images are two-dimensional arrays made up of colored squares called pixels where each pixel’s color is determined by a number. We can therefore think of a grayscale image as a matrix of numbers where the ( j, k ) th number describes the color of the ( j, k ) th pixel in the image. Since each observation is a matrix, our data set is no longer a single matrix as in linear regression – rather, it is a collection of matrices",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Since each observation is a matrix, our data set is no longer a single matrix as in linear regression – rather, it is a collection of matrices. If each image in our data set is the same shape, say r x ◊ c x , we can think of our data set as an N ◊ r x ◊ c x tensor X where X i is a matrix representing the i th image in the data set and x i,j,k is a number representing the color of the ( j, k ) th pixel in this image. In Figure 10, we visualize a tensor data set containing four different randomly generated 4 ◊ 4 grayscale images. We will also imagine that our data set includes an outcome value y i associated with image i for all i = 1 , , N . As usual, we drop all i subscripts from equations – for example, we write X i as X and y i as y . The single-layer CNN aims to produce a hidden layer representation H of the input image X that can then be used to predict y . To produce H , the single-layer CNN first applies two-dimensional discrete convolution to X to produce an intermediate representation S . This is written in equation form as s j,k = ( X ú B hx )( j, k ) = j ≠ r b ÿ m = j ≠ 1 k ≠ c b ÿ n = k ≠ 1 x m,n b hx j ≠ m,k ≠ n , j = 1 , , r x + r b ≠ 1 , k = 1 , , c x + c b ≠ 1 , (37) where ú is called the convolution operator , X is the r x ◊ c x input image, B hx is an r b ◊ c b weight matrix called the kernel , and s j,k are elements of the ( r x + r b ≠ 1) ◊ ( c x + c b ≠ 1) matrix S called the convolution of X and B hx . Equation 37 looks complicated but is fairly intuitive to understand. “ X ú B hx ” is read as “ X convolved with B hx ”. Convolving the input image X with the kernel (i.e., weight matrix) B hx produces a new matrix S , much like multiplying two matrices produces a new matrix. The kernel B hx is just a small weight matrix that “slides” around on the input image. As it “slides”, it performs element-wise multiplication with the patch of image it is currently on, then sums the result into a single output value",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As it “slides”, it performs element-wise multiplication with the patch of image it is currently on, then sums the result into a single output value. This “sliding” process is how single-layer CNNs use two-dimensional discrete convolution to summarize information from small, overlapping patches on the input image. We visualize two-dimensional discrete convolution with a 4 ◊ 4 image and a 2 ◊ 2 kernel in Figure 11. Note that our figure only visualizes the output values s j,k where the kernel fits completely inside the image, called the valid convolution . We previously mentioned that single-layer CNNs replace multiplying the input image X by a weight matrix B hx with convolving X and B hx . However, after performing convolution, single-layer CNNs do exactly what single-layer FNNs do to produce the hidden layer representation H : They add an intercept to each s j,k , then apply an activation function f : h j,k = f ( b hx j,k, 0 + s j,k ) , j = 1 , , r x + r b ≠ 1 , k = 1 , , c x + c b ≠ 1 , (38) where h j,k are elements of the resulting ( r x + r k ≠ 1) ◊ ( c x + c k ≠ 1) hidden layer representation H . As with single-layer FNNs, choosing a nonlinear activation function helps single-layer CNNs model outcomes y that vary nonlinearly with the input images X . We can now use the hidden layer H to predict the outcome y . Just like the single-layer FNN, the single-layer CNN models the outcome as a weighted sum of the hidden layer nodes, then applies a final activation function g : y = g 1 b yh 0 + r h c h ÿ j =1 b yh j h j 2 + Á, (39) h = vec( H ) , (39a) where vec, the vectorization operation, converts the r h ◊ c h hidden layer H into an r h c h ◊ 1 vector h , 5 h j are elements of h , and Á is a random error term. As with FNNs, choosing the activation function g determines whether the single-layer CNN will perform regression or 5 The vectorization operation stacks the columns of H on top of each other. That is, vec( H ) = h = [ h 1 , 1 , , h r h , 1 , h 1 , 1 , , h r h , 2 , , h 1 ,c h , , h r h ,c h ] €",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That is, vec( H ) = h = [ h 1 , 1 , , h r h , 1 , h 1 , 1 , , h r h , 2 , , h 1 ,c h , , h r h ,c h ] € . Vectorizing H just makes it easier to sum over all of its elements. classification. Deep CNNs have revolutionized predictive modeling using image data as well as video, speech, and audio data (LeCun et al., 2015). Basic deep CNNs can be constructed from single-layer CNNs just like deep FNNs can be constructed from single-layer FNNs: Simply add many hidden layers, each feeding in to the next. Additionally, modifying CNNs to process non-image data is straightforward. However, the most successful deep CNNs used in practical applications can be very complicated (e.g., Krizhevsky et al., 2012). Goodfellow et al. (2016) describe some modifications that may improve CNN predictive accuracy as well as tips for building CNNs in practice. Identifying important parts of images with two-dimensional discrete convolution. We now concretely demonstrate how the convolution operation may identify important components of images. Figure 12 presents a 3 ◊ 3 kernel B hx that is often used for identifying vertical edges in images. Directly under the kernel, we show the result of applying this kernel to a grayscale image of a face. Notice that in the convolution of the original image and the kernel, vertical edges are visible as white or light gray lines while the rest of the image is dark. At the bottom of Figure 12, we show the result of convolving the kernel with two adjacent 3 ◊ 3 patches of the original image. In the first patch, we can tell that there is a vertical edge between the second and third columns: The first column is dark and has small pixel values, while the second column is light and has large pixel values. When we convolve this patch with the kernel, the large pixel values in the first column cause the result to equal to one so that the resulting pixel is essentially black",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When we convolve this patch with the kernel, the large pixel values in the first column cause the result to equal to one so that the resulting pixel is essentially black. Note that since grayscale pixel values only range from zero to 255, negative values produced by computing the convolution are treated as zeros and the resulting pixels are set to black. In the adjacent patch, the vertical edge is now between the first and second columns. The large pixel values in the second column cause the convolution to be a large positive value so the resulting pixel is set to a light gray. Placing the light resulting pixel next to the dark resulting pixel in the convolution of the original image shows us that there was a vertical edge in this patch of the original image. We note that in practice, fitting CNNs will likely result in different fitted kernels than the one presented in Figure 12. Other kernels can identify different kinds of edges (e.g., horizontal or diagonal edges) or even other, less human-interpretable components of images. CNN toy example: Predicting emotions from facial expressions. We now provide a toy example to clarify the application of CNNs to psychological data. The data set used in this example is publicly available on Kaggle, an online community for data scientists and machine learning practitioners (https://www.kaggle.com/shawon10/ckplus). It is a small subset of the Extended Cohn-Kanade data set for facial expression recognition (Kanade et al., 2000; Lucey et al., 2010) and may be similar in size to data sets typically collected by psychologists. As with our FNN example, this example seeks to classify observations into discrete categories. Our data set contains 327 r x ◊ c x = 48 ◊ 48 grayscale images of faces. Each face is labeled with one of the following emotions: Anger (45 observations), contempt (18 observations), disgust (59 observations), fear (25 observations), happy (69 observations), sad (28 observations), or surprise (83 observations)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Each image is duplicated three times in the data set for a total of 981 images used for fitting purposes. Our goal is to use a CNN to predict which emotion a face is showing. We might build a model like this to answer any of a variety of psychological research questions. One simple use of such a classification model might be to categorize individuals’ reactions to specific stimuli without having to ask them for self-report responses, which poses a potential cognitive burden on an emotional task. Figure 13 walks through the process of modeling a single image from our data set using a single-layer CNN. We first select an image X from the tensor data set X . We then convolve our chosen image with a small kernel weight matrix B hx to get a new matrix S (i.e., the convolution). In the figure, we choose the kernel to be the same r b ◊ r b = 3 ◊ 3 kernel used in Figure 12 to detect vertical edges in images. We note that S is typically smaller than the original image X . In Figure 13, we force the kernel to stay inside of the input image (i.e., we only compute the valid convolution) so that S has size 46 ◊ 46. Once we have the convolution S , we can compute the hidden layer representation H of the input image by adding an intercept to each element of S and applying a ReLU activation function. In the figure, we set all intercepts to a large value (i.e., b hx j,k, 0 = 30 for j, k = 1 , , 48) and applied the ReLU function to demonstrate the impact of this processing step. The resulting image H is a modified version of the convolution S in which only the most salient vertical edges are now visible. Identifying salient vertical edges as shown might be useful for our toy CNN, which could, for example, utilize the lines representing the individual’s teeth to predict that they are happy. Notice that H is still square-shaped. In order to predict our emotion outcome, we first flatten H into a vector h . We then multiply h by a weight vector, add an intercept, and apply a softmax activation function (equation 12)",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In order to predict our emotion outcome, we first flatten H into a vector h . We then multiply h by a weight vector, add an intercept, and apply a softmax activation function (equation 12). The softmax function lets our model output seven probabilities, one corresponding to each possible emotion the face could be showing. The highest predicted probability corresponds to the emotion that the CNN most strongly believes the face is showing. In practice, CNNs often require many convolutional layers composed together to obtain high predictive accuracy. Although complete descriptions of state-of-the-art CNN techniques are beyond the scope of this primer, we demonstrate the feasibility of this type of analysis by fitting a deep CNN to our facial emotion data set using the R interface to Keras. Complete descriptions of the techniques used below are beyond the scope of this primer but are available in Goodfellow et al. (2016). As with our other ANN models, we constructed our deep CNN one layer at a time: cnn = keras_model_sequential () cnn %>% layer_conv_2d (filters = 6, kernel_size = c (5, 5), input_shape = c (48, 48, 1), padding = 'same ', activation = 'relu ') %>% layer_max_pooling_2d (pool_size = c (2, 2)) %>% layer_conv_2d (filters = 16, kernel_size = c (5, 5), padding = 'same ', activation = 'relu ') %>% layer_max_pooling_2d (pool_size = c (2, 2)) %>% layer_conv_2d (filters = 64, kernel_size = c (3, 3), padding = 'same ', activation = 'relu ') %>% layer_max_pooling_2d (pool_size = c (2, 2)) %>% layer_flatten () %>% layer_dense (units = 128, activation = 'relu ') %>% layer_dropout (rate = 0.5) %>% layer_dense (units = 7, activation = 'softmax ') layer_conv_2d () performs two-dimensional discrete convolution, adds intercepts, and applies an activation function to its input. Our input consists of 48 ◊ 48 grayscale images, so we set input_shape = c (48, 48, 1) . The final value of 1 tells the model that we are working with grayscale images; if we had colored images, we would set this value to 3",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The final value of 1 tells the model that we are working with grayscale images; if we had colored images, we would set this value to 3 . We choose a 5 ◊ 5 kernel by setting kernel_size = c (5, 5) and choose a ReLU activation function. Setting padding = 'same ' applies a full convolution (i.e., equation 37) instead of the valid convolution described previously. By setting filters = 6 , we choose to fit six different kernels rather than a single kernel. Each fitted kernel may learn to identify different components of the input images, such as different kinds of edges or lines. This corresponds to including six nodes in the hidden layer of a single-layer FNN. We applied a max pooling layer after our convolutional layer using layer_max_pooling_2d () . Max pooling layers are used in CNNs to condense information as it passes through the model. Choosing pool_size = c (2, 2) decreases both the height and width of the convolved image by a factor two. Goodfellow et al. (2016) provide a full description of max pooling. After applying two more convolutional and max pooling layers in succession, we flatten our convolved images into a vector using layer_flatten () . We apply a single-layer FNN to the flattened vector using layer_dense () , then apply the dropout regularization technique using layer_dropout () (Srivastava et al., 2014). The final layer_dense () outputs of vector of seven probabilities (i.e., one probability for each possible emotion) using the softmax activation function (equation 12). Next, we specified parameters for model fitting: cnn %>% compile ( loss = 'categorical_crossentropy ', optimizer = optimizer_adam (), metrics = c ('accuracy ') ) This specification is similar to the one given for our single-layer FNN and RNN toy examples. However, we now set loss = 'categorical_crossentropy ' , which extends the binary cross-entropy objective function to outcomes with more than two categories",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". However, we now set loss = 'categorical_crossentropy ' , which extends the binary cross-entropy objective function to outcomes with more than two categories. More formally, the categorical cross-entropy is the log-likelihood of an outcome that follows a multinomial distribution with number of cells equal to the number of outcome categories, cell probabilities predicted by the fitted model, and trial size one. It is the same objective function used for fitting ordinal regression models (Kutner et al., 2004). We fitted and tested the model: history = cnn %>% fit ( train_data$X, as.matrix (train_data$y), epochs = 50, batch_size = 7 ) cnn %>% evaluate (test_data$X, as.matrix (test_data$y)) The first input to fit () is a 683 ◊ 48 ◊ 48 ◊ 1 tensor of training set images and the second input is an outcome vector. We fitted the model for 50 epochs and randomly sampled 7 images for each fitting iteration. Finally, we evaluated our deep CNN on a test set of 30 percent of observations. We include a full R script for our analysis as online supplemental material. Our fitted model obtained a predictive accuracy of 0 . 84 on a test set of 30 percent of observations, suggesting that our images of faces contain objective information that can be used to predict their emotional content. We could possibly increase our model’s predictive accuracy even further either by collecting more data (which could be expensive) or by fitting our model to one of many publicly available large facial expression data sets (e.g., facial expression databases can be found at https://web.archive.org/web/20180325205102/http://emotion-research.net/wiki/Databases or https://www.ecse.rpi.edu/~cvrl/database/other_facial_expression.htm) then refining the fitted parameters on our smaller data set using a transfer learning approach. Discussion and Future Directions Deep learning is a successful machine learning paradigm that has revolutionized the psychology-related fields of computer vision and natural language processing",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_53"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Discussion and Future Directions Deep learning is a successful machine learning paradigm that has revolutionized the psychology-related fields of computer vision and natural language processing. Psychologists will likely find that state-of-the-art deep learning algorithms outperform other machine learning algorithms such as random forests (RFs) and support vector machines (SVMs) when used to predict outcomes of interest in very large data sets with many weakly correlated variables or with sequence (e.g., text, video) or image observations. Although such data sets are not common in mainstream psychological research, they are readily available through web-based and clinical sources. Additionally, psychologists with many other kinds of data sets may also benefit from deep learning: Researchers who utilize transfer learning may reap the benefits of deep learning using small data sets; survey researchers may need to spend less time developing measures when using deep learning to predict outcomes of interest; and researchers with hierarchically structured data (e.g., educational or clinical psychologists) may boost predictive accuracy using multitask deep learning approaches. As described in the introduction, computer scientists have already achieved promising results using deep learning to predict interesting psychological outcomes. In many cases, deep learning models outperformed other machine learning models such as RFs and SVMs, which suggests that the true causal structure underlying these data sets consisted of weakly correlated interactions between large numbers of variables. We anticipate that combining many data types including survey, computer and smartphone log, social media, image and video, biometric (e.g., wearable sensor, fMRI), genomic, and Gloabl Positioning System (GPS) data will produce large data sets that deep learning models can utilize to accurately predict important psychological outcomes",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_54"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We are particularly excited by the potential of deep multitask and sequence modeling approaches, which may lead to the development of personalized models for accurately detecting risk (e.g., for suicidal ideation) at the individual level in real time. In this primer, we introduced the feedforward neural network (FNN), the recurrent neural network (RNN), and the convolutional neural network (CNN) as generalizations of linear regression. These models (or modifications of these models) are fundamental building blocks of advanced artificial neural networks used in large-scale scientific and industrial applications. We did not describe these state-of-the-art model architectures because deep learning research is progressing too quickly for such descriptions to be practically useful for long. Rather, we aimed to help psychologists gain some fluency in machine learning and deep learning basics. We hope that this fluency will be a first step toward enabling psychologists to draw from the machine learning literature in the same way that they have historically drawn from the statistics literature. In future work, we will further discuss how to build deep learning models and will provide software implementations to help psychologists utilize deep learning to answer prediction-focused research questions. On a final note, we wish to emphasize that machine learning and deep learning are not panaceas. In causal modeling, the randomized controlled trial is still a powerful experimental design for understanding the causal mechanisms that give rise to psychological phenomena (e.g., Lilienfeld et al., 2018). In predictive modeling, artificial neural networks may give worse results than simpler models like linear regression in data sets with large, linear associations between a few variables. Deep learning models that “automatically” learn complicated relationships between independent and dependent variables are not a license for psychologists to feed their data into an algorithm and hope for the best",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_55"
  },
  {
    "document_type": "research_paper",
    "title": "deep-learning-a-primer-for-psychologists",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\deep-learning-a-primer-for-psychologists.pdf",
    "date_published": "2021-04-02",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Rather, valid psychological research requires attention to the same things it always has: Identifying which research hypotheses might be interesting and useful to investigate; translating abstract theoretical constructs into meaningful observable measurements; designing studies and collecting data sets ethically; choosing appropriate classical or modern statistical modeling techniques; and many more. Machine learning and deep learning combined with excellent research practices represent a key step toward helping psychologists accurately and reliably predict human behaviors, cognitions, and emotions.",
    "chunk_id": "Natural_language_processing_deep-learning-a-primer-for-psychologists.json_chunk_56"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": "Advancing NLP with Cognitive Language Processing Signals Nora Hollenstein ETH Zurich noraho@ethz.ch Maria Barrett University of Copenhagen mjb@di.ku.dk Marius Troendle University of Zurich m.troendle@uzh.ch Francesco Bigiolli ETH Zurich fbigiol@ethz.ch Nicolas Langer University of Zurich n.langer@uzh.ch Ce Zhang ETH Zurich ce.zhang@ethz.ch Abstract When we read, our brain processes language and generates cognitive processing data such as gaze patterns and brain activity. These sinals can be recorded while reading. Cognitive language processing data such as eye-tracking features have shown improvements on single NLP tasks. We analyze whether using such human features can show consistent improvment across tasks and data sources. We present an extensive investigation of the benefits and limitations of using cognitive processing data for NLP. Specifically, we use gaze and EEG features to augment models of named entity recognition, relation classification, and sentment analysis. These methods significantly outperform the baselines and show the potetial and current limitations of employing hman language processing data for NLP. 1 Introduction When reading, humans process language “autmatically” without reflecting on each step — Hmans string words together into sentences, undestand the meaning of spoken and written ideas, and process language without thinking too much about how the underlying cognitive process hapens. This process generates cognitive signals that could potentially facilitate natural language prcessing tasks. In recent years, collecting these signals has bcome increasingly easy and less expensive ( Ppoutsaki et al. , 2016 ); as a result, using cognitive features to improve NLP tasks has become more popular. For example, researchers have proposed a range of work that uses eye-tracking or gaze signals to improve part-of-speech tagging ( Barett et al. , 2016 ), sentiment analysis ( Mishra et al. , 2017 ), named entity recognition ( Hollenstein and Zhang , 2019 ), among other tasks",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2016 ), sentiment analysis ( Mishra et al. , 2017 ), named entity recognition ( Hollenstein and Zhang , 2019 ), among other tasks. Moreover, these signals have been used successfully to regulaize attention in neural networks for NLP ( Barrett et al. , 2018 ). However, most previous work leverages only eye-tracking data, presumably because it is the most accessible form of cognitive language prcessing signal. In addition, most state-of-the-art work focused on improving a single task with a single type of cognitive signal. But can cognitive processing signal bring consistent improvements across modality (e.g., eye-tracking and/or EEG) and across various NLP tasks? And if so, does the combination of different sources of cognitive signals bring incremental improvements? In this paper, we aim at shedding light on these questions. We present, to the best of our knowedge, the first comprehensive study to analyze the benefits and limitations of using cognitive laguage processing signals to improve NLP across multiple tasks and modalities (types of signals). Specifically, we go beyond state-of-the-art in two ways: (Multiple Signals) We consider both eye-tracking and electroencephalography (EEG) data as exaples of cognitive language processing data. Eytracking records the readers gaze positions on the screen and serves as an indirect measure of the cognitive reading process. EEG records electrical brain activity along the scalp and is a more direct measure of physiological processes, including laguage processing. This is also the first application leveraging EEG data to improve NLP tasks. (Multiple Tasks) We then construct named entity recognition, relation classification, and sentiment analysis models with gaze and EEG features. We analyze three methods of adding these cognitive signals to machine learning architectures for NLP. First, we simply add the features to existing sytems (Section 4 )",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We analyze three methods of adding these cognitive signals to machine learning architectures for NLP. First, we simply add the features to existing sytems (Section 4 ). Second, we show how these fearXiv:1904.02682v1 [cs.CL] 4 Apr 2019 tures can be generalized so that recorded data is not required at test data (Section 5.1 ). And third, in a multi-task setting we learn gaze and EEG fetures as auxiliary tasks to aid the main NLP task (Section 6 ). In summary, the most important insights gained from this work include: 1. Using cognitive features shows consistent iprovements over a range of NLP tasks even witout large amounts of recorded cognitive signals. 2. While integrating gaze or EEG signals seprately significantly outperforms the baselines, the combination of both does not further improve the results. 3. We identify multiple directions of future rsearch: How can cognitive signals, such as EEG data, be preprocessed and de-noised more effciently for NLP tasks? How can cognitive features of different sources be combined more effectively for natural language processing? All experiments presented in this paper are avaiable 1 to provide a foundation for future work to better understand these questions. 2 Related Work 2.1 Eye-tracking The benefits of eye movement data have been assessed in various domains, including NLP and computer vision. Eye-trackers provide millisecond-accurate records about where humans look when they are reading. Although it is mostly still being recorded in controlled environments, rcent approaches have shown substantial improvments in recording gaze data by using cameras of mobile devices ( G ́omez-Poveda and Gaudioso , 2016 ; Papoutsaki et al. , 2016 ). Hence, gaze data will become more accessible and available in much larger volumes in the next few years ( San Agustin et al. , 2009 ; Sewell and Komogorsev , 2010 ), which will facilitate the creation of siable datasets enormously",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2009 ; Sewell and Komogorsev , 2010 ), which will facilitate the creation of siable datasets enormously. The benefit of eye-tracking in human language processing is supported by intensive study in pscholinguistics during the 20th century and owards. For example, when humans read a text, they do not focus on every single word. The nuber of fixations and the fixation duration on a word depends on a number of linguistic factors ( Clifton 1 https://github.com/DS3Lab/zuco-nlp/ et al. , 2007 ; Demberg and Keller , 2008 ). Diffeent features even allow us to study early and late cognitive processing separately. First, word length, frequency and predictability from context affect fixation duration and counts. The frequency effect was first noted by Rayner ( 1977 ) and has been consistently reported in varous studies since, e.g. Just and Carpenter ( 1980 ); Rayner and Duffy ( 1986 ); Cop et al. ( 2017 ). Seond, readers are more likely to fixate on open-class words ( Carpenter and Just , 1983 ). It even appears that eye movements are reliable indicators of sytactical categories ( Barrett and Søgaard , 2015 ). Word familiarity also influences how long reaers look at a word. Although two words may have the same frequency value, they may differ in fmiliarity and predictability from context. Effects of word familiarity on fixation time have also been demonstrated in a number of studies ( Juhasz and Rayner , 2003 ; Williams and Morris , 2004 ) as have word predictability effects, e.g. McDonald and Shillcock ( 2003 ). A range of work of using eye-tracking signals to improve natural language processing tasks has been proposed and shows promising results. Gaze data has been used to improve tasks such as parof-speech tagging ( Barrett et al. , 2016 ), sentiment analysis ( Mishra et al. , 2017 ), prediction of mutiword expressions ( Rohanian et al. , 2017 ), setence compression ( Klerke et al. , 2016 ), and word embedding evaluation ( Søgaard , 2016 )",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2017 ), prediction of mutiword expressions ( Rohanian et al. , 2017 ), setence compression ( Klerke et al. , 2016 ), and word embedding evaluation ( Søgaard , 2016 ). Furthemore, gaze data has been used to regularize attetion in neural architectures on NLP classification tasks ( Barrett et al. , 2018 ). 2.2 EEG To the best of our knowledge, there are no applictions leveraging EEG data to improve NLP tasks. There are, however, good reasons to try to cobine the two sources. EEG could provide the mising information in the eye movements to disabiguate different cognitive processes. An extended fixation duration only tells us that extended cogntive processing occurs, but not which process . EEG and eye-tracking use the same temporal resolution with non-invasive technologies ( Sereno and Rayner , 2003 ). Dambacher and Kliegl ( 2007 ) found that longer fixation duration correlates with larger N400 amplitude effects. N400 is part of the normal brain response to words and other meaingful stimuli ( Kutas and Federmeier , 2000 ). Efects of word predictability on eye movements and EEG co-registration have also been studied in srialized word representation and in natural reading ( Dimigen et al. , 2011 ). Other aspects relevant for linguistic processing can be observed in the EEG signal itself. For istance, term relevance can be associated with brain activity with significant changes in certain brain areas ( Eugster et al. , 2014 ), differences in procesing verbs and noun, concrete nouns and abstract nouns, as well as common nouns and proper nouns are also observed ( Weiss and Mueller , 2003 ). Futhermore, there is a correspondence between coputational grammar models and certain EEG efects ( Hale et al. , 2018 ). Collecting EEG data is more expensive and time-consuming than collecting eye-tracking data, which is why brain activity data is commonly less accessible. Moreover, collecting EEG data from subjects in a naturalistic reading environment is even more challenging. Hence, related work in this area is very limited",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Moreover, collecting EEG data from subjects in a naturalistic reading environment is even more challenging. Hence, related work in this area is very limited. Subsequently, while we rely on standard practices when leveraging gaze data, our experiments using EEG data are more experimental. 3 Data The Zurich Cognitive Language Processing Copus (ZuCo; Hollenstein et al. ( 2018 )) is the main data source of this work. It is the first freely available dataset 2 of simultaneous eye-tracking and EEG recordings of natural sentence reading. This corpus includes recordings of 12 adult, ntive speakers reading approximately 1100 English sentences. The corpus contains both natural reading and task-solving reading paradigms. For this work, we make use of the first two reading paradigms of ZuCo, during which the subjects read natrally at their own speed and without any spcific task other than answering some control quetions testing their reading comprehension. The first paradigm includes 300 sentences (7737 tokes) from Wikipedia articles ( Culotta et al. , 2006 ) that contained semantic relations such as employer , award and job title . The second paradigm cotains 400 positive, negative and neutral sentences (8138 tokens) from the Stanford Sentiment Trebank ( Socher et al. , 2013 ), to analyze the elicittion of emotions and opinions during reading. The 2 The data is available here: https://osf.io/ q3zws/ same sentences were read by all 12 subjects",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2013 ), to analyze the elicittion of emotions and opinions during reading. The 2 The data is available here: https://osf.io/ q3zws/ same sentences were read by all 12 subjects. 3.1 Gaze features ZuCo readily provides 5 eye-tracking features: number of fixations (NFIX), the number of all fiations landing on a word; first fixation duration (FFD), the duration of the first fixation on the curent word; total reading time (TRT), the sum of all fixation durations on the current word; gaze durtion (GD), the sum of all fixations on the current word in the first-pass reading before the eye moves out of the word; and go-past time (GPT), the sum of all fixations prior to progressing to the right of the current word, including regressions to previous words that originated from the current word. Fiations shorter than 100 ms were excluded, since these are unlikely to reflect language processing ( Sereno and Rayner , 2003 ). To increase the rbustness of the signal, the eye-tracking features are averaged over all subjects. 3.2 EEG features Since eye-tracking and EEG were recorded simutaneously, we were able to extract word-level EEG features. During the preprocessing of ZuCo 23 electrodes in the outermost circumference (chin and neck) were used to detect muscular artifacts and were removed for subsequent analyses. Thus, each EEG feature, corresponding to the duration of a specific fixation, contains 105 electrode vaues. The EEG signal is split into 8 frequency bands, which are fixed ranges of wave frequecies and amplitudes over a time scale: theta1 (4-6 Hz), theta2 (6.5-8 Hz), alpha1 (8.5-10 Hz), alpha2 (10.5-13 Hz), beta1 , (13.5-18 Hz) beta2 (18.5-30 Hz), gamma1 (30.5-40 Hz) and gamma2 (40-49.5 Hz). These frequency ranges are known to corrlate with certain cognitive functions. For instance, theta activity reflects cognitive control and woring memory ( Williams et al",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". These frequency ranges are known to corrlate with certain cognitive functions. For instance, theta activity reflects cognitive control and woring memory ( Williams et al. , 2019 ), alpha activity has been related to attentiveness ( Klimesch , 2012 ), gamma-band activity has been used to detect emtions ( Li and Lu , 2009 ) and beta frequencies afect decisions regarding relevance ( Eugster et al. , 2014 ). Even though the variability between sujects is much higher in the EEG signal, we also average all features over all subjects. 4 Tasks To thoroughly evaluate the potential of gaze and brain activity data, we perform experiments on the three information extraction tasks described in this section. Current state-of-the-art systems are used for all tasks and different combinations of cogntive features are evaluated. 4.1 Named Entity Recognition The performance of named entity recognition (NER) systems can successfully be improved with eye-tracking features ( Hollenstein and Zhang , 2019 ). However, this has not been explored for EEG signals. We use the state-of-the-art neural architecture for NER by Lample et al. ( 2016 ) 3 . Their model successfully combines word-level and character-level embeddings, which we augment with embedding layers for gaze and/or EEG features. Word length and frquency are known to correlate and interact with gaze features (e.g. Just and Carpenter ( 1980 ); Rayner ( 1977 )), which is why we selected a base model that allows us to combine the cognitive features with word-level and character-level iformation. We use the named entity annottions from https://github.com/DS3Lab/ ner-at-first-sight . Features For this task, we used the 17 gaze fetures proposed by Hollenstein and Zhang ( 2019 ) for NER. These features include relevant informtion from early and late word processing as well as context features from the surrounding words",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". These features include relevant informtion from early and late word processing as well as context features from the surrounding words. We extracted 8 word-level EEG features, one for each frequency band (The neural architecture of this system does not allow for raw normalized EEG and gaze features as is the case for relation classification and sentiment analysis.). The feture values were averaged over the 105 electrode values. These features are mapped to the durtion of the gaze features. Thus, in the experiments we tested EEG features during total reading time of the words and EEG features merely during the first fixations. The latter yielded better results. The gaze and EEG features values (originally in milliseconds (for gaze) and microvolts (for EEG)) were normalized and concatenated to the character and word embeddings as one-hot vectors. Experiments All models were trained on both ZuCo paradigms described above (15875 tokens) with 10-fold cross validation (80% training, 10% development, 10% test) and early stopping was performed after 20 epochs of no improvement on the development set to reduce training time. For 3 https://github.com/glample/tagger the experiments, the default values for all paramters were maintained. The word embeddings were initialized with the pre-trained GloVe vectors of 100 dimensions ( Pennington et al. , 2014 ) and the character-based embeddings were trained on the corpus at hand (25 dimensions). 4.2 Relation Classification The second information extraction task we anlyze is classifying semantic relations in sentences. As a state-of-the art relation classification method we use the winning system from SemEval 2018 ( Rotsztejn et al. , 2018 ), which combines convlutional and recurrent neural networks to leveage the best architecture for different sentence lengths. We consider the following 11 relation types: award, employer, education, founder, viited, wife, political-affiliation, nationality, jotitle, birth-place and death-place . We use the anotations provided by Culotta et al",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We use the anotations provided by Culotta et al. ( 2006 ). Features For this task, we employed the 5 gaze features on word-level provided in the ZuCo data: number of fixations, first fixation duration, total reading time, gaze duration and go-past time. The eye-tracking feature values were normalized over all occurrences in the corpus. The EEG features were extracted by averaging the 105 electrode vaues over all fixations for each word and then nomalized. All word features in a sentence were cocatenated and finally padded to the maximum setence length. The eye-tracking and/or EEG feature vectors were appended to the word embeddings. Experiments We performed 5-fold cross validtion over 566 samples (sentences can include more than one relation type). We split the data into 80% training data and 20% test data. Due to the small size of the dataset, we used the same preprocesing steps and parameters as proposed by the SmEval 2018 system. The word embeddings were initialized with the pre-trained GloVe vectors of 300 dimensions. 4.3 Sentiment Analysis The third NLP task we choose for this work is sentiment analysis. Based on the analysis by Barnes et al. ( 2017 ), we implemented a bidiretional LSTM with an attention layer for the classfication of sentence-level sentiment labels. Features Analogous to the relation classifiction, the 5 word-level eye-tracking features were NER RelClass Sentiment (2) Sentiment (3) P R F1 P R F1 P R F1 P R F1 baseline 84.5 81.7 82.9 62.6 56.6 57.7 82.5 82.5 82.5 57.1 57.6 57.2 gaze 86.2 84.3 85.1 ** 65.1 61.9 62.0** 84.7 84.6 84.6 ** 61.4 61.7 61.5 ** EEG 86.7 81.5 83.9* 68.3 64.8 65.1 ** 83.6 83.6 83.6** 60.5 60.2 60.3** gaze+EEG 85.1 83.2 84.0** 66.3 59.3 60.8** 84.3 84.3 84.3** 59.8 60.0 59.8** Table 1: Precision (P), recall (R) and F1-score (F1) for the four tasks augmented with gaze features, EEG features, and both. Significance is indicated with the asterisks: * = p < 0.01, ** = p < 0.0008 (Bonferroni method)",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Significance is indicated with the asterisks: * = p < 0.01, ** = p < 0.0008 (Bonferroni method). normalized and concatenated before being apended to the sentence embeddings. The raw EEG data (105 electrode values per word) were aveaged and normalized. Experiments 10-fold cross validation was peformed over the 400 sentences with available setiment labels from ZuCo (123 neutral, 137 negtive and 140 positive sentences). We test ternary classification as well as binary classification. For the latter, we remove all neutral sentences from the training data. Word embeddings were initiaized with pre-trained vectors of 300 dimensions ( Mikolov et al. , 2013 ). All models are trained for 10 epochs with batch sizes of 32. The initial learning rate is set to 0.001. It was halved every 3 passes or every 10 passes, for binary classification and ternary classification respectively (due to the larger training set). 5 Evaluation For each information extraction task described in the previous section we trained baseline models, models augmented with gaze features, with EEG features, and with both. All the baseline moels were trained solely on textual information (i.e. word embeddings without any gaze or EEG fetures). We trained single-subject models and moels in which the features values are averaged over all subjects. The results of the averaged models are shown in Table 1 . We observe consistent improvements over the baselines for all tasks when augmented with cognitive features. The models with gaze fetures, EEG features and the combination thereof all outperform the baseline. Notably, while the combination of gaze and EEG features also ouperform the baseline, they do not improve over uing gaze or EEG individually. We perform statistical significance testing using permutation (as described in Dror et al. ( 2018 )) over all tasks. In addition, we apply the consevative Bonferroni correction for multiple hypothses, where the global null hypothesis is rejected if p < α/N , where N is the number of hypothses ( Dror et al. , 2017 )",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2017 ). In our setting, α = 0 . 01 and N = 12 , accounting for the combination of the 4 tasks and 3 configurations (EEG, gaze, EEG+gaze). The improvements in 11 configurtions out of 12 are also statistically significant uder the Bonferroni correction. Despite the limited amount of data, this result suggests that augmening NLP systems with cognitive features is a geeralizable approach. Subject analysis In an additional analysis we also evaluate the single-subject models to test the robustness of averaging the feature values over all readers. By the example of binary and ternary sentiment analysis, Figure 1 depicts the variabiity of the results between the subjects. In contrast to the averaged models, the best subject for bnary sentiment classification reaches an F1-score of 85% with the combination of gaze and EEG data. Moreover, it shows how the averaged models perform almost as good as the best subject. Note that the best-performing subject for gaze is not necessarily the same subject as for the best EEG model. We also trained models that only take into account the feature values of the five best subjects. However, when averaging over all subjects, the signal-to-noise ratio is higher and provides better results than training on the best five subjects only. While previous research had shown the same efect for using eye-tracking data from multiple sujects in NLP, this had no yet been shown for EEG data. 5.1 No real-time recorded data required While adding these cognitive features to a system show the potential of this type of data, it is not very practical if real-time recordings of EEG and/or eye-tracking are required at prediction time. Folowing Barrett et al. ( 2016 ), we evaluate feature aggregation on word-type level. This means that all cognitive features are averaged over the word occurrences",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Folowing Barrett et al. ( 2016 ), we evaluate feature aggregation on word-type level. This means that all cognitive features are averaged over the word occurrences. As a result a lexicon of lower-cased word types with their averaged gaze and EEG fe Figure 1: Comparison of single-subject models and features averaged over all subject for both binary sentiment classification (left) and ternary sentiment classification (right). Each dot represents a single subject model, each line an averaged feature model. Note that the best-performing subject for gaze is not necessarily the same subject as for the best EEG model. ture values was compiled. Words in the training data as well as in the test set are assigned these features if the words occurs in the type-aggregated lexicon or receives unknown features values oterwise. Thus, recorded human data is not required at test time. We evaluate the concept of type aggregation on the tasks described above. We choose 3 bencmark datasets and add the aggregated EEG and/or eye-tracking features to words occurring in ZuCo. For NER we use the CoNLL-2003 corpus ( Sang and De Meulder , 2003 ), for relation classification we use the full Wikipedia dataset provided by ( Clotta et al. , 2006 ) and for sentiment analysis we use the Stanford Sentiment Treebank (SST). The same experiment settings as above were applied here. To avoid overfitting we did not use the offcial train/test splits but performed cross validation. Table 2 shows the details about these datasets and the results. We can observe a consistent iprovement using type-aggregated gaze features. However, the effect of type-aggregated EEG fetures is mixed. Type aggregation shows not only that recorded gaze or EEG data is not necessary at test time, but also that improvements can be achieved with human data without requiring large quantities of recorded data. 6 Multi-task learning We further investigate multi-task learning (MTL) as an additional machine learning strategy to beefit from cognitive features",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6 Multi-task learning We further investigate multi-task learning (MTL) as an additional machine learning strategy to beefit from cognitive features. The intuition behind MTL is that training signals of one task, the auiliary task, improves the performance of the main task, by sharing information throughout the traiing process. In our case, we learn gaze and EEG features as auxiliary tasks to improve the main NLP task. In previous work, it has been shown that MTL can be used successfully for sequence labelling tasks ( Bingel and Søgaard , 2017 ) due to some compelling benefits, including its potential to eficiently regularize models and to reduce the need for labeled data. Moreover, gaze duration has been predicted as an auxiliary task to improve sentence compression ( Klerke et al. , 2016 ), and to better predict the readability of texts ( Gonz ́alez-Garduno and Søgaard , 2018 ). To the best of our knowledge, EEG features have not been used in MTL to iprove NLP tasks. In multi-task learning it is important that the tasks that are learned simultaneously are related to a certain extent ( Caruana , 1997 ; Collobert et al. , 2011 ). Assuming that the cognitive processes in the human brain during reading are related, there should be a gain from training on gaze and EEG data when learning to extract information from text. Thus, we assess the hypothesis that MTL might also be useful in our scenario. Experiments We utilized the Sluice networks ( Ruder et al. , 2017 ), where the network learns to which extent the layers are shared between the tasks. Thus, we re-formulated the sentiment anaysis as sequence labelling tasks on phrase level. For binary sentiment analysis, the classes NETRAL and NOT-NEUTRAL were predicted. We did not have to modify the named entity reconition task and the relation classification was not tested since only sentence level labels are avaiable",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We did not have to modify the named entity reconition task and the relation classification was not tested since only sentence level labels are avaiable. NER RelClass Sentiment (2) Sentiment (3) corpus CoNLL-2003 Wikipedia SST SST tokens 302811 32953 165165 202125 sentences 22137 1794 9612 11853 unknown tokens 41.09% 30.31% 26.02% 25.96% baseline 94.02 76.94 82.01 57.13 gaze 94.41** 77.85 ** 81.64 57.48 ** EEG 94.58** 76.40 80.07 54.27 gaze + EEG 94.63 ** 77.01 79.74 54.80 Table 2: The top part shows the size of the datasets used for the type-aggregation experiments, including the percentage of unknown tokens, i.e. tokens not in the lexicon of aggregated type features. The bottom part shows F1-scores of type aggregation on external benchmark corpora. Significance is indicated with the asterisks: * = p < 0.01, ** = p < 0.0008 (Bonferroni method). main task aux task(s) accuracy - 87.34 freq 91.29 NER FFD 87.34 FFD freq 91.87 EEG a 87.31 EEG a freq 91.79 - 60.99 freq 61.15 Sentiment TRT 61.31 binary TRT freq 61.13 EEG b 61.01 EEG b freq 61.56 - 61.03 freq 61.02 Sentiment FFD 61.05 ternary FFD freq 61.10 EEG t 61.05 EEG t freq 61.17 Table 3: Results of the multi-task learning experiments on NER, binary and ternary sentiment analysis. We ran 5-fold cross validation for all experments over the same data as described in Section 3 . As our baselines we used single-task learing and learning word frequency as an auxiliary task to an NLP task. Word frequencies were etracted from the British National Corpus ( Kilgariff , 1995 ). The experiments ran with the default settings recommended by ( Ruder et al. , 2017 ). In accordance to their results, the Sluice networks yielded consistently higher results than hard prameter sharing. As a main task the network learned to predict NER, binary or ternary sentiment labels. As auiliary tasks the network learned a single gaze or EEG feature",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As a main task the network learned to predict NER, binary or ternary sentiment labels. As auiliary tasks the network learned a single gaze or EEG feature. We used five eye-tracking features: number of fixations (NFIX), mean fixation dration (MFD), first fixation duration (FFD), ttal reading time (TRT), and fixation probability (FIXP). Additionally, we tested four EEG features, one for each combined frequency band: EEG t (i.e. the average values of theta1 and theta2 ), EEG a , EEG b , EEG g . The features were discretized and binned. Results Table 3 shows the results of these eperiments. Note that only the best feature combnations are included in the table. Learning word frequency as an auxiliary task is a strong basline. Learning gaze and EEG features as auxiliary tasks does not improve the performance over the single-task baseline for NER and only minimally for sentiment analysis. Learning two auxiliary tasks, a gaze of EEG feature and word frequency in parallel yields modest improvements over the frequency baseline. Adding further auxiliary tasks with additional gaze or EEG features did not yield better results. Moreover, the combination of learning gaze and brain activity features did also not bring further improvements. As we know that gaze and frequency band EEG features represent different cognitive processes ivolved in reading, our main and auxiliary tasks should in fact be related. However, it seems like the noise-to-signal ratio in the EEG features is too high to achieve significant results. As stated by Gonz ́alez-Garduno and Søgaard ( 2018 ), it is iportant to establish whether the same feature reresentation can yield good results for all tasks idependently. To gain further insights into these results, we analyze how well these human features can be learned. 6.1 Learning cognitive features Using the same experiment setting as for the above described MTL experiments, we first trained single-task baselines for each of the gaze and EEG features",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6.1 Learning cognitive features Using the same experiment setting as for the above described MTL experiments, we first trained single-task baselines for each of the gaze and EEG features. Then, we trained each gaze feature in 3 MTL settings: (1) word frequency as an auxiliary task, (2) the remaining gaze features as parallel auxiliary tasks and (3) the EEG features as parallel gaze features EEG features NFIX MFD FFD TRT FIXP EEG t EEG a EEG b EEG g - 64.14 84.60 55.21 65.04 46.66 40.67 36.14 39.50 30.48 freq 71.01 84.64 63.68 71.99 56.34 53.36 49.75 52.79 41.34 gaze 71.34 84.78 63.60 72.20 55.77 53.53 49.38 52.58 40.95 EEG 71.15 84.64 62.10 72.03 55.63 53.47 46.77 52.54 37.27 Table 4: Learning cognitive features in an MTL setting. Columns = main tasks, rows = auxiliary tasks. auxiliary tasks. The same applies to EEG features as main tasks. The results in Table 4 show that gaze features have far higher baselines than EEG features. Presumably EEG is harder to learn bcause it has larger variance in the data. Moreover, while the eye-tracking data is limited to the visual component of the cognitive processes, EEG data additionally contains a motor component and a smantic component during the reading process. Learning word frequency as an auxiliary task considerably helps all gaze and EEG features. The known correlation between eye-tracking and word frequency ( Rayner and Duffy , 1986 ) is clearly beneficial for learning gaze features. Moreover, a frequency effect can also be found in early EEG signals, i.e. during the first 200ms of reading a word ( Hauk and Pulverm ̈uller , 2004 ). 7 Discussion In accordance with previous work (e.g. Barrett et al. ( 2016 ); Mishra and Bhattacharyya ( 2018 )), we showed consistent improvements when using gaze data in a range of information extraction tasks, with recorded token-level features and with type-aggregated features on benchmark corpora. The patterns in the results are less consistent when enhancing NLP methods with EEG signals",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The patterns in the results are less consistent when enhancing NLP methods with EEG signals. While we can still show significant improvements over the baseline models, in general the models leveaging EEG features yield lower performance than the ones with gaze features. A plausible explantion for this is that the combination of gaze and EEG features decreases the signal-to-noise ratio even more than for only one type of cognitive data. Another interpretation is that the eye-tracking and EEG signal contain information that is (too) simlar. Thus, the combination does not improve yield better results. Consequently, there are some open questions: How can EEG signals be preprocessed and dnoised more efficiently for NLP tasks? How can EEG and eye-tracking (and other cognitive prcessing signals or fortuitous data ( Plank , 2016 )) be combined more effectively to improve NLP aplications? The models leveraging type-aggregated conitive features show that improvements can be achieved without requiring large amounts of recorded data and provide evidence that this type of data can be generalized on word type level. Athough these results indicate that huge amounts of recorded data are not necessary for perfomance gains, one of the limitations of this work is the effort of collecting cognitive processing sinals from humans. However, webcam-based eytrackers (e.g. Papoutsaki et al. ( 2016 )) and comercially available EEG devices (e.g Stytsenko et al. ( 2011 )) are becoming more accurate and user-friendly. Finally, the multi-task learning experiments provide insights into the correlation of learning NLP tasks together with word frequency and conitive features. While the results are not as promiing as our main experiments, it reveals qualities of the individual gaze and EEG features. For fture work, a possible approach to combine the ptential of exceptionally good single-subject moels and multi-task learning, would be to learn gaze and/or EEG features from multiple subjects at the same time",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This has been shown to improve accracy on brain-computer interface tasks and helps to further reduce the variability between subjects ( Panagopoulos , 2017 ). One of the challenges of NLP is to learn as much as possible from limited resources. Using cognitive language processing data may allow us take a step towards meta-reasoning, the process of discovering the cognitive processes that are used to tackle a task in the human brain ( Griffiths et al. , 2019 ), and in turn be able to improve NLP. 8 Conclusion We presented an extensive study of improing NLP tasks with eye-tracking and electroecephalography data as instances of cognitive prcessing signals. We showed how adding gaze and/or EEG features to a range of information extraction tasks, namely named entity recogn tion, relation classification and sentiment analsis, yields significant improvements over the baslines. Moreover, we showed how these features can be generalized at word type-level so that no recorded data is required during prediction time. Finally, we explored a multi-task learning setting to simultaneously learn NLP tasks and cognitive features. In conclusion, the gaze and EEG signals of hmans reading text, even though noisy and avaiable in limited amounts, show great potential in improving NLP tasks and facilitate insights into language processing which can be applied to NLP, but need to be investigated in more depth.",
    "chunk_id": "Natural_language_processing_advancing_nlp_with_cogntive.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": "5185 Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data Emily M. Bender University of Washington Department of Linguistics ebender@uw.edu Alexander Koller Saarland University Dept. of Language Science and Technology koller@coli.uni-saarland.de Abstract The success of the large neural language moels on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being dscribed as “understanding” language or captuing “meaning”. In this position paper, we ague that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the ditinction between form and meaning will help guide the field towards better science around natural language understanding. 1 Introduction The current state of affairs in NLP is that the large neural language models (LMs), such as BERT ( Dvlin et al. , 2019 ) or GPT-2 ( Radford et al. , 2019 ), are making great progress on a wide range of tasks, including those that are ostensibly meaninsensitive. This has led to claims, in both academic and popular publications, that such models “undestand” or “comprehend” natural language or learn its “meaning”. From our perspective, these are overclaims caused by a misunderstanding of the relationship between linguistic form and meaning. We argue that the language modeling task, bcause it only uses form as training data, cannot in principle lead to learning of meaning . We take the term language model to refer to any system trained only on the task of string prediction, whether it operates over characters, words or sentences, and sequentially or not. We take (linguistic) meaning to be the relation between a linguistic form and communicative intent",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We take (linguistic) meaning to be the relation between a linguistic form and communicative intent. Our aim is to advocate for an alignment of claims and methodology: Human-analogous natural laguage understanding (NLU) is a grand challenge of artificial intelligence, which involves mastery of the structure and use of language and the ability to ground it in the world. While large neural LMs may well end up being important components of an eventual full-scale solution to human-analogous NLU, they are not nearly-there solutions to this grand challenge. We argue in this paper that geuine progress in our field — climbing the right hill, not just the hill on whose slope we currently sit — depends on maintaining clarity around big picture notions such as meaning and understanding in task design and reporting of experimental results. After briefly reviewing the ways in which large LMs are spoken about and summarizing the rcent flowering of “BERTology” papers ( 2 ), we offer a working definition for “meaning” ( 3 ) and a series of thought experiments illustrating the ipossibility of learning meaning when it is not in the training signal ( 4 , 5 ). We then consider the human language acquisition literature for insight into what information humans use to bootstrap laguage learning ( 6 ) and the distributional sematics literature to discuss what is required to ground distributional models ( 7 ). 8 presents reflections on how we look at progress and direct research effort in our field, and in 9 , we address possible counterarguments to our main thesis. 2 Large LMs: Hype and analysis Publications talking about the application of large LMs to meaning-sensitive tasks tend to describe the models with terminology that, if interpreted at face value, is misleading. Here is a selection from academically-oriented pieces (emphasis added): (1) In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task. ( Devlin et al",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ( Devlin et al. , 2019 ) (2) Using BERT, a pretraining language model, has been successful for single-turn machine comprehension ( Ohsugi et al. , 2019 ) (3) The surprisingly strong ability of these models to rcall factual knowledge without any fine-tuning demo 5186 strates their potential as unsupervised open-domain QA systems. ( Petroni et al. , 2019 ) If the highlighted terms are meant to describe human-analogous understanding, comprehension, or recall of factual knowledge, then these are gross overclaims. If, instead, they are intended as techncal terms, they should be explicitly defined. One important consequence of imprudent use of terminology in our academic discourse is that it feeds AI hype in the popular press. As NLP gains public exposure and is more widely used in applied contexts, it is increasingly important that the actual capabilities of our systems be accurately represented. In some cases, NLP experts speaking with the media are being appropriately careful, as in these two quotes in the New York Times : 1 (4) These systems are still a really long way from truly understanding running prose. (Gary Marcus) (5) Though BERT passed the lab’s common-sense test, mchines are still a long way from an artificial version of a human’s common sense. (Oren Etzioni) However, there are plenty of instances where the popular press gets it wrong, such as ( 6 ) from the B2C website, 2 apparently based on the Google Blog post about BERT and search, which includes numerous statements like ( 7 ). 3 (6) BERT is a system by which Google’s algorithm uses pattern recognition to better understand how human beings communicate so that it can return more relevant results for users. (7) Here are some of the examples that showed up our evaluation process that demonstrate BERTs ability to understand the intent behind your search",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (7) Here are some of the examples that showed up our evaluation process that demonstrate BERTs ability to understand the intent behind your search. In sum, it is not clear from our academic literature whether all authors are clear on the distinction btween form and meaning, but it is clear that the way we speak about what neural LMs are doing is misleading to the public. Part of the reason for this tendency to use imprcise language may well be that we do not yet fully understand what exactly it is about language that the large LMs come to implicitly represent. Their success, however, has sparked a subfield (‘BERToogy’) that aims to answer this question. The methodology of probing tasks (e.g. Adi et al. , 2017 ; Ettinger et al. , 2018 ) has been used to show that 1 https://www.nytimes.com/2018/11/18/technology/artific ial-intelligence-language.html , accessed 2019/12/04 2 https://www.business2community.com/seo/what-t o-do-about-bert-googles-recent-local-algorithm-updat e-02259261 , accessed 2019/12/04 3 https://www.blog.google/products/search/search-langu age-understanding-bert/ , accessed 2019/12/04 large LMs learn at least some information about phenomena such as English subject-verb agreement ( Goldberg , 2019 ; Jawahar et al. , 2019 ), constituent types, dependency labels, NER, and (core) sematic role types (again, all in English) ( Tenney et al. , 2019 ). 4 Hewitt and Manning ( 2019 ) find informtion analogous to unlabeled dependency structures in the word vectors provided by ELMo and BERT (trained on English). And of course it is well estalished that vector-space representations of words pick up word classes, both syntactic (POS, e.g. Lin et al. , 2015 ) and semantic (lexical similarity, e.g. Rubenstein and Goodenough , 1965 ; Mikolov et al. , 2013 )",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Lin et al. , 2015 ) and semantic (lexical similarity, e.g. Rubenstein and Goodenough , 1965 ; Mikolov et al. , 2013 ). Others have looked more closely at the success of the large LMs on apparently meaning sensitive tasks and found that in fact, far from doing the “resoning” ostensibly required to complete the tasks, they were instead simply more effective at leveraing artifacts in the data than previous approaches. Niven and Kao ( 2019 ) find that BERT’s unreasoably good performance on the English Argument Reasoning Comprehension Task ( Habernal et al. , 2018 ) falls back to chance if the dataset is modified by adding adversarial examples that just negate one piece of the original, thus mirroring the distribution of lexical cues for each label. Similarly, McCoy et al. ( 2019 ) find that BERT’s performance on the English Multi-genre Natural Language Inference dataset ( Williams et al. , 2018 ) is predicated on its ability to leverage syntactic heuristics involving overlap (of full constituents, subsequences, or siply bags of words). In a dataset carefully designed to frustrate such heuristics, BERT’s performance falls to significantly below chance. In this brief overview of BERTology papers we have highlighted both the extent to which there is evidence that large LMs can learn aspects of linguistic formal structure (e.g. agreement, depedency structure), and how their apparent ability to “reason” is sometimes a mirage built on leveraging artifacts in the training data (i.e. form, not meaing). Our contribution is an argument on theoretical grounds that a system exposed only to form in its training cannot in principle learn meaning. 3 What is meaning? We start by defining two key terms: We take form to be any observable realization of language: marks 4 But see Warstadt et al. ’s ( 2019 ) cautionary note about how the methodology used for probing can influence the results. 5187 on a page, pixels or bytes in a digital representation of text, or movements of the articulators",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5187 on a page, pixels or bytes in a digital representation of text, or movements of the articulators. 5 We take meaning to be the relation between the form and something external to language, in a sense that we will make precise below. 3.1 Meaning and communicative intent When humans use language, we do so for a purpose: We do not talk for the joy of moving our articultors, but in order to achieve some communicative intent . There are many types of communicative intents: they may be to convey some information to the other person; or to ask them to do something; or simply to socialize. We take meaning to be the relation M ⊆ E × I which contains pairs ( e, i ) of natural language expressions e and the communictive intents i they can be used to evoke. Given this definition of meaning, we can now use understand to refer to the process of retrieving i given e . Communicative intents are about something that is outside of language . When we say Open the window! or When was Malala Yousafzai born? , the communicative intent is grounded in the real world the speaker and listener inhabit together. Commnicative intents can also be about abstract worlds, e.g. bank accounts, computer file systems, or a purely hypothetical world in the speaker’s mind. Linguists distinguish communicative intent from conventional (or standing ) meaning ( Quine , 1960 ; Grice , 1968 ). The conventional meaning of an expression (word, phrase, sentence) is what is costant across all of its possible contexts of use. Coventional meaning is an abstract object that reprsents the communicative potential of a form, given the linguistic system it is drawn from. Each liguistic system (say, English) provides a relation C ⊆ E × S , which contains pairs ( e, s ) of expresions e and their conventional meanings s . 6 The field of linguistic semantics provides many copeting theories of what conventional meanings s look like",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6 The field of linguistic semantics provides many copeting theories of what conventional meanings s look like. For our purposes, we don’t need to select among these theories; all we assume is that convetional meanings must have interpretations, such as a means of testing them for truth against a model of the world. Thus, like the meaning relation M , C connects language to objects outside of language. 5 In spoken languages, the primary articulators are the coponents of the vocal tract. In signed languages, they are principally the hands and face. 6 We abstract away here from the facts that linguistic sytems C change over time and are only incompletely shared among different speakers. They are stable enough to function as rich signals to communicative intent. Returning to the meaning relation M from above, it is best understood as mediated by the relation C of a linguistic system shared between two intelocutors. The speaker has a certain communictive intent i , and chooses an expression e with a standing meaning s which is fit to express i in the current communicative situation. Upon hearing e , the listener then reconstructs s and uses their own knowledge of the communicative situation and their hypotheses about the speaker’s state of mind and intention in an attempt to deduce i . This active participation of the listener is crcial to human communication ( Reddy , 1979 ; Clark , 1996 ). For example, to make sense of ( 8 ) and ( 9 ) (from Clark , 1996 , p.144), the listener has to calclate that Napoleon refers to a specific pose (hand inside coat flap) or that China trip refers to a person who has recently traveled to China. (8) The photographer asked me to do a Napoleon for the camera. (9) Never ask two China trips to the same party. We humans are also very willing, as we will see in 4 below, to attribute communicative intent to a linguistic signal of a language we speak, even if the originator of the signal is not an entity that could have communicative intent",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To summarize, as we strive to understand how NLU tasks and system performance on those tasks relates to the bigger picture goals of building human-analogous natural language understanding systems, it is useful to distinguish cleanly between form, conventional meaning, and communicative intent. Furthermore, we should be careful not to confuse communicative intent with ground truth about the world, as speakers can of course be mitaken, be intentionally dissembling, etc. We argue that a model of natural language that is trained purely on form will not learn meaning: if the training data is only form, there is not suffcient signal to learn the relation M between that form and the non-linguistic intent of human laguage users, nor C between form and the standing meaning the linguistic system assigns to each form. 3.2 Meaning and intelligence Meaning and understanding have long been seen as key to intelligence. Turing ( 1950 ) argued that a machine can be said to “think” if a human judge cannot distinguish it from a human interlocutor ater having an arbitrary written conversation with 5188 each. However, humans are quick to attribute meaing and even intelligence to artificial agents, even when they know them to be artificial, as evidenced by the way people formed attachments to ELIZA ( Weizenbaum , 1966 ; Block , 1981 ). This means we must be extra careful in devising evaluations for machine understanding, as Searle ( 1980 ) elaborates with his Chinese Room experment: he develops the metaphor of a “system” in which a person who does not speak Chinese aswers Chinese questions by consulting a library of Chinese books according to predefined rules. From the outside, the system seems like it “understands” Chinese, although in reality no actual understaning happens anywhere inside the system. Searle’s thought experiment begins from the premise that it is possible to manipulate forms well enough to be indistinguishable from a system that understands the meaning of the forms, reasons about it, and responds appropriately",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We observe that much recent work in NLP claims to be builing systems where not only the runtime system but in fact also the process for building it only has access to form. But language is used for communcation about the speakers’ actual (physical, social, and mental) world, and so the reasoning behind producing meaningful responses must connect the meanings of perceived inputs to information about that world. This in turn means that for a human or a machine to learn a language, they must solve what Harnad ( 1990 ) calls the symbol grounding problem . Harnad encapsulates this by pointing to the impossibility for a non-speaker of Chinese to learn the meanings of Chinese words from Chinese dictionary definitions alone. Our purpose here is to look more deeply into why meaning can’t be learned from linguistic form alone, even in the context of modern hardware and techniques for scaling connectionist models to the point where they can take in vast amounts of data. We argue that, independently of whether passing the Turing test would mean a system is intelligent, a system that is trained only on form would fail a sufficiently sensitive test, because it lacks the ability to connect its utterances to the world. 4 The octopus test In order to illustrate the challenges in attempting to learn meaning from form alone, we propose a concrete scenario. Say that A and B, both fluent speakers of English, are independently stranded on two uninhabited islands. They soon discover that previous visitors to these islands have left behind telegraphs and that they can communicate with each other via an underwater cable. A and B start happily typing messages to each other. Meanwhile, O, a hyper-intelligent deep-sea otopus who is unable to visit or observe the two islands, discovers a way to tap into the underwter cable and listen in on A and B’s conversations. O knows nothing about English initially, but is very good at detecting statistical patterns",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". O knows nothing about English initially, but is very good at detecting statistical patterns. Over time, O learns to predict with great accuracy how B will respond to each of A’s utterances. O also observes that certain words tend to occur in similar contexts, and perhaps learns to generalize across lexical patterns by hypothesizing that they can be used somewhat interchangeably. Nonetheless, O has never observed these objects, and thus would not be able to pick out the referent of a word when presented with a set of (physical) alternatives. At some point, O starts feeling lonely. He cuts the underwater cable and inserts himself into the conversation, by pretending to be B and replying to A’s messages. Can O successfully pose as B without making A suspicious? This constitutes a weak form of the Turing test (weak because A has no reason to suspect she is talking to a nohuman); the interesting question is whether O fails it because he has not learned the meaning relation, having seen only the form of A and B’s utterances. The extent to which O can fool A depends on the task — that is, on what A is trying to talk about. A and B have spent a lot of time exchanging trivial notes about their daily lives to make the long island evenings more enjoyable. It seems possible that O would be able to produce new sentences of the kind B used to produce; essentially acting as a chatbot. This is because the utterances in such conversations have a primarily social function, and do not need to be grounded in the particulars of the interlocutors’ actual physical situation nor anything else specific about the real world. It is sufficient to produce text that is internally coherent. Now say that A has invented a new device, say a coconut catapult. She excitedly sends detailed instructions on building a coconut catapult to B, and asks about B’s experiences and suggestions for improvements",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". She excitedly sends detailed instructions on building a coconut catapult to B, and asks about B’s experiences and suggestions for improvements. Even if O had a way of construcing the catapult underwater, he does not know what words such as rope and coconut refer to, and thus can’t physically reproduce the experiment. He can 5189 only resort to earlier observations about how B rsponded to similarly worded utterances. Perhaps O can recognize utterances about mangos and nails as “similarly worded” because those words appeared in similar contexts as coconut and rope . So O dcides to simply say “Cool idea, great job!”, because B said that a lot when A talked about ropes and nails. It is absolutely conceivable that A accepts this reply as meaningful — but only because A does all the work in attributing meaning to O’s response. It is not because O understood the meaning of A’s instructions or even his own reply. Finally, A faces an emergency. She is suddenly pursued by an angry bear. She grabs a couple of sticks and frantically asks B to come up with a way to construct a weapon to defend herself. Of course, O has no idea what A “means”. Solving a task like this requires the ability to map accurately between words and real-world entities (as well as reasoning and creative thinking). It is at this point that O would fail the Turing test, if A hadn’t been eaten by the bear before noticing the deception. 7 Having only form available as training data, O did not learn meaning. The language exchanged by A and B is a projection of their communicative intents through the meaning relation into linguistic forms. Without access to a means of hypothesizing and testing the underlying communicative intents, reconstructing them from the forms alone is hopless, and O’s language use will eventually diverge from the language use of an agent who can ground their language in coherent communicative intents. The thought experiment also illustrates our point from 3 about listeners’ active role in communiction",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The thought experiment also illustrates our point from 3 about listeners’ active role in communiction. When O sent signals to A pretending to be B, he exploited statistical regularities in the form, i.e. the distribution of linguistic forms he observed. Whatever O learned is a reflection of A and B’s communicative intents and the meaning relation. But reproducing this distribution is not sufficient for meaningful communication. O only fooled A into believing he was B because A was such an ative listener: Because agents who produce English sentences usually have communicative intents, she 7 To see what a large LM might reply in this situation, we prompted the GPT-2 demo with “Help! I’m being chased by a bear! All I have is these sticks. What should I do?” , and GP2 to supplied “You’re not going to get away with this!” ( ht tps://gpt2.apps.allenai.org/ , accessed 2019/12/4). Following Radford et al. ’s ( 2019 ) approach of giving explicit cues to encode the task, we also constructed a more elaborate prompt. The results, given in Appendix A , are highly entertaining but no more helpful to the hapless A. assumes that O does too, and thus she builds the conventional meaning English associates with O’s utterances. Because she assumes that O is B, she uses that conventional meaning together with her other guesses about B’s state of mind and goals to attribute communicative intent. It is not that O’s utterances make sense, but rather, that A can make sense of them. 5 More constrained thought experiments The story of the octopus considers the problem of learning not only the full communicative system, including the relations M and C , but also the resoning required to come up with answers that are both coherent and also helpful in the real world. Here, we provide two more constrained thought eperiments, to focus more narrowly on the problem of learning the meaning relation, for both natural languages and programming languages",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here, we provide two more constrained thought eperiments, to focus more narrowly on the problem of learning the meaning relation, for both natural languages and programming languages. Because programming languages are designed to be unambiguous and relatively insensitive to exection context, the distinction between standing and speaker meaning is less important than for natural languages. A Java program e , when compiled and executed on the Java Virtual Machine, can be intepreted as a function i which maps program inputs to program outputs. We take the meaning relation J ⊆ E × I of Java to contain all such pairs ( e, i ) . Java Imagine that we were to train an LM on all of the well-formed Java code published on Github. The input is only the code. It is not paired with bytecode, nor a compiler, nor sample inputs and outputs for any specific program. We can use any type of LM we like and train it for as long as we like. We then ask the model to execute a sample program, and expect correct program output. English As as second example, imagine traiing an LM (again, of any type) on English text, again with no associated independent indications of speaker intent. The system is also given access to a very large collection of unlabeled photos, but without any connection between the text and the photos. For the text data, the training task is purely one of predicting form. For the image data, the training task could be anything, so long as it only involves the images. At test time, we present the model with inputs consisting of an utterance and a photograph, like How many dogs in the picture are jumping? or Kim saw this picture and said “What a cute dog!” What is cute? and the photos 5190 Figure 1: Photo stimuli 1 (L) and 2 (R) in Figure 1 , where the appropriate answers are a number or a region of the photo, respectively. Reflections In both cases, the tests are ridiculous. It seems patently unfair to ask the model to peform them, given what it was trained on",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Reflections In both cases, the tests are ridiculous. It seems patently unfair to ask the model to peform them, given what it was trained on. But that is precisely the point we are trying to make: a sytem that has learned the meaning (semantics) of a programming language knows how to execute code in that language. And a system that has learned the meaning of a human language can do things like answer questions posed in the language about things in the world (or in this case, in pictures). In other words, what’s interesting here is not that the tasks are impossible, but rather what makes them impossible: what’s missing from the training data. The form of Java programs, to a system that has not observed the inputs and outputs of these programs, does not include information on how to execute them. Similarly, the form of English sentences, to a system that has not had a chance to acquire the meaning relation C of English, and in the absence of any signal of communicative itent, does not include any information about what language-external entities the speaker might be rferring to. Accordingly, a system trained only on the form of Java or English has no way learn their respective meaning relations. 6 Human language acquisition One common reason for believing LMs might be learning meaning is the claim that human children can acquire language just by listening to it. This is not supported by scholarly work on language acquisition: rather, we find that human language learning is not only grounded in the physical world around us, but also in interaction with other people in that world. Kids won’t pick up a language from passive exposure such as TV or radio: Snow et al. ( 1976 ) note in passing that Dutch-speaking kids who watch German TV shows by choice nonethless don’t learn German. Kuhl ( 2007 ) shows expeimentally that English-learning infants can learn Mandarin phonemic distinctions from brief interations with a Mandarin-speaking experimenter but not from exposure to Mandarin TV or radio",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Baldwin ( 1995 ) and others argue that what is critical for language learning is not just interaction but actually joint attention, i.e. situations where the child and a caregiver are both attending to the same thing and both aware of this fact. This theoretcal perspective is substantiated with experimental results showing that toddlers (observed at 15 and 21 months) whose caregivers “follow into” their attention and provide labels for the object of joint attention more have larger vocabularies ( Tomasello and Farrar , 1986 ); that toddlers (18–20 months old) don’t pick up labels uttered by someone behind a screen, but do pick up labels uttered by somone performing joint attention with them ( Baldwin , 1995 ); and that at around 10–11 months of age bbies pay attention to whether a person’s eyes are open or not in terms of whether to follow their gaze, and the degree to which infants in fact follow gaze at 10–11 months while vocalizing themselves prdicts vocabulary comprehension 7–8 months later ( Brooks and Meltzoff , 2005 ). 8 In summary, the process of acquiring a linguitic system, like human communication generally, relies on joint attention and intersubjectivity: the ability to be aware of what another human is attening to and guess what they are intending to commnicate. Human children do not learn meaning from form alone and we should not expect machines to do so either. 7 Distributional semantics Distributional semanticists have long been aware that grounding distributional representations in the real world is challenging. The lexical similarity relations learned by distributional models trained on text don’t in themselves connect any of those words to the world ( Herbelot , 2013 ; Baroni et al. , 2014 ; Erk , 2016 ; Emerson , 2020 ), and the distribtions of words may not match the distribution of things in the world (consider four-legged dogs ). One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos ( Hossain et al",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One approach to providing grounding is to train distributional models on corpora augmented with perceptual data, such as photos ( Hossain et al. , 2019 ) or other modalities ( Kiela and Clark , 2015 ; Kiela et al. , 2015 ). Another is to look to interaction data, e.g. a dialogue corpus with success annottions, including low-level success signals such as 8 These three studies do not name the language that the children were learning. It appears to have been English. 5191 emotional stress ( McDuff and Kapoor , 2019 ) or eye gaze ( Koller et al. , 2012 ), which contains a signal about the felicitous uses of forms. The idea that as the learner gets access to more and more information in addition to the text itself, it can learn more and more facets of meaning is worked out in detail by Bisk et al. ( 2020 ). We agree that this is an exciting avenue of research. From this literature we can see that the slogan “meaning is use” (often attributed to Wittgenstein , 1953 ), refers not to “use” as “distribution in a text corpus” but rather that language is used in the real world to convey communicative intents to real people. Speakers distill their past experience of language use into what we call “meaning” here, and produce new attempts at using language based on this; this attempt is successful if the listener correctly deduces the speaker’s communicative itent. Thus, standing meanings evolve over time as speakers can different experiences (e.g. McConnelGinet , 1984 ), and a reflection of such change can be observed in their changing textual distribution (e.g. Herbelot et al. , 2012 ; Hamilton et al. , 2016 )",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". McConnelGinet , 1984 ), and a reflection of such change can be observed in their changing textual distribution (e.g. Herbelot et al. , 2012 ; Hamilton et al. , 2016 ). 8 On climbing the right hills What about systems which are trained on a task that is not language modeling — say, semantic paring, or reading comprehension tests — and that use word embeddings from BERT or some other large LM as one component? Numerous papers over the past couple of years have shown that using such pretrained embeddings can boost the accuracy of the downstream system drastically, even for tasks that are clearly related to meaning. Our arguments do not apply to such scenarios: reading comprehension datasets include informtion which goes beyond just form, in that they speify semantic relations between pieces of text, and thus a sufficiently sophisticated neural model might learn some aspects of meaning when trained on such datasets. It also is conceivable that whatever information a pretrained LM captures might help the downstream task in learning meaning, without being meaning itself. Recent research suggests that it is wise to iterpret such findings with caution. As noted in 2 , both McCoy et al. ( 2019 ) and Niven and Kao ( 2019 ) found that BERT picked up idiosyncratic patterns in the data for their tasks, and not “meaing”. Beyond such diagnostic research on why large pretrained LMs boost such tasks so much, we think there is a more fundamental question to be asked here: Are we climbing the right hill? 8.1 Top-down and bottom-up theory-building There are two different perspectives from which one can look at the progress of a field. Under a bottom-up perspective, the efforts of a scientific community are driven by identifying specific rsearch challenges. A scientific result counts as a success if it solves such a specific challenge, at least partially. As long as such successes are frequent and satisfying, there is a general atmosphere of sustained progress",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As long as such successes are frequent and satisfying, there is a general atmosphere of sustained progress. By contrast, under a top-down perspective, the focus is on the remote end goal of offering a complete, unified theory for the entire field. This view invites anxiety about the fact that we have not yet fully explained all phenomena and raises the question of whether all of our bottom-up progress leads us in the right direction. There is no doubt that NLP is currently in the process of rapid hill-climbing. Every year, states of the art across many NLP tasks are being improved significantly — often through the use of better prtrained LMs — and tasks that seemed impossible not long ago are already old news. Thus, everthing is going great when we take the bottom-up view. But from a top-down perspective, the quetion is whether the hill we are climbing so rapidly is the right hill. How do we know that incremental progress on today’s tasks will take us to our end goal, whether that is “General Linguistic Intellgence” ( Yogatama et al. , 2019 ) or a system that passes the Turing test or a system that captures the meaning of English, Arapaho, Thai, or Hausa to a linguist’s satisfaction? It is instructive to look at the past to apprecate this question. Computational linguistics has gone through many fashion cycles over the course of its history. Grammaand knowledge-based methods gave way to statistical methods, and today most research incorporates neural methods. Rsearchers of each generation felt like they were solving relevant problems and making constant progress, from a bottom-up perspective. However, eventually serious shortcomings of each paradigm emerged, which could not be tackled satisfactorily with the methods of the day, and these methods were seen as obsolete. This negative judgment — we were climbing a hill, but not the right hill — can only be made from a top-down perspective",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This negative judgment — we were climbing a hill, but not the right hill — can only be made from a top-down perspective. We have discussed the question of what is required to 5192 learn meaning in an attempt to bring the top-down perspective into clearer focus. 8.2 Hillclimbing diagnostics We can only definitively tell if we’ve been climbing the right hill in hindsight, but we propose some best practices for less error-prone mountaineering: First, above all, cultivate humility towards laguage and ask top-down questions. Neural metods are not the first bottom-up success in NLP; they will probably not be the last. Second, be aware of the limitations of tasks: Artficial tasks like bAbI ( Weston et al. , 2016 ) can help get a field of research off the ground, but there is no reason to assume that the distribution of language in the test data remotely resembles the distribution of real natural language; thus evaluation results on such tasks must be interpreted very carefully. Siilar points can be made about crowdsourced NLI datasets such as SQuAD ( Rajpurkar et al. , 2016 ) or SNLI ( Bowman et al. , 2015 ), which do not reresent questions that any particular person really wanted to ask about a text, but the somewhat unatural communicative situation of crowdsourcing work. If a system does better on such a task than the inter-annotator agreement, 9 the task probably has statistical artifacts that do not represent meaning. In the vision community, Barbu et al. ( 2019 ) offer a novel dataset which explicitly tries to achieve a more realistic distribution of task data; it would be interesting to explore similar ideas for language. Third, value and support the work of carefully creating new tasks (see also Heinzerling , 2019 ). For example, the DROP reading comprehension benchmark ( Dua et al. , 2019 ) seeks to create more stringent tests of understanding by creating quetions that require the system to integrate informtion from different parts of a paragraph via simple arithmetic or similar operations",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 10 Fourth, evaluate models of meaning across tasks. (Standing) meaning is task-independent, so a sytem that captures meaning should do well on mutiple tasks. Efforts like SuperGLUE ( Wang et al. , 2019 ) seem like a good step in this direction. Finally, perform thorough analysis of both errors and successes. As McCoy et al. ( 2019 ) and Niven and Kao ( 2019 ) have shown, systems that find sucess with large pretrained LMs do not necessarily do so because the LMs have learned “meaning”. 9 https://rajpurkar.github.io/SQuAD-explorer/ 10 See Appendix B for an exploration of what GPT-2 does with arithmetic. Analyses which start from an attitude of healthy skepticism (“too good to be true”) and probing tasks which try to identify what the model actually learned can be good ways to find out whether the system performs well for the right reasons. 9 Some possible counterarguments In discussing the main thesis of this paper with various colleagues over the past 18 months, we have observed recurring counterarguments. In this section, we address those counterarguments, plus a few more that might arise. “But ‘meaning’ doesn’t mean what you say it means.” Defining “meaning” is notoriously hard. For the purposes of this paper, we chose a working definition which is as general as we could make it, capturing the crucial point that meaning is based on the link between linguistic form and something that is not language. “Meaning” cannot simply be the relation between form and some kind of “deep syntax”, e.g. semantic dependency graphs ( Oepen et al. , 2015 ); like syntax, such representtions could perhaps be learned from form alone ( He et al. , 2018 ; Hewitt and Manning , 2019 ). Equating these with meaning ignores a core function of laguage, which is to convey communicative intents. “But meaning could be learned from ”. As we discussed in 7 , if form is augmented with grounding data of some kind, then meaning can conceivably be learned to the extent that the comunicative intent is represented in that data",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As we discussed in 7 , if form is augmented with grounding data of some kind, then meaning can conceivably be learned to the extent that the comunicative intent is represented in that data. In addition, certain tasks are designed in a way that specific forms are declared as representing cetain semantic relations of interest. Examples of this include NLI datasets ( Dagan et al. , 2006 ; Rjpurkar et al. , 2016 ; Ostermann et al. , 2019 ) which pair input/output tuples of linguistic forms with an explicit semantic relation (e.g. text + hypothesis + “entailed”). Similarly, control codes, or tokens like tl;dr , have been used to prompt large LMs to perform summarization and other tasks ( Radford et al. , 2019 ; Keskar et al. , 2019 ). Here forms are explicitly declared at test time to represent certain semantic relations, which together with the ditributional similarity between e.g. tl;dr and other phrases such as in summary , may be enough to bootstrap a successful neural summarizer. Depening on one’s perspective, one may argue that such a system has learned to reliably find instances of the relation without understanding the text; or that 5193 explicitly declaring cues like entailed or tl;dr as representing certain semantic relations provides a training signal that goes beyond pure form. Analogously, it has been pointed out to us that the sum of all Java code on Github (cf. 5 ) contains unit tests, which specify input-output pairs for Java code. Thus a learner could have access to a weak form of interaction data, from which the meaning of Java could conceivably be learned. This is true, but requires a learner which has been equipped by its human developer with the ability to identify and interpret unit tests. This learner thus has access to partial grounding in addition to the form. “But there is so much form out there – surely that is enough.” We have argued for the general principle that learning meaning requires more than form",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". “But there is so much form out there – surely that is enough.” We have argued for the general principle that learning meaning requires more than form. How much form can be observed is not relevant to our point; the octopus can observe A and B for as long as he wants, and the quantity of training data in 5 is not limited. But given lots of form, could O perhaps learn to keep producing seemingly meaningful responses to A’s utterances without learning meaning? The prolem is that people constantly generate new commnicative intents to talk about their constantly evoling inner and outer worlds, and thus O would need to memorize infinitely many stimulus-response pairs. Such an approach may be an avenue towards high scores in evaluations where perfection is not expected anyway; but it is probably not an avenue towards human-analogous NLU. “But aren’t neural representations meaning too?” The internal representations of a neural network have been found to capture certain aspects of meaning, such as semantic similarity ( Mikolov et al. , 2013 ; Clark , 2015 ). As we argued in 4 , smantic similarity is only a weak reflection of actual meaning. Neural representations neither qualify as standing meanings ( s ), lacking interpretations, nor as communicative intents ( i ), being insufficient to e.g. correctly build a coconut catapult. An interesting recent development is the emegence of models for unsupervised machine transltion trained only with a language modeling objetive on monolingual corpora for the two languages ( Lample et al. , 2018 ). If such models were to reach the accuracy of supervised translation models, this would seem contradict our conclusion that meaning cannot be learned from form. A perhaps surprising consequence of our argument would then be that accurate machine translation does not actually rquire a system to understand the meaning of the source or target language sentence",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A perhaps surprising consequence of our argument would then be that accurate machine translation does not actually rquire a system to understand the meaning of the source or target language sentence. “But BERT improves performance on meaninrelated tasks, so it must have learned something about meaning.” It has probably learned somthing about meaning, in the same sense that syntax captures something about meaning and semantic similarity captures something about meaning: a potentially useful, but incomplete, reflection of the actual meaning. McCoy et al. ( 2019 ) and Niven and Kao ( 2019 ) provide cautionary tales about oveestimating what that “something” is purely based on evaluation results on existing tasks. What eactly BERT and its relatives learn about meaning is a very interesting question, and we look forward to further findings from the field of BERTology. 10 Conclusion In this paper, we have argued that in contrast to some current hype, meaning cannot be learned from form alone. This means that even large laguage models such as BERT do not learn “meaing”; they learn some reflection of meaning into the linguistic form which is very useful in applications. We have offered some thoughts on how to maitain a healthy, but not exaggerated, optimism with respect to research that builds upon these LMs. In particular, this paper can be seen as a call for prcise language use when talking about the success of current models and for humility in dealing with natural language. With this we hope to encourage a top-down perspective on our field which we think will help us select the right hill to climb towards human-analogous NLU. Acknowledgments. This paper benefitted from many inspiring and often spirited discussions",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data",
    "author": "Emily M. Bender ; Alexander Koller",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Bender-2020-Climbing-towards-nlu-on-meaning-for.pdf",
    "date_published": "2020-05-27",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Acknowledgments. This paper benefitted from many inspiring and often spirited discussions. Without implying any agreement with the cotents as presented, we thank Sam Bowman, Vera Demberg, Lucia Donatelli, Jason Eisner, Jonas Groschwitz, Kristen Howell, Angie McMillaMajor, Joakim Nivre, Stephan Oepen, Ellie Pavlick, Benjamin Roth, Dan Roth, Asad Sayeed, Hinrich Sch ̈ utze, Nina Tahmasebi, and Olga Zamaraeva. This paper originated in a Twitter mega-thread that was neatly summarized by Thomas Wolf ( 2018 ). We also thank the ACL reviewers and the partiipants of the Toulouse Workshop on Formal and Distributional Semantics (2015) and *SEM 2016 for their insightful and constructive thoughts. 5194",
    "chunk_id": "Natural_language_processing_climbing_towards_nlu_on_meaning,_form,_and_understanding_in_the_age_of_data.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": "Turning large language models into cognitive models Marcel Binz MPRG Computational Principles of Intelligence Max Planck Institute for Biological Cybernetics, Tübingen, Germany marcel.binz@tue.mpg.de Eric Schulz MPRG Computational Principles of Intelligence Max Planck Institute for Biological Cybernetics, Tübingen, Germany Abstract Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that – after finetuning them on data from psychological experiments – these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole. 1 Introduction Large language models are neural networks trained on vast amounts of data to predict the next token for a given text sequence [Brown et al., 2020]. These models display many emergent abilities that were not anticipated by extrapolating the performance of smaller models [Wei et al., 2022]. Their abilities are so impressive and far-reaching that some have argued that they show sparks of general intelligence [Bubeck et al., 2023]",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Their abilities are so impressive and far-reaching that some have argued that they show sparks of general intelligence [Bubeck et al., 2023]. We may currently witness one of the biggest revolutions in artificial intelligence, but the impact of modern language models is felt far beyond, permeating into education [Kasneci et al., 2023], medicine [Li et al., 2023], and the labor market [Eloundou et al., 2023]. In-context learning – the ability to extract information from a context and to use that information to improve the production of subsequent outputs – is one of the defining features of such models. It is through this mechanism that large language models are able to solve a variety of tasks, ranging from translation [Brown et al., 2020] to analogical reasoning [Webb et al., 2022]. Previous work has shown that these models can even successfully navigate when they are placed into classic psychological experiments [Binz and Schulz, 2023, Coda-Forno et al., 2023, Dasgupta et al., 2022, Hagendorff et al., 2022]. To provide just one example, GPT-3 – an autoregressive language model designed by OpenAI [Brown et al., 2020] – outperformed human subjects in a sequential decision-making task that required to balance between exploitative and exploratory actions [Binz and Schulz, 2023]. Even though these models show human-like behavioral characteristics in some situations, this is not always the case. In the sequential decision-making task mentioned above, for instance, GPT-3 Preprint. Under review. arXiv:2306.03917v1 [cs.CL] 6 Jun 2023 Figure 1: Illustration of our approach and main results. (a) We provided text-based descriptions of psychological experiments to a large language model and extracted the resulting embeddings. We then finetuned a linear layer on top of these embeddings to predict human choices. We refer to the resulting model as CENTaUR. (b) Example prompt for the choices13k data set. (c) Negative log-likelihoods for the choices13k data set. (d) Example prompt for the horizon task",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We refer to the resulting model as CENTaUR. (b) Example prompt for the choices13k data set. (c) Negative log-likelihoods for the choices13k data set. (d) Example prompt for the horizon task. (e) Negative log-likelihoods for the horizon task. Prompts shown in this figure are stylized for readability. Exact prompts can be found in the Supplementary Materials. relied heavily on exploitative strategies, while people applied a combination of elaborate exploration strategies [Wilson et al., 2014]. Moreover, GPT-3 stopped improving after only a few trials, while people continued learning as the task progressed. In the present paper, we investigate whether it is possible to fix the behavioral discrepancy between large language models and humans. To do so, we rely on the idea of finetuning on domain-specific data. This approach has been fruitful across a number of areas [Sanh et al., 2019, Ouyang et al., 2022] and eventually led to the creation of the term foundation models [Bommasani et al., 2021] – models trained on broad data at scale and adapted to a wide range of downstream tasks. In the context of human cognition, such domain-specific data can be readily accessed by tapping the vast amount of behavioral studies that psychologists have conducted over the last century. We made use of this and extracted data sets for several behavioral paradigms which we then used to finetune a large language model. We show that this approach can be used to create models that describe human behavior better than traditional cognitive models. We verify this result through extensive model simulations, which confirm that finetuned language models indeed show human-like behavioral characteristics. Furthermore, we find that the embeddings obtained from such models contain the information necessary to capture individual differences. Finally, we highlight that a model finetuned on two tasks is capable of predicting human behavior on a third, hold-out task",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Finally, we highlight that a model finetuned on two tasks is capable of predicting human behavior on a third, hold-out task. Taken together, our work demonstrates that it is possible to turn large language models into cognitive models, thereby opening up completely new opportunities to harvest the power of large language models for building domain-general models of human learning and decision-making. 2 2 Finetuned language models beat domain-specific models We started our investigations by testing whether it is possible to capture how people make decisions through finetuning a large language model. For our analyses, we relied on the Large Language Model Meta AI , or in short: LLaMA [Touvron et al., 2023]. LLaMA is a family of state-of-the-art foundational large language models (with either 7B, 13B, 33B, or 65B parameters) that were trained on trillions of tokens coming from exclusively publicly available data sets. We focused on the largest of these models – the 65B parameter version – for the analyses in the main text. LLaMA is publicly available, meaning that researchers are provided with complete access to the network architecture including its pre-trained weights. We utilized this feature to extract embeddings for several cognitive tasks and then finetuned a linear layer on top of these embeddings to predict human choices (see Figure 1a for a visualization). We call the resulting class of models CENTaUR, in analogy to the mythical creature that is half human and half ungulate. We considered two paradigms that have been extensively studied in the human decision-making literature for our initial analyses: decisions from descriptions [Kahneman and Tversky, 1972] and decisions from experience [Hertwig et al., 2004]. In the former, a decision-maker is asked to choose between one of two hypothetical gambles like the ones shown in Figure 1b. Thus, for both options, there is complete information about outcome probabilities and their respective values",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Thus, for both options, there is complete information about outcome probabilities and their respective values. In contrast, the decisions from experience paradigm does not provide such explicit information. Instead, the decision-maker has to learn about outcome probabilities and their respective values from repeated interactions with the task as shown in Figure 1d. Importantly, this modification calls for a change in how an ideal decision-maker should approach such problems: it is not enough to merely exploit currently available knowledge anymore but also crucial to explore options that are unfamiliar [Schulz and Gershman, 2019]. For both these paradigms, we created a data set consisting of embeddings and the corresponding human choices. We obtained embeddings by passing prompts that included all the information that people had access to on a given trial through LLaMA and then extracting the hidden activations of the final layer (see Figure 1b and d for example prompts, and the Supplementary Materials for a more detailed description about the prompt generation procedure). We relied on publicly available data from earlier studies in this process. In the decisions from descriptions setting, we used the choices13k data set [Peterson et al., 2021], which is a large-scale data set consisting of over 13,000 choice problems (all in all, 14,711 participants made over one million choices on these problems). In the decisions from experience setting, we used data from the horizon task [Wilson et al., 2014] and a replication study [Feng et al., 2021], which combined include 60 participants making a total of 67,200 choices. With these two data sets at hand, we fitted a regularized logistic regression model from the extracted embeddings to human choices. In this section, we restricted ourselves to a joint model for all participants, thereby neglecting potential individual differences (but see one of the following sections for an analysis that allows for individual differences)",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Model performance was measured through the predictive log-likelihood on hold-out data obtained using a 100-fold cross-validation procedure. We standardized all input features and furthermore applied a nested cross-validation for tuning the hyperparameter that controls the regularization strength. Further details are provided in the Materials and Methods section. We compared the goodness-of-fit of the resulting models against three baselines: a random guessing model, LLaMA without finetuning (obtained by reading out log-probabilities of the pre-trained model), and a domain-specific model ( Best Estimate and Sampling Tools , or BEAST, for the choices13k data set [Erev et al., 2017] and a hybrid model [Gershman, 2018] that involves a combination of different exploitation and exploration strategies for the horizon task). We found that LLaMA did not capture human behavior well, obtaining a negative log-likelihood (NLL) close to chance-level for the choices13k data set (NLL = 96248 . 5 ) and the horizon task (NLL = 46211 . 4 ). However, finetuning led to models that captured human behavior better than the domain-specific models under consideration. In the choices13k data set, CENTaUR achieved a negative log-likelihood of 48002 . 3 while BEAST only achieved a negative log-likelihood of 49448 . 1 (see Figure 1c). In the horizon task, CENTaUR achieved a negative log-likelihood of 25968 . 6 while the hybrid model only achieved a negative log-likelihood of 29042 . 5 (see Figure 1e). Together, these results suggest that the representations extracted from large language models are rich enough to attain state-of-the-art results for modeling human decision-making. 3 Figure 2: Model simulations. (a) Performance for different models and human participants on the choices13k data set. (b) Performance for different models and human participants on the horizon task. (c) Human choice curves in the equal information condition of the horizon task. (d) Human choice curves in the unequal information condition of the horizon task",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (c) Human choice curves in the equal information condition of the horizon task. (d) Human choice curves in the unequal information condition of the horizon task. (e) LLaMA choice curves in the equal information condition of the horizon task. (f) LLaMA choice curves in the unequal information condition of the horizon task. (g) CENTaUR choice curves in the equal information condition of the horizon task. (h) CENTaUR choice curves in the unequal information condition of the horizon task. 3 Model simulations reveal human-like behavior We next verified that CENTaUR shows human-like behavioral characteristics. To do so, we simulated the model on the experimental data. Looking at performance, we found that finetuning led to models that closely resemble human performance as shown in Figure 2a and b. For the choices-13k data set, CENTaUR obtained a regret (defined as the difference between the highest possible reward and the reward for the action selected by the model) of 1 . 35 (SE = 0 . 01 ), which was much closer to the human regret (M = 1 . 24 , SE = 0 . 01 ) than the regret of LLaMA (M = 1 . 85 , SE = 0 . 01 ). The results for the horizon task showed an identical pattern, with CENTaUR (M = 2 . 38 , SE = 0 . 01 ) matching human regret (M = 2 . 33 , SE = 0 . 05 ) more closely than LLaMA (M = 7 . 21 , SE = 0 . 02 ). In addition to looking at performance, we also inspected choice curves. For this analysis, we took the data from the first free-choice trial in the horizon task and divided it into two conditions: (1) an equal information condition that includes trials where the decision-maker had access to an equal number of observations for both options and (2) an unequal information condition that includes trials where the decision-maker previously observed one option fewer times than the other. We then fitted a separate logistic regression model for each condition with reward difference, horizon, and their interaction as independent variables onto the simulated choices",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We then fitted a separate logistic regression model for each condition with reward difference, horizon, and their interaction as independent variables onto the simulated choices. Earlier studies with human subjects [Wilson et al., 2014] identified the following two main results regarding their exploratory behavior: (1) people’s choices become more random with a longer horizon in the equal information condition (as shown in Figure 2c) and (2) people in the unequal information condition select the more informative option more frequently when the task horizon is longer (as shown in Figure 2d). While LLaMA did not show any of the two effects (see Figure 2e and f), CENTaUR exhibited both of them (see Figure 2g and h), thereby further corroborating that it accurately captures human behavior. 4 Figure 3: Individual differences. (a) Negative log-likelihood difference to the best-fitting model for each participant. Black highlights the best-fitting model, while white corresponds to a difference larger than ten. (b) Negative log-likelihoods for models that were finetuned using the random-effects structure described in the main text. 4 Language model embeddings capture individual differences We also investigated how well CENTaUR describes the behavior of each individual participant. Note that this form of analysis is only possible for the horizon task as choice information on the participant level is not available for the choices13k data set. In total, the majority of participants (N = 52 out of 60 ) was best modeled by CENTaUR (see Figure 3a for a detailed visualization). We furthermore entered the negative log-likelihoods into a random-effects model selection procedure which estimates the probability that a particular model is the most frequent explanation within a set of candidate models [Rigoux et al., 2014]. This procedure favored CENTaUR decisively, assigning a probability that it is the most frequent explanation of close to one. Thus far, we have finetuned LLaMA jointly for all participants",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This procedure favored CENTaUR decisively, assigning a probability that it is the most frequent explanation of close to one. Thus far, we have finetuned LLaMA jointly for all participants. However, people may exhibit individual differences that are not captured by this analysis. To close this gap and test whether LLaMA embeddings can account for individual differences, we incorporated random effects in the finetuned layer. We added a random effect for each participant and embedding dimension while keeping the remaining evaluation procedure the same. Figure 3b illustrates the resulting negative log-likelihoods. Including the random-effect structure improved goodness-of-fit considerably (NLL = 23929 . 5 ) compared to the fixed-effect-only model (NLL = 25968 . 6 ). Furthermore, CENTaUR remained superior to the hybrid model with an identical random-effect structure (NLL = 24166 . 0 ). Taken together, the findings reported in this section highlight that embeddings of large language models contain the information necessary to model behavior on the participant level. 5 Evaluating goodness-of-fit on hold-out tasks Finally, we examined whether CENTaUR – after being finetuned on multiple tasks – is able to predict human behavior in an entirely different task. This evaluation protocol provides a much stronger test for the generalization abilities of our approach. Following our initial analyses, we finetuned a linear layer on top of LLaMA embeddings. However, this time, we fitted a joint model using both the data from the choices13k data set and the horizon task, and then evaluated how well the finetuned model captures human choices on a third task. Further details about the fitting procedure are provided in the Materials and Methods section. For the hold-out task, we considered data from a recent study that provided participants with a choice between one option whose information is provided via a description and another option for which information is provided via a list of experienced outcomes [Garcia et al., 2023]",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Figure 4a shows an example prompt for this experimental paradigm. Finetuning was generally beneficial for modeling human behavior on the hold-out task: negative log-likelihoods for CENTaUR (NLL = 4521 . 1 ) decreased both in comparison to a random guessing model (NLL = 5977 . 7 ) and LLaMA (NLL = 6307 . 9 ). We were thus curious whether CENTaUR also captures human behavior on a qualitative level. To test this, we took a look at the key insight from the original study: people tend to overvalue options that are provided through a description 5 Figure 4: Hold-out task evaluations. (a) Example prompt for the experiential-symbolic task of Garcia et al. [2023]. (b) Human choice curves as a function of win probabilities for both options. (c) Human indifference points as a function of win probability for the E-option. Indifferent points express the win probabilities at which a decision-maker is equally likely to select both options. (d) LLaMA choice curves as a function of win probabilities for both options. (e) LLaMA indifference points as a function of win probability for the E-option. (f) CENTaUR choice curves as a function of win probabilities for both options. (g) CENTaUR indifference points as a function of win probability for the E-option. (symbolic or S-options) over the options that come with a list of experienced outcomes (experiential or E-options) as illustrated in Figure 4b and c. LLaMA does not show this characteristic and instead weighs both option types equally (Figure 4d and e). In contrast to this, CENTaUR shows human-like behavior, taking mostly the S-option into account (Figure 4f and g). This is remarkable because we never presented data from the experiment under consideration during finetuning. 6 Discussion We have demonstrated that large language models can be turned into cognitive models by finetuning their final layer. This process led to models that achieved state-of-the-art performance in two domains",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This process led to models that achieved state-of-the-art performance in two domains. Furthermore, these models were able to capture behavioral differences at the individual participant level. Finally, we have shown that our approach generalizes to previously unseen tasks. In particular, a model that was finetuned on two tasks also exhibited human-like behavior on a third, hold-out task. These results complement earlier work showing that large language model embeddings allow us to predict behavior and neural activations in linguistic settings [Schrimpf et al., 2021, Kumar et al., 2022, Tuckute et al., 2023, Antonello et al., 2023]. For example, Schrimpf et al. [2021] showed that large language models can predict neural and behavioral responses in tasks that involved reading short passages with an accuracy that was close to noise ceiling. While it may be expected that large language models explain human behavior in linguistic domains (after all these models are trained to predict future word occurrences), the observation that these results also transfer to more cognition domains like the ones studied here is highly non-trivial. We are particularly excited about one feature of CENTaUR: embeddings extracted for different tasks all lie in a common space. This property allows finetuned large language models to solve multiple tasks in a unified architecture. We have presented preliminary results in this direction, showing that a model finetuned on two tasks can predict human behavior on a third. However, we believe that our current results only hint at the potential of this approach. Ideally, we would like to scale up our approach to finetuning on a larger number of tasks from the psychology literature. If one would include enough tasks in the training set, the resulting system should – in principle – generalize to any 6 hold-out task",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". If one would include enough tasks in the training set, the resulting system should – in principle – generalize to any 6 hold-out task. Therefore, our approach provides a path towards a domain-general model of human cognition, which has been the goal of theoreticians for decades [Newell, 1992, Yang et al., 2019, Riveland and Pouget, 2022, Binz et al., 2023]. We believe that having access to such a model would transform psychology and the behavioral sciences more generally. It could, among other applications, be used to rapidly prototype the outcomes of projected experiments, thereby easing the trial-and-error process of experimental design, or to provide behavioral policy recommendations while avoiding expensive data collection procedures. Finally, we have to ask ourselves what we can learn about human cognition when finetuning large language models. For now, our insights are limited to the observation that large language model embeddings are rich enough to explain human decision-making. While this is interesting in its own right, it is certainly not the end of the story. Looking beyond the current work, having access to an accurate neural network model of human behavior provides the opportunity to apply a wide range of explainability techniques from the machine learning literature. For instance, we could pick a particular neuron in the embedding and trace back what parts of a particular input sequence excite that neuron using methods such as layer-wise relevance propagation [Bach et al., 2015, Chefer et al., 2021]. Thus, our work also opens up a new spectrum of analyses that are not possible when working with human subjects. To summarize, large language models are an immensely powerful tool for studying human behavior. We believe that our work has only scratched the surface of this potential and there is certainly much more to come",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "turning_large_language_models_into_cognitive_models",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\turning_large_language_models_into_cognitive_models.pdf",
    "date_published": "2023-06-08",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We believe that our work has only scratched the surface of this potential and there is certainly much more to come. Acknowledgements: We like to thank Robert Wilson and Basile Garcia for their help on the horizon task and the experiential-symbolic task respectively, Ido Erev and Eyal Ert for their help with the BEAST model, and Meta AI for making LLaMA accessible to the research community. Funding: This work was funded by the Max Planck Society, the Volkswagen Foundation, as well as the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy–EXC2064/1–390727645. Data and materials availability: Data and code for the current study are available through the GitHub repository https://github.com/marcelbinz/CENTaUR . 7",
    "chunk_id": "Natural_language_processing_turning_large_language_models_into_cognitive_models.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Python Data Science Handbook ESSENTIAL TOOLS FOR WORKING WITH DATA powered by Jake VanderPlas Python Data Science Handbook Essential Tools for Working with Data Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing 978-1-491-91205-8 [LSI] Python Data Science Handbook by Jake VanderPlas Copyright © 2017 Jake VanderPlas. All rights reserved. Printed in the United States of America. Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles ( http://oreilly.com/safari ). For more information, contact our corporate/insti‐ tutional sales department: 800-998-9938 or corporate@oreilly.com . Editor: Dawn Schanafelt Production Editor: Kristen Brown Copyeditor: Jasmine Kwityn Proofreader: Rachel Monaghan Indexer: WordCo Indexing Services, Inc. Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Rebecca Demarest December 2016: First Edition Revision History for the First Edition 2016-11-17: First Release See http://oreilly.com/catalog/errata.csp?isbn=9781491912058 for release details. The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Python Data Science Handbook , the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the author have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the author disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights. Table of Contents Prefacexi 1. IPython: Beyond Normal Python1 Shell or Notebook? 2 Launching the IPython Shell 2 Launching the Jupyter Notebook 2 Help and Documentation in IPython 3 Accessing Documentation with ? 3 Accessing Source Code with ?? 5 Exploring Modules with Tab Completion 6 Keyboard Shortcuts in the IPython Shell 8 Navigation Shortcuts 8 Text Entry Shortcuts 9 Command History Shortcuts 9 Miscellaneous Shortcuts 10 IPython Magic Commands 10 Pasting Code Blocks: %paste and %cpaste 11 Running External Code: %run 12 Timing Code Execution: %timeit 12 Help on Magic Functions: ?, %magic, and %lsmagic 13 Input and Output History 13 IPython’s In and Out Objects 13 Underscore Shortcuts and Previous Outputs 15 Suppressing Output 15 Related Magic Commands 16 IPython and Shell Commands 16 Quick Introduction to the Shell 16 Shell Commands in IPython 18 iii Passing Values to and from the Shell 18 Shell-Related Magic Commands 19 Errors and Debugging 20 Controlling Exceptions: %xmode 20 Debugging: When Reading Tracebacks Is Not Enough 22 Profiling and Timing Code 25 Timing Code Snippets: %timeit and %time 25 Profiling Full Scripts: %prun 27 Line-by-Line Profiling with %lprun 28 Profiling Memory Use: %memit and %mprun 29 More IPython Resources 30 Web Resources 30 Books 31 2",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Introduction to NumPy33 Understanding Data Types in Python 34 A Python Integer Is More Than Just an Integer 35 A Python List Is More Than Just a List 37 Fixed-Type Arrays in Python 38 Creating Arrays from Python Lists 39 Creating Arrays from Scratch 39 NumPy Standard Data Types 41 The Basics of NumPy Arrays 42 NumPy Array Attributes 42 Array Indexing: Accessing Single Elements 43 Array Slicing: Accessing Subarrays 44 Reshaping of Arrays 47 Array Concatenation and Splitting 48 Computation on NumPy Arrays: Universal Functions 50 The Slowness of Loops 50 Introducing UFuncs 51 Exploring NumPy’s UFuncs 52 Advanced Ufunc Features 56 Ufuncs: Learning More 58 Aggregations: Min, Max, and Everything in Between 58 Summing the Values in an Array 59 Minimum and Maximum 59 Example: What Is the Average Height of US Presidents? 61 Computation on Arrays: Broadcasting 63 Introducing Broadcasting 63 Rules of Broadcasting 65 Broadcasting in Practice 68 iv | Table of Contents Comparisons, Masks, and Boolean Logic 70 Example: Counting Rainy Days 70 Comparison Operators as ufuncs 71 Working with Boolean Arrays 73 Boolean Arrays as Masks 75 Fancy Indexing 78 Exploring Fancy Indexing 79 Combined Indexing 80 Example: Selecting Random Points 81 Modifying Values with Fancy Indexing 82 Example: Binning Data 83 Sorting Arrays 85 Fast Sorting in NumPy: np.sort and np.argsort 86 Partial Sorts: Partitioning 88 Example: k-Nearest Neighbors 88 Structured Data: NumPy’s Structured Arrays 92 Creating Structured Arrays 94 More Advanced Compound Types 95 RecordArrays: Structured Arrays with a Twist 96 On to Pandas 96 3",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Data Manipulation with Pandas97 Installing and Using Pandas 97 Introducing Pandas Objects 98 The Pandas Series Object 99 The Pandas DataFrame Object 102 The Pandas Index Object 105 Data Indexing and Selection 107 Data Selection in Series 107 Data Selection in DataFrame 110 Operating on Data in Pandas 115 Ufuncs: Index Preservation 115 UFuncs: Index Alignment 116 Ufuncs: Operations Between DataFrame and Series 118 Handling Missing Data 119 Trade-Offs in Missing Data Conventions 120 Missing Data in Pandas 120 Operating on Null Values 124 Hierarchical Indexing 128 A Multiply Indexed Series 128 Methods of MultiIndex Creation 131 Indexing and Slicing a MultiIndex 134 Table of Contents | v Rearranging Multi-Indices 137 Data Aggregations on Multi-Indices 140 Combining Datasets: Concat and Append 141 Recall: Concatenation of NumPy Arrays 142 Simple Concatenation with pd.concat 142 Combining Datasets: Merge and Join 146 Relational Algebra 146 Categories of Joins 147 Specification of the Merge Key 149 Specifying Set Arithmetic for Joins 152 Overlapping Column Names: The suffixes Keyword 153 Example: US States Data 154 Aggregation and Grouping 158 Planets Data 159 Simple Aggregation in Pandas 159 GroupBy: Split, Apply, Combine 161 Pivot Tables 170 Motivating Pivot Tables 170 Pivot Tables by Hand 171 Pivot Table Syntax 171 Example: Birthrate Data 174 Vectorized String Operations 178 Introducing Pandas String Operations 178 Tables of Pandas String Methods 180 Example: Recipe Database 184 Working with Time Series 188 Dates and Times in Python 188 Pandas Time Series: Indexing by Time 192 Pandas Time Series Data Structures 192 Frequencies and Offsets 195 Resampling, Shifting, and Windowing 196 Where to Learn More 202 Example: Visualizing Seattle Bicycle Counts 202 High-Performance Pandas: eval() and query() 208 Motivating query() and eval(): Compound Expressions 209 pandas.eval() for Efficient Operations 210 DataFrame.eval() for Column-Wise Operations 211 DataFrame.query() Method 213 Performance: When to Use These Functions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "and eval(): Compound Expressions 209 pandas.eval() for Efficient Operations 210 DataFrame.eval() for Column-Wise Operations 211 DataFrame.query() Method 213 Performance: When to Use These Functions 214 Further Resources 215 vi | Table of Contents 4",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Visualization with Matplotlib217 General Matplotlib Tips 218 Importing matplotlib 218 Setting Styles 218 show() or No show()? How to Display Your Plots 218 Saving Figures to File 221 Two Interfaces for the Price of One 222 Simple Line Plots 224 Adjusting the Plot: Line Colors and Styles 226 Adjusting the Plot: Axes Limits 228 Labeling Plots 230 Simple Scatter Plots 233 Scatter Plots with plt.plot 233 Scatter Plots with plt.scatter 235 plot Versus scatter: A Note on Efficiency 237 Visualizing Errors 237 Basic Errorbars 238 Continuous Errors 239 Density and Contour Plots 241 Visualizing a Three-Dimensional Function 241 Histograms, Binnings, and Density 245 Two-Dimensional Histograms and Binnings 247 Customizing Plot Legends 249 Choosing Elements for the Legend 251 Legend for Size of Points 252 Multiple Legends 254 Customizing Colorbars 255 Customizing Colorbars 256 Example: Handwritten Digits 261 Multiple Subplots 262 plt.axes: Subplots by Hand 263 plt.subplot: Simple Grids of Subplots 264 plt.subplots: The Whole Grid in One Go 265 plt.GridSpec: More Complicated Arrangements 266 Text and Annotation 268 Example: Effect of Holidays on US Births 269 Transforms and Text Position 270 Arrows and Annotation 272 Customizing Ticks 275 Major and Minor Ticks 276 Hiding Ticks or Labels 277 Reducing or Increasing the Number of Ticks 278 Table of Contents | vii Fancy Tick Formats 279 Summary of Formatters and Locators 281 Customizing Matplotlib: Configurations and Stylesheets 282 Plot Customization by Hand 282 Changing the Defaults: rcParams 284 Stylesheets 285 Three-Dimensional Plotting in Matplotlib 290 Three-Dimensional Points and Lines 291 Three-Dimensional Contour Plots 292 Wireframes and Surface Plots 293 Surface Triangulations 295 Geographic Data with Basemap 298 Map Projections 300 Drawing a Map Background 304 Plotting Data on Maps 307 Example: California Cities 308 Example: Surface Temperature Data 309 Visualization with Seaborn 311 Seaborn Versus Matplotlib 312 Exploring Seaborn Plots 313 Example: Exploring Marathon",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "on Maps 307 Example: California Cities 308 Example: Surface Temperature Data 309 Visualization with Seaborn 311 Seaborn Versus Matplotlib 312 Exploring Seaborn Plots 313 Example: Exploring Marathon Finishing Times 322 Further Resources 329 Matplotlib Resources 329 Other Python Graphics Libraries 330 5",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Machine Learning331 What Is Machine Learning? 332 Categories of Machine Learning 332 Qualitative Examples of Machine Learning Applications 333 Summary 342 Introducing Scikit-Learn 343 Data Representation in Scikit-Learn 343 Scikit-Learn’s Estimator API 346 Application: Exploring Handwritten Digits 354 Summary 359 Hyperparameters and Model Validation 359 Thinking About Model Validation 359 Selecting the Best Model 363 Learning Curves 370 Validation in Practice: Grid Search 373 Summary 375 Feature Engineering 375 viii | Table of Contents Categorical Features 376 Text Features 377 Image Features 378 Derived Features 378 Imputation of Missing Data 381 Feature Pipelines 381 In Depth: Naive Bayes Classification 382 Bayesian Classification 383 Gaussian Naive Bayes 383 Multinomial Naive Bayes 386 When to Use Naive Bayes 389 In Depth: Linear Regression 390 Simple Linear Regression 390 Basis Function Regression 392 Regularization 396 Example: Predicting Bicycle Traffic 400 In-Depth: Support Vector Machines 405 Motivating Support Vector Machines 405 Support Vector Machines: Maximizing the Margin 407 Example: Face Recognition 416 Support Vector Machine Summary 420 In-Depth: Decision Trees and Random Forests 421 Motivating Random Forests: Decision Trees 421 Ensembles of Estimators: Random Forests 426 Random Forest Regression 428 Example: Random Forest for Classifying Digits 430 Summary of Random Forests 432 In Depth: Principal Component Analysis 433 Introducing Principal Component Analysis 433 PCA as Noise Filtering 440 Example: Eigenfaces 442 Principal Component Analysis Summary 445 In-Depth: Manifold Learning 445 Manifold Learning: “HELLO” 446 Multidimensional Scaling (MDS) 447 MDS as Manifold Learning 450 Nonlinear Embeddings: Where MDS Fails 452 Nonlinear Manifolds: Locally Linear Embedding 453 Some Thoughts on Manifold Methods 455 Example: Isomap on Faces 456 Example: Visualizing Structure in Digits 460 In Depth: k-Means Clustering 462 Table of Contents | ix Introducing k-Means 463 k-Means Algorithm:",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "on Manifold Methods 455 Example: Isomap on Faces 456 Example: Visualizing Structure in Digits 460 In Depth: k-Means Clustering 462 Table of Contents | ix Introducing k-Means 463 k-Means Algorithm: Expectation–Maximization 465 Examples 470 In Depth: Gaussian Mixture Models 476 Motivating GMM: Weaknesses of k-Means 477 Generalizing E–M: Gaussian Mixture Models 480 GMM as Density Estimation 484 Example: GMM for Generating New Data 488 In-Depth: Kernel Density Estimation 491 Motivating KDE: Histograms 491 Kernel Density Estimation in Practice 496 Example: KDE on a Sphere 498 Example: Not-So-Naive Bayes 501 Application: A Face Detection Pipeline 506 HOG Features 506 HOG in Action: A Simple Face Detector 507 Caveats and Improvements 512 Further Machine Learning Resources 514 Machine Learning in Python 514 General Machine Learning 515 Index517 x | Table of Contents Preface What Is Data Science? This is a book about doing data science with Python, which immediately begs the question: what is data science ? It’s a surprisingly hard definition to nail down, espe‐ cially given how ubiquitous the term has become",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Vocal critics have variously dis‐ missed the term as a superfluous label (after all, what science doesn’t involve data?) or a simple buzzword that only exists to salt résumés and catch the eye of overzealous tech recruiters. In my mind, these critiques miss something important. Data science, despite its hypladen veneer, is perhaps the best label we have for the cross-disciplinary set of skills that are becoming increasingly important in many applications across industry and academia. This cross-disciplinary piece is key: in my mind, the best existing defini‐ tion of data science is illustrated by Drew Conway’s Data Science Venn Diagram, first published on his blog in September 2010 (see Figure P-1 ). Figure P-1. Drew Conway’s Data Science Venn Diagram xi While some of the intersection labels are a bit tongue-in-cheek, this diagram captures the essence of what I think people mean when they say “data science”: it is fundamen‐ tally an interdisciplinary subject. Data science comprises three distinct and overlap‐ ping areas: the skills of a statistician who knows how to model and summarize datasets (which are growing ever larger); the skills of a computer scientist who can design and use algorithms to efficiently store, process, and visualize this data; and the domain expertise —what we might think of as “classical” training in a subject—neces‐ sary both to formulate the right questions and to put their answers in context. With this in mind, I would encourage you to think of data science not as a new domain of knowledge to learn, but as a new set of skills that you can apply within your current area of expertise. Whether you are reporting election results, forecasting stock returns, optimizing online ad clicks, identifying microorganisms in microscope photos, seeking new classes of astronomical objects, or working with data in any other field, the goal of this book is to give you the ability to ask and answer new ques‐ tions about your chosen subject area",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Who Is This Book For? In my teaching both at the University of Washington and at various tech-focused conferences and meetups, one of the most common questions I have heard is this: “how should I learn Python?” The people asking are generally technically minded students, developers, or researchers, often with an already strong background in writ‐ ing code and using computational and numerical tools. Most of these folks don’t want to learn Python per se, but want to learn the language with the aim of using it as a tool for data-intensive and computational science. While a large patchwork of videos, blog posts, and tutorials for this audience is available online, I’ve long been frustrated by the lack of a single good answer to this question; that is what inspired this book. The book is not meant to be an introduction to Python or to programming in gen‐ eral; I assume the reader has familiarity with the Python language, including defining functions, assigning variables, calling methods of objects, controlling the flow of a program, and other basic tasks. Instead, it is meant to help Python users learn to use Python’s data science stack—libraries such as IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and related tools—to effectively store, manipulate, and gain insight from data. Why Python? Python has emerged over the last couple decades as a first-class tool for scientific computing tasks, including the analysis and visualization of large datasets. This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This may have come as a surprise to early proponents of the Python language: the language itself was not specifically designed with data analysis or scientific computing in mind. xii | Preface The usefulness of Python for data science stems primarily from the large and active ecosystem of third-party packages: NumPy for manipulation of homogeneous arrabased data, Pandas for manipulation of heterogeneous and labeled data, SciPy for common scientific computing tasks, Matplotlib for publication-quality visualizations, IPython for interactive execution and sharing of code, Scikit-Learn for machine learning, and many more tools that will be mentioned in the following pages. If you are looking for a guide to the Python language itself, I would suggest the sister project to this book, A Whirlwind Tour of the Python Language . This short report pro‐ vides a tour of the essential features of the Python language, aimed at data scientists who already are familiar with one or more other programming languages. Python 2 Versus Python 3 This book uses the syntax of Python 3, which contains language enhancements that are not compatible with the 2.x series of Python. Though Python 3.0 was first released in 2008, adoption has been relatively slow, particularly in the scientific and web devel‐ opment communities. This is primarily because it took some time for many of the essential third-party packages and toolkits to be made compatible with the new lan‐ guage internals. Since early 2014, however, stable releases of the most important tools in the data science ecosystem have been fully compatible with both Python 2 and 3, and so this book will use the newer Python 3 syntax. However, the vast majority of code snippets in this book will also work without modification in Python 2: in cases where a Py2-incompatible syntax is used, I will make every effort to note it explicitly. Outline of This Book Each chapter of this book focuses on a particular package or tool that contributes a fundamental piece of the Python data science story",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Outline of This Book Each chapter of this book focuses on a particular package or tool that contributes a fundamental piece of the Python data science story. IPython and Jupyter ( Chapter 1 ) These packages provide the computational environment in which many Pythousing data scientists work. NumPy ( Chapter 2 ) This library provides the ndarray object for efficient storage and manipulation of dense data arrays in Python. Pandas ( Chapter 3 ) This library provides the DataFrame object for efficient storage and manipulation of labeled/columnar data in Python. Matplotlib ( Chapter 4 ) This library provides capabilities for a flexible range of data visualizations in Python. Preface | xiii Scikit-Learn ( Chapter 5 ) This library provides efficient and clean Python implementations of the most important and established machine learning algorithms. The PyData world is certainly much larger than these five packages, and is growing every day. With this in mind, I make every attempt through these pages to provide references to other interesting efforts, projects, and packages that are pushing the boundaries of what can be done in Python. Nevertheless, these five are currently fun‐ damental to much of the work being done in the Python data science space, and I expect they will remain important even as the ecosystem continues growing around them. Using Code Examples Supplemental material (code examples, figures, etc.) is available for download at https://github.com/jakevdp/PythonDataScienceHandbook . This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for per‐ mission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing a CD-ROM of examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission. We appreciate, but do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example, “ Python Data Science Handbook by Jake VanderPlas (O’Reilly). Copyright 2017 Jake VanderPlas, 978-1-491-91205-8.” If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com . Installation Considerations Installing Python and the suite of libraries that enable scientific computing is straightforward. This section will outline some of the considerations to keep in mind when setting up your computer. Though there are various ways to install Python, the one I would suggest for use in data science is the Anaconda distribution, which works similarly whether you use Windows, Linux, or Mac OS X. The Anaconda distribution comes in two flavors: • Miniconda gives you the Python interpreter itself, along with a command-line tool called conda that operates as a cross-platform package manager geared xiv | Preface toward Python packages, similar in spirit to the apt or yum tools that Linux users might be familiar with. • Anaconda includes both Python and conda, and additionally bundles a suite of other preinstalled packages geared toward scientific computing. Because of the size of this bundle, expect the installation to consume several gigabytes of disk space. Any of the packages included with Anaconda can also be installed manually on top of Miniconda; for this reason I suggest starting with Miniconda",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Any of the packages included with Anaconda can also be installed manually on top of Miniconda; for this reason I suggest starting with Miniconda. To get started, download and install the Miniconda package (make sure to choose a version with Python 3), and then install the core packages used in this book: [~]$ conda install numpy pandas scikit-learn matplotlib seaborn ipython-notebook Throughout the text, we will also make use of other, more specialized tools in Python’s scientific ecosystem; installation is usually as easy as typing conda install packagename . For more information on conda, including information about creating and using conda environments (which I would highly recommend), refer to conda’s online documentation . Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for program listings, as well as within paragraphs to refer to program ele‐ ments such as variable or function names, databases, data types, environment variables, statements, and keywords. Constant width bold Shows commands or other text that should be typed literally by the user. Constant width italic Shows text that should be replaced with user-supplied values or by values deter‐ mined by context. O’Reilly Safari Safari (formerly Safari Books Online) is a membership-based training and reference platform for enterprise, government, educators, and individuals. Preface | xv Members have access to thousands of books, training videos, Learning Paths, interac‐ tive tutorials, and curated playlists from over 250 publishers, including O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐ sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and Course Technology, among others",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For more information, please visit http://oreilly.com/safari . How to Contact Us Please address comments and questions concerning this book to the publisher: O’Reilly Media, Inc. 1005 Gravenstein Highway North Sebastopol, CA 95472 800-998-9938 (in the United States or Canada) 707-829-0515 (international or local) 707-829-0104 (fax) We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at http://bit.ly/python-data-sci-handbook . To comment or ask technical questions about this book, send email to bookques‐ tions@oreilly.com . For more information about our books, courses, conferences, and news, see our web‐ site at http://www.oreilly.com . Find us on Facebook: http://facebook.com/oreilly Follow us on Twitter: http://twitter.com/oreillymedia Watch us on YouTube: http://www.youtube.com/oreillymedia xvi | Preface CHAPTER 1 IPython: Beyond Normal Python There are many options for development environments for Python, and I’m often asked which one I use in my own work. My answer sometimes surprises people: my preferred environment is IPython plus a text editor (in my case, Emacs or Atom depending on my mood). IPython (short for Interactive Python ) was started in 2001 by Fernando Perez as an enhanced Python interpreter, and has since grown into a project aiming to provide, in Perez’s words, “Tools for the entire lifecycle of research computing.” If Python is the engine of our data science task, you might think of IPy‐ thon as the interactive control panel. As well as being a useful interactive interface to Python, IPython also provides a number of useful syntactic additions to the language; we’ll cover the most useful of these additions here. In addition, IPython is closely tied with the Jupyter project , which provides a browser-based notebook that is useful for development, collabora‐ tion, sharing, and even publication of data science results",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The IPython notebook is actually a special case of the broader Jupyter notebook structure, which encompasses notebooks for Julia, R, and other programming languages. As an example of the use‐ fulness of the notebook format, look no further than the page you are reading: the entire manuscript for this book was composed as a set of IPython notebooks. IPython is about using Python effectively for interactive scientific and data-intensive computing. This chapter will start by stepping through some of the IPython features that are useful to the practice of data science, focusing especially on the syntax it offers beyond the standard features of Python. Next, we will go into a bit more depth on some of the more useful “magic commands” that can speed up common tasks in creating and using data science code. Finally, we will touch on some of the features of the notebook that make it useful in understanding data and sharing results. 1 Shell or Notebook? There are two primary means of using IPython that we’ll discuss in this chapter: the IPython shell and the IPython notebook. The bulk of the material in this chapter is relevant to both, and the examples will switch between them depending on what is most convenient. In the few sections that are relevant to just one or the other, I will explicitly state that fact. Before we start, some words on how to launch the IPython shell and IPython notebook. Launching the IPython Shell This chapter, like most of this book, is not designed to be absorbed passively. I recom‐ mend that as you read through it, you follow along and experiment with the tools and syntax we cover: the muscle-memory you build through doing this will be far more useful than the simple act of reading about it. Start by launching the IPython inter‐ preter by typing ipython on the command line; alternatively, if you’ve installed a dis‐ tribution like Anaconda or EPD, there may be a launcher specific to your system (we’ll discuss this more fully in “Help and Documentation in IPython” on page 3 )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Once you do this, you should see a prompt like the following: IPython 4.0.1 -- An enhanced Interactive Python. ? -> Introduction and overview of IPython's features. %quickref -> Quick reference. help -> Python's own help system. object? -> Details about 'object', use 'object??' for extra details. In [1]: With that, you’re ready to follow along. Launching the Jupyter Notebook The Jupyter notebook is a browser-based graphical interface to the IPython shell, and builds on it a rich set of dynamic display capabilities. As well as executing Python/ IPython statements, the notebook allows the user to include formatted text, static and dynamic visualizations, mathematical equations, JavaScript widgets, and much more. Furthermore, these documents can be saved in a way that lets other people open them and execute the code on their own systems. Though the IPython notebook is viewed and edited through your web browser win‐ dow, it must connect to a running Python process in order to execute code. To start this process (known as a “kernel”), run the following command in your system shell: $ jupyter notebook This command will launch a local web server that will be visible to your browser. It immediately spits out a log showing what it is doing; that log will look something like this: 2 | Chapter 1: IPython: Beyond Normal Python $ jupyter notebook [NotebookApp] Serving notebooks from local directory: /Users/jakevdp/[NotebookApp] 0 active kernels [NotebookApp] The IPython Notebook is running at: http://localhost:8888/ [NotebookApp] Use Control-C to stop this server and shut down all kernelsUpon issuing the command, your default browser should automatically open and navigate to the listed local URL; the exact address will depend on your system. If the browser does not open automatically, you can open a window and manually open this address ( http://localhost:8888/ in this example)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If the browser does not open automatically, you can open a window and manually open this address ( http://localhost:8888/ in this example). Help and Documentation in IPython If you read no other section in this chapter, read this one: I find the tools discussed here to be the most transformative contributions of IPython to my daily workflow. When a technologically minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and Stack Overflow answers contain a wealth of information, even (espe‐ cially?) if it is a topic you’ve found yourself searching before. Being an effective prac‐ titioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the informa‐ tion you don’t know, whether through a web search engine or another means. One of the most useful functions of IPython/Jupyter is to shorten the gap between the user and the type of documentation and search that will help them do their work effectively. While web searches still play a role in answering complicated questions, an amazing amount of information can be found through IPython alone. Some examples of the questions IPython can help answer in a few keystrokes: • How do I call this function? What arguments and options does it have? • What does the source code of this Python object look like? • What is in this package I imported? What attributes or methods does this object have? Here we’ll discuss IPython’s tools to quickly access this information, namely the ? character to explore documentation, the ?? characters to explore source code, and the Tab key for autocompletion",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Accessing Documentation with ? The Python language and its data science ecosystem are built with the user in mind, and one big part of that is access to documentation. Every Python object contains the Help and Documentation in IPython | 3 reference to a string, known as a docstring , which in most cases will contain a concise summary of the object and how to use it. Python has a built-in help() function that can access this information and print the results. For example, to see the documenta‐ tion of the built-in len function, you can do the following: In [ 1 ]: help ( len ) Help on built - in function len in module builtins : len ( ) len ( object ) -> integer Return the number of items of a sequence or mapping . Depending on your interpreter, this information may be displayed as inline text, or in some separate pop-up window. Because finding help on an object is so common and useful, IPython introduces the ? character as a shorthand for accessing this documentation and other relevant information: In [ 2 ]: len ? Type : builtin_function_or_method String form : < built - in function len > Namespace : Python builtin Docstring : len ( object ) -> integer Return the number of items of a sequence or mapping . This notation works for just about anything, including object methods: In [ 3 ]: L = [ 1 , 2 , 3 ] In [ 4 ]: L.insert ? Type : builtin_function_or_method String form : < built - in method insert of list object at 0x1024b8ea8 > Docstring : L",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". insert ( index , object ) -- insert object before index or even objects themselves, with the documentation from their type: In [ 5 ]: L ? Type : list String form : [ 1 , 2 , 3 ] Length : 3 Docstring : list () -> new empty list list ( iterable ) -> new list initialized from iterable 's items Importantly, this will even work for functions or other objects you create yourself! Here we’ll define a small function with a docstring: In [ 6 ]: def square ( a ): : \"\"\"Return the square of a.\"\"\" 4 | Chapter 1: IPython: Beyond Normal Python : return a ** 2 : Note that to create a docstring for our function, we simply placed a string literal in the first line. Because docstrings are usually multiple lines, by convention we used Python’s triple-quote notation for multiline strings. Now we’ll use the ? mark to find this docstring: In [ 7 ]: square ? Type : function String form : < function square at 0x103713cb0 > Definition : square ( a ) Docstring : Return the square of a . This quick access to documentation via docstrings is one reason you should get in the habit of always adding such inline documentation to the code you write! Accessing Source Code with ?? Because the Python language is so easily readable, you can usually gain another level of insight by reading the source code of the object you’re curious about. IPython pro‐ vides a shortcut to the source code with the double question mark ( ?? ): In [ 8 ]: square ?? Type : function String form : < function square at 0x103713cb0 > Definition : square ( a ) Source : def square ( a ): \"Return the square of a\" return a ** 2 For simple functions like this, the double question mark can give quick insight into the under-the-hood details. If you play with this much, you’ll notice that sometimes the ?? suffix doesn’t display any source code: this is generally because the object in question is not implemented in Python, but in C or some other compiled extension language. If this is the case, the ?? suffix gives the same output as the ? suffix",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If this is the case, the ?? suffix gives the same output as the ? suffix. You’ll find this particularly with many of Python’s built-in objects and types, for example len from above: In [ 9 ]: len ?? Type : builtin_function_or_method String form : < built - in function len > Namespace : Python builtin Docstring : len ( object ) -> integer Return the number of items of a sequence or mapping . Help and Documentation in IPython | 5 Using ? and/or ?? gives a powerful and quick interface for finding information about what any Python function or module does. Exploring Modules with Tab Completion IPython’s other useful interface is the use of the Tab key for autocompletion and exploration of the contents of objects, modules, and namespaces. In the examples that follow, we’ll use <TAB> to indicate when the Tab key should be pressed. Tab completion of object contents Every Python object has various attributes and methods associated with it. Like with the help function discussed before, Python has a built-in dir function that returns a list of these, but the tab-completion interface is much easier to use in practice. To see a list of all available attributes of an object, you can type the name of the object fol‐ lowed by a period ( . ) character and the Tab key: In [ 10 ]: L .< TAB > L . append L . copy L . extend L . insert L . remove L . sort L . clear L . count L . index L . pop L . reverse To narrow down the list, you can type the first character or several characters of the name, and the Tab key will find the matching attributes and methods: In [ 10 ]: L . c < TAB > L . clear L . copy L . count In [ 10 ]: L . co < TAB > L . copy L . count If there is only a single option, pressing the Tab key will complete the line for you. For example, the following will instantly be replaced with L.count : In [ 10 ]: L . cou < TAB > Though Python has no strictly enforced distinction between public/external attributes and private/internal attributes, by convention a preceding underscore is used to denote such methods",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". cou < TAB > Though Python has no strictly enforced distinction between public/external attributes and private/internal attributes, by convention a preceding underscore is used to denote such methods. For clarity, these private methods and special methods are omitted from the list by default, but it’s possible to list them by explicitly typing the underscore: In [ 10 ]: L . _ < TAB > L . __add__ L . __gt__ L . __reduce__ L . __class__ L . __hash__ L . __reduce_ex__ For brevity, we’ve only shown the first couple lines of the output. Most of these are Python’s special double-underscore methods (often nicknamed “dunder” methods). 6 | Chapter 1: IPython: Beyond Normal Python Tab completion when importing Tab completion is also useful when importing objects from packages. Here we’ll use it to find all possible imports in the itertools package that start with co : In [10]: from itertools import co<TAB> combinations compress combinations_with_replacement count Similarly, you can use tab completion to see which imports are available on your sys‐ tem (this will change depending on which third-party scripts and modules are visible to your Python session): In [10]: import <TAB> Display all 399 possibilities? (y or n) Crypto dis py_compile Cython distutils pyclbr difflib pwd zmq In [10]: import h<TAB> hashlib hmac http heapq html husl (Note that for brevity, I did not print here all 399 importable packages and modules on my system.) Beyond tab completion: Wildcard matching Tab completion is useful if you know the first few characters of the object or attribute you’re looking for, but is little help if you’d like to match characters at the middle or end of the word. For this use case, IPython provides a means of wildcard matching for names using the * character",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For this use case, IPython provides a means of wildcard matching for names using the * character. For example, we can use this to list every object in the namespace that ends with Warning : In [ 10 ]: *Warning ? BytesWarning RuntimeWarning DeprecationWarning SyntaxWarning FutureWarning UnicodeWarning ImportWarning UserWarning PendingDeprecationWarning Warning ResourceWarning Notice that the * character matches any string, including the empty string. Similarly, suppose we are looking for a string method that contains the word find somewhere in its name. We can search for it this way: Help and Documentation in IPython | 7 In [ 10 ]: str.*find* ? str . find str . rfind I find this type of flexible wildcard search can be very useful for finding a particular command when I’m getting to know a new package or reacquainting myself with a familiar one. Keyboard Shortcuts in the IPython Shell If you spend any amount of time on the computer, you’ve probably found a use for keyboard shortcuts in your workflow. Most familiar perhaps are Cmd-C and Cmd-V (or Ctrl-C and Ctrl-V) for copying and pasting in a wide variety of programs and sys‐ tems. Power users tend to go even further: popular text editors like Emacs, Vim, and others provide users an incredible range of operations through intricate combina‐ tions of keystrokes. The IPython shell doesn’t go this far, but does provide a number of keyboard short‐ cuts for fast navigation while you’re typing commands. These shortcuts are not in fact provided by IPython itself, but through its dependency on the GNU Readline library: thus, some of the following shortcuts may differ depending on your system configu‐ ration. Also, while some of these shortcuts do work in the browser-based notebook, this section is primarily about shortcuts in the IPython shell. Once you get accustomed to these, they can be very useful for quickly performing certain commands without moving your hands from the “home” keyboard position",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Once you get accustomed to these, they can be very useful for quickly performing certain commands without moving your hands from the “home” keyboard position. If you’re an Emacs user or if you have experience with Linux-style shells, the follow‐ ing will be very familiar. We’ll group these shortcuts into a few categories: navigation shortcuts , text entry shortcuts , command history shortcuts , and miscellaneous shortcuts . Navigation Shortcuts While the use of the left and right arrow keys to move backward and forward in the line is quite obvious, there are other options that don’t require moving your hands from the “home” keyboard position: Keystroke Action Ctrl-a Move cursor to the beginning of the line Ctrl-e Move cursor to the end of the line Ctrl-b (or the left arrow key) Move cursor back one character Ctrl-f (or the right arrow key) Move cursor forward one character 8 | Chapter 1: IPython: Beyond Normal Python Text Entry Shortcuts While everyone is familiar with using the Backspace key to delete the previous char‐ acter, reaching for the key often requires some minor finger gymnastics, and it only deletes a single character at a time. In IPython there are several shortcuts for remov‐ ing some portion of the text you’re typing. The most immediately useful of these are the commands to delete entire lines of text. You’ll know these have become second nature if you find yourself using a combination of Ctrl-b and Ctrl-d instead of reach‐ ing for the Backspace key to delete the previous character! Keystroke Action Backspace key Delete previous character in line Ctrl-d Delete next character in line Ctrl-k Cut text from cursor to end of line Ctrl-u Cut text from beginning fo line to cursor Ctrl-y Yank (i.e., paste) text that was previously cut Ctrl-t Transpose (i.e., switch) previous two characters Command History Shortcuts Perhaps the most impactful shortcuts discussed here are the ones IPython provides for navigating the command history",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This command history goes beyond your cur‐ rent IPython session: your entire command history is stored in a SQLite database in your IPython profile directory. The most straightforward way to access these is with the up and down arrow keys to step through the history, but other options exist as well: Keystroke Action Ctrl-p (or the up arrow key) Access previous command in history Ctrl-n (or the down arrow key) Access next command in history Ctrl-r Reverse-search through command history The reverse-search can be particularly useful. Recall that in the previous section we defined a function called square . Let’s reverse-search our Python history from a new IPython shell and find this definition again. When you press Ctrl-r in the IPython terminal, you’ll see the following prompt: In [ 1 ]: ( reverse - i - search ) ` ': If you start typing characters at this prompt, IPython will auto-fill the most recent command, if any, that matches those characters: Keyboard Shortcuts in the IPython Shell | 9 In [ 1 ]: ( reverse - i - search ) ` sqa ': square?? At any point, you can add more characters to refine the search, or press Ctrl-r again to search further for another command that matches the query. If you followed along in the previous section, pressing Ctrl-r twice more gives: In [ 1 ]: ( reverse - i - search ) ` sqa ': def square(a): \"\"\"Return the square of a\"\"\" return a ** 2 Once you have found the command you’re looking for, press Return and the search will end. We can then use the retrieved command, and carry on with our session: In [ 1 ]: def square ( a ): \"\"\"Return the square of a\"\"\" return a ** 2 In [ 2 ]: square ( 2 ) Out [ 2 ]: 4 Note that you can also use Ctrl-p/Ctrl-n or the up/down arrow keys to search through history, but only by matching characters at the beginning of the line. That is, if you type def and then press Ctrl-p, it would find the most recent command (if any) in your history that begins with the characters def",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That is, if you type def and then press Ctrl-p, it would find the most recent command (if any) in your history that begins with the characters def . Miscellaneous Shortcuts Finally, there are a few miscellaneous shortcuts that don’t fit into any of the preceding categories, but are nevertheless useful to know: Keystroke Action Ctrl-l Clear terminal screen Ctrl-c Interrupt current Python command Ctrl-d Exit IPython session The Ctrl-c shortcut in particular can be useful when you inadvertently start a very long-running job. While some of the shortcuts discussed here may seem a bit tedious at first, they quickly become automatic with practice. Once you develop that muscle memory, I suspect you will even find yourself wishing they were available in other contexts. IPython Magic Commands The previous two sections showed how IPython lets you use and explore Python effi‐ ciently and interactively. Here we’ll begin discussing some of the enhancements that 10 | Chapter 1: IPython: Beyond Normal Python IPython adds on top of the normal Python syntax. These are known in IPython as magic commands , and are prefixed by the % character. These magic commands are designed to succinctly solve various common problems in standard data analysis. Magic commands come in two flavors: line magics , which are denoted by a single % prefix and operate on a single line of input, and cell magics , which are denoted by a double %% prefix and operate on multiple lines of input. We’ll demonstrate and dis‐ cuss a few brief examples here, and come back to more focused discussion of several useful magic commands later in the chapter. Pasting Code Blocks: %paste and %cpaste When you’re working in the IPython interpreter, one common gotcha is that pasting multiline code blocks can lead to unexpected errors, especially when indentation and interpreter markers are involved. A common case is that you find some example code on a website and want to paste it into your interpreter",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A common case is that you find some example code on a website and want to paste it into your interpreter. Consider the following simple function: >>> def donothing ( x ): return x The code is formatted as it would appear in the Python interpreter, and if you copy and paste this directly into IPython you get an error: In [ 2 ]: >>> def donothing ( x ): : return x : File \"<ipython-input-20-5a66c8964687>\" , line 2 return x ^ SyntaxError : invalid syntax In the direct paste, the interpreter is confused by the additional prompt characters. But never fear—IPython’s %paste magic function is designed to handle this exact type of multiline, marked-up input: In [ 3 ]: % paste >>> def donothing ( x ): return x ## -- End pasted text -- The %paste command both enters and executes the code, so now the function is ready to be used: In [ 4 ]: donothing ( 10 ) Out [ 4 ]: 10 A command with a similar intent is %cpaste , which opens up an interactive multiline prompt in which you can paste one or more chunks of code to be executed in a batch: IPython Magic Commands | 11 In [ 5 ]: % cpaste Pasting code ; enter '--' alone on the line to stop or use Ctrl - D . : >>> def donothing ( x ): : return x : -- These magic commands, like others we’ll see, make available functionality that would be difficult or impossible in a standard Python interpreter. Running External Code: %run As you begin developing more extensive code, you will likely find yourself working in both IPython for interactive exploration, as well as a text editor to store code that you want to reuse. Rather than running this code in a new window, it can be convenient to run it within your IPython session. This can be done with the %run magic",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Rather than running this code in a new window, it can be convenient to run it within your IPython session. This can be done with the %run magic. For example, imagine you’ve created a myscript.py file with the following contents: #------------------------------------- # file: myscript.py def square ( x ): \"\"\"square a number\"\"\" return x ** 2 for N in range ( 1 , 4 ): print ( N , \"squared is\" , square ( N )) You can execute this from your IPython session as follows: In [ 6 ]: % run myscript.py 1 squared is 1 2 squared is 4 3 squared is 9 Note also that after you’ve run this script, any functions defined within it are available for use in your IPython session: In [ 7 ]: square ( 5 ) Out [ 7 ]: 25 There are several options to fine-tune how your code is run; you can see the docu‐ mentation in the normal way, by typing %run? in the IPython interpreter. Timing Code Execution: %timeit Another example of a useful magic function is %timeit , which will automatically determine the execution time of the single-line Python statement that follows it. For example, we may want to check the performance of a list comprehension: In [ 8 ]: % timeit L = [n ** 2 for n in range(1000)] 1000 loops , best of 3 : 325 μ s per loop 12 | Chapter 1: IPython: Beyond Normal Python The benefit of %timeit is that for short commands it will automatically perform mul‐ tiple runs in order to attain more robust results. For multiline statements, adding a second % sign will turn this into a cell magic that can handle multiple lines of input. For example, here’s the equivalent construction with a for loop: In [ 9 ]: %% timeit : L = [] : for n in range(1000): : L.append(n ** 2) : 1000 loops, best of 3: 373 μs per loop We can immediately see that list comprehensions are about 10% faster than the equivalent for loop construction in this case. We’ll explore %timeit and other approaches to timing and profiling code in “Profiling and Timing Code” on page 25",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We’ll explore %timeit and other approaches to timing and profiling code in “Profiling and Timing Code” on page 25 . Help on Magic Functions: ?, %magic, and %lsmagic Like normal Python functions, IPython magic functions have docstrings, and this useful documentation can be accessed in the standard manner. So, for example, to read the documentation of the %timeit magic, simply type this: In [ 10 ]: %timeit ? Documentation for other functions can be accessed similarly. To access a general description of available magic functions, including some examples, you can type this: In [ 11 ]: % magic For a quick and simple list of all available magic functions, type this: In [ 12 ]: % lsmagic Finally, I’ll mention that it is quite straightforward to define your own magic func‐ tions if you wish. We won’t discuss it here, but if you are interested, see the references listed in “More IPython Resources” on page 30 . Input and Output History Previously we saw that the IPython shell allows you to access previous commands with the up and down arrow keys, or equivalently the Ctrl-p/Ctrl-n shortcuts. Addi‐ tionally, in both the shell and the notebook, IPython exposes several ways to obtain the output of previous commands, as well as string versions of the commands them‐ selves. We’ll explore those here. IPython’s In and Out Objects By now I imagine you’re quite familiar with the In[1]: / Out[1]: style prompts used by IPython. But it turns out that these are not just pretty decoration: they give a clue Input and Output History | 13 as to how you can access previous inputs and outputs in your current session. Imag‐ ine you start a session that looks like this: In [ 1 ]: import math In [ 2 ]: math . sin ( 2 ) Out [ 2 ]: 0.9092974268256817 In [ 3 ]: math . cos ( 2 ) Out [ 3 ]: - 0.4161468365471424 We’ve imported the built-in math package, then computed the sine and the cosine of the number 2",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sin ( 2 ) Out [ 2 ]: 0.9092974268256817 In [ 3 ]: math . cos ( 2 ) Out [ 3 ]: - 0.4161468365471424 We’ve imported the built-in math package, then computed the sine and the cosine of the number 2. These inputs and outputs are displayed in the shell with In / Out labels, but there’s more—IPython actually creates some Python variables called In and Out that are automatically updated to reflect this history: In [ 4 ]: print ( In ) [ '' , 'import math' , 'math.sin(2)' , 'math.cos(2)' , 'print(In)' ] In [ 5 ]: Out Out [ 5 ]: { 2 : 0.9092974268256817 , 3 : - 0.4161468365471424 } The In object is a list, which keeps track of the commands in order (the first item in the list is a placeholder so that In[1] can refer to the first command): In [ 6 ]: print ( In [ 1 ]) import math The Out object is not a list but a dictionary mapping input numbers to their outputs (if any): In [ 7 ]: print ( Out [ 2 ]) 0.9092974268256817 Note that not all operations have outputs: for example, import statements and print statements don’t affect the output. The latter may be surprising, but makes sense if you consider that print is a function that returns None ; for brevity, any command that returns None is not added to Out . Where this can be useful is if you want to interact with past results. For example, let’s check the sum of sin(2) ** 2 and cos(2) ** 2 using the previously computed results: In [ 8 ]: Out [ 2 ] ** 2 + Out [ 3 ] ** 2 Out [ 8 ]: 1.0 The result is 1.0 as we’d expect from the well-known trigonometric identity",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In this case, using these previous results probably is not necessary, but it can become very handy if you execute a very expensive computation and want to reuse the result! 14 | Chapter 1: IPython: Beyond Normal Python Underscore Shortcuts and Previous Outputs The standard Python shell contains just one simple shortcut for accessing previous output; the variable _ (i.e., a single underscore) is kept updated with the previous out‐ put; this works in IPython as well: In [ 9 ]: print ( _ ) 1.0 But IPython takes this a bit further—you can use a double underscore to access the second-to-last output, and a triple underscore to access the third-to-last output (skip‐ ping any commands with no output): In [ 10 ]: print ( __ ) - 0.4161468365471424 In [ 11 ]: print ( ___ ) 0.9092974268256817 IPython stops there: more than three underscores starts to get a bit hard to count, and at that point it’s easier to refer to the output by line number. There is one more shortcut we should mention, however—a shorthand for Out[X] is _X (i.e., a single underscore followed by the line number): In [ 12 ]: Out [ 2 ] Out [ 12 ]: 0.9092974268256817 In [ 13 ]: _2 Out [ 13 ]: 0.9092974268256817 Suppressing Output Sometimes you might wish to suppress the output of a statement (this is perhaps most common with the plotting commands that we’ll explore in Chapter 4 ). Or maybe the command you’re executing produces a result that you’d prefer not to store in your output history, perhaps so that it can be deallocated when other references are removed. The easiest way to suppress the output of a command is to add a semicolon to the end of the line: In [ 14 ]: math . sin ( 2 ) + math . cos ( 2 ); Note that the result is computed silently, and the output is neither displayed on the screen or stored in the Out dictionary: In [ 15 ]: 14 in Out Out [ 15 ]: False Input and Output History | 15 Related Magic Commands For accessing a batch of previous inputs at once, the %history magic command is very helpful",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here is how you can print the first four inputs: In [ 16 ]: % history -n 1-4 1 : import math 2 : math . sin ( 2 ) 3 : math . cos ( 2 ) 4 : print ( In ) As usual, you can type %history? for more information and a description of options available. Other similar magic commands are %rerun (which will re-execute some portion of the command history) and %save (which saves some set of the command history to a file). For more information, I suggest exploring these using the ? help functionality discussed in “Help and Documentation in IPython” on page 3 . IPython and Shell Commands When working interactively with the standard Python interpreter, one of the frustra‐ tions you’ll face is the need to switch between multiple windows to access Python tools and system command-line tools. IPython bridges this gap, and gives you a syn‐ tax for executing shell commands directly from within the IPython terminal. The magic happens with the exclamation point: anything appearing after ! on a line will be executed not by the Python kernel, but by the system command line. The following assumes you’re on a Unix-like system, such as Linux or Mac OS X. Some of the examples that follow will fail on Windows, which uses a different type of shell by default (though with the 2016 announcement of native Bash shells on Win‐ dows, soon this may no longer be an issue!). If you’re unfamiliar with shell com‐ mands, I’d suggest reviewing the Shell Tutorial put together by the always excellent Software Carpentry Foundation. Quick Introduction to the Shell A full intro to using the shell/terminal/command line is well beyond the scope of this chapter, but for the uninitiated we will offer a quick introduction here. The shell is a way to interact textually with your computer. Ever since the mid-1980s, when Micro‐ soft and Apple introduced the first versions of their now ubiquitous graphical operat‐ ing systems, most computer users have interacted with their operating system through familiar clicking of menus and drag-and-drop movements",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". But operating systems existed long before these graphical user interfaces, and were primarily con‐ trolled through sequences of text input: at the prompt, the user would type a com‐ mand, and the computer would do what the user told it to. Those early prompt 16 | Chapter 1: IPython: Beyond Normal Python systems are the precursors of the shells and terminals that most active data scientists still use today. Someone unfamiliar with the shell might ask why you would bother with this, when you can accomplish many results by simply clicking on icons and menus. A shell user might reply with another question: why hunt icons and click menus when you can accomplish things much more easily by typing? While it might sound like a typical tech preference impasse, when moving beyond basic tasks it quickly becomes clear that the shell offers much more control of advanced tasks, though admittedly the learning curve can intimidate the average computer user. As an example, here is a sample of a Linux/OS X shell session where a user explores, creates, and modifies directories and files on their system ( osx:~ $ is the prompt, and everything after the $ sign is the typed command; text that is preceded by a # is meant just as description, rather than something you would actually type in): osx:~ $ echo \"hello world\" # echo is like Python's print function hello world osx:~ $ pwd # pwd = print working directory /home/jake # this is the \"path\" that we're in osx:~ $ ls # ls = list working directory contents notebooks projects osx:~ $ cd projects/ # cd = change directory osx:projects $ pwd /home/jake/projects osx:projects $ ls datasci_book mpld3 myproject.txt osx:projects $ mkdir myproject # mkdir = make new directory osx:projects $ cd myproject/ osx:myproject $ mv ../myproject.txt ./ # mv = move file",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here we're moving the # file myproject.txt from one directory # up (../) to the current directory (./) osx:myproject $ ls myproject.txt Notice that all of this is just a compact way to do familiar operations (navigating a directory structure, creating a directory, moving a file, etc.) by typing commands rather than clicking icons and menus. Note that with just a few commands ( pwd , ls , cd , mkdir , and cp ) you can do many of the most common file operations. It’s when you go beyond these basics that the shell approach becomes really powerful. IPython and Shell Commands | 17 Shell Commands in IPython You can use any command that works at the command line in IPython by prefixing it with the ! character. For example, the ls , pwd , and echo commands can be run as follows: In [ 1 ]: ! ls myproject . txt In [ 2 ]: ! pwd / home / jake / projects / myproject In [ 3 ]: ! echo \"printing from the shell\" printing from the shell Passing Values to and from the Shell Shell commands can not only be called from IPython, but can also be made to inter‐ act with the IPython namespace. For example, you can save the output of any shell command to a Python list using the assignment operator: In [ 4 ]: contents = ! ls In [ 5 ]: print ( contents ) [ 'myproject.txt' ] In [ 6 ]: directory = ! pwd In [ 7 ]: print ( directory ) [ '/Users/jakevdp/notebooks/tmp/myproject' ] Note that these results are not returned as lists, but as a special shell return type defined in IPython: In [ 8 ]: type ( directory ) IPython . utils . text . SList This looks and acts a lot like a Python list, but has additional functionality, such as the grep and fields methods and the s , n , and p properties that allow you to search, filter, and display the results in convenient ways. For more information on these, you can use IPython’s built-in help features",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For more information on these, you can use IPython’s built-in help features. Communication in the other direction—passing Python variables into the shell—is possible through the {varname} syntax: In [ 9 ]: message = \"hello from Python\" In [ 10 ]: ! echo { message } hello from Python 18 | Chapter 1: IPython: Beyond Normal Python The curly braces contain the variable name, which is replaced by the variable’s con‐ tents in the shell command. Shell-Related Magic Commands If you play with IPython’s shell commands for a while, you might notice that you can‐ not use !cd to navigate the filesystem: In [ 11 ]: ! pwd / home / jake / projects / myproject In [ 12 ]: ! cd .. In [ 13 ]: ! pwd / home / jake / projects / myproject The reason is that shell commands in the notebook are executed in a temporary sub‐ shell. If you’d like to change the working directory in a more enduring way, you can use the %cd magic command: In [ 14 ]: % cd .. / home / jake / projects In fact, by default you can even use this without the % sign: In [ 15 ]: cd myproject / home / jake / projects / myproject This is known as an automagic function, and this behavior can be toggled with the %automagic magic function. Besides %cd , other available shell-like magic functions are %cat , %cp , %env , %ls , %man , %mkdir , %more , %mv , %pwd , %rm , and %rmdir , any of which can be used without the % sign if automagic is on. This makes it so that you can almost treat the IPython prompt as if it’s a normal shell: In [ 16 ]: mkdir tmp In [ 17 ]: ls myproject . txt tmp / In [ 18 ]: cp myproject . txt tmp / In [ 19 ]: ls tmp myproject . txt In [ 20 ]: rm - r tmp This access to the shell from within the same terminal window as your Python ses‐ sion means that there is a lot less switching back and forth between interpreter and shell as you write your Python code. Shell-Related Magic Commands | 19 Errors and Debugging Code development and data analysis always require a bit of trial and error, and IPython contains tools to streamline this process",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Shell-Related Magic Commands | 19 Errors and Debugging Code development and data analysis always require a bit of trial and error, and IPython contains tools to streamline this process. This section will briefly cover some options for controlling Python’s exception reporting, followed by exploring tools for debugging errors in code. Controlling Exceptions: %xmode Most of the time when a Python script fails, it will raise an exception. When the inter‐ preter hits one of these exceptions, information about the cause of the error can be found in the traceback , which can be accessed from within Python. With the %xmode magic function, IPython allows you to control the amount of information printed when the exception is raised. Consider the following code: In [ 1 ]: def func1 ( a , b ): return a / b def func2 ( x ): a = x b = x - 1 return func1 ( a , b ) In [ 2 ]: func2 ( 1 ) --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-2-b2e110f6fc8f^gt; in <module>() ----> 1 func2(1) <ipython-input-1-d849e34d61fb> in func2(x) 5 a = x 6 b = x - 1 ----> 7 return func1(a, b) <ipython-input-1-d849e34d61fb> in func1(a, b) 1 def func1(a, b): ----> 2 return a / b 3 4 def func2(x): 5 a = x ZeroDivisionError: division by zero Calling func2 results in an error, and reading the printed trace lets us see exactly what happened. By default, this trace includes several lines showing the context of each 20 | Chapter 1: IPython: Beyond Normal Python step that led to the error. Using the %xmode magic function (short for exception mode ), we can change what information is printed. %xmode takes a single argument, the mode, and there are three possibilities: Plain , Context , and Verbose . The default is Context , and gives output like that just shown",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". %xmode takes a single argument, the mode, and there are three possibilities: Plain , Context , and Verbose . The default is Context , and gives output like that just shown. Plain is more compact and gives less information: In [ 3 ]: % xmode Plain Exception reporting mode: Plain In [ 4 ]: func2 ( 1 ) ------------------------------------------------------------ Traceback (most recent call last): File \"<ipython-input-4-b2e110f6fc8f>\", line 1, in <module> func2(1) File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2 return func1(a, b) File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1 return a / b ZeroDivisionError: division by zero The Verbose mode adds some extra information, including the arguments to any functions that are called: In [ 5 ]: % xmode Verbose Exception reporting mode: Verbose In [ 6 ]: func2 ( 1 ) --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) <ipython-input-6-b2e110f6fc8f> in <module>() ----> 1 func2(1) global func2 = <function func2 at 0x103729320> <ipython-input-1-d849e34d61fb> in func2(x=1) 5 a = x 6 b = x - 1 ----> 7 return func1(a, b) global func1 = <function func1 at 0x1037294d0> a = 1 b = 0 Errors and Debugging | 21 <ipython-input-1-d849e34d61fb> in func1(a=1, b=0) 1 def func1(a, b): ----> 2 return a / b a = 1 b = 0 3 4 def func2(x): 5 a = x ZeroDivisionError: division by zero This extra information can help you narrow in on why the exception is being raised. So why not use the Verbose mode all the time? As code gets complicated, this kind of traceback can get extremely long. Depending on the context, sometimes the brevity of Default mode is easier to work with. Debugging: When Reading Tracebacks Is Not Enough The standard Python tool for interactive debugging is pdb , the Python debugger. This debugger lets the user step through the code line by line in order to see what might be causing a more difficult error. The IPython-enhanced version of this is ipdb , the IPython debugger",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This debugger lets the user step through the code line by line in order to see what might be causing a more difficult error. The IPython-enhanced version of this is ipdb , the IPython debugger. There are many ways to launch and use both these debuggers; we won’t cover them fully here. Refer to the online documentation of these two utilities to learn more. In IPython, perhaps the most convenient interface to debugging is the %debug magic command. If you call it after hitting an exception, it will automatically open an inter‐ active debugging prompt at the point of the exception. The ipdb prompt lets you explore the current state of the stack, explore the available variables, and even run Python commands! Let’s look at the most recent exception, then do some basic tasks—print the values of a and b , and type quit to quit the debugging session: In [ 7 ]: % debug > <ipython-input-1-d849e34d61fb>(2)func1() 1 def func1(a, b): ----> 2 return a / b 3 ipdb> print(a) 1 ipdb> print(b) 0 ipdb> quit 22 | Chapter 1: IPython: Beyond Normal Python The interactive debugger allows much more than this, though—we can even step up and down through the stack and explore the values of variables there: In [ 8 ]: % debug > <ipython-input-1-d849e34d61fb>(2)func1() 1 def func1(a, b): ----> 2 return a / b 3 ipdb> up > <ipython-input-1-d849e34d61fb>(7)func2() 5 a = x 6 b = x - 1 ----> 7 return func1(a, b) ipdb> print(x) 1 ipdb> up > <ipython-input-6-b2e110f6fc8f>(1)<module>() ----> 1 func2(1) ipdb> down > <ipython-input-1-d849e34d61fb>(7)func2() 5 a = x 6 b = x - 1 ----> 7 return func1(a, b) ipdb> quit This allows you to quickly find out not only what caused the error, but also what function calls led up to the error",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If you’d like the debugger to launch automatically whenever an exception is raised, you can use the %pdb magic function to turn on this automatic behavior: In [ 9 ]: % xmode Plain % pdb on func2 ( 1 ) Exception reporting mode: Plain Automatic pdb calling has been turned ON Traceback (most recent call last): File \"<ipython-input-9-569a67d2d312>\", line 3, in <module> func2(1) File \"<ipython-input-1-d849e34d61fb>\", line 7, in func2 return func1(a, b) Errors and Debugging | 23 File \"<ipython-input-1-d849e34d61fb>\", line 2, in func1 return a / b ZeroDivisionError: division by zero > <ipython-input-1-d849e34d61fb>(2)func1() 1 def func1(a, b): ----> 2 return a / b 3 ipdb> print(b) 0 ipdb> quit Finally, if you have a script that you’d like to run from the beginning in interactive mode, you can run it with the command %run -d , and use the next command to step through the lines of code interactively. Partial list of debugging commands There are many more available commands for interactive debugging than we’ve listed here; the following table contains a description of some of the more common and useful ones: Command Description list Show the current location in the file h(elp) Show a list of commands, or find help on a specific command q(uit) Quit the debugger and the program c(ontinue) Quit the debugger; continue in the program n(ext) Go to the next step of the program <enter> Repeat the previous command p(rint) Print variables s(tep) Step into a subroutine r(eturn) Return out of a subroutine For more information, use the help command in the debugger, or take a look at ipdb ’s online documentation . 24 | Chapter 1: IPython: Beyond Normal Python Profiling and Timing Code In the process of developing code and creating data processing pipelines, there are often trade-offs you can make between various implementations. Early in developing your algorithm, it can be counterproductive to worry about such things",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Early in developing your algorithm, it can be counterproductive to worry about such things. As Donald Knuth famously quipped, “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.” But once you have your code working, it can be useful to dig into its efficiency a bit. Sometimes it’s useful to check the execution time of a given command or set of com‐ mands; other times it’s useful to dig into a multiline process and determine where the bottleneck lies in some complicated series of operations. IPython provides access to a wide array of functionality for this kind of timing and profiling of code. Here we’ll discuss the following IPython magic commands: %time Time the execution of a single statement %timeit Time repeated execution of a single statement for more accuracy %prun Run code with the profiler %lprun Run code with the line-by-line profiler %memit Measure the memory use of a single statement %mprun Run code with the line-by-line memory profiler The last four commands are not bundled with IPython—you’ll need to install the line_profiler and memory_profiler extensions, which we will discuss in the fol‐ lowing sections. Timing Code Snippets: %timeit and %time We saw the %timeit line magic and %%timeit cell magic in the introduction to magic functions in “IPython Magic Commands” on page 10 ; %%timeit can be used to time the repeated execution of snippets of code: In [ 1 ]: % timeit sum ( range ( 100 )) 100000 loops, best of 3: 1.54 μs per loop Profiling and Timing Code | 25 Note that because this operation is so fast, %timeit automatically does a large number of repetitions. For slower commands, %timeit will automatically adjust and perform fewer repetitions: In [ 2 ]: %% timeit total = 0 for i in range ( 1000 ): for j in range ( 1000 ): total += i * ( - 1 ) ** j 1 loops, best of 3: 407 ms per loop Sometimes repeating an operation is not the best option",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, if we have a list that we’d like to sort, we might be misled by a repeated operation. Sorting a prsorted list is much faster than sorting an unsorted list, so the repetition will skew the result: In [ 3 ]: import random L = [ random . random () for i in range ( 100000 )] % timeit L . sort () 100 loops, best of 3: 1.9 ms per loop For this, the %time magic function may be a better choice. It also is a good choice for longer-running commands, when short, system-related delays are unlikely to affect the result. Let’s time the sorting of an unsorted and a presorted list: In [ 4 ]: import random L = [ random . random () for i in range ( 100000 )] print ( \"sorting an unsorted list:\" ) % time L . sort () sorting an unsorted list: CPU times: user 40.6 ms, sys: 896 μs, total: 41.5 ms Wall time: 41.5 ms In [ 5 ]: print ( \"sorting an already sorted list:\" ) % time L . sort () sorting an already sorted list: CPU times: user 8.18 ms, sys: 10 μs, total: 8.19 ms Wall time: 8.24 ms Notice how much faster the presorted list is to sort, but notice also how much longer the timing takes with %time versus %timeit , even for the presorted list! This is a result of the fact that %timeit does some clever things under the hood to prevent sys‐ tem calls from interfering with the timing. For example, it prevents cleanup of unused Python objects (known as garbage collection ) that might otherwise affect the timing. For this reason, %timeit results are usually noticeably faster than %time results. For %time as with %timeit , using the double-percent-sign cell-magic syntax allows timing of multiline scripts: 26 | Chapter 1: IPython: Beyond Normal Python In [ 6 ]: %% time total = 0 for i in range ( 1000 ): for j in range ( 1000 ): total += i * ( - 1 ) ** j CPU times: user 504 ms, sys: 979 μs, total: 505 ms Wall time: 505 ms For more information on %time and %timeit , as well as their available options, use the IPython help functionality (i.e., type %time? at the IPython prompt)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Profiling Full Scripts: %prun A program is made of many single statements, and sometimes timing these state‐ ments in context is more important than timing them on their own. Python contains a built-in code profiler (which you can read about in the Python documentation), but IPython offers a much more convenient way to use this profiler, in the form of the magic function %prun . By way of example, we’ll define a simple function that does some calculations: In [ 7 ]: def sum_of_lists ( N ): total = 0 for i in range ( 5 ): L = [ j ^ ( j >> i ) for j in range ( N )] total += sum ( L ) return total Now we can call %prun with a function call to see the profiled results: In [ 8 ]: % prun sum_of_lists ( 1000000 ) In the notebook, the output is printed to the pager, and looks something like this: 14 function calls in 0.714 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 5 0.599 0.120 0.599 0.120 <ipython-input-19>:4(<listcomp>) 5 0.064 0.013 0.064 0.013 {built-in method sum} 1 0.036 0.036 0.699 0.699 <ipython-input-19>:1(sum_of_lists) 1 0.014 0.014 0.714 0.714 <string>:1(<module>) 1 0.000 0.000 0.714 0.714 {built-in method exec} The result is a table that indicates, in order of total time on each function call, where the execution is spending the most time. In this case, the bulk of execution time is in the list comprehension inside sum_of_lists . From here, we could start thinking about what changes we might make to improve the performance in the algorithm. Profiling and Timing Code | 27 For more information on %prun , as well as its available options, use the IPython help functionality (i.e., type %prun? at the IPython prompt). Line-by-Line Profiling with %lprun The function-by-function profiling of %prun is useful, but sometimes it’s more conve‐ nient to have a line-by-line profile report. This is not built into Python or IPython, but there is a line_profiler package available for installation that can do this",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is not built into Python or IPython, but there is a line_profiler package available for installation that can do this. Start by using Python’s packaging tool, pip , to install the line_profiler package: $ pip install line_profiler Next, you can use IPython to load the line_profiler IPython extension, offered as part of this package: In [ 9 ]: % load_ext line_profiler Now the %lprun command will do a line-by-line profiling of any function—in this case, we need to tell it explicitly which functions we’re interested in profiling: In [ 10 ]: % lprun - f sum_of_lists sum_of_lists ( 5000 ) As before, the notebook sends the result to the pager, but it looks something like this: Timer unit: 1e-06 s Total time: 0.009382 s File: <ipython-input-19-fa2be176cc3e> Function: sum_of_lists at line 1 Line # Hits Time Per Hit % Time Line Contents ============================================================== 1 def sum_of_lists(N): 2 1 2 2.0 0.0 total = 0 3 6 8 1.3 0.1 for i in range(5): 4 5 9001 1800.2 95.9 L = [j ^ (j >> i) 5 5 371 74.2 4.0 total += sum(L) 6 1 0 0.0 0.0 return total The information at the top gives us the key to reading the results: the time is reported in microseconds and we can see where the program is spending the most time. At this point, we may be able to use this information to modify aspects of the script and make it perform better for our desired use case. For more information on %lprun , as well as its available options, use the IPython help functionality (i.e., type %lprun? at the IPython prompt). 28 | Chapter 1: IPython: Beyond Normal Python Profiling Memory Use: %memit and %mprun Another aspect of profiling is the amount of memory an operation uses. This can be evaluated with another IPython extension, the memory_profiler",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This can be evaluated with another IPython extension, the memory_profiler . As with the line_profiler , we start by pip -installing the extension: $ pip install memory_profiler Then we can use IPython to load the extension: In [ 12 ]: % load_ext memory_profiler The memory profiler extension contains two useful magic functions: the %memit magic (which offers a memory-measuring equivalent of %timeit ) and the %mprun function (which offers a memory-measuring equivalent of %lprun ). The %memit func‐ tion can be used rather simply: In [ 13 ]: % memit sum_of_lists ( 1000000 ) peak memory: 100.08 MiB, increment: 61.36 MiB We see that this function uses about 100 MB of memory. For a line-by-line description of memory use, we can use the %mprun magic. Unfortu‐ nately, this magic works only for functions defined in separate modules rather than the notebook itself, so we’ll start by using the %%file magic to create a simple module called mprun_demo.py , which contains our sum_of_lists function, with one addition that will make our memory profiling results more clear: In [ 14 ]: %% file mprun_demo",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". py def sum_of_lists ( N ): total = 0 for i in range ( 5 ): L = [ j ^ ( j >> i ) for j in range ( N )] total += sum ( L ) del L # remove reference to L return total Overwriting mprun_demo.py We can now import the new version of this function and run the memory line profiler: In [ 15 ]: from mprun_demo import sum_of_lists % mprun - f sum_of_lists sum_of_lists ( 1000000 ) The result, printed to the pager, gives us a summary of the memory use of the func‐ tion, and looks something like this: Profiling and Timing Code | 29 Filename: ./mprun_demo.py Line # Mem usage Increment Line Contents ================================================ 4 71.9 MiB 0.0 MiB L = [j ^ (j >> i) for j in range(N)] Filename: ./mprun_demo.py Line # Mem usage Increment Line Contents ================================================ 1 39.0 MiB 0.0 MiB def sum_of_lists(N): 2 39.0 MiB 0.0 MiB total = 0 3 46.5 MiB 7.5 MiB for i in range(5): 4 71.9 MiB 25.4 MiB L = [j ^ (j >> i) for j in range(N)] 5 71.9 MiB 0.0 MiB total += sum(L) 6 46.5 MiB -25.4 MiB del L # remove reference to L 7 39.1 MiB -7.4 MiB return total Here the Increment column tells us how much each line affects the total memory budget: observe that when we create and delete the list L , we are adding about 25 MB of memory usage. This is on top of the background memory usage from the Python interpreter itself. For more information on %memit and %mprun , as well as their available options, use the IPython help functionality (i.e., type %memit? at the IPython prompt). More IPython Resources In this chapter, we’ve just scratched the surface of using IPython to enable data sci‐ ence tasks. Much more information is available both in print and on the Web, and here we’ll list some other resources that you may find helpful. Web Resources The IPython website The IPython website links to documentation, examples, tutorials, and a variety of other resources. The nbviewer website This site shows static renderings of any IPython notebook available on the Inter‐ net",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The nbviewer website This site shows static renderings of any IPython notebook available on the Inter‐ net. The front page features some example notebooks that you can browse to see what other folks are using IPython for! 30 | Chapter 1: IPython: Beyond Normal Python A Gallery of Interesting IPython Notebooks This ever-growing list of notebooks, powered by nbviewer, shows the depth and breadth of numerical analysis you can do with IPython. It includes everything from short examples and tutorials to full-blown courses and books composed in the notebook format! Video tutorials Searching the Internet, you will find many video-recorded tutorials on IPython. I’d especially recommend seeking tutorials from the PyCon, SciPy, and PyData conferences by Fernando Perez and Brian Granger, two of the primary creators and maintainers of IPython and Jupyter. Books Python for Data Analysis Wes McKinney’s book includes a chapter that covers using IPython as a data sci‐ entist. Although much of the material overlaps what we’ve discussed here, another perspective is always helpful. Learning IPython for Interactive Computing and Data Visualization This short book by Cyrille Rossant offers a good introduction to using IPython for data analysis. IPython Interactive Computing and Visualization Cookbook Also by Cyrille Rossant, this book is a longer and more advanced treatment of using IPython for data science. Despite its name, it’s not just about IPython—it also goes into some depth on a broad range of data science topics. Finally, a reminder that you can find help on your own: IPython’s ? -based help func‐ tionality (discussed in “Help and Documentation in IPython” on page 3 ) can be very useful if you use it well and use it often. As you go through the examples here and elsewhere, you can use it to familiarize yourself with all the tools that IPython has to offer",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As you go through the examples here and elsewhere, you can use it to familiarize yourself with all the tools that IPython has to offer. More IPython Resources | 31 CHAPTER 2 Introduction to NumPy This chapter, along with Chapter 3 , outlines techniques for effectively loading, stor‐ ing, and manipulating in-memory data in Python. The topic is very broad: datasets can come from a wide range of sources and a wide range of formats, including collec‐ tions of documents, collections of images, collections of sound clips, collections of numerical measurements, or nearly anything else. Despite this apparent heterogene‐ ity, it will help us to think of all data fundamentally as arrays of numbers. For example, images—particularly digital images—can be thought of as simply twdimensional arrays of numbers representing pixel brightness across the area. Sound clips can be thought of as one-dimensional arrays of intensity versus time. Text can be converted in various ways into numerical representations, perhaps binary digits rep‐ resenting the frequency of certain words or pairs of words. No matter what the data are, the first step in making them analyzable will be to transform them into arrays of numbers. (We will discuss some specific examples of this process later in “Feature Engineering” on page 375 .) For this reason, efficient storage and manipulation of numerical arrays is absolutely fundamental to the process of doing data science. We’ll now take a look at the special‐ ized tools that Python has for handling such numerical arrays: the NumPy package and the Pandas package (discussed in Chapter 3 .) This chapter will cover NumPy in detail. NumPy (short for Numerical Python ) pro‐ vides an efficient interface to store and operate on dense data buffers. In some ways, NumPy arrays are like Python’s built-in list type, but NumPy arrays provide much more efficient storage and data operations as the arrays grow larger in size",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In some ways, NumPy arrays are like Python’s built-in list type, but NumPy arrays provide much more efficient storage and data operations as the arrays grow larger in size. NumPy arrays form the core of nearly the entire ecosystem of data science tools in Python, so time spent learning to use NumPy effectively will be valuable no matter what aspect of data science interests you. 33 If you followed the advice outlined in the preface and installed the Anaconda stack, you already have NumPy installed and ready to go. If you’re more the do-it-yourself type, you can go to the NumPy website and follow the installation instructions found there. Once you do, you can import NumPy and double-check the version: In [ 1 ]: import numpy numpy . __version__ Out[1]: '1.11.1' For the pieces of the package discussed here, I’d recommend NumPy version 1.8 or later. By convention, you’ll find that most people in the SciPy/PyData world will import NumPy using np as an alias: In [ 2 ]: import numpy as np Throughout this chapter, and indeed the rest of the book, you’ll find that this is the way we will import and use NumPy. Reminder About Built-In Documentation As you read through this chapter, don’t forget that IPython gives you the ability to quickly explore the contents of a package (by using the tab-completion feature) as well as the documentation of various functions (using the ? character). Refer back to “Help and Documentation in IPython” on page 3 if you need a refresher on this. For example, to display all the contents of the numpy namespace, you can type this: In [ 3 ]: np .< TAB > And to display NumPy’s built-in documentation, you can use this: In [ 4 ]: np ? More detailed documentation, along with tutorials and other resources, can be found at http://www.numpy.org . Understanding Data Types in Python Effective data-driven science and computation requires understanding how data is stored and manipulated",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Understanding Data Types in Python Effective data-driven science and computation requires understanding how data is stored and manipulated. This section outlines and contrasts how arrays of data are handled in the Python language itself, and how NumPy improves on this. Under‐ standing this difference is fundamental to understanding much of the material throughout the rest of the book. Users of Python are often drawn in by its ease of use, one piece of which is dynamic typing. While a statically typed language like C or Java requires each variable to be 34 | Chapter 2: Introduction to NumPy explicitly declared, a dynamically typed language like Python skips this specification. For example, in C you might specify a particular operation as follows: /* C code */ int result = 0 ; for ( int i = 0 ; i < 100 ; i ++ ){ result += i ; } While in Python the equivalent operation could be written this way: # Python code result = 0 for i in range ( 100 ): result += i Notice the main difference: in C, the data types of each variable are explicitly declared, while in Python the types are dynamically inferred. This means, for exam‐ ple, that we can assign any kind of data to any variable: # Python code x = 4 x = \"four\" Here we’ve switched the contents of x from an integer to a string. The same thing in C would lead (depending on compiler settings) to a compilation error or other uninten‐ ded consequences: /* C code */ int x = 4 ; x = \"four\" ; // FAILS This sort of flexibility is one piece that makes Python and other dynamically typed languages convenient and easy to use. Understanding how this works is an important piece of learning to analyze data efficiently and effectively with Python. But what this type flexibility also points to is the fact that Python variables are more than just their value; they also contain extra information about the type of the value. We’ll explore this more in the sections that follow. A Python Integer Is More Than Just an Integer The standard Python implementation is written in C",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We’ll explore this more in the sections that follow. A Python Integer Is More Than Just an Integer The standard Python implementation is written in C. This means that every Python object is simply a cleverly disguised C structure, which contains not only its value, but other information as well. For example, when we define an integer in Python, such as x = 10000 , x is not just a “raw” integer. It’s actually a pointer to a compound C struc‐ ture, which contains several values. Looking through the Python 3.4 source code, we find that the integer (long) type definition effectively looks like this (once the C mac‐ ros are expanded): Understanding Data Types in Python | 35 struct _longobject { long ob_refcnt ; PyTypeObject * ob_type ; size_t ob_size ; long ob_digit [ 1 ]; }; A single integer in Python 3.4 actually contains four pieces: • ob_refcnt , a reference count that helps Python silently handle memory alloca‐ tion and deallocation • ob_type , which encodes the type of the variable • ob_size , which specifies the size of the following data members • ob_digit , which contains the actual integer value that we expect the Python vari‐ able to represent This means that there is some overhead in storing an integer in Python as compared to an integer in a compiled language like C, as illustrated in Figure 2-1 . Figure 2-1. The difference between C and Python integers Here PyObject_HEAD is the part of the structure containing the reference count, type code, and other pieces mentioned before. Notice the difference here: a C integer is essentially a label for a position in memory whose bytes encode an integer value. A Python integer is a pointer to a position in memory containing all the Python object information, including the bytes that con‐ tain the integer value. This extra information in the Python integer structure is what allows Python to be coded so freely and dynamically",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This extra information in the Python integer structure is what allows Python to be coded so freely and dynamically. All this additional information in Python types comes at a cost, however, which becomes especially apparent in structures that combine many of these objects. 36 | Chapter 2: Introduction to NumPy A Python List Is More Than Just a List Let’s consider now what happens when we use a Python data structure that holds many Python objects. The standard mutable multielement container in Python is the list. We can create a list of integers as follows: In [ 1 ]: L = list ( range ( 10 )) L Out[1]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] In [ 2 ]: type ( L [ 0 ]) Out[2]: int Or, similarly, a list of strings: In [ 3 ]: L2 = [ str ( c ) for c in L ] L2 Out[3]: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] In [ 4 ]: type ( L2 [ 0 ]) Out[4]: str Because of Python’s dynamic typing, we can even create heterogeneous lists: In [ 5 ]: L3 = [ True , \"2\" , 3.0 , 4 ] [ type ( item ) for item in L3 ] Out[5]: [bool, str, float, int] But this flexibility comes at a cost: to allow these flexible types, each item in the list must contain its own type info, reference count, and other information—that is, each item is a complete Python object. In the special case that all variables are of the same type, much of this information is redundant: it can be much more efficient to store data in a fixed-type array. The difference between a dynamic-type list and a fixed-type (NumPy-style) array is illustrated in Figure 2-2 . At the implementation level, the array essentially contains a single pointer to one con‐ tiguous block of data. The Python list, on the other hand, contains a pointer to a block of pointers, each of which in turn points to a full Python object like the Python integer we saw earlier. Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Again, the advantage of the list is flexibility: because each list element is a full structure containing both data and type information, the list can be filled with data of any desired type. Fixed-type NumPy-style arrays lack this flexibil‐ ity, but are much more efficient for storing and manipulating data. Understanding Data Types in Python | 37 Figure 2-2. The difference between C and Python lists Fixed-Type Arrays in Python Python offers several different options for storing data in efficient, fixed-type data buffers. The built-in array module (available since Python 3.3) can be used to create dense arrays of a uniform type: In [ 6 ]: import array L = list ( range ( 10 )) A = array . array ( 'i' , L ) A Out[6]: array('i', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Here 'i' is a type code indicating the contents are integers. Much more useful, however, is the ndarray object of the NumPy package. While Python’s array object provides efficient storage of array-based data, NumPy adds to this efficient operations on that data. We will explore these operations in later sec‐ tions; here we’ll demonstrate several ways of creating a NumPy array. We’ll start with the standard NumPy import, under the alias np : In [ 7 ]: import numpy as np 38 | Chapter 2: Introduction to NumPy Creating Arrays from Python Lists First, we can use np.array to create arrays from Python lists: In [ 8 ]: # integer array: np . array ([ 1 , 4 , 2 , 5 , 3 ]) Out[8]: array([1, 4, 2, 5, 3]) Remember that unlike Python lists, NumPy is constrained to arrays that all contain the same type. If types do not match, NumPy will upcast if possible (here, integers are upcast to floating point): In [ 9 ]: np . array ([ 3.14 , 4 , 2 , 3 ]) Out[9]: array([ 3.14, 4. , 2. , 3. ]) If we want to explicitly set the data type of the resulting array, we can use the dtype keyword: In [ 10 ]: np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_53"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". array ([ 3.14 , 4 , 2 , 3 ]) Out[9]: array([ 3.14, 4. , 2. , 3. ]) If we want to explicitly set the data type of the resulting array, we can use the dtype keyword: In [ 10 ]: np . array ([ 1 , 2 , 3 , 4 ], dtype = 'float32' ) Out[10]: array([ 1., 2., 3., 4.], dtype=float32) Finally, unlike Python lists, NumPy arrays can explicitly be multidimensional; here’s one way of initializing a multidimensional array using a list of lists: In [ 11 ]: # nested lists result in multidimensional arrays np . array ([ range ( i , i + 3 ) for i in [ 2 , 4 , 6 ]]) Out[11]: array([[2, 3, 4], [4, 5, 6], [6, 7, 8]]) The inner lists are treated as rows of the resulting two-dimensional array. Creating Arrays from Scratch Especially for larger arrays, it is more efficient to create arrays from scratch using rou‐ tines built into NumPy. Here are several examples: In [ 12 ]: # Create a length-10 integer array filled with zeros np . zeros ( 10 , dtype = int ) Out[12]: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) In [ 13 ]: # Create a 3x5 floating-point array filled with 1s np . ones (( 3 , 5 ), dtype = float ) Out[13]: array([[ 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 1.], [ 1., 1., 1., 1., 1.]]) In [ 14 ]: # Create a 3x5 array filled with 3.14 np . full (( 3 , 5 ), 3.14 ) Understanding Data Types in Python | 39 Out[14]: array([[ 3.14, 3.14, 3.14, 3.14, 3.14], [ 3.14, 3.14, 3.14, 3.14, 3.14], [ 3.14, 3.14, 3.14, 3.14, 3.14]]) In [ 15 ]: # Create an array filled with a linear sequence # Starting at 0, ending at 20, stepping by 2 # (this is similar to the built-in range() function) np . arange ( 0 , 20 , 2 ) Out[15]: array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]) In [ 16 ]: # Create an array of five values evenly spaced between 0 and 1 np . linspace ( 0 , 1 , 5 ) Out[16]: array([ 0. , 0.25, 0.5 , 0.75, 1. ]) In [ 17 ]: # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_54"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". linspace ( 0 , 1 , 5 ) Out[16]: array([ 0. , 0.25, 0.5 , 0.75, 1. ]) In [ 17 ]: # Create a 3x3 array of uniformly distributed # random values between 0 and 1 np . random . random (( 3 , 3 )) Out[17]: array([[ 0.99844933, 0.52183819, 0.22421193], [ 0.08007488, 0.45429293, 0.20941444], [ 0.14360941, 0.96910973, 0.946117 ]]) In [ 18 ]: # Create a 3x3 array of normally distributed random values # with mean 0 and standard deviation 1 np . random . normal ( 0 , 1 , ( 3 , 3 )) Out[18]: array([[ 1.51772646, 0.39614948, -0.10634696], [ 0.25671348, 0.00732722, 0.37783601], [ 0.68446945, 0.15926039, -0.70744073]]) In [ 19 ]: # Create a 3x3 array of random integers in the interval [0, 10) np . random . randint ( 0 , 10 , ( 3 , 3 )) Out[19]: array([[2, 3, 4], [5, 7, 8], [0, 5, 0]]) In [ 20 ]: # Create a 3x3 identity matrix np . eye ( 3 ) Out[20]: array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) In [ 21 ]: # Create an uninitialized array of three integers # The values will be whatever happens to already exist at that # memory location np . empty ( 3 ) Out[21]: array([ 1., 1., 1.]) 40 | Chapter 2: Introduction to NumPy NumPy Standard Data Types NumPy arrays contain values of a single type, so it is important to have detailed knowledge of those types and their limitations. Because NumPy is built in C, the types will be familiar to users of C, Fortran, and other related languages. The standard NumPy data types are listed in Table 2-1 . Note that when constructing an array, you can specify them using a string: np . zeros ( 10 , dtype = 'int16' ) Or using the associated NumPy object: np . zeros ( 10 , dtype = np . int16 ) Table 2-1",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_55"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Note that when constructing an array, you can specify them using a string: np . zeros ( 10 , dtype = 'int16' ) Or using the associated NumPy object: np . zeros ( 10 , dtype = np . int16 ) Table 2-1. Standard NumPy data types Data type Description bool_ Boolean (True or False) stored as a byte int_ Default integer type (same as C long ; normally either int64 or int32 ) intc Identical to C int (normally int32 or int64 ) intp Integer used for indexing (same as C ssize_t ; normally either int32 or int64 ) int8 Byte (–128 to 127) int16 Integer (–32768 to 32767) int32 Integer (–2147483648 to 2147483647) int64 Integer (–9223372036854775808 to 9223372036854775807) uint8 Unsigned integer (0 to 255) uint16 Unsigned integer (0 to 65535) uint32 Unsigned integer (0 to 4294967295) uint64 Unsigned integer (0 to 18446744073709551615) float_ Shorthand for float64 float16 Half-precision float: sign bit, 5 bits exponent, 10 bits mantissa float32 Single-precision float: sign bit, 8 bits exponent, 23 bits mantissa float64 Double-precision float: sign bit, 11 bits exponent, 52 bits mantissa complex_ Shorthand for complex128 complex64 Complex number, represented by two 32-bit floats complex128 Complex number, represented by two 64-bit floats More advanced type specification is possible, such as specifying big or little endian numbers; for more information, refer to the NumPy documentation . NumPy also supports compound data types, which will be covered in “Structured Data: NumPy’s Structured Arrays” on page 92 . Understanding Data Types in Python | 41 The Basics of NumPy Arrays Data manipulation in Python is nearly synonymous with NumPy array manipulation: even newer tools like Pandas ( Chapter 3 ) are built around the NumPy array. This sec‐ tion will present several examples using NumPy array manipulation to access data and subarrays, and to split, reshape, and join the arrays. While the types of operations shown here may seem a bit dry and pedantic, they comprise the building blocks of many other examples used throughout the book",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_56"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While the types of operations shown here may seem a bit dry and pedantic, they comprise the building blocks of many other examples used throughout the book. Get to know them well! We’ll cover a few categories of basic array manipulations here: Attributes of arrays Determining the size, shape, memory consumption, and data types of arrays Indexing of arrays Getting and setting the value of individual array elements Slicing of arrays Getting and setting smaller subarrays within a larger array Reshaping of arrays Changing the shape of a given array Joining and splitting of arrays Combining multiple arrays into one, and splitting one array into many NumPy Array Attributes First let’s discuss some useful array attributes. We’ll start by defining three random arrays: a one-dimensional, two-dimensional, and three-dimensional array. We’ll use NumPy’s random number generator, which we will seed with a set value in order to ensure that the same random arrays are generated each time this code is run: In [ 1 ]: import numpy as np np . random . seed ( 0 ) # seed for reproducibility x1 = np . random . randint ( 10 , size = 6 ) # One-dimensional array x2 = np . random . randint ( 10 , size = ( 3 , 4 )) # Two-dimensional array x3 = np . random . randint ( 10 , size = ( 3 , 4 , 5 )) # Three-dimensional array Each array has attributes ndim (the number of dimensions), shape (the size of each dimension), and size (the total size of the array): In [ 2 ]: print ( \"x3 ndim: \" , x3 . ndim ) print ( \"x3 shape:\" , x3 . shape ) print ( \"x3 size: \" , x3 . size ) x3 ndim: 3 x3 shape: (3, 4, 5) x3 size: 60 42 | Chapter 2: Introduction to NumPy Another useful attribute is the dtype , the data type of the array (which we discussed previously in “Understanding Data Types in Python” on page 34 ): In [ 3 ]: print ( \"dtype:\" , x3 . dtype ) dtype: int64 Other attributes include itemsize , which lists the size (in bytes) of each array ele‐ ment, and nbytes , which lists the total size (in bytes) of the array: In [ 4 ]: print ( \"itemsize:\" , x3",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_57"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". itemsize , \"bytes\" ) print ( \"nbytes:\" , x3 . nbytes , \"bytes\" ) itemsize: 8 bytes nbytes: 480 bytes In general, we expect that nbytes is equal to itemsize times size . Array Indexing: Accessing Single Elements If you are familiar with Python’s standard list indexing, indexing in NumPy will feel quite familiar. In a one-dimensional array, you can access the i th value (counting from zero) by specifying the desired index in square brackets, just as with Python lists: In [ 5 ]: x1 Out[5]: array([5, 0, 3, 3, 7, 9]) In [ 6 ]: x1 [ 0 ] Out[6]: 5 In [ 7 ]: x1 [ 4 ] Out[7]: 7 To index from the end of the array, you can use negative indices: In [ 8 ]: x1 [ - 1 ] Out[8]: 9 In [ 9 ]: x1 [ - 2 ] Out[9]: 7 In a multidimensional array, you access items using a comma-separated tuple of indices: In [ 10 ]: x2 Out[10]: array([[3, 5, 2, 4], [7, 6, 8, 8], [1, 6, 7, 7]]) In [ 11 ]: x2 [ 0 , 0 ] Out[11]: 3 The Basics of NumPy Arrays | 43 In [ 12 ]: x2 [ 2 , 0 ] Out[12]: 1 In [ 13 ]: x2 [ 2 , - 1 ] Out[13]: 7 You can also modify values using any of the above index notation: In [ 14 ]: x2 [ 0 , 0 ] = 12 x2 Out[14]: array([[12, 5, 2, 4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) Keep in mind that, unlike Python lists, NumPy arrays have a fixed type. This means, for example, that if you attempt to insert a floating-point value to an integer array, the value will be silently truncated. Don’t be caught unaware by this behavior! In [ 15 ]: x1 [ 0 ] = 3.14159 # this will be truncated! x1 Out[15]: array([3, 0, 3, 3, 7, 9]) Array Slicing: Accessing Subarrays Just as we can use square brackets to access individual array elements, we can also use them to access subarrays with the slice notation, marked by the colon ( : ) character. The NumPy slicing syntax follows that of the standard Python list; to access a slice of an array x , use this: x [ start : stop : step ] If any of these are unspecified, they default to the values start=0 , stop= size of dimension , step=1 . We’ll take a look at accessing subarrays in one dimension and in multiple dimensions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_58"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We’ll take a look at accessing subarrays in one dimension and in multiple dimensions. One-dimensional subarrays In [ 16 ]: x = np . arange ( 10 ) x Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) In [ 17 ]: x [: 5 ] # first five elements Out[17]: array([0, 1, 2, 3, 4]) In [ 18 ]: x [ 5 :] # elements after index 5 Out[18]: array([5, 6, 7, 8, 9]) In [ 19 ]: x [ 4 : 7 ] # middle subarray Out[19]: array([4, 5, 6]) 44 | Chapter 2: Introduction to NumPy In [ 20 ]: x [:: 2 ] # every other element Out[20]: array([0, 2, 4, 6, 8]) In [ 21 ]: x [ 1 :: 2 ] # every other element, starting at index 1 Out[21]: array([1, 3, 5, 7, 9]) A potentially confusing case is when the step value is negative. In this case, the defaults for start and stop are swapped. This becomes a convenient way to reverse an array: In [ 22 ]: x [:: - 1 ] # all elements, reversed Out[22]: array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0]) In [ 23 ]: x [ 5 :: - 2 ] # reversed every other from index 5 Out[23]: array([5, 3, 1]) Multidimensional subarrays Multidimensional slices work in the same way, with multiple slices separated by com‐ mas. For example: In [ 24 ]: x2 Out[24]: array([[12, 5, 2, 4], [ 7, 6, 8, 8], [ 1, 6, 7, 7]]) In [ 25 ]: x2 [: 2 , : 3 ] # two rows, three columns Out[25]: array([[12, 5, 2], [ 7, 6, 8]]) In [ 26 ]: x2 [: 3 , :: 2 ] # all rows, every other column Out[26]: array([[12, 2], [ 7, 8], [ 1, 7]]) Finally, subarray dimensions can even be reversed together: In [ 27 ]: x2 [:: - 1 , :: - 1 ] Out[27]: array([[ 7, 7, 6, 1], [ 8, 8, 6, 7], [ 4, 2, 5, 12]]) Accessing array rows and columns. One commonly needed routine is accessing single rows or columns of an array",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_59"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". One commonly needed routine is accessing single rows or columns of an array. You can do this by combining indexing and slicing, using an empty slice marked by a single colon ( : ): In [ 28 ]: print ( x2 [:, 0 ]) # first column of x2 [12 7 1] The Basics of NumPy Arrays | 45 In [ 29 ]: print ( x2 [ 0 , :]) # first row of x2 [12 5 2 4] In the case of row access, the empty slice can be omitted for a more compact syntax: In [ 30 ]: print ( x2 [ 0 ]) # equivalent to x2[0, :] [12 5 2 4] Subarrays as no-copy views One important—and extremely useful—thing to know about array slices is that they return views rather than copies of the array data. This is one area in which NumPy array slicing differs from Python list slicing: in lists, slices will be copies. Consider our two-dimensional array from before: In [ 31 ]: print ( x2 ) [[12 5 2 4] [ 7 6 8 8] [ 1 6 7 7]] Let’s extract a 2×2 subarray from this: In [ 32 ]: x2_sub = x2 [: 2 , : 2 ] print ( x2_sub ) [[12 5] [ 7 6]] Now if we modify this subarray, we’ll see that the original array is changed! Observe: In [ 33 ]: x2_sub [ 0 , 0 ] = 99 print ( x2_sub ) [[99 5] [ 7 6]] In [ 34 ]: print ( x2 ) [[99 5 2 4] [ 7 6 8 8] [ 1 6 7 7]] This default behavior is actually quite useful: it means that when we work with large datasets, we can access and process pieces of these datasets without the need to copy the underlying data buffer. Creating copies of arrays Despite the nice features of array views, it is sometimes useful to instead explicitly copy the data within an array or a subarray. This can be most easily done with the copy() method: 46 | Chapter 2: Introduction to NumPy In [ 35 ]: x2_sub_copy = x2 [: 2 , : 2 ] . copy () print ( x2_sub_copy ) [[99 5] [ 7 6]] If we now modify this subarray, the original array is not touched: In [ 36 ]: x2_sub_copy [ 0 , 0 ] = 42 print ( x2_sub_copy ) [[42 5] [ 7 6]] In [ 37 ]: print ( x2 ) [[99 5 2 4] [ 7 6 8 8] [ 1 6 7 7]] Reshaping of Arrays Another useful type of operation is reshaping of arrays",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_60"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The most flexible way of doing this is with the reshape() method. For example, if you want to put the num‐ bers 1 through 9 in a 3×3 grid, you can do the following: In [ 38 ]: grid = np . arange ( 1 , 10 ) . reshape (( 3 , 3 )) print ( grid ) [[1 2 3] [4 5 6] [7 8 9]] Note that for this to work, the size of the initial array must match the size of the reshaped array. Where possible, the reshape method will use a no-copy view of the initial array, but with noncontiguous memory buffers this is not always the case. Another common reshaping pattern is the conversion of a one-dimensional array into a two-dimensional row or column matrix. You can do this with the reshape method, or more easily by making use of the newaxis keyword within a slice opera‐ tion: In [ 39 ]: x = np . array ([ 1 , 2 , 3 ]) # row vector via reshape x . reshape (( 1 , 3 )) Out[39]: array([[1, 2, 3]]) In [ 40 ]: # row vector via newaxis x [ np . newaxis , :] Out[40]: array([[1, 2, 3]]) The Basics of NumPy Arrays | 47 In [ 41 ]: # column vector via reshape x . reshape (( 3 , 1 )) Out[41]: array([[1], [2], [3]]) In [ 42 ]: # column vector via newaxis x [:, np . newaxis ] Out[42]: array([[1], [2], [3]]) We will see this type of transformation often throughout the remainder of the book. Array Concatenation and Splitting All of the preceding routines worked on single arrays. It’s also possible to combine multiple arrays into one, and to conversely split a single array into multiple arrays. We’ll take a look at those operations here. Concatenation of arrays Concatenation, or joining of two arrays in NumPy, is primarily accomplished through the routines np.concatenate , np.vstack , and np.hstack . np.concatenate takes a tuple or list of arrays as its first argument, as we can see here: In [ 43 ]: x = np . array ([ 1 , 2 , 3 ]) y = np . array ([ 3 , 2 , 1 ]) np . concatenate ([ x , y ]) Out[43]: array([1, 2, 3, 3, 2, 1]) You can also concatenate more than two arrays at once: In [ 44 ]: z = [ 99 , 99 , 99 ] print ( np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_61"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". array ([ 3 , 2 , 1 ]) np . concatenate ([ x , y ]) Out[43]: array([1, 2, 3, 3, 2, 1]) You can also concatenate more than two arrays at once: In [ 44 ]: z = [ 99 , 99 , 99 ] print ( np . concatenate ([ x , y , z ])) [ 1 2 3 3 2 1 99 99 99] np.concatenate can also be used for two-dimensional arrays: In [ 45 ]: grid = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) In [ 46 ]: # concatenate along the first axis np . concatenate ([ grid , grid ]) Out[46]: array([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6]]) In [ 47 ]: # concatenate along the second axis (zero-indexed) np . concatenate ([ grid , grid ], axis = 1 ) 48 | Chapter 2: Introduction to NumPy Out[47]: array([[1, 2, 3, 1, 2, 3], [4, 5, 6, 4, 5, 6]]) For working with arrays of mixed dimensions, it can be clearer to use the np.vstack (vertical stack) and np.hstack (horizontal stack) functions: In [ 48 ]: x = np . array ([ 1 , 2 , 3 ]) grid = np . array ([[ 9 , 8 , 7 ], [ 6 , 5 , 4 ]]) # vertically stack the arrays np . vstack ([ x , grid ]) Out[48]: array([[1, 2, 3], [9, 8, 7], [6, 5, 4]]) In [ 49 ]: # horizontally stack the arrays y = np . array ([[ 99 ], [ 99 ]]) np . hstack ([ grid , y ]) Out[49]: array([[ 9, 8, 7, 99], [ 6, 5, 4, 99]]) Similarly, np.dstack will stack arrays along the third axis. Splitting of arrays The opposite of concatenation is splitting, which is implemented by the functions np.split , np.hsplit , and np.vsplit . For each of these, we can pass a list of indices giving the split points: In [ 50 ]: x = [ 1 , 2 , 3 , 99 , 99 , 3 , 2 , 1 ] x1 , x2 , x3 = np . split ( x , [ 3 , 5 ]) print ( x1 , x2 , x3 ) [1 2 3] [99 99] [3 2 1] Notice that N split points lead to N + 1 subarrays. The related functions np.hsplit and np.vsplit are similar: In [ 51 ]: grid = np . arange ( 16 ) . reshape (( 4 , 4 )) grid Out[51]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) In [ 52 ]: upper , lower = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_62"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". arange ( 16 ) . reshape (( 4 , 4 )) grid Out[51]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) In [ 52 ]: upper , lower = np . vsplit ( grid , [ 2 ]) print ( upper ) print ( lower ) [[0 1 2 3] [4 5 6 7]] The Basics of NumPy Arrays | 49 [[ 8 9 10 11] [12 13 14 15]] In [ 53 ]: left , right = np . hsplit ( grid , [ 2 ]) print ( left ) print ( right ) [[ 0 1] [ 4 5] [ 8 9] [12 13]] [[ 2 3] [ 6 7] [10 11] [14 15]] Similarly, np.dsplit will split arrays along the third axis. Computation on NumPy Arrays: Universal Functions Up until now, we have been discussing some of the basic nuts and bolts of NumPy; in the next few sections, we will dive into the reasons that NumPy is so important in the Python data science world. Namely, it provides an easy and flexible interface to opti‐ mized computation with arrays of data. Computation on NumPy arrays can be very fast, or it can be very slow. The key to making it fast is to use vectorized operations, generally implemented through Num‐ Py’s universal functions (ufuncs). This section motivates the need for NumPy’s ufuncs, which can be used to make repeated calculations on array elements much more effi‐ cient. It then introduces many of the most common and useful arithmetic ufuncs available in the NumPy package. The Slowness of Loops Python’s default implementation (known as CPython) does some operations very slowly. This is in part due to the dynamic, interpreted nature of the language: the fact that types are flexible, so that sequences of operations cannot be compiled down to efficient machine code as in languages like C and Fortran. Recently there have been various attempts to address this weakness: well-known examples are the PyPy project , a just-in-time compiled implementation of Python; the Cython project , which con‐ verts Python code to compilable C code; and the Numba project , which converts snippets of Python code to fast LLVM bytecode",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_63"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Each of these has its strengths and weaknesses, but it is safe to say that none of the three approaches has yet surpassed the reach and popularity of the standard CPython engine. The relative sluggishness of Python generally manifests itself in situations where many small operations are being repeated—for instance, looping over arrays to oper‐ 50 | Chapter 2: Introduction to NumPy ate on each element. For example, imagine we have an array of values and we’d like to compute the reciprocal of each. A straightforward approach might look like this: In [ 1 ]: import numpy as np np . random . seed ( 0 ) def compute_reciprocals ( values ): output = np . empty ( len ( values )) for i in range ( len ( values )): output [ i ] = 1.0 / values [ i ] return output values = np . random . randint ( 1 , 10 , size = 5 ) compute_reciprocals ( values ) Out[1]: array([ 0.16666667, 1. , 0.25 , 0.25 , 0.125 ]) This implementation probably feels fairly natural to someone from, say, a C or Java background. But if we measure the execution time of this code for a large input, we see that this operation is very slow, perhaps surprisingly so! We’ll benchmark this with IPython’s %timeit magic (discussed in “Profiling and Timing Code” on page 25 ): In [ 2 ]: big_array = np . random . randint ( 1 , 100 , size = 1000000 ) % timeit compute_reciprocals ( big_array ) 1 loop, best of 3: 2.91 s per loop It takes several seconds to compute these million operations and to store the result! When even cell phones have processing speeds measured in Giga-FLOPS (i.e., bil‐ lions of numerical operations per second), this seems almost absurdly slow. It turns out that the bottleneck here is not the operations themselves, but the type-checking and function dispatches that CPython must do at each cycle of the loop. Each time the reciprocal is computed, Python first examines the object’s type and does a dynamic lookup of the correct function to use for that type",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_64"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Each time the reciprocal is computed, Python first examines the object’s type and does a dynamic lookup of the correct function to use for that type. If we were working in compiled code instead, this type specification would be known before the code exe‐ cutes and the result could be computed much more efficiently. Introducing UFuncs For many types of operations, NumPy provides a convenient interface into just this kind of statically typed, compiled routine. This is known as a vectorized operation. You can accomplish this by simply performing an operation on the array, which will then be applied to each element. This vectorized approach is designed to push the loop into the compiled layer that underlies NumPy, leading to much faster execution. Compare the results of the following two: In [ 3 ]: print ( compute_reciprocals ( values )) print ( 1.0 / values ) Computation on NumPy Arrays: Universal Functions | 51 [ 0.16666667 1. 0.25 0.25 0.125 ] [ 0.16666667 1. 0.25 0.25 0.125 ] Looking at the execution time for our big array, we see that it completes orders of magnitude faster than the Python loop: In [ 4 ]: % timeit ( 1.0 / big_array ) 100 loops, best of 3: 4.6 ms per loop Vectorized operations in NumPy are implemented via ufuncs , whose main purpose is to quickly execute repeated operations on values in NumPy arrays. Ufuncs are extremely flexible—before we saw an operation between a scalar and an array, but we can also operate between two arrays: In [ 5 ]: np . arange ( 5 ) / np . arange ( 1 , 6 ) Out[5]: array([ 0. , 0.5 , 0.66666667, 0.75 , 0.8 ]) And ufunc operations are not limited to one-dimensional arrays—they can act on multidimensional arrays as well: In [ 6 ]: x = np . arange ( 9 ) . reshape (( 3 , 3 )) 2 ** x Out[6]: array([[ 1, 2, 4], [ 8, 16, 32], [ 64, 128, 256]]) Computations using vectorization through ufuncs are nearly always more efficient than their counterpart implemented through Python loops, especially as the arrays grow in size",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_65"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Any time you see such a loop in a Python script, you should consider whether it can be replaced with a vectorized expression. Exploring NumPy’s UFuncs Ufuncs exist in two flavors: unary ufuncs , which operate on a single input, and binary ufuncs , which operate on two inputs. We’ll see examples of both these types of func‐ tions here. Array arithmetic NumPy’s ufuncs feel very natural to use because they make use of Python’s native arithmetic operators. The standard addition, subtraction, multiplication, and division can all be used: In [ 7 ]: x = np . arange ( 4 ) print ( \"x =\" , x ) print ( \"x + 5 =\" , x + 5 ) print ( \"x - 5 =\" , x - 5 ) print ( \"x * 2 =\" , x * 2 ) 52 | Chapter 2: Introduction to NumPy print ( \"x / 2 =\" , x / 2 ) print ( \"x // 2 =\" , x // 2 ) # floor division x = [0 1 2 3] x + 5 = [5 6 7 8] x - 5 = [-5 -4 -3 -2] x * 2 = [0 2 4 6] x / 2 = [ 0. 0.5 1. 1.5] x // 2 = [0 0 1 1] There is also a unary ufunc for negation, a ** operator for exponentiation, and a % operator for modulus: In [ 8 ]: print ( \"-x = \" , - x ) print ( \"x ** 2 = \" , x ** 2 ) print ( \"x % 2 = \" , x % 2 ) -x = [ 0 -1 -2 -3] x ** 2 = [0 1 4 9] x % 2 = [0 1 0 1] In addition, these can be strung together however you wish, and the standard order of operations is respected: In [ 9 ]: - ( 0.5 * x + 1 ) ** 2 Out[9]: array([-1. , -2.25, -4. , -6.25]) All of these arithmetic operations are simply convenient wrappers around specific functions built into NumPy; for example, the + operator is a wrapper for the add function: In [ 10 ]: np . add ( x , 2 ) Out[10]: array([2, 3, 4, 5]) Table 2-2 lists the arithmetic operators implemented in NumPy. Table 2-2",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_66"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". add ( x , 2 ) Out[10]: array([2, 3, 4, 5]) Table 2-2 lists the arithmetic operators implemented in NumPy. Table 2-2. Arithmetic operators implemented in NumPy Operator Equivalent ufunc Description + np.add Addition (e.g., 1 + 1 = 2 ) - np.subtract Subtraction (e.g., 3 - 2 = 1 ) - np.negative Unary negation (e.g., -2 ) * np.multiply Multiplication (e.g., 2 * 3 = 6 ) / np.divide Division (e.g., 3 / 2 = 1.5 ) // np.floor_divide Floor division (e.g., 3 // 2 = 1 ) ** np.power Exponentiation (e.g., 2 ** 3 = 8 ) % np.mod Modulus/remainder (e.g., 9 % 4 = 1 ) Computation on NumPy Arrays: Universal Functions | 53 Additionally there are Boolean/bitwise operators; we will explore these in “Compari‐ sons, Masks, and Boolean Logic” on page 70 . Absolute value Just as NumPy understands Python’s built-in arithmetic operators, it also understands Python’s built-in absolute value function: In [ 11 ]: x = np . array ([ - 2 , - 1 , 0 , 1 , 2 ]) abs ( x ) Out[11]: array([2, 1, 0, 1, 2]) The corresponding NumPy ufunc is np.absolute , which is also available under the alias np.abs : In [ 12 ]: np . absolute ( x ) Out[12]: array([2, 1, 0, 1, 2]) In [ 13 ]: np . abs ( x ) Out[13]: array([2, 1, 0, 1, 2]) This ufunc can also handle complex data, in which the absolute value returns the magnitude: In [ 14 ]: x = np . array ([ 3 - 4j , 4 - 3j , 2 + 0j , 0 + 1j ]) np . abs ( x ) Out[14]: array([ 5., 5., 2., 1.]) Trigonometric functions NumPy provides a large number of useful ufuncs, and some of the most useful for the data scientist are the trigonometric functions. We’ll start by defining an array of angles: In [ 15 ]: theta = np . linspace ( 0 , np . pi , 3 ) Now we can compute some trigonometric functions on these values: In [ 16 ]: print ( \"theta = \" , theta ) print ( \"sin(theta) = \" , np . sin ( theta )) print ( \"cos(theta) = \" , np . cos ( theta )) print ( \"tan(theta) = \" , np . tan ( theta )) theta = [ 0",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_67"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sin ( theta )) print ( \"cos(theta) = \" , np . cos ( theta )) print ( \"tan(theta) = \" , np . tan ( theta )) theta = [ 0. 1.57079633 3.14159265] sin(theta) = [ 0.00000000e+00 1.00000000e+00 1.22464680e-16] cos(theta) = [ 1.00000000e+00 6.12323400e-17 -1.00000000e+00] tan(theta) = [ 0.00000000e+00 1.63312394e+16 -1.22464680e-16] The values are computed to within machine precision, which is why values that should be zero do not always hit exactly zero. Inverse trigonometric functions are also available: 54 | Chapter 2: Introduction to NumPy In [ 17 ]: x = [ - 1 , 0 , 1 ] print ( \"x = \" , x ) print ( \"arcsin(x) = \" , np . arcsin ( x )) print ( \"arccos(x) = \" , np . arccos ( x )) print ( \"arctan(x) = \" , np . arctan ( x )) x = [-1, 0, 1] arcsin(x) = [-1.57079633 0. 1.57079633] arccos(x) = [ 3.14159265 1.57079633 0. ] arctan(x) = [-0.78539816 0. 0.78539816] Exponents and logarithms Another common type of operation available in a NumPy ufunc are the exponentials: In [ 18 ]: x = [ 1 , 2 , 3 ] print ( \"x =\" , x ) print ( \"e^x =\" , np . exp ( x )) print ( \"2^x =\" , np . exp2 ( x )) print ( \"3^x =\" , np . power ( 3 , x )) x = [1, 2, 3] e^x = [ 2.71828183 7.3890561 20.08553692] 2^x = [ 2. 4. 8.] 3^x = [ 3 9 27] The inverse of the exponentials, the logarithms, are also available. The basic np.log gives the natural logarithm; if you prefer to compute the base-2 logarithm or the base-10 logarithm, these are available as well: In [ 19 ]: x = [ 1 , 2 , 4 , 10 ] print ( \"x =\" , x ) print ( \"ln(x) =\" , np . log ( x )) print ( \"log2(x) =\" , np . log2 ( x )) print ( \"log10(x) =\" , np . log10 ( x )) x = [1, 2, 4, 10] ln(x) = [ 0. 0.69314718 1.38629436 2.30258509] log2(x) = [ 0. 1. 2. 3.32192809] log10(x) = [ 0. 0.30103 0.60205999 1. ] There are also some specialized versions that are useful for maintaining precision with very small input: In [ 20 ]: x = [ 0 , 0.001 , 0.01 , 0.1 ] print ( \"exp(x) - 1 =\" , np . expm1 ( x )) print ( \"log(1 + x) =\" , np . log1p ( x )) exp(x) - 1 = [ 0. 0.0010005 0.01005017 0.10517092] log(1 + x) = [ 0",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_68"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". expm1 ( x )) print ( \"log(1 + x) =\" , np . log1p ( x )) exp(x) - 1 = [ 0. 0.0010005 0.01005017 0.10517092] log(1 + x) = [ 0. 0.0009995 0.00995033 0.09531018] When x is very small, these functions give more precise values than if the raw np.log or np.exp were used. Computation on NumPy Arrays: Universal Functions | 55 Specialized ufuncs NumPy has many more ufuncs available, including hyperbolic trig functions, bitwise arithmetic, comparison operators, conversions from radians to degrees, rounding and remainders, and much more. A look through the NumPy documentation reveals a lot of interesting functionality. Another excellent source for more specialized and obscure ufuncs is the submodule scipy.special . If you want to compute some obscure mathematical function on your data, chances are it is implemented in scipy.special . There are far too many functions to list them all, but the following snippet shows a couple that might come up in a statistics context: In [ 21 ]: from scipy import special In [ 22 ]: # Gamma functions (generalized factorials) and related functions x = [ 1 , 5 , 10 ] print ( \"gamma(x) =\" , special . gamma ( x )) print ( \"ln|gamma(x)| =\" , special . gammaln ( x )) print ( \"beta(x, 2) =\" , special . beta ( x , 2 )) gamma(x) = [ 1.00000000e+00 2.40000000e+01 3.62880000e+05] ln|gamma(x)| = [ 0. 3.17805383 12.80182748] beta(x, 2) = [ 0.5 0.03333333 0.00909091] In [ 23 ]: # Error function (integral of Gaussian) # its complement, and its inverse x = np . array ([ 0 , 0.3 , 0.7 , 1.0 ]) print ( \"erf(x) =\" , special . erf ( x )) print ( \"erfc(x) =\" , special . erfc ( x )) print ( \"erfinv(x) =\" , special . erfinv ( x )) erf(x) = [ 0. 0.32862676 0.67780119 0.84270079] erfc(x) = [ 1. 0.67137324 0.32219881 0.15729921] erfinv(x) = [ 0. 0.27246271 0.73286908 inf] There are many, many more ufuncs available in both NumPy and scipy.special . Because the documentation of these packages is available online, a web search along the lines of “gamma function python” will generally find the relevant information",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_69"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Because the documentation of these packages is available online, a web search along the lines of “gamma function python” will generally find the relevant information. Advanced Ufunc Features Many NumPy users make use of ufuncs without ever learning their full set of features. We’ll outline a few specialized features of ufuncs here. Specifying output For large calculations, it is sometimes useful to be able to specify the array where the result of the calculation will be stored. Rather than creating a temporary array, you can use this to write computation results directly to the memory location where you’d 56 | Chapter 2: Introduction to NumPy like them to be. For all ufuncs, you can do this using the out argument of the function: In [ 24 ]: x = np . arange ( 5 ) y = np . empty ( 5 ) np . multiply ( x , 10 , out = y ) print ( y ) [ 0. 10. 20. 30. 40.] This can even be used with array views. For example, we can write the results of a computation to every other element of a specified array: In [ 25 ]: y = np . zeros ( 10 ) np . power ( 2 , x , out = y [:: 2 ]) print ( y ) [ 1. 0. 2. 0. 4. 0. 8. 0. 16. 0.] If we had instead written y[::2] = 2 ** x , this would have resulted in the creation of a temporary array to hold the results of 2 ** x , followed by a second operation copying those values into the y array. This doesn’t make much of a difference for such a small computation, but for very large arrays the memory savings from careful use of the out argument can be significant. Aggregates For binary ufuncs, there are some interesting aggregates that can be computed directly from the object. For example, if we’d like to reduce an array with a particular operation, we can use the reduce method of any ufunc. A reduce repeatedly applies a given operation to the elements of an array until only a single result remains. For example, calling reduce on the add ufunc returns the sum of all elements in the array: In [ 26 ]: x = np . arange ( 1 , 6 ) np . add",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_70"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, calling reduce on the add ufunc returns the sum of all elements in the array: In [ 26 ]: x = np . arange ( 1 , 6 ) np . add . reduce ( x ) Out[26]: 15 Similarly, calling reduce on the multiply ufunc results in the product of all array elements: In [ 27 ]: np . multiply . reduce ( x ) Out[27]: 120 If we’d like to store all the intermediate results of the computation, we can instead use accumulate : In [ 28 ]: np . add . accumulate ( x ) Out[28]: array([ 1, 3, 6, 10, 15]) Computation on NumPy Arrays: Universal Functions | 57 In [ 29 ]: np . multiply . accumulate ( x ) Out[29]: array([ 1, 2, 6, 24, 120]) Note that for these particular cases, there are dedicated NumPy functions to compute the results ( np.sum , np.prod , np.cumsum , np.cumprod ), which we’ll explore in “Aggre‐ gations: Min, Max, and Everything in Between” on page 58 . Outer products Finally, any ufunc can compute the output of all pairs of two different inputs using the outer method. This allows you, in one line, to do things like create a multiplica‐ tion table: In [ 30 ]: x = np . arange ( 1 , 6 ) np . multiply . outer ( x , x ) Out[30]: array([[ 1, 2, 3, 4, 5], [ 2, 4, 6, 8, 10], [ 3, 6, 9, 12, 15], [ 4, 8, 12, 16, 20], [ 5, 10, 15, 20, 25]]) The ufunc.at and ufunc.reduceat methods, which we’ll explore in “Fancy Index‐ ing” on page 78 , are very helpful as well. Another extremely useful feature of ufuncs is the ability to operate between arrays of different sizes and shapes, a set of operations known as broadcasting . This subject is important enough that we will devote a whole section to it (see “Computation on Arrays: Broadcasting” on page 63 ). Ufuncs: Learning More More information on universal functions (including the full list of available func‐ tions) can be found on the NumPy and SciPy documentation websites",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_71"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Ufuncs: Learning More More information on universal functions (including the full list of available func‐ tions) can be found on the NumPy and SciPy documentation websites. Recall that you can also access information directly from within IPython by import‐ ing the packages and using IPython’s tab-completion and help ( ? ) functionality, as described in “Help and Documentation in IPython” on page 3 . Aggregations: Min, Max, and Everything in Between Often when you are faced with a large amount of data, a first step is to compute sum‐ mary statistics for the data in question. Perhaps the most common summary statistics are the mean and standard deviation, which allow you to summarize the “typical” val‐ ues in a dataset, but other aggregates are useful as well (the sum, product, median, minimum and maximum, quantiles, etc.). 58 | Chapter 2: Introduction to NumPy NumPy has fast built-in aggregation functions for working on arrays; we’ll discuss and demonstrate some of them here. Summing the Values in an Array As a quick example, consider computing the sum of all values in an array. Python itself can do this using the built-in sum function: In [ 1 ]: import numpy as np In [ 2 ]: L = np . random . random ( 100 ) sum ( L ) Out[2]: 55.61209116604941 The syntax is quite similar to that of NumPy’s sum function, and the result is the same in the simplest case: In [ 3 ]: np . sum ( L ) Out[3]: 55.612091166049424 However, because it executes the operation in compiled code, NumPy’s version of the operation is computed much more quickly: In [ 4 ]: big_array = np . random . rand ( 1000000 ) % timeit sum ( big_array ) % timeit np . sum ( big_array ) 10 loops, best of 3: 104 ms per loop 1000 loops, best of 3: 442 μs per loop Be careful, though: the sum function and the np.sum function are not identical, which can sometimes lead to confusion! In particular, their optional arguments have differ‐ ent meanings, and np.sum is aware of multiple array dimensions, as we will see in the following section",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_72"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Minimum and Maximum Similarly, Python has built-in min and max functions, used to find the minimum value and maximum value of any given array: In [ 5 ]: min ( big_array ), max ( big_array ) Out[5]: (1.1717128136634614e-06, 0.9999976784968716) NumPy’s corresponding functions have similar syntax, and again operate much more quickly: In [ 6 ]: np . min ( big_array ), np . max ( big_array ) Out[6]: (1.1717128136634614e-06, 0.9999976784968716) Aggregations: Min, Max, and Everything in Between | 59 In [ 7 ]: % timeit min ( big_array ) % timeit np . min ( big_array ) 10 loops, best of 3: 82.3 ms per loop 1000 loops, best of 3: 497 μs per loop For min , max , sum , and several other NumPy aggregates, a shorter syntax is to use methods of the array object itself: In [ 8 ]: print ( big_array . min (), big_array . max (), big_array . sum ()) 1.17171281366e-06 0.999997678497 499911.628197 Whenever possible, make sure that you are using the NumPy version of these aggre‐ gates when operating on NumPy arrays! Multidimensional aggregates One common type of aggregation operation is an aggregate along a row or column. Say you have some data stored in a two-dimensional array: In [ 9 ]: M = np . random . random (( 3 , 4 )) print ( M ) [[ 0.8967576 0.03783739 0.75952519 0.06682827] [ 0.8354065 0.99196818 0.19544769 0.43447084] [ 0.66859307 0.15038721 0.37911423 0.6687194 ]] By default, each NumPy aggregation function will return the aggregate over the entire array: In [ 10 ]: M . sum () Out[10]: 6.0850555667307118 Aggregation functions take an additional argument specifying the axis along which the aggregate is computed. For example, we can find the minimum value within each column by specifying axis=0 : In [ 11 ]: M . min ( axis = 0 ) Out[11]: array([ 0.66859307, 0.03783739, 0.19544769, 0.06682827]) The function returns four values, corresponding to the four columns of numbers. Similarly, we can find the maximum value within each row: In [ 12 ]: M",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_73"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Similarly, we can find the maximum value within each row: In [ 12 ]: M . max ( axis = 1 ) Out[12]: array([ 0.8967576 , 0.99196818, 0.6687194 ]) The way the axis is specified here can be confusing to users coming from other lan‐ guages. The axis keyword specifies the dimension of the array that will be collapsed , rather than the dimension that will be returned. So specifying axis=0 means that the 60 | Chapter 2: Introduction to NumPy first axis will be collapsed: for two-dimensional arrays, this means that values within each column will be aggregated. Other aggregation functions NumPy provides many other aggregation functions, but we won’t discuss them in detail here. Additionally, most aggregates have a NaN -safe counterpart that computes the result while ignoring missing values, which are marked by the special IEEE floating-point NaN value (for a fuller discussion of missing data, see “Handling Miss‐ ing Data” on page 119 ). Some of these NaN -safe functions were not added until NumPy 1.8, so they will not be available in older NumPy versions. Table 2-3 provides a list of useful aggregation functions available in NumPy. Table 2-3. Aggregation functions available in NumPy Function Name NaN-safe Version Description np.sum np.nansum Compute sum of elements np.prod np.nanprod Compute product of elements np.mean np.nanmean Compute median of elements np.std np.nanstd Compute standard deviation np.var np.nanvar Compute variance np.min np.nanmin Find minimum value np.max np.nanmax Find maximum value np.argmin np.nanargmin Find index of minimum value np.argmax np.nanargmax Find index of maximum value np.median np.nanmedian Compute median of elements np.percentile np.nanpercentile Compute rank-based statistics of elements np.any N/A Evaluate whether any elements are true np.all N/A Evaluate whether all elements are true We will see these aggregates often throughout the rest of the book",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_74"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Example: What Is the Average Height of US Presidents? Aggregates available in NumPy can be extremely useful for summarizing a set of val‐ ues. As a simple example, let’s consider the heights of all US presidents. This data is available in the file president_heights.csv , which is a simple comma-separated list of labels and values: In [ 13 ]: ! head - 4 data / president_heights . csv order,name,height(cm) 1,George Washington,189 Aggregations: Min, Max, and Everything in Between | 61 2,John Adams,170 3,Thomas Jefferson,189 We’ll use the Pandas package, which we’ll explore more fully in Chapter 3 , to read the file and extract this information (note that the heights are measured in centimeters): In [ 14 ]: import pandas as pd data = pd . read_csv ( 'data/president_heights.csv' ) heights = np . array ( data [ 'height(cm)' ]) print ( heights ) [189 170 189 163 183 171 185 168 173 183 173 173 175 178 183 193 178 173 174 183 183 168 170 178 182 180 183 178 182 188 175 179 183 193 182 183 177 185 188 188 182 185] Now that we have this data array, we can compute a variety of summary statistics: In [ 15 ]: print ( \"Mean height: \" , heights . mean ()) print ( \"Standard deviation:\" , heights . std ()) print ( \"Minimum height: \" , heights . min ()) print ( \"Maximum height: \" , heights . max ()) Mean height: 179.738095238 Standard deviation: 6.93184344275 Minimum height: 163 Maximum height: 193 Note that in each case, the aggregation operation reduced the entire array to a single summarizing value, which gives us information about the distribution of values. We may also wish to compute quantiles: In [ 16 ]: print ( \"25th percentile: \" , np . percentile ( heights , 25 )) print ( \"Median: \" , np . median ( heights )) print ( \"75th percentile: \" , np . percentile ( heights , 75 )) 25th percentile: 174.25 Median: 182.0 75th percentile: 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_75"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". percentile ( heights , 75 )) 25th percentile: 174.25 Median: 182.0 75th percentile: 183.0 We see that the median height of US presidents is 182 cm, or just shy of six feet. Of course, sometimes it’s more useful to see a visual representation of this data, which we can accomplish using tools in Matplotlib (we’ll discuss Matplotlib more fully in Chapter 4 ). For example, this code generates the chart shown in Figure 2-3 : In [ 17 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () # set plot style In [ 18 ]: plt . hist ( heights ) plt . title ( 'Height Distribution of US Presidents' ) plt . xlabel ( 'height (cm)' ) plt . ylabel ( 'number' ); 62 | Chapter 2: Introduction to NumPy Figure 2-3. Histogram of presidential heights These aggregates are some of the fundamental pieces of exploratory data analysis that we’ll explore in more depth in later chapters of the book. Computation on Arrays: Broadcasting We saw in the previous section how NumPy’s universal functions can be used to vec‐ torize operations and thereby remove slow Python loops. Another means of vectoriz‐ ing operations is to use NumPy’s broadcasting functionality. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes. Introducing Broadcasting Recall that for arrays of the same size, binary operations are performed on an element-by-element basis: In [ 1 ]: import numpy as np In [ 2 ]: a = np . array ([ 0 , 1 , 2 ]) b = np . array ([ 5 , 5 , 5 ]) a + b Out[2]: array([5, 6, 7]) Broadcasting allows these types of binary operations to be performed on arrays of dif‐ ferent sizes—for example, we can just as easily add a scalar (think of it as a zerdimensional array) to an array: Computation on Arrays: Broadcasting | 63 In [ 3 ]: a + 5 Out[3]: array([5, 6, 7]) We can think of this as an operation that stretches or duplicates the value 5 into the array [5, 5, 5] , and adds the results",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_76"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The advantage of NumPy’s broadcasting is that this duplication of values does not actually take place, but it is a useful mental model as we think about broadcasting. We can similarly extend this to arrays of higher dimension. Observe the result when we add a one-dimensional array to a two-dimensional array: In [ 4 ]: M = np . ones (( 3 , 3 )) M Out[4]: array([[ 1., 1., 1.], [ 1., 1., 1.], [ 1., 1., 1.]]) In [ 5 ]: M + a Out[5]: array([[ 1., 2., 3.], [ 1., 2., 3.], [ 1., 2., 3.]]) Here the one-dimensional array a is stretched, or broadcast, across the second dimension in order to match the shape of M . While these examples are relatively easy to understand, more complicated cases can involve broadcasting of both arrays. Consider the following example: In [ 6 ]: a = np . arange ( 3 ) b = np . arange ( 3 )[:, np . newaxis ] print ( a ) print ( b ) [0 1 2] [[0] [1] [2]] In [ 7 ]: a + b Out[7]: array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) 64 | Chapter 2: Introduction to NumPy 1 Code to produce this plot can be found in the online appendix, and is adapted from source published in the astroML documentation . Used with permission. Just as before we stretched or broadcasted one value to match the shape of the other, here we’ve stretched both a and b to match a common shape, and the result is a twdimensional array! The geometry of these examples is visualized in Figure 2-4 . 1 Figure 2-4. Visualization of NumPy broadcasting The light boxes represent the broadcasted values: again, this extra memory is not actually allocated in the course of the operation, but it can be useful conceptually to imagine that it is. Rules of Broadcasting Broadcasting in NumPy follows a strict set of rules to determine the interaction between the two arrays: • Rule 1: If the two arrays differ in their number of dimensions, the shape of the one with fewer dimensions is padded with ones on its leading (left) side",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_77"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Rule 2: If the shape of the two arrays does not match in any dimension, the array with shape equal to 1 in that dimension is stretched to match the other shape. • Rule 3: If in any dimension the sizes disagree and neither is equal to 1, an error is raised. Computation on Arrays: Broadcasting | 65 To make these rules clear, let’s consider a few examples in detail. Broadcasting example 1 Let’s look at adding a two-dimensional array to a one-dimensional array: In [ 8 ]: M = np . ones (( 2 , 3 )) a = np . arange ( 3 ) Let’s consider an operation on these two arrays. The shapes of the arrays are: M.shape = (2, 3) a.shape = (3,) We see by rule 1 that the array a has fewer dimensions, so we pad it on the left with ones: M.shape -> (2, 3) a.shape -> (1, 3) By rule 2, we now see that the first dimension disagrees, so we stretch this dimension to match: M.shape -> (2, 3) a.shape -> (2, 3) The shapes match, and we see that the final shape will be (2, 3) : In [ 9 ]: M + a Out[9]: array([[ 1., 2., 3.], [ 1., 2., 3.]]) Broadcasting example 2 Let’s take a look at an example where both arrays need to be broadcast: In [ 10 ]: a = np . arange ( 3 ) . reshape (( 3 , 1 )) b = np . arange ( 3 ) Again, we’ll start by writing out the shape of the arrays: a.shape = (3, 1) b.shape = (3,) 66 | Chapter 2: Introduction to NumPy Rule 1 says we must pad the shape of b with ones: a.shape -> (3, 1) b.shape -> (1, 3) And rule 2 tells us that we upgrade each of these ones to match the corresponding size of the other array: a.shape -> (3, 3) b.shape -> (3, 3) Because the result matches, these shapes are compatible. We can see this here: In [ 11 ]: a + b Out[11]: array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]) Broadcasting example 3 Now let’s take a look at an example in which the two arrays are not compatible: In [ 12 ]: M = np . ones (( 3 , 2 )) a = np . arange ( 3 ) This is just a slightly different situation than in the first example: the matrix M is transposed",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_78"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". ones (( 3 , 2 )) a = np . arange ( 3 ) This is just a slightly different situation than in the first example: the matrix M is transposed. How does this affect the calculation? The shapes of the arrays are: M.shape = (3, 2) a.shape = (3,) Again, rule 1 tells us that we must pad the shape of a with ones: M.shape -> (3, 2) a.shape -> (1, 3) By rule 2, the first dimension of a is stretched to match that of M : M.shape -> (3, 2) a.shape -> (3, 3) Now we hit rule 3—the final shapes do not match, so these two arrays are incompati‐ ble, as we can observe by attempting this operation: Computation on Arrays: Broadcasting | 67 In [ 13 ]: M + a --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-13-9e16e9f98da6> in <module>() ----> 1 M + a ValueError: operands could not be broadcast together with shapes (3,2) (3,) Note the potential confusion here: you could imagine making a and M compatible by, say, padding a ’s shape with ones on the right rather than the left. But this is not how the broadcasting rules work! That sort of flexibility might be useful in some cases, but it would lead to potential areas of ambiguity. If right-side padding is what you’d like, you can do this explicitly by reshaping the array (we’ll use the np.newaxis keyword introduced in “The Basics of NumPy Arrays” on page 42 ): In [ 14 ]: a [:, np . newaxis ] . shape Out[14]: (3, 1) In [ 15 ]: M + a [:, np . newaxis ] Out[15]: array([[ 1., 1.], [ 2., 2.], [ 3., 3.]]) Also note that while we’ve been focusing on the + operator here, these broadcasting rules apply to any binary ufunc . For example, here is the logaddexp(a, b) function, which computes log(exp(a) + exp(b)) with more precision than the naive approach: In [ 16 ]: np . logaddexp ( M , a [:, np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_79"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, here is the logaddexp(a, b) function, which computes log(exp(a) + exp(b)) with more precision than the naive approach: In [ 16 ]: np . logaddexp ( M , a [:, np . newaxis ]) Out[16]: array([[ 1.31326169, 1.31326169], [ 1.69314718, 1.69314718], [ 2.31326169, 2.31326169]]) For more information on the many available universal functions, refer to “Computa‐ tion on NumPy Arrays: Universal Functions” on page 50 . Broadcasting in Practice Broadcasting operations form the core of many examples we’ll see throughout this book. We’ll now take a look at a couple simple examples of where they can be useful. Centering an array In the previous section, we saw that ufuncs allow a NumPy user to remove the need to explicitly write slow Python loops. Broadcasting extends this ability. One com‐ 68 | Chapter 2: Introduction to NumPy monly seen example is centering an array of data. Imagine you have an array of 10 observations, each of which consists of 3 values. Using the standard convention (see “Data Representation in Scikit-Learn” on page 343 ), we’ll store this in a 10×3 array: In [ 17 ]: X = np . random . random (( 10 , 3 )) We can compute the mean of each feature using the mean aggregate across the first dimension: In [ 18 ]: Xmean = X . mean ( 0 ) Xmean Out[18]: array([ 0.53514715, 0.66567217, 0.44385899]) And now we can center the X array by subtracting the mean (this is a broadcasting operation): In [ 19 ]: X_centered = X - Xmean To double-check that we’ve done this correctly, we can check that the centered array has near zero mean: In [ 20 ]: X_centered . mean ( 0 ) Out[20]: array([ 2.22044605e-17, -7.77156117e-17, -1.66533454e-17]) To within-machine precision, the mean is now zero. Plotting a two-dimensional function One place that broadcasting is very useful is in displaying images based on twdimensional functions. If we want to define a function z = f ( x , y ), broadcasting can be used to compute the function across the grid: In [ 21 ]: # x and y have 50 steps from 0 to 5 x = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_80"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If we want to define a function z = f ( x , y ), broadcasting can be used to compute the function across the grid: In [ 21 ]: # x and y have 50 steps from 0 to 5 x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 50 )[:, np . newaxis ] z = np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) We’ll use Matplotlib to plot this two-dimensional array (these tools will be discussed in full in “Density and Contour Plots” on page 241 ): In [ 22 ]: % matplotlib inline import matplotlib.pyplot as plt In [ 23 ]: plt . imshow ( z , origin = 'lower' , extent = [ 0 , 5 , 0 , 5 ], cmap = 'viridis' ) plt . colorbar (); The result, shown in Figure 2-5 , is a compelling visualization of the two-dimensional function. Computation on Arrays: Broadcasting | 69 Figure 2-5. Visualization of a 2D array Comparisons, Masks, and Boolean Logic This section covers the use of Boolean masks to examine and manipulate values within NumPy arrays. Masking comes up when you want to extract, modify, count, or otherwise manipulate values in an array based on some criterion: for example, you might wish to count all values greater than a certain value, or perhaps remove all out‐ liers that are above some threshold. In NumPy, Boolean masking is often the most efficient way to accomplish these types of tasks. Example: Counting Rainy Days Imagine you have a series of data that represents the amount of precipitation each day for a year in a given city. For example, here we’ll load the daily rainfall statistics for the city of Seattle in 2014, using Pandas (which is covered in more detail in Chap‐ ter 3 ): In [ 1 ]: import numpy as np import pandas as pd # use Pandas to extract rainfall inches as a NumPy array rainfall = pd . read_csv ( 'data/Seattle2014.csv' )[ 'PRCP' ] . values inches = rainfall / 254 # 1/10mm -> inches inches . shape Out[1]: (365,) The array contains 365 values, giving daily rainfall in inches from January 1 to December 31, 2014",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_81"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". values inches = rainfall / 254 # 1/10mm -> inches inches . shape Out[1]: (365,) The array contains 365 values, giving daily rainfall in inches from January 1 to December 31, 2014. As a first quick visualization, let’s look at the histogram of rainy days shown in Figure 2-6 , which was generated using Matplotlib (we will explore this tool more fully in Chapter 4 ): 70 | Chapter 2: Introduction to NumPy In [ 2 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () # set plot styles In [ 3 ]: plt . hist ( inches , 40 ); Figure 2-6. Histogram of 2014 rainfall in Seattle This histogram gives us a general idea of what the data looks like: despite its reputa‐ tion, the vast majority of days in Seattle saw near zero measured rainfall in 2014. But this doesn’t do a good job of conveying some information we’d like to see: for exam‐ ple, how many rainy days were there in the year? What is the average precipitation on those rainy days? How many days were there with more than half an inch of rain? Digging into the data One approach to this would be to answer these questions by hand: loop through the data, incrementing a counter each time we see values in some desired range. For rea‐ sons discussed throughout this chapter, such an approach is very inefficient, both from the standpoint of time writing code and time computing the result. We saw in “Computation on NumPy Arrays: Universal Functions” on page 50 that NumPy’s ufuncs can be used in place of loops to do fast element-wise arithmetic operations on arrays; in the same way, we can use other ufuncs to do element-wise comparisons over arrays, and we can then manipulate the results to answer the questions we have. We’ll leave the data aside for right now, and discuss some general tools in NumPy to use masking to quickly answer these types of questions. Comparison Operators as ufuncs In “Computation on NumPy Arrays: Universal Functions” on page 50 we introduced ufuncs, and focused in particular on arithmetic operators",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_82"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Comparison Operators as ufuncs In “Computation on NumPy Arrays: Universal Functions” on page 50 we introduced ufuncs, and focused in particular on arithmetic operators. We saw that using + , - , * , / , Comparisons, Masks, and Boolean Logic | 71 and others on arrays leads to element-wise operations. NumPy also implements com‐ parison operators such as < (less than) and > (greater than) as element-wise ufuncs. The result of these comparison operators is always an array with a Boolean data type. All six of the standard comparison operations are available: In [ 4 ]: x = np . array ([ 1 , 2 , 3 , 4 , 5 ]) In [ 5 ]: x < 3 # less than Out[5]: array([ True, True, False, False, False], dtype=bool) In [ 6 ]: x > 3 # greater than Out[6]: array([False, False, False, True, True], dtype=bool) In [ 7 ]: x <= 3 # less than or equal Out[7]: array([ True, True, True, False, False], dtype=bool) In [ 8 ]: x >= 3 # greater than or equal Out[8]: array([False, False, True, True, True], dtype=bool) In [ 9 ]: x != 3 # not equal Out[9]: array([ True, True, False, True, True], dtype=bool) In [ 10 ]: x == 3 # equal Out[10]: array([False, False, True, False, False], dtype=bool) It is also possible to do an element-by-element comparison of two arrays, and to include compound expressions: In [ 11 ]: ( 2 * x ) == ( x ** 2 ) Out[11]: array([False, True, False, False, False], dtype=bool) As in the case of arithmetic operators, the comparison operators are implemented as ufuncs in NumPy; for example, when you write x < 3 , internally NumPy uses np.less(x, 3) . A summary of the comparison operators and their equivalent ufunc is shown here: Operator Equivalent ufunc == np.equal != np.not_equal < np.less <= np.less_equal > np.greater >= np.greater_equal Just as in the case of arithmetic ufuncs, these will work on arrays of any size and shape. Here is a two-dimensional example: 72 | Chapter 2: Introduction to NumPy In [ 12 ]: rng = np . random . RandomState ( 0 ) x = rng",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_83"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here is a two-dimensional example: 72 | Chapter 2: Introduction to NumPy In [ 12 ]: rng = np . random . RandomState ( 0 ) x = rng . randint ( 10 , size = ( 3 , 4 )) x Out[12]: array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) In [ 13 ]: x < 6 Out[13]: array([[ True, True, True, True], [False, False, True, True], [ True, True, False, False]], dtype=bool) In each case, the result is a Boolean array, and NumPy provides a number of straight‐ forward patterns for working with these Boolean results. Working with Boolean Arrays Given a Boolean array, there are a host of useful operations you can do. We’ll work with x , the two-dimensional array we created earlier: In [ 14 ]: print ( x ) [[5 0 3 3] [7 9 3 5] [2 4 7 6]] Counting entries To count the number of True entries in a Boolean array, np.count_nonzero is useful: In [ 15 ]: # how many values less than 6? np . count_nonzero ( x < 6 ) Out[15]: 8 We see that there are eight array entries that are less than 6. Another way to get at this information is to use np.sum ; in this case, False is interpreted as 0 , and True is inter‐ preted as 1 : In [ 16 ]: np . sum ( x < 6 ) Out[16]: 8 The benefit of sum() is that like with other NumPy aggregation functions, this sum‐ mation can be done along rows or columns as well: In [ 17 ]: # how many values less than 6 in each row? np . sum ( x < 6 , axis = 1 ) Out[17]: array([4, 2, 2]) This counts the number of values less than 6 in each row of the matrix. Comparisons, Masks, and Boolean Logic | 73 If we’re interested in quickly checking whether any or all the values are true, we can use (you guessed it) np.any() or np.all() : In [ 18 ]: # are there any values greater than 8? np . any ( x > 8 ) Out[18]: True In [ 19 ]: # are there any values less than zero? np . any ( x < 0 ) Out[19]: False In [ 20 ]: # are all values less than 10? np . all ( x < 10 ) Out[20]: True In [ 21 ]: # are all values equal to 6? np . all ( x == 6 ) Out[21]: False np.all() and np.any() can be used along particular axes as well",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_84"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". all ( x < 10 ) Out[20]: True In [ 21 ]: # are all values equal to 6? np . all ( x == 6 ) Out[21]: False np.all() and np.any() can be used along particular axes as well. For example: In [ 22 ]: # are all values in each row less than 8? np . all ( x < 8 , axis = 1 ) Out[22]: array([ True, False, True], dtype=bool) Here all the elements in the first and third rows are less than 8, while this is not the case for the second row. Finally, a quick warning: as mentioned in “Aggregations: Min, Max, and Everything in Between” on page 58 , Python has built-in sum() , any() , and all() functions. These have a different syntax than the NumPy versions, and in particular will fail or produce unintended results when used on multidimensional arrays. Be sure that you are using np.sum() , np.any() , and np.all() for these examples! Boolean operators We’ve already seen how we might count, say, all days with rain less than four inches, or all days with rain greater than two inches. But what if we want to know about all days with rain less than four inches and greater than one inch? This is accomplished through Python’s bitwise logic operators , & , | , ^ , and ~ . Like with the standard arith‐ metic operators, NumPy overloads these as ufuncs that work element-wise on (usu‐ ally Boolean) arrays. For example, we can address this sort of compound question as follows: In [ 23 ]: np . sum (( inches > 0.5 ) & ( inches < 1 )) Out[23]: 29 So we see that there are 29 days with rainfall between 0.5 and 1.0 inches. 74 | Chapter 2: Introduction to NumPy Note that the parentheses here are important—because of operator precedence rules, with parentheses removed this expression would be evaluated as follows, which results in an error: inches > ( 0.5 & inches ) < 1 Using the equivalence of A AND B and NOT (A OR B) (which you may remember if you’ve taken an introductory logic course), we can compute the same result in a dif‐ ferent manner: In [ 24 ]: np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_85"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sum ( ~ ( ( inches <= 0.5 ) | ( inches >= 1 ) )) Out[24]: 29 Combining comparison operators and Boolean operators on arrays can lead to a wide range of efficient logical operations. The following table summarizes the bitwise Boolean operators and their equivalent ufuncs: Operator Equivalent ufunc & np.bitwise_and | np.bitwise_or ^ np.bitwise_xor ~ np.bitwise_not Using these tools, we might start to answer the types of questions we have about our weather data. Here are some examples of results we can compute when combining masking with aggregations: In [ 25 ]: print ( \"Number days without rain: \" , np . sum ( inches == 0 )) print ( \"Number days with rain: \" , np . sum ( inches != 0 )) print ( \"Days with more than 0.5 inches:\" , np . sum ( inches > 0.5 )) print ( \"Rainy days with < 0.1 inches :\" , np . sum (( inches > 0 ) & ( inches < 0.2 ))) Number days without rain: 215 Number days with rain: 150 Days with more than 0.5 inches: 37 Rainy days with < 0.1 inches : 75 Boolean Arrays as Masks In the preceding section, we looked at aggregates computed directly on Boolean arrays. A more powerful pattern is to use Boolean arrays as masks, to select particular subsets of the data themselves. Returning to our x array from before, suppose we want an array of all values in the array that are less than, say, 5: Comparisons, Masks, and Boolean Logic | 75 In [ 26 ]: x Out[26]: array([[5, 0, 3, 3], [7, 9, 3, 5], [2, 4, 7, 6]]) We can obtain a Boolean array for this condition easily, as we’ve already seen: In [ 27 ]: x < 5 Out[27]: array([[False, True, True, True], [False, False, True, False], [ True, True, False, False]], dtype=bool) Now to select these values from the array, we can simply index on this Boolean array; this is known as a masking operation: In [ 28 ]: x [ x < 5 ] Out[28]: array([0, 3, 3, 3, 2, 4]) What is returned is a one-dimensional array filled with all the values that meet this condition; in other words, all the values in positions at which the mask array is True",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_86"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We are then free to operate on these values as we wish. For example, we can compute some relevant statistics on our Seattle rain data: In [ 29 ]: # construct a mask of all rainy days rainy = ( inches > 0 ) # construct a mask of all summer days (June 21st is the 172nd day) summer = ( np . arange ( 365 ) - 172 < 90 ) & ( np . arange ( 365 ) - 172 > 0 ) print ( \"Median precip on rainy days in 2014 (inches): \" , np . median ( inches [ rainy ])) print ( \"Median precip on summer days in 2014 (inches): \" , np . median ( inches [ summer ])) print ( \"Maximum precip on summer days in 2014 (inches): \" , np . max ( inches [ summer ])) print ( \"Median precip on non-summer rainy days (inches):\" , np . median ( inches [ rainy & ~ summer ])) Median precip on rainy days in 2014 (inches): 0.194881889764 Median precip on summer days in 2014 (inches): 0.0 Maximum precip on summer days in 2014 (inches): 0.850393700787 Median precip on non-summer rainy days (inches): 0.200787401575 By combining Boolean operations, masking operations, and aggregates, we can very quickly answer these sorts of questions for our dataset. 76 | Chapter 2: Introduction to NumPy Using the Keywords and/or Versus the Operators &/| One common point of confusion is the difference between the keywords and and or on one hand, and the operators & and | on the other hand. When would you use one versus the other? The difference is this: and and or gauge the truth or falsehood of entire object , while & and | refer to bits within each object . When you use and or or , it’s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_87"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When you use and or or , it’s equivalent to asking Python to treat the object as a single Boolean entity. In Python, all nonzero integers will evaluate as True . Thus: In [ 30 ]: bool ( 42 ), bool ( 0 ) Out[30]: (True, False) In [ 31 ]: bool ( 42 and 0 ) Out[31]: False In [ 32 ]: bool ( 42 or 0 ) Out[32]: True When you use & and | on integers, the expression operates on the bits of the element, applying the and or the or to the individual bits making up the number: In [ 33 ]: bin ( 42 ) Out[33]: '0b101010' In [ 34 ]: bin ( 59 ) Out[34]: '0b111011' In [ 35 ]: bin ( 42 & 59 ) Out[35]: '0b101010' In [ 36 ]: bin ( 42 | 59 ) Out[36]: '0b111011' Notice that the corresponding bits of the binary representation are compared in order to yield the result. When you have an array of Boolean values in NumPy, this can be thought of as a string of bits where 1 = True and 0 = False , and the result of & and | operates in a similar manner as before: In [ 37 ]: A = np . array ([ 1 , 0 , 1 , 0 , 1 , 0 ], dtype = bool ) B = np . array ([ 1 , 1 , 1 , 0 , 1 , 1 ], dtype = bool ) A | B Out[37]: array([ True, True, True, False, True, True], dtype=bool) Comparisons, Masks, and Boolean Logic | 77 Using or on these arrays will try to evaluate the truth or falsehood of the entire array object, which is not a well-defined value: In [ 38 ]: A or B --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-38-5d8e4f2e21c0> in <module>() ----> 1 A or B ValueError: The truth value of an array with more than one element isSimilarly, when doing a Boolean expression on a given array, you should use | or & rather than or or and : In [ 39 ]: x = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_88"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". arange ( 10 ) ( x > 4 ) & ( x < 8 ) Out[39]: array([False, False, , True, True, False, False], dtype=bool) Trying to evaluate the truth or falsehood of the entire array will give the same ValueError we saw previously: In [ 40 ]: ( x > 4 ) and ( x < 8 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-40-3d24f1ffd63d> in <module>() ----> 1 (x > 4) and (x < 8) ValueError: The truth value of an array with more than one element isSo remember this: and and or perform a single Boolean evaluation on an entire object, while & and | perform multiple Boolean evaluations on the content (the indi‐ vidual bits or bytes) of an object. For Boolean NumPy arrays, the latter is nearly always the desired operation. Fancy Indexing In the previous sections, we saw how to access and modify portions of arrays using simple indices (e.g., arr[0] ), slices (e.g., arr[:5] ), and Boolean masks (e.g., arr[arr > 0] ). In this section, we’ll look at another style of array indexing, known as fancy indexing . Fancy indexing is like the simple indexing we’ve already seen, but we pass arrays of indices in place of single scalars. This allows us to very quickly access and modify complicated subsets of an array’s values. 78 | Chapter 2: Introduction to NumPy Exploring Fancy Indexing Fancy indexing is conceptually simple: it means passing an array of indices to access multiple array elements at once. For example, consider the following array: In [ 1 ]: import numpy as np rand = np . random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] Suppose we want to access three different elements",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_89"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . RandomState ( 42 ) x = rand . randint ( 100 , size = 10 ) print ( x ) [51 92 14 71 60 20 82 86 74 74] Suppose we want to access three different elements. We could do it like this: In [ 2 ]: [ x [ 3 ], x [ 7 ], x [ 2 ]] Out[2]: [71, 86, 14] Alternatively, we can pass a single list or array of indices to obtain the same result: In [ 3 ]: ind = [ 3 , 7 , 4 ] x [ ind ] Out[3]: array([71, 86, 60]) With fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed : In [ 4 ]: ind = np . array ([[ 3 , 7 ], [ 4 , 5 ]]) x [ ind ] Out[4]: array([[71, 86], [60, 20]]) Fancy indexing also works in multiple dimensions. Consider the following array: In [ 5 ]: X = np . arange ( 12 ) . reshape (( 3 , 4 )) X Out[5]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) Like with standard indexing, the first index refers to the row, and the second to the column: In [ 6 ]: row = np . array ([ 0 , 1 , 2 ]) col = np . array ([ 2 , 1 , 3 ]) X [ row , col ] Out[6]: array([ 2, 5, 11]) Notice that the first value in the result is X[0, 2] , the second is X[1, 1] , and the third is X[2, 3] . The pairing of indices in fancy indexing follows all the broadcasting rules that were mentioned in “Computation on Arrays: Broadcasting” on page 63 . So, Fancy Indexing | 79 for example, if we combine a column vector and a row vector within the indices, we get a two-dimensional result: In [ 7 ]: X [ row [:, np . newaxis ], col ] Out[7]: array([[ 2, 1, 3], [ 6, 5, 7], [10, 9, 11]]) Here, each row value is matched with each column vector, exactly as we saw in broad‐ casting of arithmetic operations. For example: In [ 8 ]: row [:, np . newaxis ] * col Out[8]: array([[0, 0, 0], [2, 1, 3], [4, 2, 6]]) It is always important to remember with fancy indexing that the return value reflects the broadcasted shape of the indices , rather than the shape of the array being indexed",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_90"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Combined Indexing For even more powerful operations, fancy indexing can be combined with the other indexing schemes we’ve seen: In [ 9 ]: print ( X ) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] We can combine fancy and simple indices: In [ 10 ]: X [ 2 , [ 2 , 0 , 1 ]] Out[10]: array([10, 8, 9]) We can also combine fancy indexing with slicing: In [ 11 ]: X [ 1 :, [ 2 , 0 , 1 ]] Out[11]: array([[ 6, 4, 5], [10, 8, 9]]) And we can combine fancy indexing with masking: In [ 12 ]: mask = np . array ([ 1 , 0 , 1 , 0 ], dtype = bool ) X [ row [:, np . newaxis ], mask ] Out[12]: array([[ 0, 2], [ 4, 6], [ 8, 10]]) All of these indexing options combined lead to a very flexible set of operations for accessing and modifying array values. 80 | Chapter 2: Introduction to NumPy Example: Selecting Random Points One common use of fancy indexing is the selection of subsets of rows from a matrix. For example, we might have an N by D matrix representing N points in D dimen‐ sions, such as the following points drawn from a two-dimensional normal distribu‐ tion: In [ 13 ]: mean = [ 0 , 0 ] cov = [[ 1 , 2 ], [ 2 , 5 ]] X = rand . multivariate_normal ( mean , cov , 100 ) X . shape Out[13]: (100, 2) Using the plotting tools we will discuss in Chapter 4 , we can visualize these points as a scatter plot ( Figure 2-7 ): In [ 14 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () # for plot styling plt . scatter ( X [:, 0 ], X [:, 1 ]); Figure 2-7. Normally distributed points Let’s use fancy indexing to select 20 random points. We’ll do this by first choosing 20 random indices with no repeats, and use these indices to select a portion of the origi‐ nal array: In [ 15 ]: indices = np . random . choice ( X . shape [ 0 ], 20 , replace = False ) indices Out[15]: array([93, 45, 73, 81, 50, 10, 98, 94, 4, 64, 65, 89, 47, 84, 82, 80, 25, 90, 63, 20]) Fancy Indexing | 81 In [ 16 ]: selection = X [ indices ] # fancy indexing here selection",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_91"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". shape Out[16]: (20, 2) Now to see which points were selected, let’s over-plot large circles at the locations of the selected points ( Figure 2-8 ): In [ 17 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], alpha = 0.3 ) plt . scatter ( selection [:, 0 ], selection [:, 1 ], facecolor = 'none' , s = 200 ); Figure 2-8. Random selection among points This sort of strategy is often used to quickly partition datasets, as is often needed in train/test splitting for validation of statistical models (see “Hyperparameters and Model Validation” on page 359 ), and in sampling approaches to answering statistical questions. Modifying Values with Fancy Indexing Just as fancy indexing can be used to access parts of an array, it can also be used to modify parts of an array. For example, imagine we have an array of indices and we’d like to set the corresponding items in an array to some value: In [ 18 ]: x = np . arange ( 10 ) i = np . array ([ 2 , 1 , 8 , 4 ]) x [ i ] = 99 print ( x ) [ 0 99 99 3 99 5 6 7 99 9] We can use any assignment-type operator for this. For example: 82 | Chapter 2: Introduction to NumPy In [ 19 ]: x [ i ] -= 10 print ( x ) [ 0 89 89 3 89 5 6 7 89 9] Notice, though, that repeated indices with these operations can cause some poten‐ tially unexpected results. Consider the following: In [ 20 ]: x = np . zeros ( 10 ) x [[ 0 , 0 ]] = [ 4 , 6 ] print ( x ) [ 6. 0. 0. 0. 0. 0. 0. 0. 0. 0.] Where did the 4 go? The result of this operation is to first assign x[0] = 4 , followed by x[0] = 6 . The result, of course, is that x[0] contains the value 6. Fair enough, but consider this operation: In [ 21 ]: i = [ 2 , 3 , 3 , 4 , 4 , 4 ] x [ i ] += 1 x Out[21]: array([ 6., 0., 1., 1., 1., 0., 0., 0., 0., 0.]) You might expect that x[3] would contain the value 2, and x[4] would contain the value 3, as this is how many times each index is repeated. Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1 . x[i] + 1 is evaluated, and then the result is assigned to the indices in x",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_92"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Why is this not the case? Conceptually, this is because x[i] += 1 is meant as a shorthand of x[i] = x[i] + 1 . x[i] + 1 is evaluated, and then the result is assigned to the indices in x . With this in mind, it is not the augmentation that happens multiple times, but the assignment, which leads to the rather nonintuitive results. So what if you want the other behavior where the operation is repeated? For this, you can use the at() method of ufuncs (available since NumPy 1.8), and do the following: In [ 22 ]: x = np . zeros ( 10 ) np . add . at ( x , i , 1 ) print ( x ) [ 0. 0. 1. 2. 3. 0. 0. 0. 0. 0.] The at() method does an in-place application of the given operator at the specified indices (here, i ) with the specified value (here, 1). Another method that is similar in spirit is the reduceat() method of ufuncs, which you can read about in the NumPy documentation. Example: Binning Data You can use these ideas to efficiently bin data to create a histogram by hand. For example, imagine we have 1,000 values and would like to quickly find where they fall within an array of bins. We could compute it using ufunc.at like this: Fancy Indexing | 83 In [ 23 ]: np . random . seed ( 42 ) x = np . random . randn ( 100 ) # compute a histogram by hand bins = np . linspace ( - 5 , 5 , 20 ) counts = np . zeros_like ( bins ) # find the appropriate bin for each x i = np . searchsorted ( bins , x ) # add 1 to each of these bins np . add . at ( counts , i , 1 ) The counts now reflect the number of points within each bin—in other words, a his‐ togram ( Figure 2-9 ): In [ 24 ]: # plot the results plt . plot ( bins , counts , linestyle = 'steps' ); Figure 2-9. A histogram computed by hand Of course, it would be silly to have to do this each time you want to plot a histogram. This is why Matplotlib provides the plt.hist() routine, which does the same in a single line: plt . hist ( x , bins , histtype = 'step' ); This function will create a nearly identical plot to the one seen here",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_93"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". hist ( x , bins , histtype = 'step' ); This function will create a nearly identical plot to the one seen here. To compute the binning, Matplotlib uses the np.histogram function, which does a very similar com‐ putation to what we did before. Let’s compare the two here: In [ 25 ]: print ( \"NumPy routine:\" ) % timeit counts , edges = np . histogram ( x , bins ) 84 | Chapter 2: Introduction to NumPy print ( \"Custom routine:\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 10000 loops, best of 3: 97.6 μs per loop Custom routine: 10000 loops, best of 3: 19.5 μs per loop Our own one-line algorithm is several times faster than the optimized algorithm in NumPy! How can this be? If you dig into the np.histogram source code (you can do this in IPython by typing np.histogram?? ), you’ll see that it’s quite a bit more involved than the simple search-and-count that we’ve done; this is because NumPy’s algorithm is more flexible, and particularly is designed for better performance when the number of data points becomes large: In [ 26 ]: x = np . random . randn ( 1000000 ) print ( \"NumPy routine:\" ) % timeit counts , edges = np . histogram ( x , bins ) print ( \"Custom routine:\" ) % timeit np . add . at ( counts , np . searchsorted ( bins , x ), 1 ) NumPy routine: 10 loops, best of 3: 68.7 ms per loop Custom routine: 10 loops, best of 3: 135 ms per loop What this comparison shows is that algorithmic efficiency is almost never a simple question. An algorithm efficient for large datasets will not always be the best choice for small datasets, and vice versa (see “Big-O Notation” on page 92 ). But the advan‐ tage of coding this algorithm yourself is that with an understanding of these basic methods, you could use these building blocks to extend this to do some very interest‐ ing custom behaviors",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_94"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The key to efficiently using Python in data-intensive applica‐ tions is knowing about general convenience routines like np.histogram and when they’re appropriate, but also knowing how to make use of lower-level functionality when you need more pointed behavior. Sorting Arrays Up to this point we have been concerned mainly with tools to access and operate on array data with NumPy. This section covers algorithms related to sorting values in NumPy arrays. These algorithms are a favorite topic in introductory computer sci‐ ence courses: if you’ve ever taken one, you probably have had dreams (or, depending on your temperament, nightmares) about insertion sorts , selection sorts , merge sorts , quick sorts , bubble sorts , and many, many more. All are means of accomplishing a similar task: sorting the values in a list or array. Sorting Arrays | 85 For example, a simple selection sort repeatedly finds the minimum value from a list, and makes swaps until the list is sorted. We can code this in just a few lines of Python: In [ 1 ]: import numpy as np def selection_sort ( x ): for i in range ( len ( x )): swap = i + np . argmin ( x [ i :]) ( x [ i ], x [ swap ]) = ( x [ swap ], x [ i ]) return x In [ 2 ]: x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) selection_sort ( x ) Out[2]: array([1, 2, 3, 4, 5]) As any first-year computer science major will tell you, the selection sort is useful for its simplicity, but is much too slow to be useful for larger arrays. For a list of N values, it requires N loops, each of which does on the order of ~ N comparisons to find the swap value. In terms of the “big-O” notation often used to characterize these algo‐ rithms (see “Big-O Notation” on page 92 ), selection sort averages � N 2 : if you dou‐ ble the number of items in the list, the execution time will go up by about a factor of four. Even selection sort, though, is much better than my all-time favorite sorting algo‐ rithms, the bogosort : In [ 3 ]: def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_95"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Even selection sort, though, is much better than my all-time favorite sorting algo‐ rithms, the bogosort : In [ 3 ]: def bogosort ( x ): while np . any ( x [: - 1 ] > x [ 1 :]): np . random . shuffle ( x ) return x In [ 4 ]: x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) bogosort ( x ) Out[4]: array([1, 2, 3, 4, 5]) This silly sorting method relies on pure chance: it repeatedly applies a random shuf‐ fling of the array until the result happens to be sorted. With an average scaling of � N × N ! (that’s N times N factorial), this should—quite obviously—never be used for any real computation. Fortunately, Python contains built-in sorting algorithms that are much more efficient than either of the simplistic algorithms just shown. We’ll start by looking at the Python built-ins, and then take a look at the routines included in NumPy and opti‐ mized for NumPy arrays. Fast Sorting in NumPy: np.sort and np.argsort Although Python has built-in sort and sorted functions to work with lists, we won’t discuss them here because NumPy’s np.sort function turns out to be much more 86 | Chapter 2: Introduction to NumPy efficient and useful for our purposes. By default np.sort uses an � N log N , quick‐ sort algorithm, though mergesort and heapsort are also available. For most applica‐ tions, the default quicksort is more than sufficient. To return a sorted version of the array without modifying the input, you can use np.sort : In [ 5 ]: x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) np . sort ( x ) Out[5]: array([1, 2, 3, 4, 5]) If you prefer to sort the array in-place, you can instead use the sort method of arrays: In [ 6 ]: x . sort () print ( x ) [1 2 3 4 5] A related function is argsort , which instead returns the indices of the sorted elements: In [ 7 ]: x = np . array ([ 2 , 1 , 4 , 3 , 5 ]) i = np . argsort ( x ) print ( i ) [1 0 3 2 4] The first element of this result gives the index of the smallest element, the second value gives the index of the second smallest, and so on",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_96"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". argsort ( x ) print ( i ) [1 0 3 2 4] The first element of this result gives the index of the smallest element, the second value gives the index of the second smallest, and so on. These indices can then be used (via fancy indexing) to construct the sorted array if desired: In [ 8 ]: x [ i ] Out[8]: array([1, 2, 3, 4, 5]) Sorting along rows or columns A useful feature of NumPy’s sorting algorithms is the ability to sort along specific rows or columns of a multidimensional array using the axis argument. For example: In [ 9 ]: rand = np . random . RandomState ( 42 ) X = rand . randint ( 0 , 10 , ( 4 , 6 )) print ( X ) [[6 3 7 4 6 9] [2 6 7 4 3 7] [7 2 5 4 1 7] [5 1 4 0 9 5]] In [ 10 ]: # sort each column of X np . sort ( X , axis = 0 ) Out[10]: array([[2, 1, 4, 0, 1, 5], [5, 2, 5, 4, 3, 7], Sorting Arrays | 87 [6, 3, 7, 4, 6, 7], [7, 6, 7, 4, 9, 9]]) In [ 11 ]: # sort each row of X np . sort ( X , axis = 1 ) Out[11]: array([[3, 4, 6, 6, 7, 9], [2, 3, 4, 6, 7, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 5, 9]]) Keep in mind that this treats each row or column as an independent array, and any relationships between the row or column values will be lost! Partial Sorts: Partitioning Sometimes we’re not interested in sorting the entire array, but simply want to find the K smallest values in the array. NumPy provides this in the np.partition function. np.partition takes an array and a number K ; the result is a new array with the small‐ est K values to the left of the partition, and the remaining values to the right, in arbi‐ trary order: In [ 12 ]: x = np . array ([ 7 , 2 , 3 , 1 , 6 , 5 , 4 ]) np . partition ( x , 3 ) Out[12]: array([2, 1, 3, 4, 6, 5, 7]) Note that the first three values in the resulting array are the three smallest in the array, and the remaining array positions contain the remaining values. Within the two partitions, the elements have arbitrary order. Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: In [ 13 ]: np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_97"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Within the two partitions, the elements have arbitrary order. Similarly to sorting, we can partition along an arbitrary axis of a multidimensional array: In [ 13 ]: np . partition ( X , 2 , axis = 1 ) Out[13]: array([[3, 4, 6, 7, 6, 9], [2, 3, 4, 7, 6, 7], [1, 2, 4, 5, 7, 7], [0, 1, 4, 5, 9, 5]]) The result is an array where the first two slots in each row contain the smallest values from that row, with the remaining values filling the remaining slots. Finally, just as there is a np.argsort that computes indices of the sort, there is a np.argpartition that computes indices of the partition. We’ll see this in action in the following section. Example: k-Nearest Neighbors Let’s quickly see how we might use this argsort function along multiple axes to find the nearest neighbors of each point in a set. We’ll start by creating a random set of 10 88 | Chapter 2: Introduction to NumPy points on a two-dimensional plane. Using the standard convention, we’ll arrange these in a 10×2 array: In [ 14 ]: X = rand . rand ( 10 , 2 ) To get an idea of how these points look, let’s quickly scatter plot them ( Figure 2-10 ): In [ 15 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () # Plot styling plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ); Figure 2-10. Visualization of points in the k-neighbors example Now we’ll compute the distance between each pair of points. Recall that the squaredistance between two points is the sum of the squared differences in each dimension; using the efficient broadcasting ( “Computation on Arrays: Broadcasting” on page 63 ) and aggregation ( “Aggregations: Min, Max, and Everything in Between” on page 58 ) routines provided by NumPy, we can compute the matrix of square distances in a sin‐ gle line of code: In [ 16 ]: dist_sq = np . sum (( X [:, np . newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis =- 1 ) This operation has a lot packed into it, and it might be a bit confusing if you’re unfa‐ miliar with NumPy’s broadcasting rules",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_98"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". newaxis ,:] - X [ np . newaxis ,:,:]) ** 2 , axis =- 1 ) This operation has a lot packed into it, and it might be a bit confusing if you’re unfa‐ miliar with NumPy’s broadcasting rules. When you come across code like this, it can be useful to break it down into its component steps: In [ 17 ]: # for each pair of points, compute differences in their coordinates differences = X [:, np . newaxis , :] - X [ np . newaxis , :, :] differences . shape Out[17]: (10, 10, 2) Sorting Arrays | 89 In [ 18 ]: # square the coordinate differences sq_differences = differences ** 2 sq_differences . shape Out[18]: (10, 10, 2) In [ 19 ]: # sum the coordinate differences to get the squared distance dist_sq = sq_differences . sum ( - 1 ) dist_sq . shape Out[19]: (10, 10) Just to double-check what we are doing, we should see that the diagonal of this matrix (i.e., the set of distances between each point and itself) is all zero: In [ 20 ]: dist_sq . diagonal () Out[20]: array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) It checks out! With the pairwise square-distances converted, we can now use np.arg sort to sort along each row. The leftmost columns will then give the indices of the nearest neighbors: In [ 21 ]: nearest = np . argsort ( dist_sq , axis = 1 ) print ( nearest ) [[0 3 9 7 1 4 2 5 6 8] [1 4 7 9 3 6 8 5 0 2] [2 1 4 6 3 0 8 9 7 5] [3 9 7 0 1 4 5 8 6 2] [4 1 8 5 6 7 9 3 0 2] [5 8 6 4 1 7 9 3 2 0] [6 8 5 4 1 7 9 3 2 0] [7 9 3 1 4 0 5 8 6 2] [8 5 6 4 1 7 9 3 2 0] [9 7 3 0 1 4 5 8 6 2]] Notice that the first column gives the numbers 0 through 9 in order: this is due to the fact that each point’s closest neighbor is itself, as we would expect. By using a full sort here, we’ve actually done more work than we need to in this case. If we’re simply interested in the nearest k neighbors, all we need is to partition each row so that the smallest k + 1 squared distances come first, with larger distances fill‐ ing the remaining positions of the array. We can do this with the np.argpartition function: In [ 22 ]: K = 2 nearest_partition = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_99"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We can do this with the np.argpartition function: In [ 22 ]: K = 2 nearest_partition = np . argpartition ( dist_sq , K + 1 , axis = 1 ) In order to visualize this network of neighbors, let’s quickly plot the points along with lines representing the connections from each point to its two nearest neighbors ( Figure 2-11 ): 90 | Chapter 2: Introduction to NumPy In [ 23 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], s = 100 ) # draw lines from each point to its two nearest neighbors K = 2 for i in range ( X . shape [ 0 ]): for j in nearest_partition [ i , : K + 1 ]: # plot a line from X[i] to X[j] # use some zip magic to make it happen: plt . plot ( * zip ( X [ j ], X [ i ]), color = 'black' ) Figure 2-11. Visualization of the neighbors of each point Each point in the plot has lines drawn to its two nearest neighbors. At first glance, it might seem strange that some of the points have more than two lines coming out of them: this is due to the fact that if point A is one of the two nearest neighbors of point B, this does not necessarily imply that point B is one of the two nearest neighbors of point A. Although the broadcasting and row-wise sorting of this approach might seem less straightforward than writing a loop, it turns out to be a very efficient way of operating on this data in Python. You might be tempted to do the same type of operation by manually looping through the data and sorting each set of neighbors individually, but this would almost certainly lead to a slower algorithm than the vectorized version we used. The beauty of this approach is that it’s written in a way that’s agnostic to the size of the input data: we could just as easily compute the neighbors among 100 or 1,000,000 points in any number of dimensions, and the code would look the same. Finally, I’ll note that when doing very large nearest-neighbor searches, there are trebased and/or approximate algorithms that can scale as � N log N or better rather Sorting Arrays | 91 than the � N 2 of the brute-force algorithm",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_100"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". One example of this is the KD-Tree, implemented in Scikit-Learn . Big-O Notation Big-O notation is a means of describing how the number of operations required for an algorithm scales as the input grows in size. To use it correctly is to dive deeply into the realm of computer science theory, and to carefully distinguish it from the related small-o notation, biθ notation, biΩ notation, and probably many mutant hybrids thereof. While these distinctions add precision to statements about algorithmic scal‐ ing, outside computer science theory exams and the remarks of pedantic blog com‐ menters, you’ll rarely see such distinctions made in practice. Far more common in the data science world is a less rigid use of big-O notation: as a general (if imprecise) description of the scaling of an algorithm. With apologies to theorists and pedants, this is the interpretation we’ll use throughout this book. Big-O notation, in this loose sense, tells you how much time your algorithm will take as you increase the amount of data. If you have an � N (read “order N ”) algorithm that takes 1 second to operate on a list of length N =1,000, then you should expect it to take roughly 5 seconds for a list of length N =5,000. If you have an � N 2 (read “order N squared”) algorithm that takes 1 second for N =1,000, then you should expect it to take about 25 seconds for N =5,000. For our purposes, the N will usually indicate some aspect of the size of the dataset (the number of points, the number of dimensions, etc.). When trying to analyze billions or trillions of samples, the difference between � N and � N 2 can be far from trivial! Notice that the big-O notation by itself tells you nothing about the actual wall-clock time of a computation, but only about its scaling as you change N . Generally, for example, an � N algorithm is considered to have better scaling than an � N 2 algo‐ rithm, and for good reason. But for small datasets in particular, the algorithm with better scaling might not be faster",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_101"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". But for small datasets in particular, the algorithm with better scaling might not be faster. For example, in a given problem an � N 2 algo‐ rithm might take 0.01 seconds, while a “better” � N algorithm might take 1 second. Scale up N by a factor of 1,000, though, and the � N algorithm will win out. Even this loose version of Big-O notation can be very useful for comparing the per‐ formance of algorithms, and we’ll use this notation throughout the book when talking about how algorithms scale. Structured Data: NumPy’s Structured Arrays While often our data can be well represented by a homogeneous array of values, sometimes this is not the case. This section demonstrates the use of NumPy’s struc‐ tured arrays and record arrays , which provide efficient storage for compound, hetero‐ 92 | Chapter 2: Introduction to NumPy geneous data. While the patterns shown here are useful for simple operations, scenarios like this often lend themselves to the use of Pandas DataFrame s, which we’ll explore in Chapter 3 . Imagine that we have several categories of data on a number of people (say, name, age, and weight), and we’d like to store these values for use in a Python program. It would be possible to store these in three separate arrays: In [ 2 ]: name = [ 'Alice' , 'Bob' , 'Cathy' , 'Doug' ] age = [ 25 , 45 , 37 , 19 ] weight = [ 55.0 , 85.5 , 68.0 , 61.5 ] But this is a bit clumsy. There’s nothing here that tells us that the three arrays are related; it would be more natural if we could use a single structure to store all of this data. NumPy can handle this through structured arrays, which are arrays with com‐ pound data types. Recall that previously we created a simple array using an expression like this: In [ 3 ]: x = np . zeros ( 4 , dtype = int ) We can similarly create a structured array using a compound data type specification: In [ 4 ]: # Use a compound data type for structured arrays data = np . zeros ( 4 , dtype = { 'names' :( 'name' , 'age' , 'weight' ), 'formats' :( 'U10' , 'i4' , 'f8' )}) print ( data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_102"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". zeros ( 4 , dtype = { 'names' :( 'name' , 'age' , 'weight' ), 'formats' :( 'U10' , 'i4' , 'f8' )}) print ( data . dtype ) [('name', '<U10'), ('age', '<i4'), ('weight', '<f8')] Here 'U10' translates to “Unicode string of maximum length 10,” 'i4' translates to “4-byte (i.e., 32 bit) integer,” and 'f8' translates to “8-byte (i.e., 64 bit) float.” We’ll discuss other options for these type codes in the following section. Now that we’ve created an empty container array, we can fill the array with our lists of values: In [ 5 ]: data [ 'name' ] = name data [ 'age' ] = age data [ 'weight' ] = weight print ( data ) [('Alice', 25, 55.0) ('Bob', 45, 85.5) ('Cathy', 37, 68.0) ('Doug', 19, 61.5)] As we had hoped, the data is now arranged together in one convenient block of memory. The handy thing with structured arrays is that you can now refer to values either by index or by name: In [ 6 ]: # Get all names data [ 'name' ] Structured Data: NumPy’s Structured Arrays | 93 Out[6]: array(['Alice', 'Bob', 'Cathy', 'Doug'], dtype='<U10') In [ 7 ]: # Get first row of data data [ 0 ] Out[7]: ('Alice', 25, 55.0) In [ 8 ]: # Get the name from the last row data [ - 1 ][ 'name' ] Out[8]: 'Doug' Using Boolean masking, this even allows you to do some more sophisticated opera‐ tions such as filtering on age: In [ 9 ]: # Get names where age is under 30 data [ data [ 'age' ] < 30 ][ 'name' ] Out[9]: array(['Alice', 'Doug'], dtype='<U10') Note that if you’d like to do any operations that are any more complicated than these, you should probably consider the Pandas package, covered in the next chapter. As we’ll see, Pandas provides a DataFrame object, which is a structure built on NumPy arrays that offers a variety of useful data manipulation functionality similar to what we’ve shown here, as well as much, much more. Creating Structured Arrays Structured array data types can be specified in a number of ways. Earlier, we saw the dictionary method: In [ 10 ]: np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_103"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Creating Structured Arrays Structured array data types can be specified in a number of ways. Earlier, we saw the dictionary method: In [ 10 ]: np . dtype ({ 'names' :( 'name' , 'age' , 'weight' ), 'formats' :( 'U10' , 'i4' , 'f8' )}) Out[10]: dtype([('name', '<U10'), ('age', '<i4'), ('weight', '<f8')]) For clarity, numerical types can be specified with Python types or NumPy dtype s instead: In [ 11 ]: np . dtype ({ 'names' :( 'name' , 'age' , 'weight' ), 'formats' :(( np . str_ , 10 ), int , np . float32 )}) Out[11]: dtype([('name', '<U10'), ('age', '<i8'), ('weight', '<f4')]) A compound type can also be specified as a list of tuples: In [ 12 ]: np . dtype ([( 'name' , 'S10' ), ( 'age' , 'i4' ), ( 'weight' , 'f8' )]) Out[12]: dtype([('name', 'S10'), ('age', '<i4'), ('weight', '<f8')]) If the names of the types do not matter to you, you can specify the types alone in a comma-separated string: In [ 13 ]: np . dtype ( 'S10,i4,f8' ) 94 | Chapter 2: Introduction to NumPy Out[13]: dtype([('f0', 'S10'), ('f1', '<i4'), ('f2', '<f8')]) The shortened string format codes may seem confusing, but they are built on simple principles. The first (optional) character is < or > , which means “little endian” or “big endian,” respectively, and specifies the ordering convention for significant bits. The next character specifies the type of data: characters, bytes, ints, floating points, and so on (see Table 2-4 ). The last character or characters represents the size of the object in bytes. Table 2-4. NumPy data types Character Description Example 'b' Byte np.dtype('b') 'i' Signed integer np.dtype('i4') == np.int32 'u' Unsigned integer np.dtype('u1') == np.uint8 'f' Floating point np.dtype('f8') == np.int64 'c' Complex floating point np.dtype('c16') == np.complex128 'S' , 'a' string np.dtype('S5') 'U' Unicode string np.dtype('U') == np.str_ 'V' Raw data (void) np.dtype('V') == np.void More Advanced Compound Types It is possible to define even more advanced compound types",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_104"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, you can create a type where each element contains an array or matrix of values. Here, we’ll create a data type with a mat component consisting of a 3×3 floating-point matrix: In [ 14 ]: tp = np . dtype ([( 'id' , 'i8' ), ( 'mat' , 'f8' , ( 3 , 3 ))]) X = np . zeros ( 1 , dtype = tp ) print ( X [ 0 ]) print ( X [ 'mat' ][ 0 ]) (0, [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]) [[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]] Now each element in the X array consists of an id and a 3×3 matrix. Why would you use this rather than a simple multidimensional array, or perhaps a Python dictionary? The reason is that this NumPy dtype directly maps onto a C structure definition, so the buffer containing the array content can be accessed directly within an appropri‐ ately written C program. If you find yourself writing a Python interface to a legacy C or Fortran library that manipulates structured data, you’ll probably find structured arrays quite useful! Structured Data: NumPy’s Structured Arrays | 95 RecordArrays: Structured Arrays with a Twist NumPy also provides the np.recarray class, which is almost identical to the struc‐ tured arrays just described, but with one additional feature: fields can be accessed as attributes rather than as dictionary keys. Recall that we previously accessed the ages by writing: In [ 15 ]: data [ 'age' ] Out[15]: array([25, 45, 37, 19], dtype=int32) If we view our data as a record array instead, we can access this with slightly fewer keystrokes: In [ 16 ]: data_rec = data . view ( np . recarray ) data_rec . age Out[16]: array([25, 45, 37, 19], dtype=int32) The downside is that for record arrays, there is some extra overhead involved in accessing the fields, even when using the same syntax. We can see this here: In [ 17 ]: % timeit data [ 'age' ] % timeit data_rec [ 'age' ] % timeit data_rec",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_105"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We can see this here: In [ 17 ]: % timeit data [ 'age' ] % timeit data_rec [ 'age' ] % timeit data_rec . age 1000000 loops, best of 3: 241 ns per loop 100000 loops, best of 3: 4.61 μs per loop 100000 loops, best of 3: 7.27 μs per loop Whether the more convenient notation is worth the additional overhead will depend on your own application. On to Pandas This section on structured and record arrays is purposely at the end of this chapter, because it leads so well into the next package we will cover: Pandas. Structured arrays like the ones discussed here are good to know about for certain situations, especially in case you’re using NumPy arrays to map onto binary data formats in C, Fortran, or another language. For day-to-day use of structured data, the Pandas package is a much better choice, and we’ll dive into a full discussion of it in the next chapter. 96 | Chapter 2: Introduction to NumPy CHAPTER 3 Data Manipulation with Pandas In the previous chapter, we dove into detail on NumPy and its ndarray object, which provides efficient storage and manipulation of dense typed arrays in Python. Here we’ll build on this knowledge by looking in detail at the data structures provided by the Pandas library. Pandas is a newer package built on top of NumPy, and provides an efficient implementation of a DataFrame . DataFrame s are essentially multidimen‐ sional arrays with attached row and column labels, and often with heterogeneous types and/or missing data. As well as offering a convenient storage interface for labeled data, Pandas implements a number of powerful data operations familiar to users of both database frameworks and spreadsheet programs. As we saw, NumPy’s ndarray data structure provides essential features for the type of clean, well-organized data typically seen in numerical computing tasks",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_106"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As we saw, NumPy’s ndarray data structure provides essential features for the type of clean, well-organized data typically seen in numerical computing tasks. While it serves this purpose very well, its limitations become clear when we need more flexi‐ bility (attaching labels to data, working with missing data, etc.) and when attempting operations that do not map well to element-wise broadcasting (groupings, pivots, etc.), each of which is an important piece of analyzing the less structured data avail‐ able in many forms in the world around us. Pandas, and in particular its Series and DataFrame objects, builds on the NumPy array structure and provides efficient access to these sorts of “data munging” tasks that occupy much of a data scientist’s time. In this chapter, we will focus on the mechanics of using Series , DataFrame , and related structures effectively. We will use examples drawn from real datasets where appropriate, but these examples are not necessarily the focus. Installing and Using Pandas Installing Pandas on your system requires NumPy to be installed, and if you’re build‐ ing the library from source, requires the appropriate tools to compile the C and 97 Cython sources on which Pandas is built. Details on this installation can be found in the Pandas documentation . If you followed the advice outlined in the preface and used the Anaconda stack, you already have Pandas installed. Once Pandas is installed, you can import it and check the version: In [ 1 ]: import pandas pandas . __version__ Out[1]: '0.18.1' Just as we generally import NumPy under the alias np , we will import Pandas under the alias pd : In [ 2 ]: import pandas as pd This import convention will be used throughout the remainder of this book. Reminder About Built-In Documentation As you read through this chapter, don’t forget that IPython gives you the ability to quickly explore the contents of a package (by using the tab-completion feature) as well as the documentation of various functions (using the ? character)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_107"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". (Refer back to “Help and Documentation in IPython” on page 3 if you need a refresher on this.) For example, to display all the contents of the pandas namespace, you can type this: In [ 3 ]: pd .< TAB > And to display the built-in Pandas documentation, you can use this: In [ 4 ]: pd ? More detailed documentation, along with tutorials and other resources, can be found at http://pandas.pydata.org/ . Introducing Pandas Objects At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices. As we will see during the course of this chapter, Pandas provides a host of useful tools, methods, and functionality on top of the basic data structures, but nearly everything that follows will require an understanding of what these structures are. Thus, before we go any further, let’s introduce these three fundamental Pandas data structures: the Series , DataFrame , and Index . We will start our code sessions with the standard NumPy and Pandas imports: In [ 1 ]: import numpy as np import pandas as pd 98 | Chapter 3: Data Manipulation with Pandas The Pandas Series Object A Pandas Series is a one-dimensional array of indexed data. It can be created from a list or array as follows: In [ 2 ]: data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ]) data Out[2]: 0 0.25 1 0.50 2 0.75 3 1.00 dtype: float64 As we see in the preceding output, the Series wraps both a sequence of values and a sequence of indices, which we can access with the values and index attributes. The values are simply a familiar NumPy array: In [ 3 ]: data . values Out[3]: array([ 0.25, 0.5 , 0.75, 1. ]) The index is an array-like object of type pd.Index , which we’ll discuss in more detail momentarily: In [ 4 ]: data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_108"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". values Out[3]: array([ 0.25, 0.5 , 0.75, 1. ]) The index is an array-like object of type pd.Index , which we’ll discuss in more detail momentarily: In [ 4 ]: data . index Out[4]: RangeIndex(start=0, stop=4, step=1) Like with a NumPy array, data can be accessed by the associated index via the familiar Python square-bracket notation: In [ 5 ]: data [ 1 ] Out[5]: 0.5 In [ 6 ]: data [ 1 : 3 ] Out[6]: 1 0.50 2 0.75 dtype: float64 As we will see, though, the Pandas Series is much more general and flexible than the one-dimensional NumPy array that it emulates. Series as generalized NumPy array From what we’ve seen so far, it may look like the Series object is basically inter‐ changeable with a one-dimensional NumPy array. The essential difference is the pres‐ ence of the index: while the NumPy array has an implicitly defined integer index used to access the values, the Pandas Series has an explicitly defined index associated with the values. Introducing Pandas Objects | 99 This explicit index definition gives the Series object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type. For example, if we wish, we can use strings as an index: In [ 7 ]: data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data Out[7]: a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 And the item access works as expected: In [ 8 ]: data [ 'b' ] Out[8]: 0.5 We can even use noncontiguous or nonsequential indices: In [ 9 ]: data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 2 , 5 , 3 , 7 ]) data Out[9]: 2 0.25 5 0.50 3 0.75 7 1.00 dtype: float64 In [ 10 ]: data [ 5 ] Out[10]: 0.5 Series as specialized dictionary In this way, you can think of a Pandas Series a bit like a specialization of a Python dictionary. A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_109"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a Series is a structure that maps typed keys to a set of typed values. This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas Series makes it much more efficient than Python dictionaries for certain operations. We can make the Series -as-dictionary analogy even more clear by constructing a Series object directly from a Python dictionary: 100 | Chapter 3: Data Manipulation with Pandas In [ 11 ]: population_dict = { 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 } population = pd . Series ( population_dict ) population Out[11]: California 38332521 Florida 19552860 Illinois 12882135 New York 19651127 Texas 26448193 dtype: int64 By default, a Series will be created where the index is drawn from the sorted keys. From here, typical dictionary-style item access can be performed: In [ 12 ]: population [ 'California' ] Out[12]: 38332521 Unlike a dictionary, though, the Series also supports array-style operations such as slicing: In [ 13 ]: population [ 'California' : 'Illinois' ] Out[13]: California 38332521 Florida 19552860 Illinois 12882135 dtype: int64 We’ll discuss some of the quirks of Pandas indexing and slicing in “Data Indexing and Selection” on page 107 . Constructing Series objects We’ve already seen a few ways of constructing a Pandas Series from scratch; all of them are some version of the following: >>> pd . Series ( data , index = index ) where index is an optional argument, and data can be one of many entities. For example, data can be a list or NumPy array, in which case index defaults to an integer sequence: In [ 14 ]: pd . Series ([ 2 , 4 , 6 ]) Out[14]: 0 2 1 4 2 6 dtype: int64 Introducing Pandas Objects | 101 data can be a scalar, which is repeated to fill the specified index: In [ 15 ]: pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_110"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Series ([ 2 , 4 , 6 ]) Out[14]: 0 2 1 4 2 6 dtype: int64 Introducing Pandas Objects | 101 data can be a scalar, which is repeated to fill the specified index: In [ 15 ]: pd . Series ( 5 , index = [ 100 , 200 , 300 ]) Out[15]: 100 5 200 5 300 5 dtype: int64 data can be a dictionary, in which index defaults to the sorted dictionary keys: In [ 16 ]: pd . Series ({ 2 : 'a' , 1 : 'b' , 3 : 'c' }) Out[16]: 1 b 2 a 3 c dtype: object In each case, the index can be explicitly set if a different result is preferred: In [ 17 ]: pd . Series ({ 2 : 'a' , 1 : 'b' , 3 : 'c' }, index = [ 3 , 2 ]) Out[17]: 3 c 2 a dtype: object Notice that in this case, the Series is populated only with the explicitly identified keys. The Pandas DataFrame Object The next fundamental structure in Pandas is the DataFrame . Like the Series object discussed in the previous section, the DataFrame can be thought of either as a gener‐ alization of a NumPy array, or as a specialization of a Python dictionary. We’ll now take a look at each of these perspectives. DataFrame as a generalized NumPy array If a Series is an analog of a one-dimensional array with flexible indices, a DataFrame is an analog of a two-dimensional array with both flexible row indices and flexible column names. Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a DataFrame as a sequence of aligned Series objects. Here, by “aligned” we mean that they share the same index. To demonstrate this, let’s first construct a new Series listing the area of each of the five states discussed in the previous section: In [ 18 ]: area_dict = { 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 } 102 | Chapter 3: Data Manipulation with Pandas area = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_111"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Series ( area_dict ) area Out[18]: California 423967 Florida 170312 Illinois 149995 New York 141297 Texas 695662 dtype: int64 Now that we have this along with the population Series from before, we can use a dictionary to construct a single two-dimensional object containing this information: In [ 19 ]: states = pd . DataFrame ({ 'population' : population , 'area' : area }) states Out[19]: area population California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 New York 141297 19651127 Texas 695662 26448193 Like the Series object, the DataFrame has an index attribute that gives access to the index labels: In [ 20 ]: states . index Out[20]: Index(['California', 'Florida', 'Illinois', 'New York', 'Texas'], dtype='object') Additionally, the DataFrame has a columns attribute, which is an Index object holding the column labels: In [ 21 ]: states . columns Out[21]: Index(['area', 'population'], dtype='object') Thus the DataFrame can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for access‐ ing the data. DataFrame as specialized dictionary Similarly, we can also think of a DataFrame as a specialization of a dictionary. Where a dictionary maps a key to a value, a DataFrame maps a column name to a Series of column data. For example, asking for the 'area' attribute returns the Series object containing the areas we saw earlier: In [ 22 ]: states [ 'area' ] Out[22]: California 423967 Florida 170312 Introducing Pandas Objects | 103 Illinois 149995 New York 141297 Texas 695662 Name: area, dtype: int64 Notice the potential point of confusion here: in a two-dimensional NumPy array, data[0] will return the first row . For a DataFrame , data['col0'] will return the first column . Because of this, it is probably better to think about DataFrame s as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa‐ tion can be useful",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_112"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Because of this, it is probably better to think about DataFrame s as generalized dictionaries rather than generalized arrays, though both ways of looking at the situa‐ tion can be useful. We’ll explore more flexible means of indexing DataFrame s in “Data Indexing and Selection” on page 107 . Constructing DataFrame objects A Pandas DataFrame can be constructed in a variety of ways. Here we’ll give several examples. From a single Series object. A DataFrame is a collection of Series objects, and a singlcolumn DataFrame can be constructed from a single Series : In [ 23 ]: pd . DataFrame ( population , columns = [ 'population' ]) Out[23]: population California 38332521 Florida 19552860 Illinois 12882135 New York 19651127 Texas 26448193 From a list of dicts. Any list of dictionaries can be made into a DataFrame . We’ll use a simple list comprehension to create some data: In [ 24 ]: data = [{ 'a' : i , 'b' : 2 * i } for i in range ( 3 )] pd . DataFrame ( data ) Out[24]: a b 0 0 0 1 1 2 2 2 4 Even if some keys in the dictionary are missing, Pandas will fill them in with NaN (i.e., “not a number”) values: In [ 25 ]: pd . DataFrame ([{ 'a' : 1 , 'b' : 2 }, { 'b' : 3 , 'c' : 4 }]) Out[25]: a b c 0 1.0 2 NaN 1 NaN 3 4.0 104 | Chapter 3: Data Manipulation with Pandas From a dictionary of Series objects. As we saw before, a DataFrame can be constructed from a dictionary of Series objects as well: In [ 26 ]: pd . DataFrame ({ 'population' : population , 'area' : area }) Out[26]: area population California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 New York 141297 19651127 Texas 695662 26448193 From a two-dimensional NumPy array. Given a two-dimensional array of data, we can create a DataFrame with any specified column and index names. If omitted, an integer index will be used for each: In [ 27 ]: pd . DataFrame ( np . random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) Out[27]: foo bar a 0.865257 0.213169 b 0.442759 0.108267 c 0.047110 0.905718 From a NumPy structured array",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_113"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . rand ( 3 , 2 ), columns = [ 'foo' , 'bar' ], index = [ 'a' , 'b' , 'c' ]) Out[27]: foo bar a 0.865257 0.213169 b 0.442759 0.108267 c 0.047110 0.905718 From a NumPy structured array. We covered structured arrays in “Structured Data: NumPy’s Structured Arrays” on page 92 . A Pandas DataFrame operates much like a structured array, and can be created directly from one: In [ 28 ]: A = np . zeros ( 3 , dtype = [( 'A' , 'i8' ), ( 'B' , 'f8' )]) A Out[28]: array([(0, 0.0), (0, 0.0), (0, 0.0)], dtype=[('A', '<i8'), ('B', '<f8')]) In [ 29 ]: pd . DataFrame ( A ) Out[29]: A B 0 0 0.0 1 0 0.0 2 0 0.0 The Pandas Index Object We have seen here that both the Series and DataFrame objects contain an explicit index that lets you reference and modify data. This Index object is an interesting structure in itself, and it can be thought of either as an immutable array or as an ordered set (technically a multiset, as Index objects may contain repeated values). Those views have some interesting consequences in the operations available on Index objects. As a simple example, let’s construct an Index from a list of integers: Introducing Pandas Objects | 105 In [ 30 ]: ind = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) ind Out[30]: Int64Index([2, 3, 5, 7, 11], dtype='int64') Index as immutable array The Index object in many ways operates like an array. For example, we can use stan‐ dard Python indexing notation to retrieve values or slices: In [ 31 ]: ind [ 1 ] Out[31]: 3 In [ 32 ]: ind [:: 2 ] Out[32]: Int64Index([2, 5, 11], dtype='int64') Index objects also have many of the attributes familiar from NumPy arrays: In [ 33 ]: print ( ind . size , ind . shape , ind . ndim , ind",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_114"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". size , ind . shape , ind . ndim , ind . dtype ) 5 (5,) 1 int64 One difference between Index objects and NumPy arrays is that indices are immuta‐ ble—that is, they cannot be modified via the normal means: In [ 34 ]: ind [ 1 ] = 0 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-34-40e631c82e8a> in <module>() ----> 1 ind[1] = 0 /Users/jakevdp/anaconda/lib/python3.5/site-packages/pandas/indexes/base.py 1243 1244 def __setitem__(self, key, value): -> 1245 raise TypeError(\"Index does not support mutable operations\") 1246 1247 def __getitem__(self, key): TypeError: Index does not support mutable operations This immutability makes it safer to share indices between multiple DataFrame s and arrays, without the potential for side effects from inadvertent index modification. Index as ordered set Pandas objects are designed to facilitate operations such as joins across datasets, which depend on many aspects of set arithmetic. The Index object follows many of 106 | Chapter 3: Data Manipulation with Pandas the conventions used by Python’s built-in set data structure, so that unions, intersec‐ tions, differences, and other combinations can be computed in a familiar way: In [ 35 ]: indA = pd . Index ([ 1 , 3 , 5 , 7 , 9 ]) indB = pd . Index ([ 2 , 3 , 5 , 7 , 11 ]) In [ 36 ]: indA & indB # intersection Out[36]: Int64Index([3, 5, 7], dtype='int64') In [ 37 ]: indA | indB # union Out[37]: Int64Index([1, 2, 3, 5, 7, 9, 11], dtype='int64') In [ 38 ]: indA ^ indB # symmetric difference Out[38]: Int64Index([1, 2, 9, 11], dtype='int64') These operations may also be accessed via object methods—for example, indA.inter section(indB) . Data Indexing and Selection In Chapter 2 , we looked in detail at methods and tools to access, set, and modify val‐ ues in NumPy arrays",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_115"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Data Indexing and Selection In Chapter 2 , we looked in detail at methods and tools to access, set, and modify val‐ ues in NumPy arrays. These included indexing (e.g., arr[2, 1] ), slicing (e.g., arr[:, 1:5] ), masking (e.g., arr[arr > 0] ), fancy indexing (e.g., arr[0, [1, 5]] ), and combinations thereof (e.g., arr[:, [1, 5]] ). Here we’ll look at similar means of accessing and modifying values in Pandas Series and DataFrame objects. If you have used the NumPy patterns, the corresponding patterns in Pandas will feel very famil‐ iar, though there are a few quirks to be aware of. We’ll start with the simple case of the one-dimensional Series object, and then move on to the more complicated two-dimensional DataFrame object. Data Selection in Series As we saw in the previous section, a Series object acts in many ways like a ondimensional NumPy array, and in many ways like a standard Python dictionary. If we keep these two overlapping analogies in mind, it will help us to understand the pat‐ terns of data indexing and selection in these arrays. Series as dictionary Like a dictionary, the Series object provides a mapping from a collection of keys to a collection of values: In [ 1 ]: import pandas as pd data = pd . Series ([ 0.25 , 0.5 , 0.75 , 1.0 ], index = [ 'a' , 'b' , 'c' , 'd' ]) data Data Indexing and Selection | 107 Out[1]: a 0.25 b 0.50 c 0.75 d 1.00 dtype: float64 In [ 2 ]: data [ 'b' ] Out[2]: 0.5 We can also use dictionary-like Python expressions and methods to examine the keys/indices and values: In [ 3 ]: 'a' in data Out[3]: True In [ 4 ]: data . keys () Out[4]: Index(['a', 'b', 'c', 'd'], dtype='object') In [ 5 ]: list ( data . items ()) Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_116"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". items ()) Out[5]: [('a', 0.25), ('b', 0.5), ('c', 0.75), ('d', 1.0)] Series objects can even be modified with a dictionary-like syntax. Just as you can extend a dictionary by assigning to a new key, you can extend a Series by assigning to a new index value: In [ 6 ]: data [ 'e' ] = 1.25 data Out[6]: a 0.25 b 0.50 c 0.75 d 1.00 e 1.25 dtype: float64 This easy mutability of the objects is a convenient feature: under the hood, Pandas is making decisions about memory layout and data copying that might need to take place; the user generally does not need to worry about these issues. Series as one-dimensional array A Series builds on this dictionary-like interface and provides array-style item selec‐ tion via the same basic mechanisms as NumPy arrays—that is, slices , masking , and fancy indexing . Examples of these are as follows: In [ 7 ]: # slicing by explicit index data [ 'a' : 'c' ] Out[7]: a 0.25 b 0.50 c 0.75 dtype: float64 108 | Chapter 3: Data Manipulation with Pandas In [ 8 ]: # slicing by implicit integer index data [ 0 : 2 ] Out[8]: a 0.25 b 0.50 dtype: float64 In [ 9 ]: # masking data [( data > 0.3 ) & ( data < 0.8 )] Out[9]: b 0.50 c 0.75 dtype: float64 In [ 10 ]: # fancy indexing data [[ 'a' , 'e' ]] Out[10]: a 0.25 e 1.25 dtype: float64 Among these, slicing may be the source of the most confusion. Notice that when you are slicing with an explicit index (i.e., data['a':'c'] ), the final index is included in the slice, while when you’re slicing with an implicit index (i.e., data[0:2] ), the final index is excluded from the slice. Indexers: loc, iloc, and ix These slicing and indexing conventions can be a source of confusion. For example, if your Series has an explicit integer index, an indexing operation such as data[1] will use the explicit indices, while a slicing operation like data[1:3] will use the implicit Python-style index. In [ 11 ]: data = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_117"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In [ 11 ]: data = pd . Series ([ 'a' , 'b' , 'c' ], index = [ 1 , 3 , 5 ]) data Out[11]: 1 a 3 b 5 c dtype: object In [ 12 ]: # explicit index when indexing data [ 1 ] Out[12]: 'a' In [ 13 ]: # implicit index when slicing data [ 1 : 3 ] Out[13]: 3 b 5 c dtype: object Because of this potential confusion in the case of integer indexes, Pandas provides some special indexer attributes that explicitly expose certain indexing schemes. These Data Indexing and Selection | 109 are not functional methods, but attributes that expose a particular slicing interface to the data in the Series . First, the loc attribute allows indexing and slicing that always references the explicit index: In [ 14 ]: data . loc [ 1 ] Out[14]: 'a' In [ 15 ]: data . loc [ 1 : 3 ] Out[15]: 1 a 3 b dtype: object The iloc attribute allows indexing and slicing that always references the implicit Python-style index: In [ 16 ]: data . iloc [ 1 ] Out[16]: 'b' In [ 17 ]: data . iloc [ 1 : 3 ] Out[17]: 3 b 5 c dtype: object A third indexing attribute, ix , is a hybrid of the two, and for Series objects is equiva‐ lent to standard [] -based indexing. The purpose of the ix indexer will become more apparent in the context of DataFrame objects, which we will discuss in a moment. One guiding principle of Python code is that “explicit is better than implicit.” The explicit nature of loc and iloc make them very useful in maintaining clean and read‐ able code; especially in the case of integer indexes, I recommend using these both to make code easier to read and understand, and to prevent subtle bugs due to the mixed indexing/slicing convention. Data Selection in DataFrame Recall that a DataFrame acts in many ways like a two-dimensional or structured array, and in other ways like a dictionary of Series structures sharing the same index. These analogies can be helpful to keep in mind as we explore data selection within this structure. DataFrame as a dictionary The first analogy we will consider is the DataFrame as a dictionary of related Series objects",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_118"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". DataFrame as a dictionary The first analogy we will consider is the DataFrame as a dictionary of related Series objects. Let’s return to our example of areas and populations of states: 110 | Chapter 3: Data Manipulation with Pandas In [ 18 ]: area = pd . Series ({ 'California' : 423967 , 'Texas' : 695662 , 'New York' : 141297 , 'Florida' : 170312 , 'Illinois' : 149995 }) pop = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 , 'Florida' : 19552860 , 'Illinois' : 12882135 }) data = pd . DataFrame ({ 'area' : area , 'pop' : pop }) data Out[18]: area pop California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 New York 141297 19651127 Texas 695662 26448193 The individual Series that make up the columns of the DataFrame can be accessed via dictionary-style indexing of the column name: In [ 19 ]: data [ 'area' ] Out[19]: California 423967 Florida 170312 Illinois 149995 New York 141297 Texas 695662 Name: area, dtype: int64 Equivalently, we can use attribute-style access with column names that are strings: In [ 20 ]: data . area Out[20]: California 423967 Florida 170312 Illinois 149995 New York 141297 Texas 695662 Name: area, dtype: int64 This attribute-style column access actually accesses the exact same object as the dictionary-style access: In [ 21 ]: data . area is data [ 'area' ] Out[21]: True Though this is a useful shorthand, keep in mind that it does not work for all cases! For example, if the column names are not strings, or if the column names conflict with methods of the DataFrame , this attribute-style access is not possible. For exam‐ ple, the DataFrame has a pop() method, so data.pop will point to this rather than the \"pop\" column: In [ 22 ]: data . pop is data [ 'pop' ] Out[22]: False Data Indexing and Selection | 111 In particular, you should avoid the temptation to try column assignment via attribute (i.e., use data['pop'] = z rather than data.pop = z )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_119"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Like with the Series objects discussed earlier, this dictionary-style syntax can also be used to modify the object, in this case to add a new column: In [ 23 ]: data [ 'density' ] = data [ 'pop' ] / data [ 'area' ] data Out[23]: area pop density California 423967 38332521 90.413926 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 New York 141297 19651127 139.076746 Texas 695662 26448193 38.018740 This shows a preview of the straightforward syntax of element-by-element arithmetic between Series objects; we’ll dig into this further in “Operating on Data in Pandas” on page 115 . DataFrame as two-dimensional array As mentioned previously, we can also view the DataFrame as an enhanced twdimensional array. We can examine the raw underlying data array using the values attribute: In [ 24 ]: data . values Out[24]: array([[ 4.23967000e+05, 3.83325210e+07, 9.04139261e+01], [ 1.70312000e+05, 1.95528600e+07, 1.14806121e+02], [ 1.49995000e+05, 1.28821350e+07, 8.58837628e+01], [ 1.41297000e+05, 1.96511270e+07, 1.39076746e+02], [ 6.95662000e+05, 2.64481930e+07, 3.80187404e+01]]) With this picture in mind, we can do many familiar array-like observations on the DataFrame itself. For example, we can transpose the full DataFrame to swap rows and columns: In [ 25 ]: data . T Out[25]: California Florida Illinois New York Texas area 4.239670e+05 1.703120e+05 1.499950e+05 1.412970e+05 6.956620e+05 pop 3.833252e+07 1.955286e+07 1.288214e+07 1.965113e+07 2.644819e+07 density 9.041393e+01 1.148061e+02 8.588376e+01 1.390767e+02 3.801874e+01 When it comes to indexing of DataFrame objects, however, it is clear that the dictionary-style indexing of columns precludes our ability to simply treat it as a NumPy array. In particular, passing a single index to an array accesses a row: In [ 26 ]: data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_120"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, passing a single index to an array accesses a row: In [ 26 ]: data . values [ 0 ] Out[26]: array([ 4.23967000e+05, 3.83325210e+07, 9.04139261e+01]) 112 | Chapter 3: Data Manipulation with Pandas and passing a single “index” to a DataFrame accesses a column: In [ 27 ]: data [ 'area' ] Out[27]: California 423967 Florida 170312 Illinois 149995 New York 141297 Texas 695662 Name: area, dtype: int64 Thus for array-style indexing, we need another convention. Here Pandas again uses the loc , iloc , and ix indexers mentioned earlier. Using the iloc indexer, we can index the underlying array as if it is a simple NumPy array (using the implicit Python-style index), but the DataFrame index and column labels are maintained in the result: In [ 28 ]: data . iloc [: 3 , : 2 ] Out[28]: area pop California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 In [ 29 ]: data . loc [: 'Illinois' , : 'pop' ] Out[29]: area pop California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 The ix indexer allows a hybrid of these two approaches: In [ 30 ]: data . ix [: 3 , : 'pop' ] Out[30]: area pop California 423967 38332521 Florida 170312 19552860 Illinois 149995 12882135 Keep in mind that for integer indices, the ix indexer is subject to the same potential sources of confusion as discussed for integer-indexed Series objects. Any of the familiar NumPy-style data access patterns can be used within these index‐ ers. For example, in the loc indexer we can combine masking and fancy indexing as in the following: In [ 31 ]: data . loc [ data . density > 100 , [ 'pop' , 'density' ]] Out[31]: pop density Florida 19552860 114.806121 New York 19651127 139.076746 Data Indexing and Selection | 113 Any of these indexing conventions may also be used to set or modify values; this is done in the standard way that you might be accustomed to from working with NumPy: In [ 32 ]: data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_121"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". iloc [ 0 , 2 ] = 90 data Out[32]: area pop density California 423967 38332521 90.000000 Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 New York 141297 19651127 139.076746 Texas 695662 26448193 38.018740 To build up your fluency in Pandas data manipulation, I suggest spending some time with a simple DataFrame and exploring the types of indexing, slicing, masking, and fancy indexing that are allowed by these various indexing approaches. Additional indexing conventions There are a couple extra indexing conventions that might seem at odds with the pre‐ ceding discussion, but nevertheless can be very useful in practice. First, while index‐ ing refers to columns, slicing refers to rows: In [ 33 ]: data [ 'Florida' : 'Illinois' ] Out[33]: area pop density Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 Such slices can also refer to rows by number rather than by index: In [ 34 ]: data [ 1 : 3 ] Out[34]: area pop density Florida 170312 19552860 114.806121 Illinois 149995 12882135 85.883763 Similarly, direct masking operations are also interpreted row-wise rather than column-wise: In [ 35 ]: data [ data . density > 100 ] Out[35]: area pop density Florida 170312 19552860 114.806121 New York 141297 19651127 139.076746 These two conventions are syntactically similar to those on a NumPy array, and while these may not precisely fit the mold of the Pandas conventions, they are nevertheless quite useful in practice. 114 | Chapter 3: Data Manipulation with Pandas Operating on Data in Pandas One of the essential pieces of NumPy is the ability to perform quick element-wise operations, both with basic arithmetic (addition, subtraction, multiplication, etc.) and with more sophisticated operations (trigonometric functions, exponential and loga‐ rithmic functions, etc.). Pandas inherits much of this functionality from NumPy, and the ufuncs that we introduced in “Computation on NumPy Arrays: Universal Func‐ tions” on page 50 are key to this",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_122"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Pandas inherits much of this functionality from NumPy, and the ufuncs that we introduced in “Computation on NumPy Arrays: Universal Func‐ tions” on page 50 are key to this. Pandas includes a couple useful twists, however: for unary operations like negation and trigonometric functions, these ufuncs will preserve index and column labels in the output, and for binary operations such as addition and multiplication, Pandas will automatically align indices when passing the objects to the ufunc. This means that keeping the context of data and combining data from different sources—both poten‐ tially error-prone tasks with raw NumPy arrays—become essentially foolproof ones with Pandas. We will additionally see that there are well-defined operations between one-dimensional Series structures and two-dimensional DataFrame structures. Ufuncs: Index Preservation Because Pandas is designed to work with NumPy, any NumPy ufunc will work on Pandas Series and DataFrame objects. Let’s start by defining a simple Series and DataFrame on which to demonstrate this: In [ 1 ]: import pandas as pd import numpy as np In [ 2 ]: rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . randint ( 0 , 10 , 4 )) ser Out[2]: 0 6 1 3 2 7 3 4 dtype: int64 In [ 3 ]: df = pd . DataFrame ( rng . randint ( 0 , 10 , ( 3 , 4 )), columns = [ 'A' , 'B' , 'C' , 'D' ]) df Out[3]: A B C D 0 6 9 2 6 1 7 4 3 7 2 7 2 5 4 If we apply a NumPy ufunc on either of these objects, the result will be another Pan‐ das object with the indices preserved: In [ 4 ]: np . exp ( ser ) Operating on Data in Pandas | 115 Out[4]: 0 403.428793 1 20.085537 2 1096.633158 3 54.598150 dtype: float64 Or, for a slightly more complex calculation: In [ 5 ]: np . sin ( df * np . pi / 4 ) Out[5]: A B C D 0 -1.000000 7.071068e-01 1.000000 -1.000000e+00 1 -0.707107 1.224647e-16 0.707107 -7.071068e-01 2 -0.707107 1.000000e+00 -0.707107 1.224647e-16 Any of the ufuncs discussed in “Computation on NumPy Arrays: Universal Func‐ tions” on page 50 can be used in a similar manner",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_123"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". UFuncs: Index Alignment For binary operations on two Series or DataFrame objects, Pandas will align indices in the process of performing the operation. This is very convenient when you are working with incomplete data, as we’ll see in some of the examples that follow. Index alignment in Series As an example, suppose we are combining two different data sources, and find only the top three US states by area and the top three US states by population : In [ 6 ]: area = pd . Series ({ 'Alaska' : 1723337 , 'Texas' : 695662 , 'California' : 423967 }, name = 'area' ) population = pd . Series ({ 'California' : 38332521 , 'Texas' : 26448193 , 'New York' : 19651127 }, name = 'population' ) Let’s see what happens when we divide these to compute the population density: In [ 7 ]: population / area Out[7]: Alaska NaN California 90.413926 New York NaN Texas 38.018740 dtype: float64 The resulting array contains the union of indices of the two input arrays, which we could determine using standard Python set arithmetic on these indices: In [ 8 ]: area . index | population . index Out[8]: Index(['Alaska', 'California', 'New York', 'Texas'], dtype='object') Any item for which one or the other does not have an entry is marked with NaN , or “Not a Number,” which is how Pandas marks missing data (see further discussion of missing data in “Handling Missing Data” on page 119 ). This index matching is imple‐ 116 | Chapter 3: Data Manipulation with Pandas mented this way for any of Python’s built-in arithmetic expressions; any missing val‐ ues are filled in with NaN by default: In [ 9 ]: A = pd . Series ([ 2 , 4 , 6 ], index = [ 0 , 1 , 2 ]) B = pd . Series ([ 1 , 3 , 5 ], index = [ 1 , 2 , 3 ]) A + B Out[9]: 0 NaN 1 5.0 2 9.0 3 NaN dtype: float64 If using NaN values is not the desired behavior, we can modify the fill value using appropriate object methods in place of the operators",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_124"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, calling A.add(B) is equivalent to calling A + B , but allows optional explicit specification of the fill value for any elements in A or B that might be missing: In [ 10 ]: A . add ( B , fill_value = 0 ) Out[10]: 0 2.0 1 5.0 2 9.0 3 5.0 dtype: float64 Index alignment in DataFrame A similar type of alignment takes place for both columns and indices when you are performing operations on DataFrame s: In [ 11 ]: A = pd . DataFrame ( rng . randint ( 0 , 20 , ( 2 , 2 )), columns = list ( 'AB' )) A Out[11]: A B 0 1 11 1 5 1 In [ 12 ]: B = pd . DataFrame ( rng . randint ( 0 , 10 , ( 3 , 3 )), columns = list ( 'BAC' )) B Out[12]: B A C 0 4 0 9 1 5 8 0 2 9 2 6 In [ 13 ]: A + B Out[13]: A B C 0 1.0 15.0 NaN 1 13.0 6.0 NaN 2 NaN NaN NaN Operating on Data in Pandas | 117 Notice that indices are aligned correctly irrespective of their order in the two objects, and indices in the result are sorted. As was the case with Series , we can use the asso‐ ciated object’s arithmetic method and pass any desired fill_value to be used in place of missing entries. Here we’ll fill with the mean of all values in A (which we compute by first stacking the rows of A ): In [ 14 ]: fill = A . stack () . mean () A . add ( B , fill_value = fill ) Out[14]: A B C 0 1.0 15.0 13.5 1 13.0 6.0 4.5 2 6.5 13.5 10.5 Table 3-1 lists Python operators and their equivalent Pandas object methods. Table 3-1. Mapping between Python operators and Pandas methods Python operator Pandas method(s) + add() - sub() , subtract() * mul() , multiply() / truediv() , div() , divide() // floordiv() % mod() ** pow() Ufuncs: Operations Between DataFrame and Series When you are performing operations between a DataFrame and a Series , the index and column alignment is similarly maintained. Operations between a DataFrame and a Series are similar to operations between a two-dimensional and one-dimensional NumPy array. Consider one common operation, where we find the difference of a two-dimensional array and one of its rows: In [ 15 ]: A = rng",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_125"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Consider one common operation, where we find the difference of a two-dimensional array and one of its rows: In [ 15 ]: A = rng . randint ( 10 , size = ( 3 , 4 )) A Out[15]: array([[3, 8, 2, 4], [2, 6, 4, 8], [6, 1, 3, 8]]) In [ 16 ]: A - A [ 0 ] Out[16]: array([[ 0, 0, 0, 0], [-1, -2, 2, 4], [ 3, -7, 1, 4]]) 118 | Chapter 3: Data Manipulation with Pandas According to NumPy’s broadcasting rules (see “Computation on Arrays: Broadcast‐ ing” on page 63 ), subtraction between a two-dimensional array and one of its rows is applied row-wise. In Pandas, the convention similarly operates row-wise by default: In [ 17 ]: df = pd . DataFrame ( A , columns = list ( 'QRST' )) df - df . iloc [ 0 ] Out[17]: Q R S T 0 0 0 0 0 1 -1 -2 2 4 2 3 -7 1 4 If you would instead like to operate column-wise, you can use the object methods mentioned earlier, while specifying the axis keyword: In [ 18 ]: df . subtract ( df [ 'R' ], axis = 0 ) Out[18]: Q R S T 0 -5 0 -6 -4 1 -4 0 -2 2 2 5 0 2 7 Note that these DataFrame / Series operations, like the operations discussed before, will automatically align indices between the two elements: In [ 19 ]: halfrow = df . iloc [ 0 , :: 2 ] halfrow Out[19]: Q 3 S 2 Name: 0, dtype: int64 In [ 20 ]: df - halfrow Out[20]: Q R S T 0 0.0 NaN 0.0 NaN 1 -1.0 NaN 2.0 NaN 2 3.0 NaN 1.0 NaN This preservation and alignment of indices and columns means that operations on data in Pandas will always maintain the data context, which prevents the types of silly errors that might come up when you are working with heterogeneous and/or mis‐ aligned data in raw NumPy arrays. Handling Missing Data The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous. In particular, many interesting datasets will have some amount of data missing. To make matters even more compli‐ cated, different data sources may indicate missing data in different ways",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_126"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, many interesting datasets will have some amount of data missing. To make matters even more compli‐ cated, different data sources may indicate missing data in different ways. Handling Missing Data | 119 In this section, we will discuss some general considerations for missing data, discuss how Pandas chooses to represent it, and demonstrate some built-in Pandas tools for handling missing data in Python. Here and throughout the book, we’ll refer to miss‐ ing data in general as null , NaN , or NA values. Trade-Offs in Missing Data Conventions A number of schemes have been developed to indicate the presence of missing data in a table or DataFrame . Generally, they revolve around one of two strategies: using a mask that globally indicates missing values, or choosing a sentinel value that indicates a missing entry. In the masking approach, the mask might be an entirely separate Boolean array, or it may involve appropriation of one bit in the data representation to locally indicate the null status of a value. In the sentinel approach, the sentinel value could be some data-specific convention, such as indicating a missing integer value with –9999 or some rare bit pattern, or it could be a more global convention, such as indicating a missing floating-point value with NaN (Not a Number), a special value which is part of the IEEE floating-point specification. None of these approaches is without trade-offs: use of a separate mask array requires allocation of an additional Boolean array, which adds overhead in both storage and computation. A sentinel value reduces the range of valid values that can be repre‐ sented, and may require extra (often non-optimized) logic in CPU and GPU arith‐ metic. Common special values like NaN are not available for all data types. As in most cases where no universally optimal choice exists, different languages and systems use different conventions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_127"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Common special values like NaN are not available for all data types. As in most cases where no universally optimal choice exists, different languages and systems use different conventions. For example, the R language uses reserved bit pat‐ terns within each data type as sentinel values indicating missing data, while the SciDB system uses an extra byte attached to every cell to indicate a NA state. Missing Data in Pandas The way in which Pandas handles missing values is constrained by its reliance on the NumPy package, which does not have a built-in notion of NA values for nofloating-point data types. Pandas could have followed R’s lead in specifying bit patterns for each individual data type to indicate nullness, but this approach turns out to be rather unwieldy. While R contains four basic data types, NumPy supports far more than this: for example, while R has a single integer type, NumPy supports fourteen basic integer types once you account for available precisions, signedness, and endianness of the encoding. Reserving a specific bit pattern in all available NumPy types would lead to an unwieldy amount of overhead in special-casing various operations for various types, 120 | Chapter 3: Data Manipulation with Pandas likely even requiring a new fork of the NumPy package. Further, for the smaller data types (such as 8-bit integers), sacrificing a bit to use as a mask will significantly reduce the range of values it can represent. NumPy does have support for masked arrays—that is, arrays that have a separate Boolean mask array attached for marking data as “good” or “bad.” Pandas could have derived from this, but the overhead in both storage, computation, and code mainte‐ nance makes that an unattractive choice. With these constraints in mind, Pandas chose to use sentinels for missing data, and further chose to use two already-existing Python null values: the special floatinpoint NaN value, and the Python None object",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_128"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This choice has some side effects, as we will see, but in practice ends up being a good compromise in most cases of interest. None: Pythonic missing data The first sentinel value used by Pandas is None , a Python singleton object that is often used for missing data in Python code. Because None is a Python object, it cannot be used in any arbitrary NumPy/Pandas array, but only in arrays with data type 'object' (i.e., arrays of Python objects): In [ 1 ]: import numpy as np import pandas as pd In [ 2 ]: vals1 = np . array ([ 1 , None , 3 , 4 ]) vals1 Out[2]: array([1, None, 3, 4], dtype=object) This dtype=object means that the best common type representation NumPy could infer for the contents of the array is that they are Python objects. While this kind of object array is useful for some purposes, any operations on the data will be done at the Python level, with much more overhead than the typically fast operations seen for arrays with native types: In [ 3 ]: for dtype in [ 'object' , 'int' ]: print ( \"dtype =\" , dtype ) % timeit np . arange ( 1E6 , dtype = dtype ) . sum () print () dtype = object 10 loops, best of 3: 78.2 ms per loop dtype = int 100 loops, best of 3: 3.06 ms per loop The use of Python objects in an array also means that if you perform aggregations like sum() or min() across an array with a None value, you will generally get an error: Handling Missing Data | 121 In [ 4 ]: vals1 . sum () TypeError Traceback (most recent call last) <ipython-input-4-749fd8ae6030> in <module>() ----> 1 vals1.sum() /Users/jakevdp/anaconda/lib/python3.5/site-packages/numpy/core/_methods.py 30 31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False): ---> 32 return umr_sum(a, axis, dtype, out, keepdims) 33 34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False): TypeError: unsupported operand type(s) for +: 'int' and 'NoneType' This reflects the fact that addition between an integer and None is undefined",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_129"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". NaN: Missing numerical data The other missing data representation, NaN (acronym for Not a Number ), is different; it is a special floating-point value recognized by all systems that use the standard IEEE floating-point representation: In [ 5 ]: vals2 = np . array ([ 1 , np . nan , 3 , 4 ]) vals2 . dtype Out[5]: dtype('float64') Notice that NumPy chose a native floating-point type for this array: this means that unlike the object array from before, this array supports fast operations pushed into compiled code. You should be aware that NaN is a bit like a data virus—it infects any other object it touches. Regardless of the operation, the result of arithmetic with NaN will be another NaN : In [ 6 ]: 1 + np . nan Out[6]: nan In [ 7 ]: 0 * np . nan Out[7]: nan Note that this means that aggregates over the values are well defined (i.e., they don’t result in an error) but not always useful: In [ 8 ]: vals2 . sum (), vals2 . min (), vals2 . max () Out[8]: (nan, nan, nan) NumPy does provide some special aggregations that will ignore these missing values: 122 | Chapter 3: Data Manipulation with Pandas In [ 9 ]: np . nansum ( vals2 ), np . nanmin ( vals2 ), np . nanmax ( vals2 ) Out[9]: (8.0, 1.0, 4.0) Keep in mind that NaN is specifically a floating-point value; there is no equivalent NaN value for integers, strings, or other types. NaN and None in Pandas NaN and None both have their place, and Pandas is built to handle the two of them nearly interchangeably, converting between them where appropriate: In [ 10 ]: pd . Series ([ 1 , np . nan , 2 , None ]) Out[10]: 0 1.0 1 NaN 2 2.0 3 NaN dtype: float64 For types that don’t have an available sentinel value, Pandas automatically type-casts when NA values are present. For example, if we set a value in an integer array to np.nan , it will automatically be upcast to a floating-point type to accommodate the NA: In [ 11 ]: x = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_130"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, if we set a value in an integer array to np.nan , it will automatically be upcast to a floating-point type to accommodate the NA: In [ 11 ]: x = pd . Series ( range ( 2 ), dtype = int ) x Out[11]: 0 0 1 1 dtype: int64 In [ 12 ]: x [ 0 ] = None x Out[12]: 0 NaN 1 1.0 dtype: float64 Notice that in addition to casting the integer array to floating point, Pandas automati‐ cally converts the None to a NaN value. (Be aware that there is a proposal to add a native integer NA to Pandas in the future; as of this writing, it has not been included.) While this type of magic may feel a bit hackish compared to the more unified approach to NA values in domain-specific languages like R, the Pandas sentinel/cast‐ ing approach works quite well in practice and in my experience only rarely causes issues. Table 3-2 lists the upcasting conventions in Pandas when NA values are introduced. Handling Missing Data | 123 Table 3-2. Pandas handling of NAs by type Typeclass Conversion when storing NAs NA sentinel value floating No change np.nan object No change None or np.nan integer Cast to float64 np.nan boolean Cast to object None or np.nan Keep in mind that in Pandas, string data is always stored with an object dtype. Operating on Null Values As we have seen, Pandas treats None and NaN as essentially interchangeable for indi‐ cating missing or null values. To facilitate this convention, there are several useful methods for detecting, removing, and replacing null values in Pandas data structures. They are: isnull() Generate a Boolean mask indicating missing values notnull() Opposite of isnull() dropna() Return a filtered version of the data fillna() Return a copy of the data with missing values filled or imputed We will conclude this section with a brief exploration and demonstration of these routines. Detecting null values Pandas data structures have two useful methods for detecting null data: isnull() and notnull() . Either one will return a Boolean mask over the data. For example: In [ 13 ]: data = pd . Series ([ 1 , np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_131"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Either one will return a Boolean mask over the data. For example: In [ 13 ]: data = pd . Series ([ 1 , np . nan , 'hello' , None ]) In [ 14 ]: data . isnull () Out[14]: 0 False 1 True 2 False 3 True dtype: bool As mentioned in “Data Indexing and Selection” on page 107 , Boolean masks can be used directly as a Series or DataFrame index: 124 | Chapter 3: Data Manipulation with Pandas In [ 15 ]: data [ data . notnull ()] Out[15]: 0 1 2 hello dtype: object The isnull() and notnull() methods produce similar Boolean results for Data Frame s. Dropping null values In addition to the masking used before, there are the convenience methods, dropna() (which removes NA values) and fillna() (which fills in NA values). For a Series , the result is straightforward: In [ 16 ]: data . dropna () Out[16]: 0 1 2 hello dtype: object For a DataFrame , there are more options. Consider the following DataFrame : In [ 17 ]: df = pd . DataFrame ([[ 1 , np . nan , 2 ], [ 2 , 3 , 5 ], [ np . nan , 4 , 6 ]]) df Out[17]: 0 1 2 0 1.0 NaN 2 1 2.0 3.0 5 2 NaN 4.0 6 We cannot drop single values from a DataFrame ; we can only drop full rows or full columns. Depending on the application, you might want one or the other, so dropna() gives a number of options for a DataFrame . By default, dropna() will drop all rows in which any null value is present: In [ 18 ]: df . dropna () Out[18]: 0 1 2 1 2.0 3.0 5 Alternatively, you can drop NA values along a different axis; axis=1 drops all col‐ umns containing a null value: In [ 19 ]: df . dropna ( axis = 'columns' ) Out[19]: 2 0 2 1 5 2 6 Handling Missing Data | 125 But this drops some good data as well; you might rather be interested in dropping rows or columns with all NA values, or a majority of NA values. This can be specified through the how or thresh parameters, which allow fine control of the number of nulls to allow through. The default is how='any' , such that any row or column (depending on the axis key‐ word) containing a null value will be dropped",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_132"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The default is how='any' , such that any row or column (depending on the axis key‐ word) containing a null value will be dropped. You can also specify how='all' , which will only drop rows/columns that are all null values: In [ 20 ]: df [ 3 ] = np . nan df Out[20]: 0 1 2 3 0 1.0 NaN 2 NaN 1 2.0 3.0 5 NaN 2 NaN 4.0 6 NaN In [ 21 ]: df . dropna ( axis = 'columns' , how = 'all' ) Out[21]: 0 1 2 0 1.0 NaN 2 1 2.0 3.0 5 2 NaN 4.0 6 For finer-grained control, the thresh parameter lets you specify a minimum number of non-null values for the row/column to be kept: In [ 22 ]: df . dropna ( axis = 'rows' , thresh = 3 ) Out[22]: 0 1 2 3 1 2.0 3.0 5 NaN Here the first and last row have been dropped, because they contain only two nonull values. Filling null values Sometimes rather than dropping NA values, you’d rather replace them with a valid value. This value might be a single number like zero, or it might be some sort of imputation or interpolation from the good values. You could do this in-place using the isnull() method as a mask, but because it is such a common operation Pandas provides the fillna() method, which returns a copy of the array with the null values replaced. Consider the following Series : In [ 23 ]: data = pd . Series ([ 1 , np . nan , 2 , None , 3 ], index = list ( 'abcde' )) data Out[23]: a 1.0 b NaN c 2.0 d NaN 126 | Chapter 3: Data Manipulation with Pandas e 3.0 dtype: float64 We can fill NA entries with a single value, such as zero: In [ 24 ]: data . fillna ( 0 ) Out[24]: a 1.0 b 0.0 c 2.0 d 0.0 e 3.0 dtype: float64 We can specify a forward-fill to propagate the previous value forward: In [ 25 ]: # forward-fill data . fillna ( method = 'ffill' ) Out[25]: a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 Or we can specify a back-fill to propagate the next values backward: In [ 26 ]: # back-fill data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_133"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fillna ( method = 'ffill' ) Out[25]: a 1.0 b 1.0 c 2.0 d 2.0 e 3.0 dtype: float64 Or we can specify a back-fill to propagate the next values backward: In [ 26 ]: # back-fill data . fillna ( method = 'bfill' ) Out[26]: a 1.0 b 2.0 c 2.0 d 3.0 e 3.0 dtype: float64 For DataFrame s, the options are similar, but we can also specify an axis along which the fills take place: In [ 27 ]: df Out[27]: 0 1 2 3 0 1.0 NaN 2 NaN 1 2.0 3.0 5 NaN 2 NaN 4.0 6 NaN In [ 28 ]: df . fillna ( method = 'ffill' , axis = 1 ) Out[28]: 0 1 2 3 0 1.0 1.0 2.0 2.0 1 2.0 3.0 5.0 5.0 2 NaN 4.0 6.0 6.0 Notice that if a previous value is not available during a forward fill, the NA value remains. Handling Missing Data | 127 Hierarchical Indexing Up to this point we’ve been focused primarily on one-dimensional and twdimensional data, stored in Pandas Series and DataFrame objects, respectively. Often it is useful to go beyond this and store higher-dimensional data—that is, data indexed by more than one or two keys. While Pandas does provide Panel and Panel4D objects that natively handle three-dimensional and four-dimensional data (see “Panel Data” on page 141 ), a far more common pattern in practice is to make use of hierarchical indexing (also known as multi-indexing ) to incorporate multiple index levels within a single index. In this way, higher-dimensional data can be compactly represented within the familiar one-dimensional Series and two-dimensional DataFrame objects. In this section, we’ll explore the direct creation of MultiIndex objects; considerations around indexing, slicing, and computing statistics across multiply indexed data; and useful routines for converting between simple and hierarchically indexed representa‐ tions of your data. We begin with the standard imports: In [ 1 ]: import pandas as pd import numpy as np A Multiply Indexed Series Let’s start by considering how we might represent two-dimensional data within a one-dimensional Series",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_134"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For concreteness, we will consider a series of data where each point has a character and numerical key. The bad way Suppose you would like to track data about states from two different years. Using the Pandas tools we’ve already covered, you might be tempted to simply use Python tuples as keys: In [ 2 ]: index = [( 'California' , 2000 ), ( 'California' , 2010 ), ( 'New York' , 2000 ), ( 'New York' , 2010 ), ( 'Texas' , 2000 ), ( 'Texas' , 2010 )] populations = [ 33871648 , 37253956 , 18976457 , 19378102 , 20851820 , 25145561 ] pop = pd . Series ( populations , index = index ) pop Out[2]: (California, 2000) 33871648 (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 128 | Chapter 3: Data Manipulation with Pandas (Texas, 2010) 25145561 dtype: int64 With this indexing scheme, you can straightforwardly index or slice the series based on this multiple index: In [ 3 ]: pop [( 'California' , 2010 ):( 'Texas' , 2000 )] Out[3]: (California, 2010) 37253956 (New York, 2000) 18976457 (New York, 2010) 19378102 (Texas, 2000) 20851820 dtype: int64 But the convenience ends there. For example, if you need to select all values from 2010, you’ll need to do some messy (and potentially slow) munging to make it happen: In [ 4 ]: pop [[ i for i in pop . index if i [ 1 ] == 2010 ]] Out[4]: (California, 2010) 37253956 (New York, 2010) 19378102 (Texas, 2010) 25145561 dtype: int64 This produces the desired result, but is not as clean (or as efficient for large datasets) as the slicing syntax we’ve grown to love in Pandas. The better way: Pandas MultiIndex Fortunately, Pandas provides a better way. Our tuple-based indexing is essentially a rudimentary multi-index, and the Pandas MultiIndex type gives us the type of opera‐ tions we wish to have. We can create a multi-index from the tuples as follows: In [ 5 ]: index = pd . MultiIndex",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_135"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We can create a multi-index from the tuples as follows: In [ 5 ]: index = pd . MultiIndex . from_tuples ( index ) index Out[5]: MultiIndex(levels=[['California', 'New York', 'Texas'], [2000, 2010]], labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]]) Notice that the MultiIndex contains multiple levels of indexing—in this case, the state names and the years, as well as multiple labels for each data point which encode these levels. If we reindex our series with this MultiIndex , we see the hierarchical representation of the data: In [ 6 ]: pop = pop . reindex ( index ) pop Out[6]: California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Hierarchical Indexing | 129 Texas 2000 20851820 2010 25145561 dtype: int64 Here the first two columns of the Series representation show the multiple index val‐ ues, while the third column shows the data. Notice that some entries are missing in the first column: in this multi-index representation, any blank entry indicates the same value as the line above it. Now to access all data for which the second index is 2010, we can simply use the Pan‐ das slicing notation: In [ 7 ]: pop [:, 2010 ] Out[7]: California 37253956 New York 19378102 Texas 25145561 dtype: int64 The result is a singly indexed array with just the keys we’re interested in. This syntax is much more convenient (and the operation is much more efficient!) than the homspun tuple-based multi-indexing solution that we started with. We’ll now further dis‐ cuss this sort of indexing operation on hierarchically indexed data. MultiIndex as extra dimension You might notice something else here: we could easily have stored the same data using a simple DataFrame with index and column labels. In fact, Pandas is built with this equivalence in mind. The unstack() method will quickly convert a multiplindexed Series into a conventionally indexed DataFrame : In [ 8 ]: pop_df = pop",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_136"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In fact, Pandas is built with this equivalence in mind. The unstack() method will quickly convert a multiplindexed Series into a conventionally indexed DataFrame : In [ 8 ]: pop_df = pop . unstack () pop_df Out[8]: 2000 2010 California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 Naturally, the stack() method provides the opposite operation: In [ 9 ]: pop_df . stack () Out[9]: California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 Seeing this, you might wonder why would we would bother with hierarchical index‐ ing at all. The reason is simple: just as we were able to use multi-indexing to represent 130 | Chapter 3: Data Manipulation with Pandas two-dimensional data within a one-dimensional Series , we can also use it to repre‐ sent data of three or more dimensions in a Series or DataFrame . Each extra level in a multi-index represents an extra dimension of data; taking advantage of this property gives us much more flexibility in the types of data we can represent. Concretely, we might want to add another column of demographic data for each state at each year (say, population under 18); with a MultiIndex this is as easy as adding another col‐ umn to the DataFrame : In [ 10 ]: pop_df = pd . DataFrame ({ 'total' : pop , 'under18' : [ 9267089 , 9284094 , 4687374 , 4318033 , 5906301 , 6879014 ]}) pop_df Out[10]: total under18 California 2000 33871648 9267089 2010 37253956 9284094 New York 2000 18976457 4687374 2010 19378102 4318033 Texas 2000 20851820 5906301 2010 25145561 6879014 In addition, all the ufuncs and other functionality discussed in “Operating on Data in Pandas” on page 115 work with hierarchical indices as well. Here we compute the fraction of people under 18 by year, given the above data: In [ 11 ]: f_u18 = pop_df [ 'under18' ] / pop_df [ 'total' ] f_u18",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_137"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here we compute the fraction of people under 18 by year, given the above data: In [ 11 ]: f_u18 = pop_df [ 'under18' ] / pop_df [ 'total' ] f_u18 . unstack () Out[11]: 2000 2010 California 0.273594 0.249211 New York 0.247010 0.222831 Texas 0.283251 0.273568 This allows us to easily and quickly manipulate and explore even high-dimensional data. Methods of MultiIndex Creation The most straightforward way to construct a multiply indexed Series or DataFrame is to simply pass a list of two or more index arrays to the constructor. For example: In [ 12 ]: df = pd . DataFrame ( np . random . rand ( 4 , 2 ), index = [[ 'a' , 'a' , 'b' , 'b' ], [ 1 , 2 , 1 , 2 ]], columns = [ 'data1' , 'data2' ]) df Out[12]: data1 data2 a 1 0.554233 0.356072 2 0.925244 0.219474 b 1 0.441759 0.610054 2 0.171495 0.886688 Hierarchical Indexing | 131 The work of creating the MultiIndex is done in the background. Similarly, if you pass a dictionary with appropriate tuples as keys, Pandas will auto‐ matically recognize this and use a MultiIndex by default: In [ 13 ]: data = {( 'California' , 2000 ): 33871648 , ( 'California' , 2010 ): 37253956 , ( 'Texas' , 2000 ): 20851820 , ( 'Texas' , 2010 ): 25145561 , ( 'New York' , 2000 ): 18976457 , ( 'New York' , 2010 ): 19378102 } pd . Series ( data ) Out[13]: California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 Nevertheless, it is sometimes useful to explicitly create a MultiIndex ; we’ll see a cou‐ ple of these methods here. Explicit MultiIndex constructors For more flexibility in how the index is constructed, you can instead use the class method constructors available in the pd.MultiIndex . For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: In [ 14 ]: pd . MultiIndex",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_138"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, as we did before, you can construct the MultiIndex from a simple list of arrays, giving the index values within each level: In [ 14 ]: pd . MultiIndex . from_arrays ([[ 'a' , 'a' , 'b' , 'b' ], [ 1 , 2 , 1 , 2 ]]) Out[14]: MultiIndex(levels=[['a', 'b'], [1, 2]], labels=[[0, 0, 1, 1], [0, 1, 0, 1]]) You can construct it from a list of tuples, giving the multiple index values of each point: In [ 15 ]: pd . MultiIndex . from_tuples ([( 'a' , 1 ), ( 'a' , 2 ), ( 'b' , 1 ), ( 'b' , 2 )]) Out[15]: MultiIndex(levels=[['a', 'b'], [1, 2]], labels=[[0, 0, 1, 1], [0, 1, 0, 1]]) You can even construct it from a Cartesian product of single indices: In [ 16 ]: pd . MultiIndex . from_product ([[ 'a' , 'b' ], [ 1 , 2 ]]) Out[16]: MultiIndex(levels=[['a', 'b'], [1, 2]], labels=[[0, 0, 1, 1], [0, 1, 0, 1]]) Similarly, you can construct the MultiIndex directly using its internal encoding by passing levels (a list of lists containing available index values for each level) and labels (a list of lists that reference these labels): 132 | Chapter 3: Data Manipulation with Pandas In [ 17 ]: pd . MultiIndex ( levels = [[ 'a' , 'b' ], [ 1 , 2 ]], labels = [[ 0 , 0 , 1 , 1 ], [ 0 , 1 , 0 , 1 ]]) Out[17]: MultiIndex(levels=[['a', 'b'], [1, 2]], labels=[[0, 0, 1, 1], [0, 1, 0, 1]]) You can pass any of these objects as the index argument when creating a Series or DataFrame , or to the reindex method of an existing Series or DataFrame . MultiIndex level names Sometimes it is convenient to name the levels of the MultiIndex . You can accomplish this by passing the names argument to any of the above MultiIndex constructors, or by setting the names attribute of the index after the fact: In [ 18 ]: pop . index . names = [ 'state' , 'year' ] pop Out[18]: state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 With more involved datasets, this can be a useful way to keep track of the meaning of various index values",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_139"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". MultiIndex for columns In a DataFrame , the rows and columns are completely symmetric, and just as the rows can have multiple levels of indices, the columns can have multiple levels as well. Con‐ sider the following, which is a mock-up of some (somewhat realistic) medical data: In [ 19 ]: # hierarchical indices and columns index = pd . MultiIndex . from_product ([[ 2013 , 2014 ], [ 1 , 2 ]], names = [ 'year' , 'visit' ]) columns = pd . MultiIndex . from_product ([[ 'Bob' , 'Guido' , 'Sue' ], [ 'HR' , 'Temp' ]], names = [ 'subject' , 'type' ]) # mock some data data = np . round ( np . random . randn ( 4 , 6 ), 1 ) data [:, :: 2 ] *= 10 data += 37 # create the DataFrame health_data = pd . DataFrame ( data , index = index , columns = columns ) health_data Hierarchical Indexing | 133 Out[19]: subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 31.0 38.7 32.0 36.7 35.0 37.2 2 44.0 37.7 50.0 35.0 29.0 36.7 2014 1 30.0 37.4 39.0 37.8 61.0 36.9 2 47.0 37.8 48.0 37.3 51.0 36.5 Here we see where the multi-indexing for both rows and columns can come in very handy. This is fundamentally four-dimensional data, where the dimensions are the subject, the measurement type, the year, and the visit number. With this in place we can, for example, index the top-level column by the person’s name and get a full Data Frame containing just that person’s information: In [ 20 ]: health_data [ 'Guido' ] Out[20]: type HR Temp year visit 2013 1 32.0 36.7 2 50.0 35.0 2014 1 39.0 37.8 2 48.0 37.3 For complicated records containing multiple labeled measurements across multiple times for many subjects (people, countries, cities, etc.), use of hierarchical rows and columns can be extremely convenient! Indexing and Slicing a MultiIndex Indexing and slicing on a MultiIndex is designed to be intuitive, and it helps if you think about the indices as added dimensions. We’ll first look at indexing multiply indexed Series , and then multiply indexed DataFrame s",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_140"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We’ll first look at indexing multiply indexed Series , and then multiply indexed DataFrame s. Multiply indexed Series Consider the multiply indexed Series of state populations we saw earlier: In [ 21 ]: pop Out[21]: state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 We can access single elements by indexing with multiple terms: In [ 22 ]: pop [ 'California' , 2000 ] Out[22]: 33871648 134 | Chapter 3: Data Manipulation with Pandas The MultiIndex also supports partial indexing , or indexing just one of the levels in the index. The result is another Series , with the lower-level indices maintained: In [ 23 ]: pop [ 'California' ] Out[23]: year 2000 33871648 2010 37253956 dtype: int64 Partial slicing is available as well, as long as the MultiIndex is sorted (see discussion in “Sorted and unsorted indices” on page 137 ): In [ 24 ]: pop . loc [ 'California' : 'New York' ] Out[24]: state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 dtype: int64 With sorted indices, we can perform partial indexing on lower levels by passing an empty slice in the first index: In [ 25 ]: pop [:, 2000 ] Out[25]: state California 33871648 New York 18976457 Texas 20851820 dtype: int64 Other types of indexing and selection (discussed in “Data Indexing and Selection” on page 107 ) work as well; for example, selection based on Boolean masks: In [ 26 ]: pop [ pop > 22000000 ] Out[26]: state year California 2000 33871648 2010 37253956 Texas 2010 25145561 dtype: int64 Selection based on fancy indexing also works: In [ 27 ]: pop [[ 'California' , 'Texas' ]] Out[27]: state year California 2000 33871648 2010 37253956 Texas 2000 20851820 2010 25145561 dtype: int64 Hierarchical Indexing | 135 Multiply indexed DataFrames A multiply indexed DataFrame behaves in a similar manner",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_141"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Consider our toy medi‐ cal DataFrame from before: In [ 28 ]: health_data Out[28]: subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 31.0 38.7 32.0 36.7 35.0 37.2 2 44.0 37.7 50.0 35.0 29.0 36.7 2014 1 30.0 37.4 39.0 37.8 61.0 36.9 2 47.0 37.8 48.0 37.3 51.0 36.5 Remember that columns are primary in a DataFrame , and the syntax used for multi‐ ply indexed Series applies to the columns. For example, we can recover Guido’s heart rate data with a simple operation: In [ 29 ]: health_data [ 'Guido' , 'HR' ] Out[29]: year visit 2013 1 32.0 2 50.0 2014 1 39.0 2 48.0 Name: (Guido, HR), dtype: float64 Also, as with the single-index case, we can use the loc , iloc , and ix indexers intro‐ duced in “Data Indexing and Selection” on page 107 . For example: In [ 30 ]: health_data . iloc [: 2 , : 2 ] Out[30]: subject Bob type HR Temp year visit 2013 1 31.0 38.7 2 44.0 37.7 These indexers provide an array-like view of the underlying two-dimensional data, but each individual index in loc or iloc can be passed a tuple of multiple indices. For example: In [ 31 ]: health_data . loc [:, ( 'Bob' , 'HR' )] Out[31]: year visit 2013 1 31.0 2 44.0 2014 1 30.0 2 47.0 Name: (Bob, HR), dtype: float64 Working with slices within these index tuples is not especially convenient; trying to create a slice within a tuple will lead to a syntax error: 136 | Chapter 3: Data Manipulation with Pandas In [ 32 ]: health_data . loc [(:, 1 ), (:, 'HR' )] File \"<ipython-input-32-8e3cc151e316>\", line 1 health_data.loc[(:, 1), (:, 'HR')] ^ SyntaxError: invalid syntax You could get around this by building the desired slice explicitly using Python’s builin slice() function, but a better way in this context is to use an IndexSlice object, which Pandas provides for precisely this situation. For example: In [ 33 ]: idx = pd . IndexSlice health_data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_142"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example: In [ 33 ]: idx = pd . IndexSlice health_data . loc [ idx [:, 1 ], idx [:, 'HR' ]] Out[33]: subject Bob Guido Sue type HR HR HR year visit 2013 1 31.0 32.0 35.0 2014 1 30.0 39.0 61.0 There are so many ways to interact with data in multiply indexed Series and Data Frame s, and as with many tools in this book the best way to become familiar with them is to try them out! Rearranging Multi-Indices One of the keys to working with multiply indexed data is knowing how to effectively transform the data. There are a number of operations that will preserve all the infor‐ mation in the dataset, but rearrange it for the purposes of various computations. We saw a brief example of this in the stack() and unstack() methods, but there are many more ways to finely control the rearrangement of data between hierarchical indices and columns, and we’ll explore them here. Sorted and unsorted indices Earlier, we briefly mentioned a caveat, but we should emphasize it more here. Many of the MultiIndex slicing operations will fail if the index is not sorted. Let’s take a look at this here. We’ll start by creating some simple multiply indexed data where the indices are not lexographically sorted : In [ 34 ]: index = pd . MultiIndex . from_product ([[ 'a' , 'c' , 'b' ], [ 1 , 2 ]]) data = pd . Series ( np . random . rand ( 6 ), index = index ) data . index . names = [ 'char' , 'int' ] data Out[34]: char int a 1 0.003001 2 0.164974 c 1 0.741650 Hierarchical Indexing | 137 2 0.569264 b 1 0.001693 2 0.526226 dtype: float64 If we try to take a partial slice of this index, it will result in an error: In [ 35 ]: try : data [ 'a' : 'b' ] except KeyError as e : print ( type ( e )) print ( e ) <class 'KeyError'> 'Key length (1) was greater than MultiIndex lexsort depth (0)' Although it is not entirely clear from the error message, this is the result of the Multi Index not being sorted. For various reasons, partial slices and other similar opera‐ tions require the levels in the MultiIndex to be in sorted (i.e., lexographical) order",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_143"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For various reasons, partial slices and other similar opera‐ tions require the levels in the MultiIndex to be in sorted (i.e., lexographical) order. Pandas provides a number of convenience routines to perform this type of sorting; examples are the sort_index() and sortlevel() methods of the DataFrame . We’ll use the simplest, sort_index() , here: In [ 36 ]: data = data . sort_index () data Out[36]: char int a 1 0.003001 2 0.164974 b 1 0.001693 2 0.526226 c 1 0.741650 2 0.569264 dtype: float64 With the index sorted in this way, partial slicing will work as expected: In [ 37 ]: data [ 'a' : 'b' ] Out[37]: char int a 1 0.003001 2 0.164974 b 1 0.001693 2 0.526226 dtype: float64 Stacking and unstacking indices As we saw briefly before, it is possible to convert a dataset from a stacked multi-index to a simple two-dimensional representation, optionally specifying the level to use: 138 | Chapter 3: Data Manipulation with Pandas In [ 38 ]: pop . unstack ( level = 0 ) Out[38]: state California New York Texas year 2000 33871648 18976457 20851820 2010 37253956 19378102 25145561 In [ 39 ]: pop . unstack ( level = 1 ) Out[39]: year 2000 2010 state California 33871648 37253956 New York 18976457 19378102 Texas 20851820 25145561 The opposite of unstack() is stack() , which here can be used to recover the original series: In [ 40 ]: pop . unstack () . stack () Out[40]: state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 dtype: int64 Index setting and resetting Another way to rearrange hierarchical data is to turn the index labels into columns; this can be accomplished with the reset_index method. Calling this on the popula‐ tion dictionary will result in a DataFrame with a state and year column holding the information that was formerly in the index. For clarity, we can optionally specify the name of the data for the column representation: In [ 41 ]: pop_flat = pop",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_144"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For clarity, we can optionally specify the name of the data for the column representation: In [ 41 ]: pop_flat = pop . reset_index ( name = 'population' ) pop_flat Out[41]: state year population 0 California 2000 33871648 1 California 2010 37253956 2 New York 2000 18976457 3 New York 2010 19378102 4 Texas 2000 20851820 5 Texas 2010 25145561 Often when you are working with data in the real world, the raw input data looks like this and it’s useful to build a MultiIndex from the column values. This can be done with the set_index method of the DataFrame , which returns a multiply indexed Data Frame : Hierarchical Indexing | 139 In [ 42 ]: pop_flat . set_index ([ 'state' , 'year' ]) Out[42]: population state year California 2000 33871648 2010 37253956 New York 2000 18976457 2010 19378102 Texas 2000 20851820 2010 25145561 In practice, I find this type of reindexing to be one of the more useful patterns when I encounter real-world datasets. Data Aggregations on Multi-Indices We’ve previously seen that Pandas has built-in data aggregation methods, such as mean() , sum() , and max() . For hierarchically indexed data, these can be passed a level parameter that controls which subset of the data the aggregate is computed on. For example, let’s return to our health data: In [ 43 ]: health_data Out[43]: subject Bob Guido Sue type HR Temp HR Temp HR Temp year visit 2013 1 31.0 38.7 32.0 36.7 35.0 37.2 2 44.0 37.7 50.0 35.0 29.0 36.7 2014 1 30.0 37.4 39.0 37.8 61.0 36.9 2 47.0 37.8 48.0 37.3 51.0 36.5 Perhaps we’d like to average out the measurements in the two visits each year. We can do this by naming the index level we’d like to explore, in this case the year: In [ 44 ]: data_mean = health_data . mean ( level = 'year' ) data_mean Out[44]: subject Bob Guido Sue type HR Temp HR Temp HR Temp year 2013 37.5 38.2 41.0 35.85 32.0 36.95 2014 38.5 37.6 43.5 37.55 56.0 36.70 By further making use of the axis keyword, we can take the mean among levels on the columns as well: In [ 45 ]: data_mean",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_145"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". mean ( axis = 1 , level = 'type' ) Out[45]: type HR Temp year 2013 36.833333 37.000000 2014 46.000000 37.283333 140 | Chapter 3: Data Manipulation with Pandas Thus in two lines, we’ve been able to find the average heart rate and temperature measured among all subjects in all visits each year. This syntax is actually a shortcut to the GroupBy functionality, which we will discuss in “Aggregation and Grouping” on page 158 . While this is a toy example, many real-world datasets have similar hierarch‐ ical structure. Panel Data Pandas has a few other fundamental data structures that we have not yet discussed, namely the pd.Panel and pd.Panel4D objects. These can be thought of, respectively, as three-dimensional and four-dimensional generalizations of the (one-dimensional) Series and (two-dimensional) DataFrame structures. Once you are familiar with indexing and manipulation of data in a Series and DataFrame , Panel and Panel4D are relatively straightforward to use. In particular, the ix , loc , and iloc indexers dis‐ cussed in “Data Indexing and Selection” on page 107 extend readily to these highedimensional structures. We won’t cover these panel structures further in this text, as I’ve found in the majority of cases that multi-indexing is a more useful and conceptually simpler representation for higher-dimensional data. Additionally, panel data is fundamentally a dense data representation, while multi-indexing is fundamentally a sparse data representation. As the number of dimensions increases, the dense representation can become very inefficient for the majority of real-world datasets. For the occasional specialized appli‐ cation, however, these structures can be useful. If you’d like to read more about the Panel and Panel4D structures, see the references listed in “Further Resources” on page 215 . Combining Datasets: Concat and Append Some of the most interesting studies of data come from combining different data sources",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_146"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Combining Datasets: Concat and Append Some of the most interesting studies of data come from combining different data sources. These operations can involve anything from very straightforward concatena‐ tion of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets. Series and DataFrame s are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward. Here we’ll take a look at simple concatenation of Series and DataFrame s with the pd.concat function; later we’ll dive into more sophisticated in-memory merges and joins implemented in Pandas. We begin with the standard imports: In [ 1 ]: import pandas as pd import numpy as np Combining Datasets: Concat and Append | 141 For convenience, we’ll define this function, which creates a DataFrame of a particular form that will be useful below: In [ 2 ]: def make_df ( cols , ind ): \"\"\"Quickly make a DataFrame\"\"\" data = { c : [ str ( c ) + str ( i ) for i in ind ] for c in cols } return pd . DataFrame ( data , ind ) # example DataFrame make_df ( 'ABC' , range ( 3 )) Out[2]: A B C 0 A0 B0 C0 1 A1 B1 C1 2 A2 B2 C2 Recall: Concatenation of NumPy Arrays Concatenation of Series and DataFrame objects is very similar to concatenation of NumPy arrays, which can be done via the np.concatenate function as discussed in “The Basics of NumPy Arrays” on page 42 . Recall that with it, you can combine the contents of two or more arrays into a single array: In [ 4 ]: x = [ 1 , 2 , 3 ] y = [ 4 , 5 , 6 ] z = [ 7 , 8 , 9 ] np . concatenate ([ x , y , z ]) Out[4]: array([1, 2, 3, 4, 5, 6, 7, 8, 9]) The first argument is a list or tuple of arrays to concatenate. Additionally, it takes an axis keyword that allows you to specify the axis along which the result will be concatenated: In [ 5 ]: x = [[ 1 , 2 ], [ 3 , 4 ]] np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_147"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Additionally, it takes an axis keyword that allows you to specify the axis along which the result will be concatenated: In [ 5 ]: x = [[ 1 , 2 ], [ 3 , 4 ]] np . concatenate ([ x , x ], axis = 1 ) Out[5]: array([[1, 2, 1, 2], [3, 4, 3, 4]]) Simple Concatenation with pd.concat Pandas has a function, pd.concat() , which has a similar syntax to np.concatenate but contains a number of options that we’ll discuss momentarily: # Signature in Pandas v0.18 pd . concat ( objs , axis = 0 , join = 'outer' , join_axes = None , ignore_index = False , keys = None , levels = None , names = None , verify_integrity = False , copy = True ) 142 | Chapter 3: Data Manipulation with Pandas pd.concat() can be used for a simple concatenation of Series or DataFrame objects, just as np.concatenate() can be used for simple concatenations of arrays: In [ 6 ]: ser1 = pd . Series ([ 'A' , 'B' , 'C' ], index = [ 1 , 2 , 3 ]) ser2 = pd . Series ([ 'D' , 'E' , 'F' ], index = [ 4 , 5 , 6 ]) pd . concat ([ ser1 , ser2 ]) Out[6]: 1 A 2 B 3 C 4 D 5 E 6 F dtype: object It also works to concatenate higher-dimensional objects, such as DataFrame s: In [ 7 ]: df1 = make_df ( 'AB' , [ 1 , 2 ]) df2 = make_df ( 'AB' , [ 3 , 4 ]) print ( df1 ); print ( df2 ); print ( pd . concat ([ df1 , df2 ])) df1 df2 pd.concat([df1, df2]) A B A B A B 1 A1 B1 3 A3 B3 1 A1 B1 2 A2 B2 4 A4 B4 2 A2 B2 3 A3 B3 4 A4 B4 By default, the concatenation takes place row-wise within the DataFrame (i.e., axis=0 ). Like np.concatenate , pd.concat allows specification of an axis along which concatenation will take place. Consider the following example: In [ 8 ]: df3 = make_df ( 'AB' , [ 0 , 1 ]) df4 = make_df ( 'CD' , [ 0 , 1 ]) print ( df3 ); print ( df4 ); print ( pd . concat ([ df3 , df4 ], axis = 'col' )) df3 df4 pd.concat([df3, df4], axis='col') A B C D A B C D 0 A0 B0 0 C0 D0 0 A0 B0 C0 D0 1 A1 B1 1 C1 D1 1 A1 B1 C1 D1 We could have equivalently specified axis=1 ; here we’ve used the more intuitive axis='col'",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_148"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Duplicate indices One important difference between np.concatenate and pd.concat is that Pandas concatenation preserves indices , even if the result will have duplicate indices! Consider this simple example: In [ 9 ]: x = make_df ( 'AB' , [ 0 , 1 ]) y = make_df ( 'AB' , [ 2 , 3 ]) Combining Datasets: Concat and Append | 143 y . index = x . index # make duplicate indices! print ( x ); print ( y ); print ( pd . concat ([ x , y ])) x y pd.concat([x, y]) A B A B A B 0 A0 B0 0 A2 B2 0 A0 B0 1 A1 B1 1 A3 B3 1 A1 B1 0 A2 B2 1 A3 B3 Notice the repeated indices in the result. While this is valid within DataFrame s, the outcome is often undesirable. pd.concat() gives us a few ways to handle it. Catching the repeats as an error. If you’d like to simply verify that the indices in the result of pd.concat() do not overlap, you can specify the verify_integrity flag. With this set to True , the concatenation will raise an exception if there are duplicate indices. Here is an example, where for clarity we’ll catch and print the error message: In [ 10 ]: try : pd . concat ([ x , y ], verify_integrity = True ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Indexes have overlapping values: [0, 1] Ignoring the index. Sometimes the index itself does not matter, and you would prefer it to simply be ignored. You can specify this option using the ignore_index flag. With this set to True , the concatenation will create a new integer index for the resulting Series : In [ 11 ]: print ( x ); print ( y ); print ( pd . concat ([ x , y ], ignore_index = True )) x y pd.concat([x, y], ignore_index=True) A B A B A B 0 A0 B0 0 A2 B2 0 A0 B0 1 A1 B1 1 A3 B3 1 A1 B1 2 A2 B2 3 A3 B3 Adding MultiIndex keys. Another alternative is to use the keys option to specify a label for the data sources; the result will be a hierarchically indexed series containing the data: In [ 12 ]: print ( x ); print ( y ); print ( pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_149"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". concat ([ x , y ], keys = [ 'x' , 'y' ])) x y pd.concat([x, y], keys=['x', 'y']) A B A B A B 0 A0 B0 0 A2 B2 x 0 A0 B0 1 A1 B1 1 A3 B3 1 A1 B1 y 0 A2 B2 1 A3 B3 144 | Chapter 3: Data Manipulation with Pandas The result is a multiply indexed DataFrame , and we can use the tools discussed in “Hierarchical Indexing” on page 128 to transform this data into the representation we’re interested in. Concatenation with joins In the simple examples we just looked at, we were mainly concatenating DataFrame s with shared column names. In practice, data from different sources might have differ‐ ent sets of column names, and pd.concat offers several options in this case. Consider the concatenation of the following two DataFrame s, which have some (but not all!) columns in common: In [ 13 ]: df5 = make_df ( 'ABC' , [ 1 , 2 ]) df6 = make_df ( 'BCD' , [ 3 , 4 ]) print ( df5 ); print ( df6 ); print ( pd . concat ([ df5 , df6 ]) df5 df6 pd.concat([df5, df6]) A B C B C D A B C D 1 A1 B1 C1 3 B3 C3 D3 1 A1 B1 C1 NaN 2 A2 B2 C2 4 B4 C4 D4 2 A2 B2 C2 NaN 3 NaN B3 C3 D3 4 NaN B4 C4 D4 By default, the entries for which no data is available are filled with NA values. To change this, we can specify one of several options for the join and join_axes param‐ eters of the concatenate function. By default, the join is a union of the input columns ( join='outer' ), but we can change this to an intersection of the columns using join='inner' : In [ 14 ]: print ( df5 ); print ( df6 ); print ( pd . concat ([ df5 , df6 ], join = 'inner' )) df5 df6 pd.concat([df5, df6], join='inner') A B C B C D B C 1 A1 B1 C1 3 B3 C3 D3 1 B1 C1 2 A2 B2 C2 4 B4 C4 D4 2 B2 C2 3 B3 C3 4 B4 C4 Another option is to directly specify the index of the remaining colums using the join_axes argument, which takes a list of index objects. Here we’ll specify that the returned columns should be the same as those of the first input: In [ 15 ]: print ( df5 ); print ( df6 ); print ( pd . concat ([ df5 , df6 ], join_axes = [ df5",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_150"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here we’ll specify that the returned columns should be the same as those of the first input: In [ 15 ]: print ( df5 ); print ( df6 ); print ( pd . concat ([ df5 , df6 ], join_axes = [ df5 . columns ])) df5 df6 pd.concat([df5, df6], join_axes=[df5.columns]) A B C B C D A B C 1 A1 B1 C1 3 B3 C3 D3 1 A1 B1 C1 2 A2 B2 C2 4 B4 C4 D4 2 A2 B2 C2 Combining Datasets: Concat and Append | 145 3 NaN B3 C3 4 NaN B4 C4 The combination of options of the pd.concat function allows a wide range of possi‐ ble behaviors when you are joining two datasets; keep these in mind as you use these tools for your own data. The append() method Because direct array concatenation is so common, Series and DataFrame objects have an append method that can accomplish the same thing in fewer keystrokes. For example, rather than calling pd.concat([df1, df2]) , you can simply call df1.append(df2) : In [ 16 ]: print ( df1 ); print ( df2 ); print ( df1 . append ( df2 )) df1 df2 df1.append(df2) A B A B A B 1 A1 B1 3 A3 B3 1 A1 B1 2 A2 B2 4 A4 B4 2 A2 B2 3 A3 B3 4 A4 B4 Keep in mind that unlike the append() and extend() methods of Python lists, the append() method in Pandas does not modify the original object—instead, it creates a new object with the combined data. It also is not a very efficient method, because it involves creation of a new index and data buffer. Thus, if you plan to do multiple append operations, it is generally better to build a list of DataFrame s and pass them all at once to the concat() function. In the next section, we’ll look at another more powerful approach to combining data from multiple sources, the database-style merges/joins implemented in pd.merge . For more information on concat() , append() , and related functionality, see the “Merge, Join, and Concatenate” section of the Pandas documentation. Combining Datasets: Merge and Join One essential feature offered by Pandas is its high-performance, in-memory join and merge operations",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_151"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Combining Datasets: Merge and Join One essential feature offered by Pandas is its high-performance, in-memory join and merge operations. If you have ever worked with databases, you should be familiar with this type of data interaction. The main interface for this is the pd.merge func‐ tion, and we’ll see a few examples of how this can work in practice. Relational Algebra The behavior implemented in pd.merge() is a subset of what is known as relational algebra , which is a formal set of rules for manipulating relational data, and forms the conceptual foundation of operations available in most databases. The strength of the 146 | Chapter 3: Data Manipulation with Pandas relational algebra approach is that it proposes several primitive operations, which become the building blocks of more complicated operations on any dataset. With this lexicon of fundamental operations implemented efficiently in a database or other pro‐ gram, a wide range of fairly complicated composite operations can be performed. Pandas implements several of these fundamental building blocks in the pd.merge() function and the related join() method of Series and DataFrame s. As we will see, these let you efficiently link data from different sources. Categories of Joins The pd.merge() function implements a number of types of joins: the one-to-one , many-to-one , and many-to-many joins. All three types of joins are accessed via an identical call to the pd.merge() interface; the type of join performed depends on the form of the input data. Here we will show simple examples of the three types of merges, and discuss detailed options further below. One-to-one joins Perhaps the simplest type of merge expression is the one-to-one join, which is in many ways very similar to the column-wise concatenation seen in “Combining Data‐ sets: Concat and Append” on page 141 . As a concrete example, consider the following two DataFrame s, which contain information on several employees in a company: In [ 2 ]: df1 = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_152"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As a concrete example, consider the following two DataFrame s, which contain information on several employees in a company: In [ 2 ]: df1 = pd . DataFrame ({ 'employee' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'group' : [ 'Accounting' , 'Engineering' , 'Engineering' , 'HR' ]}) df2 = pd . DataFrame ({ 'employee' : [ 'Lisa' , 'Bob' , 'Jake' , 'Sue' ], 'hire_date' : [ 2004 , 2008 , 2012 , 2014 ]}) print ( df1 ); print ( df2 ) df1 df2 employee group employee hire_date 0 Bob Accounting 0 Lisa 2004 1 Jake Engineering 1 Bob 2008 2 Lisa Engineering 2 Jake 2012 3 Sue HR 3 Sue 2014 To combine this information into a single DataFrame , we can use the pd.merge() function: In [ 3 ]: df3 = pd . merge ( df1 , df2 ) df3 Out[3]: employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 Combining Datasets: Merge and Join | 147 The pd.merge() function recognizes that each DataFrame has an “employee” column, and automatically joins using this column as a key. The result of the merge is a new DataFrame that combines the information from the two inputs. Notice that the order of entries in each column is not necessarily maintained: in this case, the order of the “employee” column differs between df1 and df2 , and the pd.merge() function cor‐ rectly accounts for this. Additionally, keep in mind that the merge in general discards the index, except in the special case of merges by index (see “The left_index and right_index keywords” on page 151 ). Many-to-one joins Many-to-one joins are joins in which one of the two key columns contains duplicate entries. For the many-to-one case, the resulting DataFrame will preserve those dupli‐ cate entries as appropriate. Consider the following example of a many-to-one join: In [ 4 ]: df4 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) print ( df3 ); print ( df4 ); print ( pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_153"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". DataFrame ({ 'group' : [ 'Accounting' , 'Engineering' , 'HR' ], 'supervisor' : [ 'Carly' , 'Guido' , 'Steve' ]}) print ( df3 ); print ( df4 ); print ( pd . merge ( df3 , df4 )) df3 df4 employee group hire_date group supervisor 0 Bob Accounting 2008 0 Accounting Carly 1 Jake Engineering 2012 1 Engineering Guido 2 Lisa Engineering 2004 2 HR Steve 3 Sue HR 2014 pd.merge(df3, df4) employee group hire_date supervisor 0 Bob Accounting 2008 Carly 1 Jake Engineering 2012 Guido 2 Lisa Engineering 2004 Guido 3 Sue HR 2014 Steve The resulting DataFrame has an additional column with the “supervisor” information, where the information is repeated in one or more locations as required by the inputs. Many-to-many joins Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined. If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge. This will be perhaps most clear with a concrete example. Consider the following, where we have a DataFrame showing one or more skills associated with a particular group. By performing a many-to-many join, we can recover the skills associated with any individual person: In [ 5 ]: df5 = pd . DataFrame ({ 'group' : [ 'Accounting' , 'Accounting' , 'Engineering' , 'Engineering' , 'HR' , 'HR' ], 148 | Chapter 3: Data Manipulation with Pandas 'skills' : [ 'math' , 'spreadsheets' , 'coding' , 'linux' , 'spreadsheets' , 'organization' ]}) print ( df1 ); print ( df5 ); print ( pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_154"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". merge ( df1 , df5 )) df1 df5 employee group group skills 0 Bob Accounting 0 Accounting math 1 Jake Engineering 1 Accounting spreadsheets 2 Lisa Engineering 2 Engineering coding 3 Sue HR 3 Engineering linux 4 HR spreadsheets 5 HR organization pd.merge(df1, df5) employee group skills 0 Bob Accounting math 1 Bob Accounting spreadsheets 2 Jake Engineering coding 3 Jake Engineering linux 4 Lisa Engineering coding 5 Lisa Engineering linux 6 Sue HR spreadsheets 7 Sue HR organization These three types of joins can be used with other Pandas tools to implement a wide array of functionality. But in practice, datasets are rarely as clean as the one we’re working with here. In the following section, we’ll consider some of the options pro‐ vided by pd.merge() that enable you to tune how the join operations work. Specification of the Merge Key We’ve already seen the default behavior of pd.merge() : it looks for one or more matching column names between the two inputs, and uses this as the key. However, often the column names will not match so nicely, and pd.merge() provides a variety of options for handling this. The on keyword Most simply, you can explicitly specify the name of the key column using the on key‐ word, which takes a column name or a list of column names: In [ 6 ]: print ( df1 ); print ( df2 ); print ( pd . merge ( df1 , df2 , on = 'employee' )) df1 df2 employee group employee hire_date 0 Bob Accounting 0 Lisa 2004 1 Jake Engineering 1 Bob 2008 2 Lisa Engineering 2 Jake 2012 3 Sue HR 3 Sue 2014 Combining Datasets: Merge and Join | 149 pd.merge(df1, df2, on='employee') employee group hire_date 0 Bob Accounting 2008 1 Jake Engineering 2012 2 Lisa Engineering 2004 3 Sue HR 2014 This option works only if both the left and right DataFrame s have the specified col‐ umn name. The left_on and right_on keywords At times you may wish to merge two datasets with different column names; for exam‐ ple, we may have a dataset in which the employee name is labeled as “name” rather than “employee”",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_155"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In this case, we can use the left_on and right_on keywords to specify the two column names: In [ 7 ]: df3 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'salary' : [ 70000 , 80000 , 120000 , 90000 ]}) print ( df1 ); print ( df3 ); print ( pd . merge ( df1 , df3 , left_on = \"employee\" , right_on = \"name\" )) df1 df3 employee group name salary 0 Bob Accounting 0 Bob 70000 1 Jake Engineering 1 Jake 80000 2 Lisa Engineering 2 Lisa 120000 3 Sue HR 3 Sue 90000 pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\") employee group name salary 0 Bob Accounting Bob 70000 1 Jake Engineering Jake 80000 2 Lisa Engineering Lisa 120000 3 Sue HR Sue 90000 The result has a redundant column that we can drop if desired—for example, by using the drop() method of DataFrame s: In [ 8 ]: pd . merge ( df1 , df3 , left_on = \"employee\" , right_on = \"name\" ) . drop ( 'name' , axis = 1 ) Out[8]: employee group salary 0 Bob Accounting 70000 1 Jake Engineering 80000 2 Lisa Engineering 120000 3 Sue HR 90000 150 | Chapter 3: Data Manipulation with Pandas The left_index and right_index keywords Sometimes, rather than merging on a column, you would instead like to merge on an index. For example, your data might look like this: In [ 9 ]: df1a = df1 . set_index ( 'employee' ) df2a = df2 . set_index ( 'employee' ) print ( df1a ); print ( df2a ) df1a df2a group hire_date employee employee Bob Accounting Lisa 2004 Jake Engineering Bob 2008 Lisa Engineering Jake 2012 Sue HR Sue 2014 You can use the index as the key for merging by specifying the left_index and/or right_index flags in pd.merge() : In [ 10 ]: print ( df1a ); print ( df2a ); print ( pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_156"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". merge ( df1a , df2a , left_index = True , right_index = True )) df1a df2a group hire_date employee employee Bob Accounting Lisa 2004 Jake Engineering Bob 2008 Lisa Engineering Jake 2012 Sue HR Sue 2014 pd.merge(df1a, df2a, left_index=True, right_index=True) group hire_date employee Lisa Engineering 2004 Bob Accounting 2008 Jake Engineering 2012 Sue HR 2014 For convenience, DataFrame s implement the join() method, which performs a merge that defaults to joining on indices: In [ 11 ]: print ( df1a ); print ( df2a ); print ( df1a . join ( df2a )) df1a df2a group hire_date employee employee Bob Accounting Lisa 2004 Jake Engineering Bob 2008 Lisa Engineering Jake 2012 Sue HR Sue 2014 Combining Datasets: Merge and Join | 151 df1a.join(df2a) group hire_date employee Bob Accounting 2008 Jake Engineering 2012 Lisa Engineering 2004 Sue HR 2014 If you’d like to mix indices and columns, you can combine left_index with right_on or left_on with right_index to get the desired behavior: In [ 12 ]: print ( df1a ); print ( df3 ); print ( pd . merge ( df1a , df3 , left_index = True , right_on = 'name' )) df1a df3 group employee name salary Bob Accounting 0 Bob 70000 Jake Engineering 1 Jake 80000 Lisa Engineering 2 Lisa 120000 Sue HR 3 Sue 90000 pd.merge(df1a, df3, left_index=True, right_on='name') group name salary 0 Accounting Bob 70000 1 Engineering Jake 80000 2 Engineering Lisa 120000 3 HR Sue 90000 All of these options also work with multiple indices and/or multiple columns; the interface for this behavior is very intuitive. For more information on this, see the “Merge, Join, and Concatenate” section of the Pandas documentation. Specifying Set Arithmetic for Joins In all the preceding examples we have glossed over one important consideration in performing a join: the type of set arithmetic used in the join. This comes up when a value appears in one key column but not the other. Consider this example: In [ 13 ]: df6 = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_157"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This comes up when a value appears in one key column but not the other. Consider this example: In [ 13 ]: df6 = pd . DataFrame ({ 'name' : [ 'Peter' , 'Paul' , 'Mary' ], 'food' : [ 'fish' , 'beans' , 'bread' ]}, columns = [ 'name' , 'food' ]) df7 = pd . DataFrame ({ 'name' : [ 'Mary' , 'Joseph' ], 'drink' : [ 'wine' , 'beer' ]}, columns = [ 'name' , 'drink' ]) print ( df6 ); print ( df7 ); print ( pd . merge ( df6 , df7 )) 152 | Chapter 3: Data Manipulation with Pandas df6 df7 pd.merge(df6, df7) name food name drink name food drink 0 Peter fish 0 Mary wine 0 Mary bread wine 1 Paul beans 1 Joseph beer 2 Mary bread Here we have merged two datasets that have only a single “name” entry in common: Mary. By default, the result contains the intersection of the two sets of inputs; this is what is known as an inner join . We can specify this explicitly using the how keyword, which defaults to 'inner' : In [ 14 ]: pd . merge ( df6 , df7 , how = 'inner' ) Out[14]: name food drink 0 Mary bread wine Other options for the how keyword are 'outer' , 'left' , and 'right' . An outer join returns a join over the union of the input columns, and fills in all missing values with NAs: In [ 15 ]: print ( df6 ); print ( df7 ); print ( pd . merge ( df6 , df7 , how = 'outer' )) df6 df7 pd.merge(df6, df7, how='outer') name food name drink name food drink 0 Peter fish 0 Mary wine 0 Peter fish NaN 1 Paul beans 1 Joseph beer 1 Paul beans NaN 2 Mary bread 2 Mary bread wine 3 Joseph NaN beer The left join and right join return join over the left entries and right entries, respec‐ tively. For example: In [ 16 ]: print ( df6 ); print ( df7 ); print ( pd . merge ( df6 , df7 , how = 'left' )) df6 df7 pd.merge(df6, df7, how='left') name food name drink name food drink 0 Peter fish 0 Mary wine 0 Peter fish NaN 1 Paul beans 1 Joseph beer 1 Paul beans NaN 2 Mary bread 2 Mary bread wine The output rows now correspond to the entries in the left input. Using how='right' works in a similar manner",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_158"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Using how='right' works in a similar manner. All of these options can be applied straightforwardly to any of the preceding join types. Overlapping Column Names: The suffixes Keyword Finally, you may end up in a case where your two input DataFrame s have conflicting column names. Consider this example: In [ 17 ]: df8 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 1 , 2 , 3 , 4 ]}) Combining Datasets: Merge and Join | 153 df9 = pd . DataFrame ({ 'name' : [ 'Bob' , 'Jake' , 'Lisa' , 'Sue' ], 'rank' : [ 3 , 1 , 4 , 2 ]}) print ( df8 ); print ( df9 ); print ( pd . merge ( df8 , df9 , on = \"name\" )) df8 df9 pd.merge(df8, df9, on=\"name\") name rank name rank name rank_x rank_y 0 Bob 1 0 Bob 3 0 Bob 1 3 1 Jake 2 1 Jake 1 1 Jake 2 1 2 Lisa 3 2 Lisa 4 2 Lisa 3 4 3 Sue 4 3 Sue 2 3 Sue 4 2 Because the output would have two conflicting column names, the merge function automatically appends a suffix _x or _y to make the output columns unique. If these defaults are inappropriate, it is possible to specify a custom suffix using the suffixes keyword: In [ 18 ]: print ( df8 ); print ( df9 ); print ( pd . merge ( df8 , df9 , on = \"name\" , suffixes = [ \"_L\" , \"_R\" ])) df8 df9 name rank name rank 0 Bob 1 0 Bob 3 1 Jake 2 1 Jake 1 2 Lisa 3 2 Lisa 4 3 Sue 4 3 Sue 2 pd.merge(df8, df9, on=\"name\", suffixes=[\"_L\", \"_R\"]) name rank_L rank_R 0 Bob 1 3 1 Jake 2 1 2 Lisa 3 4 3 Sue 4 2 These suffixes work in any of the possible join patterns, and work also if there are multiple overlapping columns. For more information on these patterns, see “Aggregation and Grouping” on page 158 , where we dive a bit deeper into relational algebra. Also see the “Merge, Join, and Concatenate” section of the Pandas documentation for further discussion of these topics. Example: US States Data Merge and join operations come up most often when one is combining data from dif‐ ferent sources. Here we will consider an example of some data about US states and their populations",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_159"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here we will consider an example of some data about US states and their populations. The data files can be found at http://github.com/jakevdp/datUSstates/ : In [ 19 ]: # Following are shell commands to download the data 154 | Chapter 3: Data Manipulation with Pandas # !curl -O https://raw.githubusercontent.com/jakevdp/ # data-USstates/master/state-population.csv # !curl -O https://raw.githubusercontent.com/jakevdp/ # data-USstates/master/state-areas.csv # !curl -O https://raw.githubusercontent.com/jakevdp/ # data-USstates/master/state-abbrevs.csv Let’s take a look at the three datasets, using the Pandas read_csv() function: In [ 20 ]: pop = pd . read_csv ( 'state-population.csv' ) areas = pd . read_csv ( 'state-areas.csv' ) abbrevs = pd . read_csv ( 'state-abbrevs.csv' ) print ( pop . head ()); print ( areas . head ()); print ( abbrevs . head ()) pop.head() areas.head() state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 0 Alabama 52423 1 AL total 2012 4817528.0 1 Alaska 656425 2 AL under18 2010 1130966.0 2 Arizona 114006 3 AL total 2010 4785570.0 3 Arkansas 53182 4 AL under18 2011 1125763.0 3 Arkansas 53182 4 California 163707 abbrevs.head() state abbreviation 0 Alabama AL 1 Alaska AK 2 Arizona AZ 3 Arkansas AR 4 California CA Given this information, say we want to compute a relatively straightforward result: rank US states and territories by their 2010 population density. We clearly have the data here to find this result, but we’ll have to combine the datasets to get it. We’ll start with a many-to-one merge that will give us the full state name within the population DataFrame . We want to merge based on the state/region column of pop , and the abbreviation column of abbrevs . We’ll use how='outer' to make sure no data is thrown away due to mismatched labels. In [ 21 ]: merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged = merged . drop ( 'abbreviation' , 1 ) # drop duplicate info merged",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_160"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In [ 21 ]: merged = pd . merge ( pop , abbrevs , how = 'outer' , left_on = 'state/region' , right_on = 'abbreviation' ) merged = merged . drop ( 'abbreviation' , 1 ) # drop duplicate info merged . head () Out[21]: state/region ages year population state 0 AL under18 2012 1117489.0 Alabama 1 AL total 2012 4817528.0 Alabama 2 AL under18 2010 1130966.0 Alabama 3 AL total 2010 4785570.0 Alabama 4 AL under18 2011 1125763.0 Alabama Combining Datasets: Merge and Join | 155 Let’s double-check whether there were any mismatches here, which we can do by looking for rows with nulls: In [ 22 ]: merged . isnull () . any () Out[22]: state/region False ages False year False population True state True dtype: bool Some of the population info is null; let’s figure out which these are! In [ 23 ]: merged [ merged [ 'population' ] . isnull ()] . head () Out[23]: state/region ages year population state 2448 PR under18 1990 NaN NaN 2449 PR total 1990 NaN NaN 2450 PR total 1991 NaN NaN 2451 PR under18 1991 NaN NaN 2452 PR total 1993 NaN NaN It appears that all the null population values are from Puerto Rico prior to the year 2000; this is likely due to this data not being available from the original source. More importantly, we see also that some of the new state entries are also null, which means that there was no corresponding entry in the abbrevs key! Let’s figure out which regions lack this match: In [ 24 ]: merged . loc [ merged [ 'state' ] . isnull (), 'state/region' ] . unique () Out[24]: array(['PR', 'USA'], dtype=object) We can quickly infer the issue: our population data includes entries for Puerto Rico (PR) and the United States as a whole (USA), while these entries do not appear in the state abbreviation key. We can fix these quickly by filling in appropriate entries: In [ 25 ]: merged . loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United States' merged . isnull ()",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_161"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". loc [ merged [ 'state/region' ] == 'PR' , 'state' ] = 'Puerto Rico' merged . loc [ merged [ 'state/region' ] == 'USA' , 'state' ] = 'United States' merged . isnull () . any () Out[25]: state/region False ages False year False population True state False dtype: bool No more nulls in the state column: we’re all set! Now we can merge the result with the area data using a similar procedure. Examining our results, we will want to join on the state column in both: 156 | Chapter 3: Data Manipulation with Pandas In [ 26 ]: final = pd . merge ( merged , areas , on = 'state' , how = 'left' ) final . head () Out[26]: state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Again, let’s check for nulls to see if there were any mismatches: In [ 27 ]: final . isnull () . any () Out[27]: state/region False ages False year False population True state False area (sq. mi) True dtype: bool There are nulls in the area column; we can take a look to see which regions were ignored here: In [ 28 ]: final [ 'state' ][ final [ 'area (sq. mi)' ] . isnull ()] . unique () Out[28]: array(['United States'], dtype=object) We see that our areas DataFrame does not contain the area of the United States as a whole. We could insert the appropriate value (using the sum of all state areas, for instance), but in this case we’ll just drop the null values because the population den‐ sity of the entire United States is not relevant to our current discussion: In [ 29 ]: final . dropna ( inplace = True ) final . head () Out[29]: state/region ages year population state area (sq. mi) 0 AL under18 2012 1117489.0 Alabama 52423.0 1 AL total 2012 4817528.0 Alabama 52423.0 2 AL under18 2010 1130966.0 Alabama 52423.0 3 AL total 2010 4785570.0 Alabama 52423.0 4 AL under18 2011 1125763.0 Alabama 52423.0 Now we have all the data we need",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_162"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To answer the question of interest, let’s first select the portion of the data corresponding with the year 2000, and the total population. We’ll use the query() function to do this quickly (this requires the numexpr package to be installed; see “High-Performance Pandas: eval() and query()” on page 208 ): In [ 30 ]: data2010 = final . query ( \"year == 2010 & ages == 'total'\" ) data2010 . head () Out[30]: state/region ages year population state area (sq. mi) 3 AL total 2010 4785570.0 Alabama 52423.0 91 AK total 2010 713868.0 Alaska 656425.0 Combining Datasets: Merge and Join | 157 101 AZ total 2010 6408790.0 Arizona 114006.0 189 AR total 2010 2922280.0 Arkansas 53182.0 197 CA total 2010 37333601.0 California 163707.0 Now let’s compute the population density and display it in order. We’ll start by rein‐ dexing our data on the state, and then compute the result: In [ 31 ]: data2010 . set_index ( 'state' , inplace = True ) density = data2010 [ 'population' ] / data2010 [ 'area (sq. mi)' ] In [ 32 ]: density . sort_values ( ascending = False , inplace = True ) density . head () Out[32]: state District of Columbia 8898.897059 Puerto Rico 1058.665149 New Jersey 1009.253268 Rhode Island 681.339159 Connecticut 645.600649 dtype: float64 The result is a ranking of US states plus Washington, DC, and Puerto Rico in order of their 2010 population density, in residents per square mile. We can see that by far the densest region in this dataset is Washington, DC (i.e., the District of Columbia); among states, the densest is New Jersey. We can also check the end of the list: In [ 33 ]: density . tail () Out[33]: state South Dakota 10.583512 North Dakota 9.537565 Montana 6.736171 Wyoming 5.768079 Alaska 1.087509 dtype: float64 We see that the least dense state, by far, is Alaska, averaging slightly over one resident per square mile. This type of messy data merging is a common task when one is trying to answer questions using real-world data sources",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_163"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This type of messy data merging is a common task when one is trying to answer questions using real-world data sources. I hope that this example has given you an idea of the ways you can combine tools we’ve covered in order to gain insight from your data! Aggregation and Grouping An essential piece of analysis of large data is efficient summarization: computing aggregations like sum() , mean() , median() , min() , and max() , in which a single num‐ ber gives insight into the nature of a potentially large dataset. In this section, we’ll 158 | Chapter 3: Data Manipulation with Pandas explore aggregations in Pandas, from simple operations akin to what we’ve seen on NumPy arrays, to more sophisticated operations based on the concept of a groupby . Planets Data Here we will use the Planets dataset, available via the Seaborn package (see “Visuali‐ zation with Seaborn” on page 311 ). It gives information on planets that astronomers have discovered around other stars (known as extrasolar planets or exoplanets for short). It can be downloaded with a simple Seaborn command: In [ 2 ]: import seaborn as sns planets = sns . load_dataset ( 'planets' ) planets . shape Out[2]: (1035, 6) In [ 3 ]: planets . head () Out[3]: method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 This has some details on the 1,000+ exoplanets discovered up to 2014. Simple Aggregation in Pandas Earlier we explored some of the data aggregations available for NumPy arrays ( “Aggregations: Min, Max, and Everything in Between” on page 58 ). As with a ondimensional NumPy array, for a Pandas Series the aggregates return a single value: In [ 4 ]: rng = np . random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser Out[4]: 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 In [ 5 ]: ser",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_164"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . RandomState ( 42 ) ser = pd . Series ( rng . rand ( 5 )) ser Out[4]: 0 0.374540 1 0.950714 2 0.731994 3 0.598658 4 0.156019 dtype: float64 In [ 5 ]: ser . sum () Out[5]: 2.8119254917081569 In [ 6 ]: ser . mean () Out[6]: 0.56238509834163142 For a DataFrame , by default the aggregates return results within each column: Aggregation and Grouping | 159 In [ 7 ]: df = pd . DataFrame ({ 'A' : rng . rand ( 5 ), 'B' : rng . rand ( 5 )}) df Out[7]: A B 0 0.155995 0.020584 1 0.058084 0.969910 2 0.866176 0.832443 3 0.601115 0.212339 4 0.708073 0.181825 In [ 8 ]: df . mean () Out[8]: A 0.477888 B 0.443420 dtype: float64 By specifying the axis argument, you can instead aggregate within each row: In [ 9 ]: df . mean ( axis = 'columns' ) Out[9]: 0 0.088290 1 0.513997 2 0.849309 3 0.406727 4 0.444949 dtype: float64 Pandas Series and DataFrame s include all of the common aggregates mentioned in “Aggregations: Min, Max, and Everything in Between” on page 58 ; in addition, there is a convenience method describe() that computes several common aggregates for each column and returns the result. Let’s use this on the Planets data, for now drop‐ ping rows with missing values: In [ 10 ]: planets . dropna () . describe () Out[10]: number orbital_period mass distance year count 498.00000 498.000000 498.000000 498.000000 498.000000 mean 1.73494 835.778671 2.509320 52.068213 2007.377510 std 1.17572 1469.128259 3.636274 46.596041 4.167284 min 1.00000 1.328300 0.003600 1.350000 1989.000000 25% 1.00000 38.272250 0.212500 24.497500 2005.000000 50% 1.00000 357.000000 1.245000 39.940000 2009.000000 75% 2.00000 999.600000 2.867500 59.332500 2011.000000 max 6.00000 17337.500000 25.000000 354.000000 2014.000000 This can be a useful way to begin understanding the overall properties of a dataset. For example, we see in the year column that although exoplanets were discovered as far back as 1989, half of all known exoplanets were not discovered until 2010 or after",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_165"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, we see in the year column that although exoplanets were discovered as far back as 1989, half of all known exoplanets were not discovered until 2010 or after. This is largely thanks to the Kepler mission, which is a space-based telescope specifi‐ cally designed for finding eclipsing planets around other stars. Table 3-3 summarizes some other built-in Pandas aggregations. 160 | Chapter 3: Data Manipulation with Pandas Table 3-3. Listing of Pandas aggregation methods Aggregation Description count() Total number of items first() , last() First and last item mean() , median() Mean and median min() , max() Minimum and maximum std() , var() Standard deviation and variance mad() Mean absolute deviation prod() Product of all items sum() Sum of all items These are all methods of DataFrame and Series objects. To go deeper into the data, however, simple aggregates are often not enough. The next level of data summarization is the groupby operation, which allows you to quickly and efficiently compute aggregates on subsets of data. GroupBy: Split, Apply, Combine Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the scalled groupby operation. The name “group by” comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: split, apply, combine . Split, apply, combine A canonical example of this split-apply-combine operation, where the “apply” is a summation aggregation, is illustrated in Figure 3-1 . Figure 3-1 makes clear what the GroupBy accomplishes: • The split step involves breaking up and grouping a DataFrame depending on the value of the specified key. • The apply step involves computing some function, usually an aggregate, transfor‐ mation, or filtering, within the individual groups. • The combine step merges the results of these operations into an output array",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_166"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • The combine step merges the results of these operations into an output array. Aggregation and Grouping | 161 Figure 3-1. A visual representation of a groupby operation While we could certainly do this manually using some combination of the masking, aggregation, and merging commands covered earlier, it’s important to realize that the intermediate splits do not need to be explicitly instantiated . Rather, the GroupBy can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way. The power of the GroupBy is that it abstracts away these steps: the user need not think about how the computation is done under the hood, but rather thinks about the operation as a whole . As a concrete example, let’s take a look at using Pandas for the computation shown in Figure 3-1 . We’ll start by creating the input DataFrame : In [ 11 ]: df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data' : range ( 6 )}, columns = [ 'key' , 'data' ]) df Out[11]: key data 0 A 0 1 B 1 2 C 2 3 A 3 4 B 4 5 C 5 We can compute the most basic split-apply-combine operation with the groupby() method of DataFrame s, passing the name of the desired key column: In [ 12 ]: df . groupby ( 'key' ) Out[12]: <pandas.core.groupby.DataFrameGroupBy object at 0x117272160> 162 | Chapter 3: Data Manipulation with Pandas Notice that what is returned is not a set of DataFrame s, but a DataFrameGroupBy object. This object is where the magic is: you can think of it as a special view of the DataFrame , which is poised to dig into the groups but does no actual computation until the aggregation is applied. This “lazy evaluation” approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user. To produce a result, we can apply an aggregate to this DataFrameGroupBy object, which will perform the appropriate apply/combine steps to produce the desired result: In [ 13 ]: df . groupby ( 'key' )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_167"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". groupby ( 'key' ) . sum () Out[13]: data key A 3 B 5 C 7 The sum() method is just one possibility here; you can apply virtually any common Pandas or NumPy aggregation function, as well as virtually any valid DataFrame operation, as we will see in the following discussion. The GroupBy object The GroupBy object is a very flexible abstraction. In many ways, you can simply treat it as if it’s a collection of DataFrame s, and it does the difficult things under the hood. Let’s see some examples using the Planets data. Perhaps the most important operations made available by a GroupBy are aggregate , filter , transform , and apply . We’ll discuss each of these more fully in “Aggregate, filter, transform, apply” on page 165 , but before that let’s introduce some of the other func‐ tionality that can be used with the basic GroupBy operation. Column indexing. The GroupBy object supports column indexing in the same way as the DataFrame , and returns a modified GroupBy object. For example: In [ 14 ]: planets . groupby ( 'method' ) Out[14]: <pandas.core.groupby.DataFrameGroupBy object at 0x1172727b8> In [ 15 ]: planets . groupby ( 'method' )[ 'orbital_period' ] Out[15]: <pandas.core.groupby.SeriesGroupBy object at 0x117272da0> Here we’ve selected a particular Series group from the original DataFrame group by reference to its column name. As with the GroupBy object, no computation is done until we call some aggregate on the object: In [ 16 ]: planets . groupby ( 'method' )[ 'orbital_period' ] . median () Aggregation and Grouping | 163 Out[16]: method Astrometry 631.180000 Eclipse Timing Variations 4343.500000 Imaging 27500.000000 Microlensing 3300.000000 Orbital Brightness Modulation 0.342887 Pulsar Timing 66.541900 Pulsation Timing Variations 1170.000000 Radial Velocity 360.200000 Transit 5.714932 Transit Timing Variations 57.011000 Name: orbital_period, dtype: float64 This gives an idea of the general scale of orbital periods (in days) that each method is sensitive to. Iteration over groups",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_168"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Iteration over groups. The GroupBy object supports direct iteration over the groups, returning each group as a Series or DataFrame : In [ 17 ]: for ( method , group ) in planets . groupby ( 'method' ): print ( \"{0:30s} shape={1}\" . format ( method , group . shape )) Astrometry shape=(2, 6) Eclipse Timing Variations shape=(9, 6) Imaging shape=(38, 6) Microlensing shape=(23, 6) Orbital Brightness Modulation shape=(3, 6) Pulsar Timing shape=(5, 6) Pulsation Timing Variations shape=(1, 6) Radial Velocity shape=(553, 6) Transit shape=(397, 6) Transit Timing Variations shape=(4, 6) This can be useful for doing certain things manually, though it is often much faster to use the built-in apply functionality, which we will discuss momentarily. Dispatch methods. Through some Python class magic, any method not explicitly implemented by the GroupBy object will be passed through and called on the groups, whether they are DataFrame or Series objects. For example, you can use the describe() method of DataFrame s to perform a set of aggregations that describe each group in the data: In [ 18 ]: planets . groupby ( 'method' )[ 'year' ] . describe ()",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_169"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". groupby ( 'method' )[ 'year' ] . describe () . unstack () Out[18]: count mean std min 25% \\\\ method Astrometry 2.0 2011.500000 2.121320 2010.0 2010.75 Eclipse Timing Variations 9.0 2010.000000 1.414214 2008.0 2009.00 Imaging 38.0 2009.131579 2.781901 2004.0 2008.00 Microlensing 23.0 2009.782609 2.859697 2004.0 2008.00 Orbital Brightness Modulation 3.0 2011.666667 1.154701 2011.0 2011.00 164 | Chapter 3: Data Manipulation with Pandas Pulsar Timing 5.0 1998.400000 8.384510 1992.0 1992.00 Pulsation Timing Variations 1.0 2007.000000 NaN 2007.0 2007.00 Radial Velocity 553.0 2007.518987 4.249052 1989.0 2005.00 Transit 397.0 2011.236776 2.077867 2002.0 2010.00 Transit Timing Variations 4.0 2012.500000 1.290994 2011.0 2011.75 50% 75% max method Astrometry 2011.5 2012.25 2013.0 Eclipse Timing Variations 2010.0 2011.00 2012.0 Imaging 2009.0 2011.00 2013.0 Microlensing 2010.0 2012.00 2013.0 Orbital Brightness Modulation 2011.0 2012.00 2013.0 Pulsar Timing 1994.0 2003.00 2011.0 Pulsation Timing Variations 2007.0 2007.00 2007.0 Radial Velocity 2009.0 2011.00 2014.0 Transit 2012.0 2013.00 2014.0 Transit Timing Variations 2012.5 2013.25 2014.0 Looking at this table helps us to better understand the data: for example, the vast majority of planets have been discovered by the Radial Velocity and Transit methods, though the latter only became common (due to new, more accurate telescopes) in the last decade. The newest methods seem to be Transit Timing Variation and Orbital Brightness Modulation, which were not used to discover a new planet until 2011. This is just one example of the utility of dispatch methods. Notice that they are applied to each individual group , and the results are then combined within GroupBy and returned. Again, any valid DataFrame / Series method can be used on the corre‐ sponding GroupBy object, which allows for some very flexible and powerful operations! Aggregate, filter, transform, apply The preceding discussion focused on aggregation for the combine operation, but there are more options available",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_170"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, GroupBy objects have aggregate() , filter() , transform() , and apply() methods that efficiently implement a variety of useful operations before combining the grouped data. For the purpose of the following subsections, we’ll use this DataFrame : In [ 19 ]: rng = np . random . RandomState ( 0 ) df = pd . DataFrame ({ 'key' : [ 'A' , 'B' , 'C' , 'A' , 'B' , 'C' ], 'data1' : range ( 6 ), 'data2' : rng . randint ( 0 , 10 , 6 )}, columns = [ 'key' , 'data1' , 'data2' ]) df Out[19]: key data1 data2 0 A 0 5 1 B 1 0 2 C 2 3 Aggregation and Grouping | 165 3 A 3 3 4 B 4 7 5 C 5 9 Aggregation. We’re now familiar with GroupBy aggregations with sum() , median() , and the like, but the aggregate() method allows for even more flexibility. It can take a string, a function, or a list thereof, and compute all the aggregates at once. Here is a quick example combining all these: In [ 20 ]: df . groupby ( 'key' ) . aggregate ([ 'min' , np . median , max ]) Out[20]: data1 data2 min median max min median max key A 0 1.5 3 3 4.0 5 B 1 2.5 4 0 3.5 7 C 2 3.5 5 3 6.0 9 Another useful pattern is to pass a dictionary mapping column names to operations to be applied on that column: In [ 21 ]: df . groupby ( 'key' ) . aggregate ({ 'data1' : 'min' , 'data2' : 'max' }) Out[21]: data1 data2 key A 0 5 B 1 7 C 2 9 Filtering. A filtering operation allows you to drop data based on the group proper‐ ties. For example, we might want to keep all groups in which the standard deviation is larger than some critical value: In [ 22 ]: def filter_func ( x ): return x [ 'data2' ] . std () > 4 print ( df ); print ( df . groupby ( 'key' ) . std ()); print ( df . groupby ( 'key' )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_171"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". std () > 4 print ( df ); print ( df . groupby ( 'key' ) . std ()); print ( df . groupby ( 'key' ) . filter ( filter_func )) df df.groupby('key').std() key data1 data2 key data1 data2 0 A 0 5 A 2.12132 1.414214 1 B 1 0 B 2.12132 4.949747 2 C 2 3 C 2.12132 4.242641 3 A 3 3 4 B 4 7 5 C 5 9 df.groupby('key').filter(filter_func) key data1 data2 1 B 1 0 166 | Chapter 3: Data Manipulation with Pandas 2 C 2 3 4 B 4 7 5 C 5 9 The filter() function should return a Boolean value specifying whether the group passes the filtering. Here because group A does not have a standard deviation greater than 4, it is dropped from the result. Transformation. While aggregation must return a reduced version of the data, trans‐ formation can return some transformed version of the full data to recombine. For such a transformation, the output is the same shape as the input. A common example is to center the data by subtracting the group-wise mean: In [ 23 ]: df . groupby ( 'key' ) . transform ( lambda x : x - x . mean ()) Out[23]: data1 data2 0 -1.5 1.0 1 -1.5 -3.5 2 -1.5 -3.0 3 1.5 -1.0 4 1.5 3.5 5 1.5 3.0 The apply() method. The apply() method lets you apply an arbitrary function to the group results. The function should take a DataFrame , and return either a Pandas object (e.g., DataFrame , Series ) or a scalar; the combine operation will be tailored to the type of output returned. For example, here is an apply() that normalizes the first column by the sum of the second: In [ 24 ]: def norm_by_data2 ( x ): # x is a DataFrame of group values x [ 'data1' ] /= x [ 'data2' ] . sum () return x print ( df ); print ( df . groupby ( 'key' )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_172"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sum () return x print ( df ); print ( df . groupby ( 'key' ) . apply ( norm_by_data2 )) df df.groupby('key').apply(norm_by_data2) key data1 data2 key data1 data2 0 A 0 5 0 A 0.000000 5 1 B 1 0 1 B 0.142857 0 2 C 2 3 2 C 0.166667 3 3 A 3 3 3 A 0.375000 3 4 B 4 7 4 B 0.571429 7 5 C 5 9 5 C 0.416667 9 apply() within a GroupBy is quite flexible: the only criterion is that the function takes a DataFrame and returns a Pandas object or scalar; what you do in the middle is up to you! Aggregation and Grouping | 167 Specifying the split key In the simple examples presented before, we split the DataFrame on a single column name. This is just one of many options by which the groups can be defined, and we’ll go through some other options for group specification here. A list, array, series, or index providing the grouping keys. The key can be any series or list with a length matching that of the DataFrame . For example: In [ 25 ]: L = [ 0 , 1 , 0 , 1 , 2 , 0 ] print ( df ); print ( df . groupby ( L ) . sum ()) df df.groupby(L).sum() key data1 data2 data1 data2 0 A 0 5 0 7 17 1 B 1 0 1 4 3 2 C 2 3 2 4 7 3 A 3 3 4 B 4 7 5 C 5 9 Of course, this means there’s another, more verbose way of accomplishing the df.groupby('key') from before: In [ 26 ]: print ( df ); print ( df . groupby ( df [ 'key' ]) . sum ()) df df.groupby(df['key']).sum() key data1 data2 data1 data2 0 A 0 5 A 3 8 1 B 1 0 B 5 7 2 C 2 3 C 7 12 3 A 3 3 4 B 4 7 5 C 5 9 A dictionary or series mapping index to group. Another method is to provide a dictionary that maps index values to the group keys: In [ 27 ]: df2 = df . set_index ( 'key' ) mapping = { 'A' : 'vowel' , 'B' : 'consonant' , 'C' : 'consonant' } print ( df2 ); print ( df2 . groupby ( mapping ) . sum ()) df2 df2.groupby(mapping).sum() key data1 data2 data1 data2 A 0 5 consonant 12 19 B 1 0 vowel 3 8 C 2 3 A 3 3 B 4 7 C 5 9 168 | Chapter 3: Data Manipulation with Pandas Any Python function",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_173"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sum ()) df2 df2.groupby(mapping).sum() key data1 data2 data1 data2 A 0 5 consonant 12 19 B 1 0 vowel 3 8 C 2 3 A 3 3 B 4 7 C 5 9 168 | Chapter 3: Data Manipulation with Pandas Any Python function. Similar to mapping, you can pass any Python function that will input the index value and output the group: In [ 28 ]: print ( df2 ); print ( df2 . groupby ( str . lower ) . mean ()) df2 df2.groupby(str.lower).mean() key data1 data2 data1 data2 A 0 5 a 1.5 4.0 B 1 0 b 2.5 3.5 C 2 3 c 3.5 6.0 A 3 3 B 4 7 C 5 9 A list of valid keys. Further, any of the preceding key choices can be combined to group on a multi-index: In [ 29 ]: df2 . groupby ([ str . lower , mapping ]) . mean () Out[29]: data1 data2 a vowel 1.5 4.0 b consonant 2.5 3.5 c consonant 3.5 6.0 Grouping example As an example of this, in a couple lines of Python code we can put all these together and count discovered planets by method and by decade: In [ 30 ]: decade = 10 * ( planets [ 'year' ] // 10 ) decade = decade . astype ( str ) + 's' decade . name = 'decade' planets . groupby ([ 'method' , decade ])[ 'number' ] . sum () . unstack () . fillna ( 0 ) Out[30]: decade 1980s 1990s 2000s 2010s method Astrometry 0.0 0.0 0.0 2.0 Eclipse Timing Variations 0.0 0.0 5.0 10.0 Imaging 0.0 0.0 29.0 21.0 Microlensing 0.0 0.0 12.0 15.0 Orbital Brightness Modulation 0.0 0.0 0.0 5.0 Pulsar Timing 0.0 9.0 1.0 1.0 Pulsation Timing Variations 0.0 0.0 1.0 0.0 Radial Velocity 1.0 52.0 475.0 424.0 Transit 0.0 0.0 64.0 712.0 Transit Timing Variations 0.0 0.0 0.0 9.0 This shows the power of combining many of the operations we’ve discussed up to this point when looking at realistic datasets. We immediately gain a coarse understanding of when and how planets have been discovered over the past several decades! Aggregation and Grouping | 169 Here I would suggest digging into these few lines of code, and evaluating the individ‐ ual steps to make sure you understand exactly what they are doing to the result",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_174"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It’s certainly a somewhat complicated example, but understanding these pieces will give you the means to similarly explore your own data. Pivot Tables We have seen how the GroupBy abstraction lets us explore relationships within a data‐ set. A pivot table is a similar operation that is commonly seen in spreadsheets and other programs that operate on tabular data. The pivot table takes simple columwise data as input, and groups the entries into a two-dimensional table that provides a multidimensional summarization of the data. The difference between pivot tables and GroupBy can sometimes cause confusion; it helps me to think of pivot tables as essentially a multidimensional version of GroupBy aggregation. That is, you spliapply-combine, but both the split and the combine happen across not a ondimensional index, but across a two-dimensional grid. Motivating Pivot Tables For the examples in this section, we’ll use the database of passengers on the Titanic , available through the Seaborn library (see “Visualization with Seaborn” on page 311 ): In [ 1 ]: import numpy as np import pandas as pd import seaborn as sns titanic = sns . load_dataset ( 'titanic' ) In [ 2 ]: titanic . head () Out[2]: survived pclass sex age sibsp parch fare embarked class \\\\ 0 0 3 male 22.0 1 0 7.2500 S Third 1 1 1 female 38.0 1 0 71.2833 C First 2 1 3 female 26.0 0 0 7.9250 S Third 3 1 1 female 35.0 1 0 53.1000 S First 4 0 3 male 35.0 0 0 8.0500 S Third who adult_male deck embark_town alive alone 0 man True NaN Southampton no False 1 woman False C Cherbourg yes False 2 woman False NaN Southampton yes True 3 woman False C Southampton yes False 4 man True NaN Southampton no True This contains a wealth of information on each passenger of that ill-fated voyage, including gender, age, class, fare paid, and much more. 170 | Chapter 3: Data Manipulation with Pandas Pivot Tables by Hand To start learning more about this data, we might begin by grouping it according to gender, survival status, or some combination thereof",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_175"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If you have read the previous section, you might be tempted to apply a GroupBy operation—for example, let’s look at survival rate by gender: In [ 3 ]: titanic . groupby ( 'sex' )[[ 'survived' ]] . mean () Out[3]: survived sex female 0.742038 male 0.188908 This immediately gives us some insight: overall, three of every four females on board survived, while only one in five males survived! This is useful, but we might like to go one step deeper and look at survival by both sex and, say, class. Using the vocabulary of GroupBy , we might proceed using something like this: we group by class and gender, select survival, apply a mean aggregate, com‐ bine the resulting groups, and then unstack the hierarchical index to reveal the hidden multidimensionality. In code: In [ 4 ]: titanic . groupby ([ 'sex' , 'class' ])[ 'survived' ] . aggregate ( 'mean' ) . unstack () Out[4]: class First Second Third sex female 0.968085 0.921053 0.500000 male 0.368852 0.157407 0.135447 This gives us a better idea of how both gender and class affected survival, but the code is starting to look a bit garbled. While each step of this pipeline makes sense in light of the tools we’ve previously discussed, the long string of code is not particularly easy to read or use. This two-dimensional GroupBy is common enough that Pandas includes a convenience routine, pivot_table , which succinctly handles this type of multidimensional aggregation. Pivot Table Syntax Here is the equivalent to the preceding operation using the pivot_table method of DataFrame s: In [ 5 ]: titanic . pivot_table ( 'survived' , index = 'sex' , columns = 'class' ) Out[5]: class First Second Third sex female 0.968085 0.921053 0.500000 male 0.368852 0.157407 0.135447 This is eminently more readable than the GroupBy approach, and produces the same result. As you might expect of an early 20th-century transatlantic cruise, the survival Pivot Tables | 171 gradient favors both women and higher classes",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_176"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As you might expect of an early 20th-century transatlantic cruise, the survival Pivot Tables | 171 gradient favors both women and higher classes. First-class women survived with near certainty (hi, Rose!), while only one in ten third-class men survived (sorry, Jack!). Multilevel pivot tables Just as in the GroupBy , the grouping in pivot tables can be specified with multiple lev‐ els, and via a number of options. For example, we might be interested in looking at age as a third dimension. We’ll bin the age using the pd.cut function: In [ 6 ]: age = pd . cut ( titanic [ 'age' ], [ 0 , 18 , 80 ]) titanic . pivot_table ( 'survived' , [ 'sex' , age ], 'class' ) Out[6]: class First Second Third sex age female (0, 18] 0.909091 1.000000 0.511628 (18, 80] 0.972973 0.900000 0.423729 male (0, 18] 0.800000 0.600000 0.215686 (18, 80] 0.375000 0.071429 0.133663 We can apply this same strategy when working with the columns as well; let’s add info on the fare paid using pd.qcut to automatically compute quantiles: In [ 7 ]: fare = pd . qcut ( titanic [ 'fare' ], 2 ) titanic . pivot_table ( 'survived' , [ 'sex' , age ], [ fare , 'class' ]) Out[7]: fare [0, 14.454] class First Second Third \\\\ sex age female (0, 18] NaN 1.000000 0.714286 (18, 80] NaN 0.880000 0.444444 male (0, 18] NaN 0.000000 0.260870 (18, 80] 0.0 0.098039 0.125000 fare (14.454, 512.329] class First Second Third sex age female (0, 18] 0.909091 1.000000 0.318182 (18, 80] 0.972973 0.914286 0.391304 male (0, 18] 0.800000 0.818182 0.178571 (18, 80] 0.391304 0.030303 0.192308 The result is a four-dimensional aggregation with hierarchical indices (see “Hierarch‐ ical Indexing” on page 128 ), shown in a grid demonstrating the relationship between the values. 172 | Chapter 3: Data Manipulation with Pandas Additional pivot table options The full call signature of the pivot_table method of DataFrame s is as follows: # call signature as of Pandas 0.18 DataFrame",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_177"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". pivot_table ( data , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' ) We’ve already seen examples of the first three arguments; here we’ll take a quick look at the remaining ones. Two of the options, fill_value and dropna , have to do with missing data and are fairly straightforward; we will not show examples of them here. The aggfunc keyword controls what type of aggregation is applied, which is a mean by default. As in the GroupBy , the aggregation specification can be a string represent‐ ing one of several common choices ( 'sum' , 'mean' , 'count' , 'min' , 'max' , etc.) or a function that implements an aggregation ( np.sum() , min() , sum() , etc.). Additionally, it can be specified as a dictionary mapping a column to any of the above desired options: In [ 8 ]: titanic . pivot_table ( index = 'sex' , columns = 'class' , aggfunc = { 'survived' : sum , 'fare' : 'mean' }) Out[8]: fare survived class First Second Third First Second Third sex female 106.125798 21.970121 16.118810 91.0 70.0 72.0 male 67.226127 19.741782 12.661633 45.0 17.0 47.0 Notice also here that we’ve omitted the values keyword; when you’re specifying a mapping for aggfunc , this is determined automatically. At times it’s useful to compute totals along each grouping. This can be done via the margins keyword: In [ 9 ]: titanic . pivot_table ( 'survived' , index = 'sex' , columns = 'class' , margins = True ) Out[9]: class First Second Third All sex female 0.968085 0.921053 0.500000 0.742038 male 0.368852 0.157407 0.135447 0.188908 All 0.629630 0.472826 0.242363 0.383838 Here this automatically gives us information about the class-agnostic survival rate by gender, the gender-agnostic survival rate by class, and the overall survival rate of 38%. The margin label can be specified with the margins_name keyword, which defaults to \"All\"",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_178"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The margin label can be specified with the margins_name keyword, which defaults to \"All\" . Pivot Tables | 173 Example: Birthrate Data As a more interesting example, let’s take a look at the freely available data on births in the United States, provided by the Centers for Disease Control (CDC). This data can be found at https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/ births.csv (this dataset has been analyzed rather extensively by Andrew Gelman and his group; see, for example, this blog post ): In [ 10 ]: # shell command to download the data: # !curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/ # master/births.csv In [ 11 ]: births = pd . read_csv ( 'births.csv' ) Taking a look at the data, we see that it’s relatively simple—it contains the number of births grouped by date and gender: In [ 12 ]: births . head () Out[12]: year month day gender births 0 1969 1 1 F 4046 1 1969 1 1 M 4440 2 1969 1 2 F 4454 3 1969 1 2 M 4548 4 1969 1 3 F 4548 We can start to understand this data a bit more by using a pivot table. Let’s add a dec‐ ade column, and take a look at male and female births as a function of decade: In [ 13 ]: births [ 'decade' ] = 10 * ( births [ 'year' ] // 10 ) births . pivot_table ( 'births' , index = 'decade' , columns = 'gender' , aggfunc = 'sum' ) Out[13]: gender F M decade 1960 1753634 1846572 1970 16263075 17121550 1980 18310351 19243452 1990 19479454 20420553 2000 18229309 19106428 We immediately see that male births outnumber female births in every decade. To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to visual‐ ize the total number of births by year ( Figure 3-2 ; see Chapter 4 for a discussion of plotting with Matplotlib): In [ 14 ]: % matplotlib inline import matplotlib.pyplot as plt sns . set () # use Seaborn styles births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_179"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". set () # use Seaborn styles births . pivot_table ( 'births' , index = 'year' , columns = 'gender' , aggfunc = 'sum' ) . plot () plt . ylabel ( 'total births per year' ); 174 | Chapter 3: Data Manipulation with Pandas 1 You can learn more about sigma-clipping operations in a book I coauthored with Željko Ivezić, Andrew J. Connolly, and Alexander Gray: Statistics, Data Mining, and Machine Learning in Astronomy: A Practical Python Guide for the Analysis of Survey Data (Princeton University Press, 2014). Figure 3-2. Total number of US births by year and gender With a simple pivot table and plot() method, we can immediately see the annual trend in births by gender. By eye, it appears that over the past 50 years male births have outnumbered female births by around 5%. Further data exploration Though this doesn’t necessarily relate to the pivot table, there are a few more interest‐ ing features we can pull out of this dataset using the Pandas tools covered up to this point. We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g., June 31st) or missing values (e.g., June 99th). One easy way to remove these all at once is to cut outliers; we’ll do this via a robust sigma-clipping operation: 1 In [ 15 ]: quartiles = np . percentile ( births [ 'births' ], [ 25 , 50 , 75 ]) mu = quartiles [ 1 ] sig = 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) This final line is a robust estimate of the sample mean, where the 0.74 comes from the interquartile range of a Gaussian distribution. With this we can use the query() method (discussed further in “High-Performance Pandas: eval() and query()” on page 208 ) to filter out rows with births outside these values: In [ 16 ]: births = births",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_180"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". query ( '(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)' ) Pivot Tables | 175 Next we set the day column to integers; previously it had been a string because some columns in the dataset contained the value 'null' : In [ 17 ]: # set 'day' column to integer; it originally was a string due to nulls births [ 'day' ] = births [ 'day' ] . astype ( int ) Finally, we can combine the day, month, and year to create a Date index (see “Work‐ ing with Time Series” on page 188 ). This allows us to quickly compute the weekday corresponding to each row: In [ 18 ]: # create a datetime index from the year, month, day births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = ' %Y%m%d ' ) births [ 'dayofweek' ] = births . index . dayofweek Using this we can plot births by weekday for several decades ( Figure 3-3 ): In [ 19 ]: import matplotlib.pyplot as plt import matplotlib as mpl births . pivot_table ( 'births' , index = 'dayofweek' , columns = 'decade' , aggfunc = 'mean' ) . plot () plt . gca () . set_xticklabels ([ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ]) plt . ylabel ( 'mean births by day' ); Figure 3-3. Average daily births by day of week and decade Apparently births are slightly less common on weekends than on weekdays! Note that the 1990s and 2000s are missing because the CDC data contains only the month of birth starting in 1989. 176 | Chapter 3: Data Manipulation with Pandas Another interesting view is to plot the mean number of births by the day of the year . Let’s first group the data by month and day separately: In [ 20 ]: births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . head () Out[20]: 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 Name: births, dtype: float64 The result is a multi-index over months and days",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_181"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". index . day ]) births_by_date . head () Out[20]: 1 1 4009.225 2 4247.400 3 4500.900 4 4571.350 5 4603.625 Name: births, dtype: float64 The result is a multi-index over months and days. To make this easily plottable, let’s turn these months and days into a date by associating them with a dummy year vari‐ able (making sure to choose a leap year so February 29th is correctly handled!) In [ 21 ]: births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] births_by_date . head () Out[21]: 2012-01-01 4009.225 2012-01-02 4247.400 2012-01-03 4500.900 2012-01-04 4571.350 2012-01-05 4603.625 Name: births, dtype: float64 Focusing on the month and day only, we now have a time series reflecting the average number of births by date of the year. From this, we can use the plot method to plot the data ( Figure 3-4 ). It reveals some interesting trends: In [ 22 ]: # Plot the results fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ); Figure 3-4. Average daily births by date Pivot Tables | 177 In particular, the striking feature of this graph is the dip in birthrate on US holidays (e.g., Independence Day, Labor Day, Thanksgiving, Christmas, New Year’s Day) although this likely reflects trends in scheduled/induced births rather than some deep psychosomatic effect on natural births. For more discussion on this trend, see the analysis and links in Andrew Gelman’s blog post on the subject. We’ll return to this figure in “Example: Effect of Holidays on US Births” on page 269 , where we will use Matplotlib’s tools to annotate this plot. Looking at this short example, you can see that many of the Python and Pandas tools we’ve seen to this point can be combined and used to gain insight from a variety of datasets. We will see some more sophisticated applications of these data manipula‐ tions in future sections! Vectorized String Operations One strength of Python is its relative ease in handling and manipulating string data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_182"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Pandas builds on this and provides a comprehensive set of vectorized string operations that become an essential piece of the type of munging required when one is working with (read: cleaning up) real-world data. In this section, we’ll walk through some of the Pandas string operations, and then take a look at using them to partially clean up a very messy dataset of recipes collected from the Internet. Introducing Pandas String Operations We saw in previous sections how tools like NumPy and Pandas generalize arithmetic operations so that we can easily and quickly perform the same operation on many array elements. For example: In [ 1 ]: import numpy as np x = np . array ([ 2 , 3 , 5 , 7 , 11 , 13 ]) x * 2 Out[1]: array([ 4, 6, 10, 14, 22, 26]) This vectorization of operations simplifies the syntax of operating on arrays of data: we no longer have to worry about the size or shape of the array, but just about what operation we want done. For arrays of strings, NumPy does not provide such simple access, and thus you’re stuck using a more verbose loop syntax: In [ 2 ]: data = [ 'peter' , 'Paul' , 'MARY' , 'gUIDO' ] [ s . capitalize () for s in data ] Out[2]: ['Peter', 'Paul', 'Mary', 'Guido'] This is perhaps sufficient to work with some data, but it will break if there are any missing values. For example: In [ 3 ]: data = [ 'peter' , 'Paul' , None , 'MARY' , 'gUIDO' ] [ s",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_183"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example: In [ 3 ]: data = [ 'peter' , 'Paul' , None , 'MARY' , 'gUIDO' ] [ s . capitalize () for s in data ] 178 | Chapter 3: Data Manipulation with Pandas --------------------------------------------------------------------------- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-3-fc1d891ab539> in <module>() 1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO'] ----> 2 [s.capitalize() for s in data] <ipython-input-3-fc1d891ab539> in <listcomp>(.0) 1 data = ['peter', 'Paul', None, 'MARY', 'gUIDO'] ----> 2 [s.capitalize() for s in data] AttributeError: 'NoneType' object has no attribute 'capitalize' --------------------------------------------------------------------------- Pandas includes features to address both this need for vectorized string operations and for correctly handling missing data via the str attribute of Pandas Series and Index objects containing strings. So, for example, suppose we create a Pandas Series with this data: In [ 4 ]: import pandas as pd names = pd . Series ( data ) names Out[4]: 0 peter 1 Paul 2 None 3 MARY 4 gUIDO dtype: object We can now call a single method that will capitalize all the entries, while skipping over any missing values: In [ 5 ]: names . str . capitalize () Out[5]: 0 Peter 1 Paul 2 None 3 Mary 4 Guido dtype: object Using tab completion on this str attribute will list all the vectorized string methods available to Pandas. Vectorized String Operations | 179 Tables of Pandas String Methods If you have a good understanding of string manipulation in Python, most of Pandas’ string syntax is intuitive enough that it’s probably sufficient to just list a table of avail‐ able methods; we will start with that here, before diving deeper into a few of the sub‐ tleties. The examples in this section use the following series of names: In [ 6 ]: monte = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_184"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The examples in this section use the following series of names: In [ 6 ]: monte = pd . Series ([ 'Graham Chapman' , 'John Cleese' , 'Terry Gilliam' , 'Eric Idle' , 'Terry Jones' , 'Michael Palin' ]) Methods similar to Python string methods Nearly all Python’s built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods: len() lower() translate() islower() ljust() upper() startswith() isupper() rjust() find() endswith() isnumeric() center() rfind() isalnum() isdecimal() zfill() index() isalpha() split() strip() rindex() isdigit() rsplit() rstrip() capitalize() isspace() partition() lstrip() swapcase() istitle() rpartition() Notice that these have various return values. Some, like lower() , return a series of strings: In [ 7 ]: monte . str . lower () Out[7]: 0 graham chapman 1 john cleese 2 terry gilliam 3 eric idle 4 terry jones 5 michael palin dtype: object But some others return numbers: In [ 8 ]: monte . str . len () Out[8]: 0 14 1 11 2 13 3 9 4 11 5 13 dtype: int64 180 | Chapter 3: Data Manipulation with Pandas Or Boolean values: In [ 9 ]: monte . str . startswith ( 'T' ) Out[9]: 0 False 1 False 2 True 3 False 4 True 5 False dtype: bool Still others return lists or other compound values for each element: In [ 10 ]: monte . str . split () Out[10]: 0 [Graham, Chapman] 1 [John, Cleese] 2 [Terry, Gilliam] 3 [Eric, Idle] 4 [Terry, Jones] 5 [Michael, Palin] dtype: object We’ll see further manipulations of this kind of series-of-lists object as we continue our discussion. Methods using regular expressions In addition, there are several methods that accept regular expressions to examine the content of each string element, and follow some of the API conventions of Python’s built-in re module (see Table 3-4 ). Table 3-4. Mapping between Pandas methods and functions in Python’s re module Method Description match() Call re.match() on each element, returning a Boolean",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_185"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Table 3-4. Mapping between Pandas methods and functions in Python’s re module Method Description match() Call re.match() on each element, returning a Boolean. extract() Call re.match() on each element, returning matched groups as strings. findall() Call re.findall() on each element. replace() Replace occurrences of pattern with some other string. contains() Call re.search() on each element, returning a Boolean. count() Count occurrences of pattern. split() Equivalent to str.split() , but accepts regexps. rsplit() Equivalent to str.rsplit() , but accepts regexps. With these, you can do a wide range of interesting operations. For example, we can extract the first name from each by asking for a contiguous group of characters at the beginning of each element: Vectorized String Operations | 181 In [ 11 ]: monte . str . extract ( '([A-Za-z]+)' ) Out[11]: 0 Graham 1 John 2 Terry 3 Eric 4 Terry 5 Michael dtype: object Or we can do something more complicated, like finding all names that start and end with a consonant, making use of the start-of-string ( ^ ) and end-of-string ( $ ) regular expression characters: In [ 12 ]: monte . str . findall (r '^[^AEIOU].*[^aeiou]$' ) Out[12]: 0 [Graham Chapman] 1 [] 2 [Terry Gilliam] 3 [] 4 [Terry Jones] 5 [Michael Palin] dtype: object The ability to concisely apply regular expressions across Series or DataFrame entries opens up many possibilities for analysis and cleaning of data. Miscellaneous methods Finally, there are some miscellaneous methods that enable other convenient opera‐ tions (see Table 3-5 ). Table 3-5",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_186"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Miscellaneous methods Finally, there are some miscellaneous methods that enable other convenient opera‐ tions (see Table 3-5 ). Table 3-5. Other Pandas string methods Method Description get() Index each element slice() Slice each element slice_replace() Replace slice in each element with passed value cat() Concatenate strings repeat() Repeat values normalize() Return Unicode form of string pad() Add whitespace to left, right, or both sides of strings wrap() Split long strings into lines with length less than a given width join() Join strings in each element of the Series with passed separator get_dummies() Extract dummy variables as a DataFrame 182 | Chapter 3: Data Manipulation with Pandas Vectorized item access and slicing. The get() and slice() operations, in particular, enable vectorized element access from each array. For example, we can get a slice of the first three characters of each array using str.slice(0, 3) . Note that this behav‐ ior is also available through Python’s normal indexing syntax—for example, df.str.slice(0, 3) is equivalent to df.str[0:3] : In [ 13 ]: monte . str [ 0 : 3 ] Out[13]: 0 Gra 1 Joh 2 Ter 3 Eri 4 Ter 5 Mic dtype: object Indexing via df.str.get(i) and df.str[i] is similar. These get() and slice() methods also let you access elements of arrays returned by split() . For example, to extract the last name of each entry, we can combine split() and get() : In [ 14 ]: monte . str . split () . str . get ( - 1 ) Out[14]: 0 Chapman 1 Cleese 2 Gilliam 3 Idle 4 Jones 5 Palin dtype: object Indicator variables. Another method that requires a bit of extra explanation is the get_dummies() method. This is useful when your data has a column containing some sort of coded indicator. For example, we might have a dataset that contains informa‐ tion in the form of codes, such as A=“born in America,” B=“born in the United King‐ dom,” C=“likes cheese,” D=“likes spam”: In [ 15 ]: full_monte = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_187"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". DataFrame ({ 'name' : monte , 'info' : [ 'B|C|D' , 'B|D' , 'A|C' , 'B|D' , 'B|C' , 'B|C|D' ]}) full_monte Out[15]: info name 0 B|C|D Graham Chapman 1 B|D John Cleese 2 A|C Terry Gilliam 3 B|D Eric Idle 4 B|C Terry Jones 5 B|C|D Michael Palin Vectorized String Operations | 183 The get_dummies() routine lets you quickly split out these indicator variables into a DataFrame : In [ 16 ]: full_monte [ 'info' ] . str . get_dummies ( '|' ) Out[16]: A B C D 0 0 1 1 1 1 0 1 0 1 2 1 0 1 0 3 0 1 0 1 4 0 1 1 0 5 0 1 1 1 With these operations as building blocks, you can construct an endless range of string processing procedures when cleaning your data. We won’t dive further into these methods here, but I encourage you to read through “Working with Text Data” in the pandas online documentation , or to refer to the resources listed in “Further Resources” on page 215 . Example: Recipe Database These vectorized string operations become most useful in the process of cleaning up messy, real-world data. Here I’ll walk through an example of that, using an open recipe database compiled from various sources on the Web. Our goal will be to parse the recipe data into ingredient lists, so we can quickly find a recipe based on some ingredients we have on hand. The scripts used to compile this can be found at https://github.com/fictivekin/openre cipes , and the link to the current version of the database is found there as well. As of spring 2016, this database is about 30 MB, and can be downloaded and unzip‐ ped with these commands: In [ 17 ]: # !curl -O http://openrecipes.s3.amazonaws.com/recipeitems-latest.json.gz # !gunzip recipeitems-latest.json.gz The database is in JSON format, so we will try pd.read_json to read it: In [ 18 ]: try : recipes = pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_188"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". read_json ( 'recipeitems-latest.json' ) except ValueError as e : print ( \"ValueError:\" , e ) ValueError: Trailing data Oops! We get a ValueError mentioning that there is “trailing data.” Searching for this error on the Internet, it seems that it’s due to using a file in which each line is itself a valid JSON, but the full file is not. Let’s check if this interpretation is true: 184 | Chapter 3: Data Manipulation with Pandas In [ 19 ]: with open ( 'recipeitems-latest.json' ) as f : line = f . readline () pd . read_json ( line ) . shape Out[19]: (2, 12) Yes, apparently each line is a valid JSON, so we’ll need to string them together. One way we can do this is to actually construct a string representation containing all these JSON entries, and then load the whole thing with pd.read_json : In [ 20 ]: # read the entire file into a Python array with open ( 'recipeitems-latest.json' , 'r' ) as f : # Extract each line data = ( line . strip () for line in f ) # Reformat so each line is the element of a list data_json = \"[{0}]\" . format ( ',' . join ( data )) # read the result as a JSON recipes = pd . read_json ( data_json ) In [ 21 ]: recipes . shape Out[21]: (173278, 17) We see there are nearly 200,000 recipes, and 17 columns. Let’s take a look at one row to see what we have: In [ 22 ]: recipes . iloc [ 0 ] Out[22]: _id {'$oid': '5160756b96cc62079cc2db15'} cookTime PT30M creator NaN dateModified NaN datePublished 2013-03-11 description Late Saturday afternoon, after Marlboro Man haimage http://static.thepioneerwoman.com/cooking/fileingredients Biscuits\\n3 cups All-purpose Flour\\n2 Tablesponame Drop Biscuits and Sausage Gravy prepTime PT10M recipeCategory NaN recipeInstructions NaN recipeYield 12 source thepioneerwoman totalTime NaN ts {'$date': 1365276011104} url http://thepioneerwoman.com/cooking/2013/03/droName: 0, dtype: object There is a lot of information there, but much of it is in a very messy form, as is typical of data scraped from the Web",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_189"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, the ingredient list is in string format; we’re going to have to carefully extract the information we’re interested in. Let’s start by taking a closer look at the ingredients: In [ 23 ]: recipes . ingredients . str . len () . describe () Vectorized String Operations | 185 Out[23]: count 173278.000000 mean 244.617926 std 146.705285 min 0.000000 25% 147.000000 50% 221.000000 75% 314.000000 max 9067.000000 Name: ingredients, dtype: float64 The ingredient lists average 250 characters long, with a minimum of 0 and a maxi‐ mum of nearly 10,000 characters! Just out of curiosity, let’s see which recipe has the longest ingredient list: In [ 24 ]: recipes . name [ np . argmax ( recipes . ingredients . str . len ())] Out[24]: 'Carrot Pineapple Spice &amp; Brownie Layer Cake with Whipped Cream &amp; Cream Cheese Frosting and Marzipan Carrots' That certainly looks like an involved recipe. We can do other aggregate explorations; for example, let’s see how many of the rec‐ ipes are for breakfast food: In [ 33 ]: recipes . description . str . contains ( '[Bb]reakfast' ) . sum () Out[33]: 3524 Or how many of the recipes list cinnamon as an ingredient: In [ 34 ]: recipes . ingredients . str . contains ( '[Cc]innamon' ) . sum () Out[34]: 10526 We could even look to see whether any recipes misspell the ingredient as “cinamon”: In [ 27 ]: recipes . ingredients . str . contains ( '[Cc]inamon' ) . sum () Out[27]: 11 This is the type of essential data exploration that is possible with Pandas string tools. It is data munging like this that Python really excels at. A simple recipe recommender Let’s go a bit further, and start working on a simple recipe recommendation system: given a list of ingredients, find a recipe that uses all those ingredients. While concep‐ tually straightforward, the task is complicated by the heterogeneity of the data: there is no easy operation, for example, to extract a clean list of ingredients from each row",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_190"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While concep‐ tually straightforward, the task is complicated by the heterogeneity of the data: there is no easy operation, for example, to extract a clean list of ingredients from each row. So we will cheat a bit: we’ll start with a list of common ingredients, and simply search to see whether they are in each recipe’s ingredient list. For simplicity, let’s just stick with herbs and spices for the time being: 186 | Chapter 3: Data Manipulation with Pandas In [ 28 ]: spice_list = [ 'salt' , 'pepper' , 'oregano' , 'sage' , 'parsley' , 'rosemary' , 'tarragon' , 'thyme' , 'paprika' , 'cumin' ] We can then build a Boolean DataFrame consisting of True and False values, indicat‐ ing whether this ingredient appears in the list: In [ 29 ]: import re spice_df = pd . DataFrame ( dict (( spice , recipes . ingredients . str . contains ( spice , re . IGNORECASE )) for spice in spice_list )) spice_df . head () Out[29]: cumin oregano paprika parsley pepper rosemary sage salt tarragon thyme 0 False False False False False False True False False False 1 False False False False False False False False False False 2 True False False False True False False True False False 3 False False False False False False False False False False 4 False False False False False False False False False False Now, as an example, let’s say we’d like to find a recipe that uses parsley, paprika, and tarragon. We can compute this very quickly using the query() method of Data Frame s, discussed in “High-Performance Pandas: eval() and query()” on page 208 : In [ 30 ]: selection = spice_df . query ( 'parsley & paprika & tarragon' ) len ( selection ) Out[30]: 10 We find only 10 recipes with this combination; let’s use the index returned by this selection to discover the names of the recipes that have this combination: In [ 31 ]: recipes . name [ selection",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_191"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". name [ selection . index ] Out[31]: 2069 All cremat with a Little Gem, dandelion and wa74964 Lobster with Thermidor butter 93768 Burton's Southern Fried Chicken with White Gravy 113926 Mijo's Slow Cooker Shredded Beef 137686 Asparagus Soup with Poached Eggs 140530 Fried Oyster Po’boys 158475 Lamb shank tagine with herb tabbouleh 158486 Southern fried chicken in buttermilk 163175 Fried Chicken Sliders with Pickles + Slaw 165243 Bar Tartine Cauliflower Salad Name: name, dtype: object Now that we have narrowed down our recipe selection by a factor of almost 20,000, we are in a position to make a more informed decision about what we’d like to cook for dinner. Vectorized String Operations | 187 Going further with recipes Hopefully this example has given you a bit of a flavor (ba-dum!) for the types of data cleaning operations that are efficiently enabled by Pandas string methods. Of course, building a very robust recipe recommendation system would require a lot more work! Extracting full ingredient lists from each recipe would be an important piece of the task; unfortunately, the wide variety of formats used makes this a relatively timconsuming process. This points to the truism that in data science, cleaning and munging of real-world data often comprises the majority of the work, and Pandas provides the tools that can help you do this efficiently. Working with Time Series Pandas was developed in the context of financial modeling, so as you might expect, it contains a fairly extensive set of tools for working with dates, times, and timindexed data. Date and time data comes in a few flavors, which we will discuss here: • Time stamps reference particular moments in time (e.g., July 4th, 2015, at 7:00 a.m.). • Time intervals and periods reference a length of time between a particular begin‐ ning and end point—for example, the year 2015. Periods usually reference a spe‐ cial case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_192"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Periods usually reference a spe‐ cial case of time intervals in which each interval is of uniform length and does not overlap (e.g., 24 hour-long periods constituting days). • Time deltas or durations reference an exact length of time (e.g., a duration of 22.56 seconds). In this section, we will introduce how to work with each of these types of date/time data in Pandas. This short section is by no means a complete guide to the time series tools available in Python or Pandas, but instead is intended as a broad overview of how you as a user should approach working with time series. We will start with a brief discussion of tools for dealing with dates and times in Python, before moving more specifically to a discussion of the tools provided by Pandas. After listing some resources that go into more depth, we will review some short examples of working with time series data in Pandas. Dates and Times in Python The Python world has a number of available representations of dates, times, deltas, and timespans. While the time series tools provided by Pandas tend to be the most useful for data science applications, it is helpful to see their relationship to other packages used in Python. 188 | Chapter 3: Data Manipulation with Pandas Native Python dates and times: datetime and dateutil Python’s basic objects for working with dates and times reside in the built-in date time module. Along with the third-party dateutil module, you can use it to quickly perform a host of useful functionalities on dates and times. For example, you can manually build a date using the datetime type: In [ 1 ]: from datetime import datetime datetime ( year = 2015 , month = 7 , day = 4 ) Out[1]: datetime.datetime(2015, 7, 4, 0, 0) Or, using the dateutil module, you can parse dates from a variety of string formats: In [ 2 ]: from dateutil import parser date = parser . parse ( \"4th of July, 2015\" ) date Out[2]: datetime.datetime(2015, 7, 4, 0, 0) Once you have a datetime object, you can do things like printing the day of the week: In [ 3 ]: date",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_193"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". parse ( \"4th of July, 2015\" ) date Out[2]: datetime.datetime(2015, 7, 4, 0, 0) Once you have a datetime object, you can do things like printing the day of the week: In [ 3 ]: date . strftime ( '%A' ) Out[3]: 'Saturday' In the final line, we’ve used one of the standard string format codes for printing dates ( \"%A\" ), which you can read about in the strftime section of Python’s datetime docu‐ mentation . Documentation of other useful date utilities can be found in dateutil’s online documentation . A related package to be aware of is pytz , which contains tools for working with the most migraine-inducing piece of time series data: time zones. The power of datetime and dateutil lies in their flexibility and easy syntax: you can use these objects and their built-in methods to easily perform nearly any operation you might be interested in. Where they break down is when you wish to work with large arrays of dates and times: just as lists of Python numerical variables are subopti‐ mal compared to NumPy-style typed numerical arrays, lists of Python datetime objects are suboptimal compared to typed arrays of encoded dates. Typed arrays of times: NumPy’s datetime64 The weaknesses of Python’s datetime format inspired the NumPy team to add a set of native time series data type to NumPy. The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be represented very compactly. The date time64 requires a very specific input format: In [ 4 ]: import numpy as np date = np . array ( '2015-07-04' , dtype = np . datetime64 ) date Out[4]: array(datetime.date(2015, 7, 4), dtype='datetime64[D]') Working with Time Series | 189 Once we have this date formatted, however, we can quickly do vectorized operations on it: In [ 5 ]: date + np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_194"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". arange ( 12 ) Out[5]: array(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11', '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'], dtype='datetime64[D]') Because of the uniform type in NumPy datetime64 arrays, this type of operation can be accomplished much more quickly than if we were working directly with Python’s datetime objects, especially as arrays get large (we introduced this type of vectoriza‐ tion in “Computation on NumPy Arrays: Universal Functions” on page 50 ). One detail of the datetime64 and timedelta64 objects is that they are built on a fun‐ damental time unit . Because the datetime64 object is limited to 64-bit precision, the range of encodable times is 2 64 times this fundamental unit. In other words, date time64 imposes a trade-off between time resolution and maximum time span . For example, if you want a time resolution of one nanosecond, you only have enough information to encode a range of 2 64 nanoseconds, or just under 600 years. NumPy will infer the desired unit from the input; for example, here is a day-based datetime: In [ 6 ]: np . datetime64 ( '2015-07-04' ) Out[6]: numpy.datetime64('2015-07-04') Here is a minute-based datetime: In [ 7 ]: np . datetime64 ( '2015-07-04 12:00' ) Out[7]: numpy.datetime64('2015-07-04T12:00') Notice that the time zone is automatically set to the local time on the computer exe‐ cuting the code. You can force any desired fundamental unit using one of many for‐ mat codes; for example, here we’ll force a nanosecond-based time: In [ 8 ]: np . datetime64 ( '2015-07-04 12:59:59.50' , 'ns' ) Out[8]: numpy.datetime64('2015-07-04T12:59:59.500000000') Table 3-6 , drawn from the NumPy datetime64 documentation , lists the available for‐ mat codes along with the relative and absolute timespans that they can encode. Table 3-6",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_195"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Table 3-6. Description of date and time codes Code Meaning Time span (relative) Time span (absolute) Y Year ± 9.2e18 years [9.2e18 BC, 9.2e18 AD] M Month ± 7.6e17 years [7.6e17 BC, 7.6e17 AD] W Week ± 1.7e17 years [1.7e17 BC, 1.7e17 AD] 190 | Chapter 3: Data Manipulation with Pandas Code Meaning Time span (relative) Time span (absolute) D Day ± 2.5e16 years [2.5e16 BC, 2.5e16 AD] h Hour ± 1.0e15 years [1.0e15 BC, 1.0e15 AD] m Minute ± 1.7e13 years [1.7e13 BC, 1.7e13 AD] s Second ± 2.9e12 years [ 2.9e9 BC, 2.9e9 AD] ms Millisecond ± 2.9e9 years [ 2.9e6 BC, 2.9e6 AD] us Microsecond ± 2.9e6 years [290301 BC, 294241 AD] ns Nanosecond ± 292 years [ 1678 AD, 2262 AD] ps Picosecond ± 106 days [ 1969 AD, 1970 AD] fs Femtosecond ± 2.6 hours [ 1969 AD, 1970 AD] as Attosecond ± 9.2 seconds [ 1969 AD, 1970 AD] For the types of data we see in the real world, a useful default is datetime64[ns] , as it can encode a useful range of modern dates with a suitably fine precision. Finally, we will note that while the datetime64 data type addresses some of the defi‐ ciencies of the built-in Python datetime type, it lacks many of the convenient meth‐ ods and functions provided by datetime and especially dateutil . More information can be found in NumPy’s datetime64 documentation . Dates and times in Pandas: Best of both worlds Pandas builds upon all the tools just discussed to provide a Timestamp object, which combines the ease of use of datetime and dateutil with the efficient storage and vectorized interface of numpy.datetime64 . From a group of these Timestamp objects, Pandas can construct a DatetimeIndex that can be used to index data in a Series or DataFrame ; we’ll see many examples of this below. For example, we can use Pandas tools to repeat the demonstration from above. We can parse a flexibly formatted string date, and use format codes to output the day of the week: In [ 9 ]: import pandas as pd date = pd . to_datetime ( \"4th of July, 2015\" ) date Out[9]: Timestamp('2015-07-04 00:00:00') In [ 10 ]: date",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_196"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". to_datetime ( \"4th of July, 2015\" ) date Out[9]: Timestamp('2015-07-04 00:00:00') In [ 10 ]: date . strftime ( '%A' ) Out[10]: 'Saturday' Additionally, we can do NumPy-style vectorized operations directly on this same object: In [ 11 ]: date + pd . to_timedelta ( np . arange ( 12 ), 'D' ) Working with Time Series | 191 Out[11]: DatetimeIndex(['2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10', '2015-07-11', '2015-07-12', '2015-07-13', '2015-07-14', '2015-07-15'], dtype='datetime64[ns]', freq=None) In the next section, we will take a closer look at manipulating time series data with the tools provided by Pandas. Pandas Time Series: Indexing by Time Where the Pandas time series tools really become useful is when you begin to index data by timestamps . For example, we can construct a Series object that has timindexed data: In [ 12 ]: index = pd . DatetimeIndex ([ '2014-07-04' , '2014-08-04' , '2015-07-04' , '2015-08-04' ]) data = pd . Series ([ 0 , 1 , 2 , 3 ], index = index ) data Out[12]: 2014-07-04 0 2014-08-04 1 2015-07-04 2 2015-08-04 3 dtype: int64 Now that we have this data in a Series , we can make use of any of the Series index‐ ing patterns we discussed in previous sections, passing values that can be coerced into dates: In [ 13 ]: data [ '2014-07-04' : '2015-07-04' ] Out[13]: 2014-07-04 0 2014-08-04 1 2015-07-04 2 dtype: int64 There are additional special date-only indexing operations, such as passing a year to obtain a slice of all data from that year: In [ 14 ]: data [ '2015' ] Out[14]: 2015-07-04 2 2015-08-04 3 dtype: int64 Later, we will see additional examples of the convenience of dates-as-indices. But first, let’s take a closer look at the available time series data structures. Pandas Time Series Data Structures This section will introduce the fundamental Pandas data structures for working with time series data: 192 | Chapter 3: Data Manipulation with Pandas • For time stamps , Pandas provides the Timestamp type",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_197"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As mentioned before, it is essentially a replacement for Python’s native datetime , but is based on the more efficient numpy.datetime64 data type. The associated index structure is DatetimeIndex . • For time periods , Pandas provides the Period type. This encodes a fixefrequency interval based on numpy.datetime64 . The associated index structure is PeriodIndex . • For time deltas or durations , Pandas provides the Timedelta type. Timedelta is a more efficient replacement for Python’s native datetime.timedelta type, and is based on numpy.timedelta64 . The associated index structure is TimedeltaIndex . The most fundamental of these date/time objects are the Timestamp and DatetimeIn dex objects. While these class objects can be invoked directly, it is more common to use the pd.to_datetime() function, which can parse a wide variety of formats. Pass‐ ing a single date to pd.to_datetime() yields a Timestamp ; passing a series of dates by default yields a DatetimeIndex : In [ 15 ]: dates = pd . to_datetime ([ datetime ( 2015 , 7 , 3 ), '4th of July, 2015' , '2015-Jul-6' , '07-07-2015' , '20150708' ]) dates Out[15]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07', '2015-07-08'], dtype='datetime64[ns]', freq=None) Any DatetimeIndex can be converted to a PeriodIndex with the to_period() func‐ tion with the addition of a frequency code; here we’ll use 'D' to indicate daily frequency: In [ 16 ]: dates",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_198"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". to_period ( 'D' ) Out[16]: PeriodIndex(['2015-07-03', '2015-07-04', '2015-07-06', '2015-07-07', '2015-07-08'], dtype='int64', freq='D') A TimedeltaIndex is created, for example, when one date is subtracted from another: In [ 17 ]: dates - dates [ 0 ] Out[17]: TimedeltaIndex(['0 days', '1 days', '3 days', '4 days', '5 days'], dtype='timedelta64[ns]', freq=None) Regular sequences: pd.date_range() To make the creation of regular date sequences more convenient, Pandas offers a few functions for this purpose: pd.date_range() for timestamps, pd.period_range() for periods, and pd.timedelta_range() for time deltas. We’ve seen that Python’s Working with Time Series | 193 range() and NumPy’s np.arange() turn a startpoint, endpoint, and optional stepsize into a sequence. Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to create a regular sequence of dates. By default, the fre‐ quency is one day: In [ 18 ]: pd . date_range ( '2015-07-03' , '2015-07-10' ) Out[18]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'], dtype='datetime64[ns]', freq='D') Alternatively, the date range can be specified not with a starand endpoint, but with a startpoint and a number of periods: In [ 19 ]: pd . date_range ( '2015-07-03' , periods = 8 ) Out[19]: DatetimeIndex(['2015-07-03', '2015-07-04', '2015-07-05', '2015-07-06', '2015-07-07', '2015-07-08', '2015-07-09', '2015-07-10'], dtype='datetime64[ns]', freq='D') You can modify the spacing by altering the freq argument, which defaults to D . For example, here we will construct a range of hourly timestamps: In [ 20 ]: pd",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_199"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, here we will construct a range of hourly timestamps: In [ 20 ]: pd . date_range ( '2015-07-03' , periods = 8 , freq = 'H' ) Out[20]: DatetimeIndex(['2015-07-03 00:00:00', '2015-07-03 01:00:00', '2015-07-03 02:00:00', '2015-07-03 03:00:00', '2015-07-03 04:00:00', '2015-07-03 05:00:00', '2015-07-03 06:00:00', '2015-07-03 07:00:00'], dtype='datetime64[ns]', freq='H') To create regular sequences of period or time delta values, the very similar pd.period_range() and pd.timedelta_range() functions are useful. Here are some monthly periods: In [ 21 ]: pd . period_range ( '2015-07' , periods = 8 , freq = 'M' ) Out[21]: PeriodIndex(['2015-07', '2015-08', '2015-09', '2015-10', '2015-11', '2015-12', '2016-01', '2016-02'], dtype='int64', freq='M') And a sequence of durations increasing by an hour: In [ 22 ]: pd . timedelta_range ( 0 , periods = 10 , freq = 'H' ) Out[22]: TimedeltaIndex(['00:00:00', '01:00:00', '02:00:00', '03:00:00', '04:00:00', '05:00:00', '06:00:00', '07:00:00', '08:00:00', '09:00:00'], dtype='timedelta64[ns]', freq='H') All of these require an understanding of Pandas frequency codes, which we’ll summa‐ rize in the next section. 194 | Chapter 3: Data Manipulation with Pandas Frequencies and Offsets Fundamental to these Pandas time series tools is the concept of a frequency or date offset. Just as we saw the D (day) and H (hour) codes previously, we can use such codes to specify any desired frequency spacing. Table 3-7 summarizes the main codes available. Table 3-7. Listing of Pandas frequency codes Code Description Code Description D Calendar day B Business day W Weekly M Month end BM Business month end Q Quarter end BQ Business quarter end A Year end BA Business year end H Hours BH Business hours T Minutes S Seconds L Milliseonds U Microseconds N Nanoseconds The monthly, quarterly, and annual frequencies are all marked at the end of the speci‐ fied period. Adding an S suffix to any of these marks it instead at the beginning ( Table 3-8 ). Table 3-8",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_200"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Adding an S suffix to any of these marks it instead at the beginning ( Table 3-8 ). Table 3-8. Listing of start-indexed frequency codes Code Description MS Month start BMS Business month start QS Quarter start BQS Business quarter start AS Year start BAS Business year start Working with Time Series | 195 Additionally, you can change the month used to mark any quarterly or annual code by adding a three-letter month code as a suffix: • Q-JAN , BQ-FEB , QS-MAR , BQS-APR , etc. • A-JAN , BA-FEB , AS-MAR , BAS-APR , etc. In the same way, you can modify the split-point of the weekly frequency by adding a three-letter weekday code: • W-SUN , W-MON , W-TUE , W-WED , etc. On top of this, codes can be combined with numbers to specify other frequencies. For example, for a frequency of 2 hours 30 minutes, we can combine the hour ( H ) and minute ( T ) codes as follows: In [ 23 ]: pd . timedelta_range ( 0 , periods = 9 , freq = \"2H30T\" ) Out[23]: TimedeltaIndex(['00:00:00', '02:30:00', '05:00:00', '07:30:00', '10:00:00', '12:30:00', '15:00:00', '17:30:00', '20:00:00'], dtype='timedelta64[ns]', freq='150T') All of these short codes refer to specific instances of Pandas time series offsets, which can be found in the pd.tseries.offsets module. For example, we can create a busi‐ ness day offset directly as follows: In [ 24 ]: from pandas.tseries.offsets import BDay pd . date_range ( '2015-07-01' , periods = 5 , freq = BDay ()) Out[24]: DatetimeIndex(['2015-07-01', '2015-07-02', '2015-07-03', '2015-07-06', '2015-07-07'], dtype='datetime64[ns]', freq='B') For more discussion of the use of frequencies and offsets, see the “DateOffset objects” section of the Pandas online documentation . Resampling, Shifting, and Windowing The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_201"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Resampling, Shifting, and Windowing The ability to use dates and times as indices to intuitively organize and access data is an important piece of the Pandas time series tools. The benefits of indexed data in general (automatic alignment during operations, intuitive data slicing and access, etc.) still apply, and Pandas provides several additional time series–specific operations. We will take a look at a few of those here, using some stock price data as an example. Because Pandas was developed largely in a finance context, it includes some very spe‐ cific tools for financial data. For example, the accompanying pandas-datareader package (installable via conda install pandas-datareader ) knows how to import 196 | Chapter 3: Data Manipulation with Pandas financial data from a number of available sources, including Yahoo finance, Google Finance, and others. Here we will load Google’s closing price history: In [ 25 ]: from pandas_datareader import data goog = data . DataReader ( 'GOOG' , start = '2004' , end = '2016' , data_source = 'google' ) goog . head () Out[25]: Open High Low Close Volume Date 2004-08-19 49.96 51.98 47.93 50.12 NaN 2004-08-20 50.69 54.49 50.20 54.10 NaN 2004-08-23 55.32 56.68 54.47 54.65 NaN 2004-08-24 55.56 55.74 51.73 52.38 NaN 2004-08-25 52.43 53.95 51.89 52.95 NaN For simplicity, we’ll use just the closing price: In [ 26 ]: goog = goog [ 'Close' ] We can visualize this using the plot() method, after the normal Matplotlib setup boilerplate ( Figure 3-5 ): In [ 27 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () In [ 28 ]: goog . plot (); Figure 3-5. Google’s closing stock price over time Resampling and converting frequencies One common need for time series data is resampling at a higher or lower frequency. You can do this using the resample() method, or the much simpler asfreq() Working with Time Series | 197 method",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_202"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You can do this using the resample() method, or the much simpler asfreq() Working with Time Series | 197 method. The primary difference between the two is that resample() is fundamentally a data aggregation , while asfreq() is fundamentally a data selection . Taking a look at the Google closing price, let’s compare what the two return when we down-sample the data. Here we will resample the data at the end of business year ( Figure 3-6 ): In [ 29 ]: goog . plot ( alpha = 0.5 , style = '-' ) goog . resample ( 'BA' ) . mean () . plot ( style = ':' ) goog . asfreq ( 'BA' ) . plot ( style = '--' ); plt . legend ([ 'input' , 'resample' , 'asfreq' ], loc = 'upper left' ); Figure 3-6. Resamplings of Google’s stock price Notice the difference: at each point, resample reports the average of the previous year , while asfreq reports the value at the end of the year . For up-sampling, resample() and asfreq() are largely equivalent, though resample has many more options available. In this case, the default for both methods is to leave the up-sampled points empty—that is, filled with NA values. Just as with the pd.fillna() function discussed previously, asfreq() accepts a method argument to specify how values are imputed. Here, we will resample the business day data at a daily frequency (i.e., including weekends); see Figure 3-7 : In [ 30 ]: fig , ax = plt . subplots ( 2 , sharex = True ) data = goog . iloc [: 10 ] data . asfreq ( 'D' ) . plot ( ax = ax [ 0 ], marker = 'o' ) data . asfreq ( 'D' , method = 'bfill' ) . plot ( ax = ax [ 1 ], style = '-o' ) data . asfreq ( 'D' , method = 'ffill' ) . plot ( ax = ax [ 1 ], style = '--o' ) ax [ 1 ] . legend ([ \"back-fill\" , \"forward-fill\" ]); 198 | Chapter 3: Data Manipulation with Pandas Figure 3-7. Comparison between forward-fill and back-fill interpolation The top panel is the default: non-business days are left as NA values and do not appear on the plot. The bottom panel shows the differences between two strategies for filling the gaps: forward-filling and backward-filling",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_203"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The bottom panel shows the differences between two strategies for filling the gaps: forward-filling and backward-filling. Time-shifts Another common time series–specific operation is shifting of data in time. Pandas has two closely related methods for computing this: shift() and tshift() . In short, the difference between them is that shift() shifts the data , while tshift() shifts the index . In both cases, the shift is specified in multiples of the frequency. Here we will both shift() and tshift() by 900 days ( Figure 3-8 ): In [ 31 ]: fig , ax = plt . subplots ( 3 , sharey = True ) # apply a frequency to the data goog = goog . asfreq ( 'D' , method = 'pad' ) goog . plot ( ax = ax [ 0 ]) goog . shift ( 900 ) . plot ( ax = ax [ 1 ]) goog . tshift ( 900 ) . plot ( ax = ax [ 2 ]) # legends and annotations local_max = pd . to_datetime ( '2007-11-05' ) offset = pd . Timedelta ( 900 , 'D' ) ax [ 0 ] . legend ([ 'input' ], loc = 2 ) ax [ 0 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 0 ] . axvline ( local_max , alpha = 0.3 , color = 'red' ) Working with Time Series | 199 ax [ 1 ] . legend ([ 'shift(900)' ], loc = 2 ) ax [ 1 ] . get_xticklabels ()[ 4 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 1 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ) ax [ 2 ] . legend ([ 'tshift(900)' ], loc = 2 ) ax [ 2 ] . get_xticklabels ()[ 1 ] . set ( weight = 'heavy' , color = 'red' ) ax [ 2 ] . axvline ( local_max + offset , alpha = 0.3 , color = 'red' ); Figure 3-8. Comparison between shift and tshift We see here that shift(900) shifts the data by 900 days, pushing some of it off the end of the graph (and leaving NA values at the other end), while tshift(900) shifts the index values by 900 days. A common context for this type of shift is computing differences over time. For example, we use shifted values to compute the one-year return on investment for Google stock over the course of the dataset ( Figure 3-9 ): In [ 32 ]: ROI = 100 * ( goog . tshift ( - 365 ) / goog - 1 ) ROI",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_204"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". tshift ( - 365 ) / goog - 1 ) ROI . plot () plt . ylabel ( '% Return on Investment' ); 200 | Chapter 3: Data Manipulation with Pandas Figure 3-9. Return on investment to present day for Google stock This helps us to see the overall trend in Google stock: thus far, the most profitable times to invest in Google have been (unsurprisingly, in retrospect) shortly after its IPO, and in the middle of the 2009 recession. Rolling windows Rolling statistics are a third type of time series–specific operation implemented by Pandas. These can be accomplished via the rolling() attribute of Series and Data Frame objects, which returns a view similar to what we saw with the groupby opera‐ tion (see “Aggregation and Grouping” on page 158 ). This rolling view makes available a number of aggregation operations by default. For example, here is the one-year centered rolling mean and standard deviation of the Google stock prices ( Figure 3-10 ): In [ 33 ]: rolling = goog . rolling ( 365 , center = True ) data = pd . DataFrame ({ 'input' : goog , 'one-year rolling_mean' : rolling . mean (), 'one-year rolling_std' : rolling . std ()}) ax = data . plot ( style = [ '-' , '--' , ':' ]) ax . lines [ 0 ] . set_alpha ( 0.3 ) Working with Time Series | 201 Figure 3-10. Rolling statistics on Google stock prices As with groupby operations, the aggregate() and apply() methods can be used for custom rolling computations. Where to Learn More This section has provided only a brief summary of some of the most essential features of time series tools provided by Pandas; for a more complete discussion, you can refer to the “Time Series/Date” section of the Pandas online documentation . Another excellent resource is the textbook Python for Data Analysis by Wes McKin‐ ney (O’Reilly, 2012). Although it is now a few years old, it is an invaluable resource on the use of Pandas",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_205"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Another excellent resource is the textbook Python for Data Analysis by Wes McKin‐ ney (O’Reilly, 2012). Although it is now a few years old, it is an invaluable resource on the use of Pandas. In particular, this book emphasizes time series tools in the context of business and finance, and focuses much more on particular details of business cal‐ endars, time zones, and related topics. As always, you can also use the IPython help functionality to explore and try further options available to the functions and methods discussed here. I find this often is the best way to learn a new Python tool. Example: Visualizing Seattle Bicycle Counts As a more involved example of working with some time series data, let’s take a look at bicycle counts on Seattle’s Fremont Bridge . This data comes from an automated bicy‐ cle counter, installed in late 2012, which has inductive sensors on the east and west sidewalks of the bridge. The hourly bicycle counts can be downloaded from http:// data.seattle.gov/; here is the direct link to the dataset . As of summer 2016, the CSV can be downloaded as follows: 202 | Chapter 3: Data Manipulation with Pandas In [ 34 ]: # !curl -o FremontBridge.csv # https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD Once this dataset is downloaded, we can use Pandas to read the CSV output into a DataFrame . We will specify that we want the Date as an index, and we want these dates to be automatically parsed: In [ 35 ]: data = pd . read_csv ( 'FremontBridge.csv' , index_col = 'Date' , parse_dates = True ) data . head () Out[35]: Fremont Bridge West Sidewalk \\\\ Date 2012-10-03 00:00:00 4.0 2012-10-03 01:00:00 4.0 2012-10-03 02:00:00 1.0 2012-10-03 03:00:00 2.0 2012-10-03 04:00:00 6.0 Fremont Bridge East Sidewalk Date 2012-10-03 00:00:00 9.0 2012-10-03 01:00:00 6.0 2012-10-03 02:00:00 1.0 2012-10-03 03:00:00 3.0 2012-10-03 04:00:00 1.0 For convenience, we’ll further process this dataset by shortening the column names and adding a “Total” column: In [ 36 ]: data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_206"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". columns = [ 'West' , 'East' ] data [ 'Total' ] = data . eval ( 'West + East' ) Now let’s take a look at the summary statistics for this data: In [ 37 ]: data . dropna () . describe () Out[37]: West East Total count 33544.000000 33544.000000 33544.000000 mean 61.726568 53.541706 115.268275 std 83.210813 76.380678 144.773983 min 0.000000 0.000000 0.000000 25% 8.000000 7.000000 16.000000 50% 33.000000 28.000000 64.000000 75% 80.000000 66.000000 151.000000 max 825.000000 717.000000 1186.000000 Working with Time Series | 203 Visualizing the data We can gain some insight into the dataset by visualizing it. Let’s start by plotting the raw data ( Figure 3-11 ): In [ 38 ]: % matplotlib inline import seaborn ; seaborn . set () In [ 39 ]: data . plot () plt . ylabel ( 'Hourly Bicycle Count' ); Figure 3-11. Hourly bicycle counts on Seattle’s Fremont bridge The ~25,000 hourly samples are far too dense for us to make much sense of. We can gain more insight by resampling the data to a coarser grid. Let’s resample by week ( Figure 3-12 ): In [ 40 ]: weekly = data . resample ( 'W' ) . sum () weekly . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'Weekly bicycle count' ); This shows us some interesting seasonal trends: as you might expect, people bicycle more in the summer than in the winter, and even within a particular season the bicy‐ cle use varies from week to week (likely dependent on weather; see “In Depth: Linear Regression” on page 390 where we explore this further). 204 | Chapter 3: Data Manipulation with Pandas Figure 3-12. Weekly bicycle crossings of Seattle’s Fremont bridge Another way that comes in handy for aggregating the data is to use a rolling mean, utilizing the pd.rolling_mean() function. Here we’ll do a 30-day rolling mean of our data, making sure to center the window ( Figure 3-13 ): In [ 41 ]: daily = data . resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'mean hourly count' ); Figure 3-13",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_207"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". resample ( 'D' ) . sum () daily . rolling ( 30 , center = True ) . sum () . plot ( style = [ ':' , '--' , '-' ]) plt . ylabel ( 'mean hourly count' ); Figure 3-13. Rolling mean of weekly bicycle counts Working with Time Series | 205 The jaggedness of the result is due to the hard cutoff of the window. We can get a smoother version of a rolling mean using a window function—for example, a Gaus‐ sian window. The following code (visualized in Figure 3-14 ) specifies both the width of the window (we chose 50 days) and the width of the Gaussian within the window (we chose 10 days): In [ 42 ]: daily . rolling ( 50 , center = True , win_type = 'gaussian' ) . sum ( std = 10 ) . plot ( style = [ ':' , '--' , '-' ]); Figure 3-14. Gaussian smoothed weekly bicycle counts Digging into the data While the smoothed data views in Figure 3-14 are useful to get an idea of the general trend in the data, they hide much of the interesting structure. For example, we might want to look at the average traffic as a function of the time of day. We can do this using the GroupBy functionality discussed in “Aggregation and Grouping” on page 158 ( Figure 3-15 ): In [ 43 ]: by_time = data . groupby ( data . index . time ) . mean () hourly_ticks = 4 * 60 * 60 * np . arange ( 6 ) by_time . plot ( xticks = hourly_ticks , style = [ ':' , '--' , '-' ]); The hourly traffic is a strongly bimodal distribution, with peaks around 8:00 in the morning and 5:00 in the evening. This is likely evidence of a strong component of commuter traffic crossing the bridge. This is further evidenced by the differences between the western sidewalk (generally used going toward downtown Seattle), which peaks more strongly in the morning, and the eastern sidewalk (generally used going away from downtown Seattle), which peaks more strongly in the evening. 206 | Chapter 3: Data Manipulation with Pandas Figure 3-15. Average hourly bicycle counts We also might be curious about how things change based on the day of the week",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_208"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 206 | Chapter 3: Data Manipulation with Pandas Figure 3-15. Average hourly bicycle counts We also might be curious about how things change based on the day of the week. Again, we can do this with a simple groupby ( Figure 3-16 ): In [ 44 ]: by_weekday = data . groupby ( data . index . dayofweek ) . mean () by_weekday . index = [ 'Mon' , 'Tues' , 'Wed' , 'Thurs' , 'Fri' , 'Sat' , 'Sun' ] by_weekday . plot ( style = [ ':' , '--' , '-' ]); Figure 3-16. Average daily bicycle counts This shows a strong distinction between weekday and weekend totals, with around twice as many average riders crossing the bridge on Monday through Friday than on Saturday and Sunday. Working with Time Series | 207 With this in mind, let’s do a compound groupby and look at the hourly trend on weekdays versus weekends. We’ll start by grouping by both a flag marking the week‐ end, and the time of day: In [ 45 ]: weekend = np . where ( data . index . weekday < 5 , 'Weekday' , 'Weekend' ) by_time = data . groupby ([ weekend , data . index . time ]) . mean () Now we’ll use some of the Matplotlib tools described in “Multiple Subplots” on page 262 to plot two panels side by side ( Figure 3-17 ): In [ 46 ]: import matplotlib.pyplot as plt fig , ax = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) by_time . ix [ 'Weekday' ] . plot ( ax = ax [ 0 ], title = 'Weekdays' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]) by_time . ix [ 'Weekend' ] . plot ( ax = ax [ 1 ], title = 'Weekends' , xticks = hourly_ticks , style = [ ':' , '--' , '-' ]); Figure 3-17. Average hourly bicycle counts by weekday and weekend The result is very interesting: we see a bimodal commute pattern during the work week, and a unimodal recreational pattern during the weekends",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_209"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Average hourly bicycle counts by weekday and weekend The result is very interesting: we see a bimodal commute pattern during the work week, and a unimodal recreational pattern during the weekends. It would be interest‐ ing to dig through this data in more detail, and examine the effect of weather, temper‐ ature, time of year, and other factors on people’s commuting patterns; for further discussion, see my blog post “Is Seattle Really Seeing an Uptick In Cycling?” , which uses a subset of this data. We will also revisit this dataset in the context of modeling in “In Depth: Linear Regression” on page 390 . High-Performance Pandas: eval() and query() As we’ve already seen in previous chapters, the power of the PyData stack is built upon the ability of NumPy and Pandas to push basic operations into C via an intu‐ itive syntax: examples are vectorized/broadcasted operations in NumPy, and grouping-type operations in Pandas. While these abstractions are efficient and effec‐ 208 | Chapter 3: Data Manipulation with Pandas tive for many common use cases, they often rely on the creation of temporary inter‐ mediate objects, which can cause undue overhead in computational time and memory use. As of version 0.13 (released January 2014), Pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of inter‐ mediate arrays. These are the eval() and query() functions, which rely on the Numexpr package. In this notebook we will walk through their use and give some rules of thumb about when you might think about using them. Motivating query() and eval(): Compound Expressions We’ve seen previously that NumPy and Pandas support fast vectorized operations; for example, when you are adding the elements of two arrays: In [ 1 ]: import numpy as np rng = np . random . RandomState ( 42 ) x = rng . rand ( 1E6 ) y = rng",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_210"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . RandomState ( 42 ) x = rng . rand ( 1E6 ) y = rng . rand ( 1E6 ) % timeit x + y 100 loops, best of 3: 3.39 ms per loop As discussed in “Computation on NumPy Arrays: Universal Functions” on page 50 , this is much faster than doing the addition via a Python loop or comprehension: In [ 2 ]: % timeit np . fromiter (( xi + yi for xi , yi in zip ( x , y )), dtype = x . dtype , count = len ( x )) 1 loop, best of 3: 266 ms per loop But this abstraction can become less efficient when you are computing compound expressions. For example, consider the following expression: In [ 3 ]: mask = ( x > 0.5 ) & ( y < 0.5 ) Because NumPy evaluates each subexpression, this is roughly equivalent to the following: In [ 4 ]: tmp1 = ( x > 0.5 ) tmp2 = ( y < 0.5 ) mask = tmp1 & tmp2 In other words, every intermediate step is explicitly allocated in memory . If the x and y arrays are very large, this can lead to significant memory and computational over‐ head. The Numexpr library gives you the ability to compute this type of compound expression element by element, without the need to allocate full intermediate arrays. The Numexpr documentation has more details, but for the time being it is sufficient to say that the library accepts a string giving the NumPy-style expression you’d like to compute: High-Performance Pandas: eval() and query() | 209 In [ 5 ]: import numexpr mask_numexpr = numexpr . evaluate ( '(x > 0.5) & (y < 0.5)' ) np . allclose ( mask , mask_numexpr ) Out[5]: True The benefit here is that Numexpr evaluates the expression in a way that does not use full-sized temporary arrays, and thus can be much more efficient than NumPy, espe‐ cially for large arrays. The Pandas eval() and query() tools that we will discuss here are conceptually similar, and depend on the Numexpr package. pandas.eval() for Efficient Operations The eval() function in Pandas uses string expressions to efficiently compute opera‐ tions using DataFrame s",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_211"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". pandas.eval() for Efficient Operations The eval() function in Pandas uses string expressions to efficiently compute opera‐ tions using DataFrame s. For example, consider the following DataFrame s: In [ 6 ]: import pandas as pd nrows , ncols = 100000 , 100 rng = np . random . RandomState ( 42 ) df1 , df2 , df3 , df4 = ( pd . DataFrame ( rng . rand ( nrows , ncols )) for i in range ( 4 )) To compute the sum of all four DataFrame s using the typical Pandas approach, we can just write the sum: In [ 7 ]: % timeit df1 + df2 + df3 + df4 10 loops, best of 3: 87.1 ms per loop We can compute the same result via pd.eval by constructing the expression as a string: In [ 8 ]: % timeit pd . eval ( 'df1 + df2 + df3 + df4' ) 10 loops, best of 3: 42.2 ms per loop The eval() version of this expression is about 50% faster (and uses much less mem‐ ory), while giving the same result: In [ 9 ]: np . allclose ( df1 + df2 + df3 + df4 , pd . eval ( 'df1 + df2 + df3 + df4' )) Out[9]: True Operations supported by pd.eval() As of Pandas v0.16, pd.eval() supports a wide range of operations. To demonstrate these, we’ll use the following integer DataFrame s: In [ 10 ]: df1 , df2 , df3 , df4 , df5 = ( pd . DataFrame ( rng . randint ( 0 , 1000 , ( 100 , 3 ))) for i in range ( 5 )) Arithmetic operators. pd.eval() supports all arithmetic operators. For example: 210 | Chapter 3: Data Manipulation with Pandas In [ 11 ]: result1 = - df1 * df2 / ( df3 + df4 ) - df5 result2 = pd . eval ( '-df1 * df2 / (df3 + df4) - df5' ) np . allclose ( result1 , result2 ) Out[11]: True Comparison operators. pd.eval() supports all comparison operators, including chained expressions: In [ 12 ]: result1 = ( df1 < df2 ) & ( df2 <= df3 ) & ( df3 != df4 ) result2 = pd . eval ( 'df1 < df2 <= df3 != df4' ) np . allclose ( result1 , result2 ) Out[12]: True Bitwise operators. pd.eval() supports the & and | bitwise operators: In [ 13 ]: result1 = ( df1 < 0.5 ) & ( df2 < 0.5 ) | ( df3 < df4 ) result2 = pd . eval ( '(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)' ) np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_212"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". pd.eval() supports the & and | bitwise operators: In [ 13 ]: result1 = ( df1 < 0.5 ) & ( df2 < 0.5 ) | ( df3 < df4 ) result2 = pd . eval ( '(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)' ) np . allclose ( result1 , result2 ) Out[13]: True In addition, it supports the use of the literal and and or in Boolean expressions: In [ 14 ]: result3 = pd . eval ( '(df1 < 0.5) and (df2 < 0.5) or (df3 < df4)' ) np . allclose ( result1 , result3 ) Out[14]: True Object attributes and indices. pd.eval() supports access to object attributes via the obj.attr syntax, and indexes via the obj[index] syntax: In [ 15 ]: result1 = df2 . T [ 0 ] + df3 . iloc [ 1 ] result2 = pd . eval ( 'df2.T[0] + df3.iloc[1]' ) np . allclose ( result1 , result2 ) Out[15]: True Other operations. Other operations, such as function calls, conditional statements, loops, and other more involved constructs, are currently not implemented in pd.eval() . If you’d like to execute these more complicated types of expressions, you can use the Numexpr library itself. DataFrame.eval() for Column-Wise Operations Just as Pandas has a top-level pd.eval() function, DataFrame s have an eval() method that works in similar ways. The benefit of the eval() method is that columns can be referred to by name . We’ll use this labeled array as an example: In [ 16 ]: df = pd . DataFrame ( rng . rand ( 1000 , 3 ), columns = [ 'A' , 'B' , 'C' ]) df . head () High-Performance Pandas: eval() and query() | 211 Out[16]: A B C 0 0.375506 0.406939 0.069938 1 0.069087 0.235615 0.154374 2 0.677945 0.433839 0.652324 3 0.264038 0.808055 0.347197 4 0.589161 0.252418 0.557789 Using pd.eval() as above, we can compute expressions with the three columns like this: In [ 17 ]: result1 = ( df [ 'A' ] + df [ 'B' ]) / ( df [ 'C' ] - 1 ) result2 = pd . eval ( \"(df.A + df.B) / (df.C - 1)\" ) np . allclose ( result1 , result2 ) Out[17]: True The DataFrame.eval() method allows much more succinct evaluation of expressions with the columns: In [ 18 ]: result3 = df . eval ( '(A + B) / (C - 1)' ) np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_213"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". eval ( '(A + B) / (C - 1)' ) np . allclose ( result1 , result3 ) Out[18]: True Notice here that we treat column names as variables within the evaluated expression, and the result is what we would wish. Assignment in DataFrame.eval() In addition to the options just discussed, DataFrame.eval() also allows assignment to any column. Let’s use the DataFrame from before, which has columns 'A' , 'B' , and 'C' : In [ 19 ]: df . head () Out[19]: A B C 0 0.375506 0.406939 0.069938 1 0.069087 0.235615 0.154374 2 0.677945 0.433839 0.652324 3 0.264038 0.808055 0.347197 4 0.589161 0.252418 0.557789 We can use df.eval() to create a new column 'D' and assign to it a value computed from the other columns: In [ 20 ]: df . eval ( 'D = (A + B) / C' , inplace = True ) df . head () Out[20]: A B C D 0 0.375506 0.406939 0.069938 11.187620 1 0.069087 0.235615 0.154374 1.973796 2 0.677945 0.433839 0.652324 1.704344 3 0.264038 0.808055 0.347197 3.087857 4 0.589161 0.252418 0.557789 1.508776 212 | Chapter 3: Data Manipulation with Pandas In the same way, any existing column can be modified: In [ 21 ]: df . eval ( 'D = (A - B) / C' , inplace = True ) df . head () Out[21]: A B C D 0 0.375506 0.406939 0.069938 -0.449425 1 0.069087 0.235615 0.154374 -1.078728 2 0.677945 0.433839 0.652324 0.374209 3 0.264038 0.808055 0.347197 -1.566886 4 0.589161 0.252418 0.557789 0.603708 Local variables in DataFrame.eval() The DataFrame.eval() method supports an additional syntax that lets it work with local Python variables. Consider the following: In [ 22 ]: column_mean = df . mean ( 1 ) result1 = df [ 'A' ] + column_mean result2 = df . eval ( 'A + @column_mean' ) np . allclose ( result1 , result2 ) Out[22]: True The @ character here marks a variable name rather than a column name , and lets you efficiently evaluate expressions involving the two “namespaces”: the namespace of columns, and the namespace of Python objects",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_214"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Notice that this @ character is only supported by the DataFrame.eval() method , not by the pandas.eval() function , because the pandas.eval() function only has access to the one (Python) namespace. DataFrame.query() Method The DataFrame has another method based on evaluated strings, called the query() method. Consider the following: In [ 23 ]: result1 = df [( df . A < 0.5 ) & ( df . B < 0.5 )] result2 = pd . eval ( 'df[(df.A < 0.5) & (df.B < 0.5)]' ) np . allclose ( result1 , result2 ) Out[23]: True As with the example used in our discussion of DataFrame.eval() , this is an expres‐ sion involving columns of the DataFrame . It cannot be expressed using the Data Frame.eval() syntax, however! Instead, for this type of filtering operation, you can use the query() method: In [ 24 ]: result2 = df . query ( 'A < 0.5 and B < 0.5' ) np . allclose ( result1 , result2 ) Out[24]: True High-Performance Pandas: eval() and query() | 213 In addition to being a more efficient computation, compared to the masking expres‐ sion this is much easier to read and understand. Note that the query() method also accepts the @ flag to mark local variables: In [ 25 ]: Cmean = df [ 'C' ] . mean () result1 = df [( df . A < Cmean ) & ( df . B < Cmean )] result2 = df . query ( 'A < @Cmean and B < @Cmean' ) np . allclose ( result1 , result2 ) Out[25]: True Performance: When to Use These Functions When considering whether to use these functions, there are two considerations: com‐ putation time and memory use . Memory use is the most predictable aspect. As already mentioned, every compound expression involving NumPy arrays or Pandas Data Frame s will result in implicit creation of temporary arrays: For example, this: In [ 26 ]: x = df [( df . A < 0.5 ) & ( df . B < 0.5 )] is roughly equivalent to this: In [ 27 ]: tmp1 = df . A < 0.5 tmp2 = df",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_215"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A < 0.5 ) & ( df . B < 0.5 )] is roughly equivalent to this: In [ 27 ]: tmp1 = df . A < 0.5 tmp2 = df . B < 0.5 tmp3 = tmp1 & tmp2 x = df [ tmp3 ] If the size of the temporary DataFrame s is significant compared to your available sys‐ tem memory (typically several gigabytes), then it’s a good idea to use an eval() or query() expression. You can check the approximate size of your array in bytes using this: In [ 28 ]: df . values . nbytes Out[28]: 32000 On the performance side, eval() can be faster even when you are not maxing out your system memory. The issue is how your temporary DataFrame s compare to the size of the L1 or L2 CPU cache on your system (typically a few megabytes in 2016); if they are much bigger, then eval() can avoid some potentially slow movement of val‐ ues between the different memory caches. In practice, I find that the difference in computation time between the traditional methods and the eval / query method is usually not significant—if anything, the traditional method is faster for smaller arrays! The benefit of eval / query is mainly in the saved memory, and the sometimes cleaner syntax they offer. We’ve covered most of the details of eval() and query() here; for more information on these, you can refer to the Pandas documentation. In particular, different parsers and engines can be specified for running these queries; for details on this, see the dis‐ cussion within the “Enhancing Performance” section . 214 | Chapter 3: Data Manipulation with Pandas Further Resources In this chapter, we’ve covered many of the basics of using Pandas effectively for data analysis. Still, much has been omitted from our discussion. To learn more about Pan‐ das, I recommend the following resources: Pandas online documentation This is the go-to source for complete documentation of the package. While the examples in the documentation tend to be small generated datasets, the descrip‐ tion of the options is complete and generally very useful for understanding the use of various functions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_216"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While the examples in the documentation tend to be small generated datasets, the descrip‐ tion of the options is complete and generally very useful for understanding the use of various functions. Python for Data Analysis Written by Wes McKinney (the original creator of Pandas), this book contains much more detail on the package than we had room for in this chapter. In partic‐ ular, he takes a deep dive into tools for time series, which were his bread and but‐ ter as a financial consultant. The book also has many entertaining examples of applying Pandas to gain insight from real-world datasets. Keep in mind, though, that the book is now several years old, and the Pandas package has quite a few new features that this book does not cover (but be on the lookout for a new edi‐ tion in 2017). Pandas on Stack Overflow Pandas has so many users that any question you have has likely been asked and answered on Stack Overflow. Using Pandas is a case where some Google-Fu is your best friend. Simply go to your favorite search engine and type in the ques‐ tion, problem, or error you’re coming across—more than likely you’ll find your answer on a Stack Overflow page. Pandas on PyVideo From PyCon to SciPy to PyData, many conferences have featured tutorials from Pandas developers and power users. The PyCon tutorials in particular tend to be given by very well-vetted presenters. My hope is that, by using these resources, combined with the walk-through given in this chapter, you’ll be poised to use Pandas to tackle any data analysis problem you come across! Further Resources | 215 CHAPTER 4 Visualization with Matplotlib We’ll now take an in-depth look at the Matplotlib tool for visualization in Python. Matplotlib is a multiplatform data visualization library built on NumPy arrays, and designed to work with the broader SciPy stack. It was conceived by John Hunter in 2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting via gnuplot from the IPython command line",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_217"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It was conceived by John Hunter in 2002, originally as a patch to IPython for enabling interactive MATLAB-style plotting via gnuplot from the IPython command line. IPython’s creator, Fernando Perez, was at the time scrambling to finish his PhD, and let John know he wouldn’t have time to review the patch for several months. John took this as a cue to set out on his own, and the Matplotlib package was born, with version 0.1 released in 2003. It received an early boost when it was adopted as the plotting package of choice of the Space Tele‐ scope Science Institute (the folks behind the Hubble Telescope), which financially supported Matplotlib’s development and greatly expanded its capabilities. One of Matplotlib’s most important features is its ability to play well with many oper‐ ating systems and graphics backends. Matplotlib supports dozens of backends and output types, which means you can count on it to work regardless of which operating system you are using or which output format you wish. This cross-platform, everything-to-everyone approach has been one of the great strengths of Matplotlib. It has led to a large userbase, which in turn has led to an active developer base and Mat‐ plotlib’s powerful tools and ubiquity within the scientific Python world. In recent years, however, the interface and style of Matplotlib have begun to show their age. Newer tools like ggplot and ggvis in the R language, along with web visuali‐ zation toolkits based on D3js and HTML5 canvas, often make Matplotlib feel clunky and old-fashioned. Still, I’m of the opinion that we cannot ignore Matplotlib’s strength as a well-tested, cross-platform graphics engine",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_218"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Still, I’m of the opinion that we cannot ignore Matplotlib’s strength as a well-tested, cross-platform graphics engine. Recent Matplotlib versions make it relatively easy to set new global plotting styles (see “Customizing Matplotlib: Configurations and Stylesheets” on page 282 ), and people have been developing new packages that build on its powerful internals to drive Matplotlib via cleaner, more 217 modern APIs—for example, Seaborn (discussed in “Visualization with Seaborn” on page 311 ), ggplot , HoloViews , Altair , and even Pandas itself can be used as wrappers around Matplotlib’s API. Even with wrappers like these, it is still often useful to dive into Matplotlib’s syntax to adjust the final plot output. For this reason, I believe that Matplotlib itself will remain a vital piece of the data visualization stack, even if new tools mean the community gradually moves away from using the Matplotlib API directly. General Matplotlib Tips Before we dive into the details of creating visualizations with Matplotlib, there are a few useful things you should know about using the package. Importing matplotlib Just as we use the np shorthand for NumPy and the pd shorthand for Pandas, we will use some standard shorthands for Matplotlib imports: In [ 1 ]: import matplotlib as mpl import matplotlib.pyplot as plt The plt interface is what we will use most often, as we’ll see throughout this chapter. Setting Styles We will use the plt.style directive to choose appropriate aesthetic styles for our fig‐ ures. Here we will set the classic style, which ensures that the plots we create use the classic Matplotlib style: In [ 2 ]: plt . style . use ( 'classic' ) Throughout this section, we will adjust this style as needed. Note that the stylesheets used here are supported as of Matplotlib version 1.5; if you are using an earlier ver‐ sion of Matplotlib, only the default style is available. For more information on style‐ sheets, see “Customizing Matplotlib: Configurations and Stylesheets” on page 282",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_219"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For more information on style‐ sheets, see “Customizing Matplotlib: Configurations and Stylesheets” on page 282 . show() or No show()? How to Display Your Plots A visualization you can’t see won’t be of much use, but just how you view your Mat‐ plotlib plots depends on the context. The best use of Matplotlib differs depending on how you are using it; roughly, the three applicable contexts are using Matplotlib in a script, in an IPython terminal, or in an IPython notebook. 218 | Chapter 4: Visualization with Matplotlib Plotting from a script If you are using Matplotlib from within a script, the function plt.show() is your friend. plt.show() starts an event loop, looks for all currently active figure objects, and opens one or more interactive windows that display your figure or figures. So, for example, you may have a file called myplot.py containing the following: # ------- file: myplot.py ------ import matplotlib.pyplot as plt import numpy as np x = np . linspace ( 0 , 10 , 100 ) plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )) plt . show () You can then run this script from the command-line prompt, which will result in a window opening with your figure displayed: $ python myplot.py The plt.show() command does a lot under the hood, as it must interact with your system’s interactive graphical backend. The details of this operation can vary greatly from system to system and even installation to installation, but Matplotlib does its best to hide all these details from you. One thing to be aware of: the plt.show() command should be used only once per Python session, and is most often seen at the very end of the script. Multiple show() commands can lead to unpredictable backend-dependent behavior, and should mostly be avoided. Plotting from an IPython shell It can be very convenient to use Matplotlib interactively within an IPython shell (see Chapter 1 ). IPython is built to work well with Matplotlib if you specify Matplotlib mode",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_220"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". IPython is built to work well with Matplotlib if you specify Matplotlib mode. To enable this mode, you can use the %matplotlib magic command after start‐ ing ipython : In [ 1 ]: % matplotlib Using matplotlib backend : TkAgg In [ 2 ]: import matplotlib.pyplot as plt At this point, any plt plot command will cause a figure window to open, and further commands can be run to update the plot. Some changes (such as modifying proper‐ ties of lines that are already drawn) will not draw automatically; to force an update, use plt.draw() . Using plt.show() in Matplotlib mode is not required. General Matplotlib Tips | 219 Plotting from an IPython notebook The IPython notebook is a browser-based interactive data analysis tool that can com‐ bine narrative, code, graphics, HTML elements, and much more into a single exe‐ cutable document (see Chapter 1 ). Plotting interactively within an IPython notebook can be done with the %matplotlib command, and works in a similar way to the IPython shell. In the IPython notebook, you also have the option of embedding graphics directly in the notebook, with two possible options: • %matplotlib notebook will lead to interactive plots embedded within the notebook • %matplotlib inline will lead to static images of your plot embedded in the notebook For this book, we will generally opt for %matplotlib inline : In [ 3 ]: % matplotlib inline After you run this command (it needs to be done only once per kernel/session), any cell within the notebook that creates a plot will embed a PNG image of the resulting graphic ( Figure 4-1 ): In [ 4 ]: import numpy as np x = np . linspace ( 0 , 10 , 100 ) fig = plt . figure () plt . plot ( x , np . sin ( x ), '-' ) plt . plot ( x , np . cos ( x ), '--' ); Figure 4-1. Basic plotting example 220 | Chapter 4: Visualization with Matplotlib Saving Figures to File One nice feature of Matplotlib is the ability to save figures in a wide variety of for‐ mats. You can save a figure using the savefig() command",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_221"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You can save a figure using the savefig() command. For example, to save the previous figure as a PNG file, you can run this: In [ 5 ]: fig . savefig ( 'my_figure.png' ) We now have a file called my_figure.png in the current working directory: In [ 6 ]: ! ls - lh my_figure . png -rw-r--r-- 1 jakevdp staff 16K Aug 11 10:59 my_figure.png To confirm that it contains what we think it contains, let’s use the IPython Image object to display the contents of this file ( Figure 4-2 ): In [ 7 ]: from IPython.display import Image Image ( 'my_figure.png' ) Figure 4-2. PNG rendering of the basic plot In savefig() , the file format is inferred from the extension of the given filename. Depending on what backends you have installed, many different file formats are available. You can find the list of supported file types for your system by using the following method of the figure canvas object: In [ 8 ]: fig . canvas . get_supported_filetypes () Out[8]: {'eps': 'Encapsulated Postscript', 'jpeg': 'Joint Photographic Experts Group', 'jpg': 'Joint Photographic Experts Group', 'pdf': 'Portable Document Format', 'pgf': 'PGF code for LaTeX', 'png': 'Portable Network Graphics', 'ps': 'Postscript', 'raw': 'Raw RGBA bitmap', 'rgba': 'Raw RGBA bitmap', General Matplotlib Tips | 221 'svg': 'Scalable Vector Graphics', 'svgz': 'Scalable Vector Graphics', 'tif': 'Tagged Image File Format', 'tiff': 'Tagged Image File Format'} Note that when saving your figure, it’s not necessary to use plt.show() or related commands discussed earlier. Two Interfaces for the Price of One A potentially confusing feature of Matplotlib is its dual interfaces: a convenient MATLAB-style state-based interface, and a more powerful object-oriented interface. We’ll quickly highlight the differences between the two here. MATLAB-style interface Matplotlib was originally written as a Python alternative for MATLAB users, and much of its syntax reflects that fact. The MATLAB-style tools are contained in the pyplot ( plt ) interface",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_222"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The MATLAB-style tools are contained in the pyplot ( plt ) interface. For example, the following code will probably look quite familiar to MATLAB users ( Figure 4-3 ): In [ 9 ]: plt . figure () # create a plot figure # create the first of two panels and set current axis plt . subplot ( 2 , 1 , 1 ) # (rows, columns, panel number) plt . plot ( x , np . sin ( x )) # create the second panel and set current axis plt . subplot ( 2 , 1 , 2 ) plt . plot ( x , np . cos ( x )); Figure 4-3. Subplots using the MATLAB-style interface It’s important to note that this interface is stateful : it keeps track of the “current” figure and axes, which are where all plt commands are applied. You can get a reference to 222 | Chapter 4: Visualization with Matplotlib these using the plt.gcf() (get current figure) and plt.gca() (get current axes) routines. While this stateful interface is fast and convenient for simple plots, it is easy to run into problems. For example, once the second panel is created, how can we go back and add something to the first? This is possible within the MATLAB-style interface, but a bit clunky. Fortunately, there is a better way. Object-oriented interface The object-oriented interface is available for these more complicated situations, and for when you want more control over your figure. Rather than depending on some notion of an “active” figure or axes, in the object-oriented interface the plotting func‐ tions are methods of explicit Figure and Axes objects. To re-create the previous plot using this style of plotting, you might do the following ( Figure 4-4 ): In [ 10 ]: # First create a grid of plots # ax will be an array of two Axes objects fig , ax = plt . subplots ( 2 ) # Call plot() method on the appropriate object ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )); Figure 4-4",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_223"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( 2 ) # Call plot() method on the appropriate object ax [ 0 ] . plot ( x , np . sin ( x )) ax [ 1 ] . plot ( x , np . cos ( x )); Figure 4-4. Subplots using the object-oriented interface For more simple plots, the choice of which style to use is largely a matter of prefer‐ ence, but the object-oriented approach can become a necessity as plots become more complicated. Throughout this chapter, we will switch between the MATLAB-style and object-oriented interfaces, depending on what is most convenient. In most cases, the difference is as small as switching plt.plot() to ax.plot() , but there are a few gotchas that we will highlight as they come up in the following sections. Two Interfaces for the Price of One | 223 Simple Line Plots Perhaps the simplest of all plots is the visualization of a single function y = f x . Here we will take a first look at creating a simple plot of this type. As with all the following sections, we’ll start by setting up the notebook for plotting and importing the func‐ tions we will use: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np For all Matplotlib plots, we start by creating a figure and an axes. In their simplest form, a figure and axes can be created as follows ( Figure 4-5 ): In [ 2 ]: fig = plt . figure () ax = plt . axes () Figure 4-5. An empty gridded axes In Matplotlib, the figure (an instance of the class plt.Figure ) can be thought of as a single container that contains all the objects representing axes, graphics, text, and labels. The axes (an instance of the class plt.Axes ) is what we see above: a bounding box with ticks and labels, which will eventually contain the plot elements that make up our visualization. Throughout this book, we’ll commonly use the variable name fig to refer to a figure instance, and ax to refer to an axes instance or group of axes instances. Once we have created an axes, we can use the ax.plot function to plot some data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_224"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Once we have created an axes, we can use the ax.plot function to plot some data. Let’s start with a simple sinusoid ( Figure 4-6 ): In [ 3 ]: fig = plt . figure () ax = plt . axes () x = np . linspace ( 0 , 10 , 1000 ) ax . plot ( x , np . sin ( x )); 224 | Chapter 4: Visualization with Matplotlib Figure 4-6. A simple sinusoid Alternatively, we can use the pylab interface and let the figure and axes be created for us in the background ( Figure 4-7 ; see “Two Interfaces for the Price of One” on page 222 for a discussion of these two interfaces): In [ 4 ]: plt . plot ( x , np . sin ( x )); Figure 4-7. A simple sinusoid via the object-oriented interface If we want to create a single figure with multiple lines, we can simply call the plot function multiple times ( Figure 4-8 ): In [ 5 ]: plt . plot ( x , np . sin ( x )) plt . plot ( x , np . cos ( x )); Simple Line Plots | 225 Figure 4-8. Over-plotting multiple lines That’s all there is to plotting simple functions in Matplotlib! We’ll now dive into some more details about how to control the appearance of the axes and lines. Adjusting the Plot: Line Colors and Styles The first adjustment you might wish to make to a plot is to control the line colors and styles. The plt.plot() function takes additional arguments that can be used to spec‐ ify these. To adjust the color, you can use the color keyword, which accepts a string argument representing virtually any imaginable color. The color can be specified in a variety of ways ( Figure 4-9 ): In [ 6 ]: plt . plot ( x , np . sin ( x - 0 ), color = 'blue' ) # specify color by name plt . plot ( x , np . sin ( x - 1 ), color = 'g' ) # short color code (rgbcmyk) plt . plot ( x , np . sin ( x - 2 ), color = '0.75' ) # Grayscale between 0 and 1 plt . plot ( x , np . sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_225"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". sin ( x - 3 ), color = '#FFDD44' ) # Hex code (RRGGBB from 00 to FF) plt . plot ( x , np . sin ( x - 4 ), color = ( 1.0 , 0.2 , 0.3 )) # RGB tuple, values 0 and 1 plt . plot ( x , np . sin ( x - 5 ), color = 'chartreuse' ); # all HTML color names supported Figure 4-9. Controlling the color of plot elements 226 | Chapter 4: Visualization with Matplotlib If no color is specified, Matplotlib will automatically cycle through a set of default colors for multiple lines. Similarly, you can adjust the line style using the linestyle keyword ( Figure 4-10 ): In [ 7 ]: plt . plot ( x , x + 0 , linestyle = 'solid' ) plt . plot ( x , x + 1 , linestyle = 'dashed' ) plt . plot ( x , x + 2 , linestyle = 'dashdot' ) plt . plot ( x , x + 3 , linestyle = 'dotted' ); # For short, you can use the following codes: plt . plot ( x , x + 4 , linestyle = '-' ) # solid plt . plot ( x , x + 5 , linestyle = '--' ) # dashed plt . plot ( x , x + 6 , linestyle = '-.' ) # dashdot plt . plot ( x , x + 7 , linestyle = ':' ); # dotted Figure 4-10. Example of various line styles If you would like to be extremely terse, these linestyle and color codes can be com‐ bined into a single nonkeyword argument to the plt.plot() function ( Figure 4-11 ): In [ 8 ]: plt . plot ( x , x + 0 , '-g' ) # solid green plt . plot ( x , x + 1 , '--c' ) # dashed cyan plt . plot ( x , x + 2 , '-.k' ) # dashdot black plt . plot ( x , x + 3 , ':r' ); # dotted red Simple Line Plots | 227 Figure 4-11. Controlling colors and styles with the shorthand syntax These single-character color codes reflect the standard abbreviations in the RGB (Red/Green/Blue) and CMYK (Cyan/Magenta/Yellow/blacK) color systems, com‐ monly used for digital color graphics. There are many other keyword arguments that can be used to fine-tune the appear‐ ance of the plot; for more details, I’d suggest viewing the docstring of the plt.plot() function using IPython’s help tools (see “Help and Documentation in IPython” on page 3 )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_226"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Adjusting the Plot: Axes Limits Matplotlib does a decent job of choosing default axes limits for your plot, but some‐ times it’s nice to have finer control. The most basic way to adjust axis limits is to use the plt.xlim() and plt.ylim() methods ( Figure 4-12 ): In [ 9 ]: plt . plot ( x , np . sin ( x )) plt . xlim ( - 1 , 11 ) plt . ylim ( - 1.5 , 1.5 ); Figure 4-12. Example of setting axis limits 228 | Chapter 4: Visualization with Matplotlib If for some reason you’d like either axis to be displayed in reverse, you can simply reverse the order of the arguments ( Figure 4-13 ): In [ 10 ]: plt . plot ( x , np . sin ( x )) plt . xlim ( 10 , 0 ) plt . ylim ( 1.2 , - 1.2 ); Figure 4-13. Example of reversing the y-axis A useful related method is plt.axis() (note here the potential confusion between axes with an e , and axis with an i ). The plt.axis() method allows you to set the x and y limits with a single call, by passing a list that specifies [xmin, xmax, ymin, ymax] ( Figure 4-14 ): In [ 11 ]: plt . plot ( x , np . sin ( x )) plt . axis ([ - 1 , 11 , - 1.5 , 1.5 ]); Figure 4-14. Setting the axis limits with plt.axis The plt.axis() method goes even beyond this, allowing you to do things like auto‐ matically tighten the bounds around the current plot ( Figure 4-15 ): In [ 12 ]: plt . plot ( x , np . sin ( x )) plt . axis ( 'tight' ); Simple Line Plots | 229 Figure 4-15. Example of a “tight” layout It allows even higher-level specifications, such as ensuring an equal aspect ratio so that on your screen, one unit in x is equal to one unit in y ( Figure 4-16 ): In [ 13 ]: plt . plot ( x , np . sin ( x )) plt . axis ( 'equal' ); Figure 4-16. Example of an “equal” layout, with units matched to the output resolution For more information on axis limits and the other capabilities of the plt.axis() method, refer to the plt.axis() docstring. Labeling Plots As the last piece of this section, we’ll briefly look at the labeling of plots: titles, axis labels, and simple legends",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_227"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Labeling Plots As the last piece of this section, we’ll briefly look at the labeling of plots: titles, axis labels, and simple legends. Titles and axis labels are the simplest such labels—there are methods that can be used to quickly set them ( Figure 4-17 ): In [ 14 ]: plt . plot ( x , np . sin ( x )) plt . title ( \"A Sine Curve\" ) 230 | Chapter 4: Visualization with Matplotlib plt . xlabel ( \"x\" ) plt . ylabel ( \"sin(x)\" ); Figure 4-17. Examples of axis labels and title You can adjust the position, size, and style of these labels using optional arguments to the function. For more information, see the Matplotlib documentation and the doc‐ strings of each of these functions. When multiple lines are being shown within a single axes, it can be useful to create a plot legend that labels each line type. Again, Matplotlib has a built-in way of quickly creating such a legend. It is done via the (you guessed it) plt.legend() method. Though there are several valid ways of using this, I find it easiest to specify the label of each line using the label keyword of the plot function ( Figure 4-18 ): In [ 15 ]: plt . plot ( x , np . sin ( x ), '-g' , label = 'sin(x)' ) plt . plot ( x , np . cos ( x ), ':b' , label = 'cos(x)' ) plt . axis ( 'equal' ) plt . legend (); Figure 4-18. Plot legend example Simple Line Plots | 231 As you can see, the plt.legend() function keeps track of the line style and color, and matches these with the correct label. More information on specifying and formatting plot legends can be found in the plt.legend() docstring; additionally, we will cover some more advanced legend options in “Customizing Plot Legends” on page 249 . Matplotlib Gotchas While most plt functions translate directly to ax methods (such as plt.plot() → ax.plot() , plt.legend() → ax.legend() , etc.), this is not the case for all com‐ mands. In particular, functions to set limits, labels, and titles are slightly modified",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_228"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, functions to set limits, labels, and titles are slightly modified. For transitioning between MATLAB-style functions and object-oriented methods, make the following changes: • plt.xlabel() → ax.set_xlabel() • plt.ylabel() → ax.set_ylabel() • plt.xlim() → ax.set_xlim() • plt.ylim() → ax.set_ylim() • plt.title() → ax.set_title() In the object-oriented interface to plotting, rather than calling these functions indi‐ vidually, it is often more convenient to use the ax.set() method to set all these prop‐ erties at once ( Figure 4-19 ): In [ 16 ]: ax = plt . axes () ax . plot ( x , np . sin ( x )) ax . set ( xlim = ( 0 , 10 ), ylim = ( - 2 , 2 ), xlabel = 'x' , ylabel = 'sin(x)' , title = 'A Simple Plot' ); Figure 4-19. Example of using ax.set to set multiple properties at once 232 | Chapter 4: Visualization with Matplotlib Simple Scatter Plots Another commonly used plot type is the simple scatter plot, a close cousin of the line plot. Instead of points being joined by line segments, here the points are represented individually with a dot, circle, or other shape. We’ll start by setting up the notebook for plotting and importing the functions we will use: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np Scatter Plots with plt.plot In the previous section, we looked at plt.plot / ax.plot to produce line plots. It turns out that this same function can produce scatter plots as well ( Figure 4-20 ): In [ 2 ]: x = np . linspace ( 0 , 10 , 30 ) y = np . sin ( x ) plt . plot ( x , y , 'o' , color = 'black' ); Figure 4-20. Scatter plot example The third argument in the function call is a character that represents the type of sym‐ bol used for the plotting. Just as you can specify options such as '-' and '--' to con‐ trol the line style, the marker style has its own set of short string codes. The full list of available symbols can be seen in the documentation of plt.plot , or in Matplotlib’s online documentation",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_229"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The full list of available symbols can be seen in the documentation of plt.plot , or in Matplotlib’s online documentation. Most of the possibilities are fairly intuitive, and we’ll show a number of the more common ones here ( Figure 4-21 ): In [ 3 ]: rng = np . random . RandomState ( 0 ) for marker in [ 'o' , '.' , ',' , 'x' , '+' , 'v' , '^' , '<' , '>' , 's' , 'd' ]: plt . plot ( rng . rand ( 5 ), rng . rand ( 5 ), marker , label = \"marker='{0}'\" . format ( marker )) Simple Scatter Plots | 233 plt . legend ( numpoints = 1 ) plt . xlim ( 0 , 1.8 ); Figure 4-21. Demonstration of point numbers For even more possibilities, these character codes can be used together with line and color codes to plot points along with a line connecting them ( Figure 4-22 ): In [ 4 ]: plt . plot ( x , y , '-ok' ); # line (-), circle marker (o), black (k) Figure 4-22. Combining line and point markers Additional keyword arguments to plt.plot specify a wide range of properties of the lines and markers ( Figure 4-23 ): In [ 5 ]: plt . plot ( x , y , '-p' , color = 'gray' , markersize = 15 , linewidth = 4 , markerfacecolor = 'white' , markeredgecolor = 'gray' , markeredgewidth = 2 ) plt . ylim ( - 1.2 , 1.2 ); 234 | Chapter 4: Visualization with Matplotlib Figure 4-23. Customizing line and point numbers This type of flexibility in the plt.plot function allows for a wide variety of possible visualization options. For a full description of the options available, refer to the plt.plot documentation. Scatter Plots with plt.scatter A second, more powerful method of creating scatter plots is the plt.scatter func‐ tion, which can be used very similarly to the plt.plot function ( Figure 4-24 ): In [ 6 ]: plt . scatter ( x , y , marker = 'o' ); Figure 4-24. A simple scatter plot The primary difference of plt.scatter from plt.plot is that it can be used to create scatter plots where the properties of each individual point (size, face color, edge color, etc.) can be individually controlled or mapped to data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_230"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Let’s show this by creating a random scatter plot with points of many colors and sizes. In order to better see the overlapping results, we’ll also use the alpha keyword to adjust the transparency level ( Figure 4-25 ): Simple Scatter Plots | 235 In [ 7 ]: rng = np . random . RandomState ( 0 ) x = rng . randn ( 100 ) y = rng . randn ( 100 ) colors = rng . rand ( 100 ) sizes = 1000 * rng . rand ( 100 ) plt . scatter ( x , y , c = colors , s = sizes , alpha = 0.3 , cmap = 'viridis' ) plt . colorbar (); # show color scale Figure 4-25. Changing size, color, and transparency in scatter points Notice that the color argument is automatically mapped to a color scale (shown here by the colorbar() command), and the size argument is given in pixels. In this way, the color and size of points can be used to convey information in the visualization, in order to illustrate multidimensional data. For example, we might use the Iris data from Scikit-Learn, where each sample is one of three types of flowers that has had the size of its petals and sepals carefully meas‐ ured ( Figure 4-26 ): In [ 8 ]: from sklearn.datasets import load_iris iris = load_iris () features = iris . data . T plt . scatter ( features [ 0 ], features [ 1 ], alpha = 0.2 , s = 100 * features [ 3 ], c = iris . target , cmap = 'viridis' ) plt . xlabel ( iris . feature_names [ 0 ]) plt . ylabel ( iris . feature_names [ 1 ]); 236 | Chapter 4: Visualization with Matplotlib Figure 4-26. Using point properties to encode features of the Iris data We can see that this scatter plot has given us the ability to simultaneously explore four different dimensions of the data: the (x, y) location of each point corresponds to the sepal length and width, the size of the point is related to the petal width, and the color is related to the particular species of flower. Multicolor and multifeature scatter plots like this can be useful for both exploration and presentation of data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_231"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Multicolor and multifeature scatter plots like this can be useful for both exploration and presentation of data. plot Versus scatter: A Note on Efficiency Aside from the different features available in plt.plot and plt.scatter , why might you choose to use one over the other? While it doesn’t matter as much for small amounts of data, as datasets get larger than a few thousand points, plt.plot can be noticeably more efficient than plt.scatter . The reason is that plt.scatter has the capability to render a different size and/or color for each point, so the renderer must do the extra work of constructing each point individually. In plt.plot , on the other hand, the points are always essentially clones of each other, so the work of determin‐ ing the appearance of the points is done only once for the entire set of data. For large datasets, the difference between these two can lead to vastly different performance, and for this reason, plt.plot should be preferred over plt.scatter for large datasets. Visualizing Errors For any scientific measurement, accurate accounting for errors is nearly as important, if not more important, than accurate reporting of the number itself. For example, imagine that I am using some astrophysical observations to estimate the Hubble Con‐ stant, the local measurement of the expansion rate of the universe. I know that the current literature suggests a value of around 71 (km/s)/Mpc, and I measure a value of 74 (km/s)/Mpc with my method. Are the values consistent? The only correct answer, given this information, is this: there is no way to know. Visualizing Errors | 237 Suppose I augment this information with reported uncertainties: the current litera‐ ture suggests a value of around 71 ± 2.5 (km/s)/Mpc, and my method has measured a value of 74 ± 5 (km/s)/Mpc. Now are the values consistent? That is a question that can be quantitatively answered. In visualization of data and results, showing these errors effectively can make a plot convey much more complete information",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_232"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In visualization of data and results, showing these errors effectively can make a plot convey much more complete information. Basic Errorbars A basic errorbar can be created with a single Matplotlib function call ( Figure 4-27 ): In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np In [ 2 ]: x = np . linspace ( 0 , 10 , 50 ) dy = 0.8 y = np . sin ( x ) + dy * np . random . randn ( 50 ) plt . errorbar ( x , y , yerr = dy , fmt = '.k' ); Figure 4-27. An errorbar example Here the fmt is a format code controlling the appearance of lines and points, and has the same syntax as the shorthand used in plt.plot , outlined in “Simple Line Plots” on page 224 and “Simple Scatter Plots” on page 233 . In addition to these basic options, the errorbar function has many options to fintune the outputs. Using these additional options you can easily customize the aesthet‐ ics of your errorbar plot. I often find it helpful, especially in crowded plots, to make the errorbars lighter than the points themselves ( Figure 4-28 ): In [ 3 ]: plt . errorbar ( x , y , yerr = dy , fmt = 'o' , color = 'black' , ecolor = 'lightgray' , elinewidth = 3 , capsize = 0 ); 238 | Chapter 4: Visualization with Matplotlib Figure 4-28. Customizing errorbars In addition to these options, you can also specify horizontal errorbars ( xerr ), onsided errorbars, and many other variants. For more information on the options avail‐ able, refer to the docstring of plt.errorbar . Continuous Errors In some situations it is desirable to show errorbars on continuous quantities. Though Matplotlib does not have a built-in convenience routine for this type of application, it’s relatively easy to combine primitives like plt.plot and plt.fill_between for a useful result. Here we’ll perform a simple Gaussian process regression (GPR), using the Scikit-Learn API (see “Introducing Scikit-Learn” on page 343 for details)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_233"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here we’ll perform a simple Gaussian process regression (GPR), using the Scikit-Learn API (see “Introducing Scikit-Learn” on page 343 for details). This is a method of fit‐ ting a very flexible nonparametric function to data with a continuous measure of the uncertainty. We won’t delve into the details of Gaussian process regression at this point, but will focus instead on how you might visualize such a continuous error measurement: In [ 4 ]: from sklearn.gaussian_process import GaussianProcess # define the model and draw some data model = lambda x : x * np . sin ( x ) xdata = np . array ([ 1 , 3 , 5 , 6 , 8 ]) ydata = model ( xdata ) # Compute the Gaussian process fit gp = GaussianProcess ( corr = 'cubic' , theta0 = 1e-2 , thetaL = 1e-4 , thetaU = 1E-1 , random_start = 100 ) gp . fit ( xdata [:, np . newaxis ], ydata ) xfit = np . linspace ( 0 , 10 , 1000 ) yfit , MSE = gp . predict ( xfit [:, np . newaxis ], eval_MSE = True ) dyfit = 2 * np . sqrt ( MSE ) # 2*sigma ~ 95% confidence region Visualizing Errors | 239 We now have xfit , yfit , and dyfit , which sample the continuous fit to our data. We could pass these to the plt.errorbar function as above, but we don’t really want to plot 1,000 points with 1,000 errorbars. Instead, we can use the plt.fill_between function with a light color to visualize this continuous error ( Figure 4-29 ): In [ 5 ]: # Visualize the result plt . plot ( xdata , ydata , 'or' ) plt . plot ( xfit , yfit , '-' , color = 'gray' ) plt . fill_between ( xfit , yfit - dyfit , yfit + dyfit , color = 'gray' , alpha = 0.2 ) plt . xlim ( 0 , 10 ); Figure 4-29. Representing continuous uncertainty with filled regions Note what we’ve done here with the fill_between function: we pass an x value, then the lower y-bound, then the upper y-bound, and the result is that the area between these regions is filled",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_234"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The resulting figure gives a very intuitive view into what the Gaussian process regres‐ sion algorithm is doing: in regions near a measured data point, the model is strongly constrained and this is reflected in the small model errors. In regions far from a measured data point, the model is not strongly constrained, and the model errors increase. For more information on the options available in plt.fill_between() (and the closely related plt.fill() function), see the function docstring or the Matplotlib documentation. Finally, if this seems a bit too low level for your taste, refer to “Visualization with Sea‐ born” on page 311 , where we discuss the Seaborn package, which has a more stream‐ lined API for visualizing this type of continuous errorbar. 240 | Chapter 4: Visualization with Matplotlib Density and Contour Plots Sometimes it is useful to display three-dimensional data in two dimensions using contours or color-coded regions. There are three Matplotlib functions that can be helpful for this task: plt.contour for contour plots, plt.contourf for filled contour plots, and plt.imshow for showing images. This section looks at several examples of using these. We’ll start by setting up the notebook for plotting and importing the functions we will use: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np Visualizing a Three-Dimensional Function We’ll start by demonstrating a contour plot using a function z = f x , y , using the fol‐ lowing particular choice for f (we’ve seen this before in “Computation on Arrays: Broadcasting” on page 63 , when we used it as a motivating example for array broadcasting): In [ 2 ]: def f ( x , y ): return np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu‐ ments: a grid of x values, a grid of y values, and a grid of z values",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_235"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". cos ( 10 + y * x ) * np . cos ( x ) A contour plot can be created with the plt.contour function. It takes three argu‐ ments: a grid of x values, a grid of y values, and a grid of z values. The x and y values represent positions on the plot, and the z values will be represented by the contour levels. Perhaps the most straightforward way to prepare such data is to use the np.meshgrid function, which builds two-dimensional grids from one-dimensional arrays: In [ 3 ]: x = np . linspace ( 0 , 5 , 50 ) y = np . linspace ( 0 , 5 , 40 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) Now let’s look at this with a standard line-only contour plot ( Figure 4-30 ): In [ 4 ]: plt . contour ( X , Y , Z , colors = 'black' ); Density and Contour Plots | 241 Figure 4-30. Visualizing three-dimensional data with contours Notice that by default when a single color is used, negative values are represented by dashed lines, and positive values by solid lines. Alternatively, you can color-code the lines by specifying a colormap with the cmap argument. Here, we’ll also specify that we want more lines to be drawn—20 equally spaced intervals within the data range ( Figure 4-31 ): In [ 5 ]: plt . contour ( X , Y , Z , 20 , cmap = 'RdGy' ); Figure 4-31. Visualizing three-dimensional data with colored contours Here we chose the RdGy (short for Red-Gray ) colormap, which is a good choice for centered data. Matplotlib has a wide range of colormaps available, which you can easily browse in IPython by doing a tab completion on the plt.cm module: plt.cm.<TAB> Our plot is looking nicer, but the spaces between the lines may be a bit distracting. We can change this by switching to a filled contour plot using the plt.contourf() function (notice the f at the end), which uses largely the same syntax as plt.con tour() . 242 | Chapter 4: Visualization with Matplotlib Additionally, we’ll add a plt.colorbar() command, which automatically creates an additional axis with labeled color information for the plot ( Figure 4-32 ): In [ 6 ]: plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_236"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". contourf ( X , Y , Z , 20 , cmap = 'RdGy' ) plt . colorbar (); Figure 4-32. Visualizing three-dimensional data with filled contours The colorbar makes it clear that the black regions are “peaks,” while the red regions are “valleys.” One potential issue with this plot is that it is a bit “splotchy.” That is, the color steps are discrete rather than continuous, which is not always what is desired. You could remedy this by setting the number of contours to a very high number, but this results in a rather inefficient plot: Matplotlib must render a new polygon for each step in the level. A better way to handle this is to use the plt.imshow() function, which inter‐ prets a two-dimensional grid of data as an image. Figure 4-33 shows the result of the following code: In [ 7 ]: plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' ) plt . colorbar () plt . axis ( aspect = 'image' ); There are a few potential gotchas with imshow() , however: • plt.imshow() doesn’t accept an x and y grid, so you must manually specify the extent [ xmin , xmax , ymin , ymax ] of the image on the plot. • plt.imshow() by default follows the standard image array definition where the origin is in the upper left, not in the lower left as in most contour plots. This must be changed when showing gridded data. Density and Contour Plots | 243 • plt.imshow() will automatically adjust the axis aspect ratio to match the input data; you can change this by setting, for example, plt.axis(aspect='image') to make x and y units match. Figure 4-33. Representing three-dimensional data as an image Finally, it can sometimes be useful to combine contour plots and image plots. For example, to create the effect shown in Figure 4-34 , we’ll use a partially transparent background image (with transparency set via the alpha parameter) and over-plot contours with labels on the contours themselves (using the plt.clabel() function): In [ 8 ]: contours = plt . contour ( X , Y , Z , 3 , colors = 'black' ) plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_237"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". contour ( X , Y , Z , 3 , colors = 'black' ) plt . clabel ( contours , inline = True , fontsize = 8 ) plt . imshow ( Z , extent = [ 0 , 5 , 0 , 5 ], origin = 'lower' , cmap = 'RdGy' , alpha = 0.5 ) plt . colorbar (); Figure 4-34. Labeled contours on top of an image The combination of these three functions— plt.contour , plt.contourf , and plt.imshow —gives nearly limitless possibilities for displaying this sort of thredimensional data within a two-dimensional plot. For more information on the 244 | Chapter 4: Visualization with Matplotlib options available in these functions, refer to their docstrings. If you are interested in three-dimensional visualizations of this type of data, see “Three-Dimensional Plot‐ ting in Matplotlib” on page 290 . Histograms, Binnings, and Density A simple histogram can be a great first step in understanding a dataset. Earlier, we saw a preview of Matplotlib’s histogram function (see “Comparisons, Masks, and Boolean Logic” on page 70 ), which creates a basic histogram in one line, once the normal boilerplate imports are done ( Figure 4-35 ): In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) data = np . random . randn ( 1000 ) In [ 2 ]: plt . hist ( data ); Figure 4-35. A simple histogram The hist() function has many options to tune both the calculation and the display; here’s an example of a more customized histogram ( Figure 4-36 ): In [ 3 ]: plt . hist ( data , bins = 30 , normed = True , alpha = 0.5 , histtype = 'stepfilled' , color = 'steelblue' , edgecolor = 'none' ); Histograms, Binnings, and Density | 245 Figure 4-36. A customized histogram The plt.hist docstring has more information on other customization options avail‐ able. I find this combination of histtype='stepfilled' along with some transpar‐ ency alpha to be very useful when comparing histograms of several distributions ( Figure 4-37 ): In [ 4 ]: x1 = np . random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_238"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . normal ( 0 , 0.8 , 1000 ) x2 = np . random . normal ( - 2 , 1 , 1000 ) x3 = np . random . normal ( 3 , 2 , 1000 ) kwargs = dict ( histtype = 'stepfilled' , alpha = 0.3 , normed = True , bins = 40 ) plt . hist ( x1 , ** kwargs ) plt . hist ( x2 , ** kwargs ) plt . hist ( x3 , ** kwargs ); Figure 4-37. Over-plotting multiple histograms If you would like to simply compute the histogram (that is, count the number of points in a given bin) and not display it, the np.histogram() function is available: 246 | Chapter 4: Visualization with Matplotlib In [ 5 ]: counts , bin_edges = np . histogram ( data , bins = 5 ) print ( counts ) [ 12 190 468 301 29] Two-Dimensional Histograms and Binnings Just as we create histograms in one dimension by dividing the number line into bins, we can also create histograms in two dimensions by dividing points among twdimensional bins. We’ll take a brief look at several ways to do this here. We’ll start by defining some data—an x and y array drawn from a multivariate Gaussian distribution: In [ 6 ]: mean = [ 0 , 0 ] cov = [[ 1 , 1 ], [ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 10000 ) . T plt.hist2d: Two-dimensional histogram One straightforward way to plot a two-dimensional histogram is to use Matplotlib’s plt.hist2d function ( Figure 4-38 ): In [ 12 ]: plt . hist2d ( x , y , bins = 30 , cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( 'counts in bin' ) Figure 4-38. A two-dimensional histogram with plt.hist2d Just as with plt.hist , plt.hist2d has a number of extra options to fine-tune the plot and the binning, which are nicely outlined in the function docstring. Further, just as plt.hist has a counterpart in np.histogram , plt.hist2d has a counterpart in np.histogram2d , which can be used as follows: In [ 8 ]: counts , xedges , yedges = np . histogram2d ( x , y , bins = 30 ) For the generalization of this histogram binning in dimensions higher than two, see the np.histogramdd function",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_239"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". histogram2d ( x , y , bins = 30 ) For the generalization of this histogram binning in dimensions higher than two, see the np.histogramdd function. Histograms, Binnings, and Density | 247 plt.hexbin: Hexagonal binnings The two-dimensional histogram creates a tessellation of squares across the axes. Another natural shape for such a tessellation is the regular hexagon. For this purpose, Matplotlib provides the plt.hexbin routine, which represents a two-dimensional dataset binned within a grid of hexagons ( Figure 4-39 ): In [ 9 ]: plt . hexbin ( x , y , gridsize = 30 , cmap = 'Blues' ) cb = plt . colorbar ( label = 'count in bin' ) Figure 4-39. A two-dimensional histogram with plt.hexbin plt.hexbin has a number of interesting options, including the ability to specify weights for each point, and to change the output in each bin to any NumPy aggregate (mean of weights, standard deviation of weights, etc.). Kernel density estimation Another common method of evaluating densities in multiple dimensions is kernel density estimation (KDE). This will be discussed more fully in “In-Depth: Kernel Density Estimation” on page 491 , but for now we’ll simply mention that KDE can be thought of as a way to “smear out” the points in space and add up the result to obtain a smooth function. One extremely quick and simple KDE implementation exists in the scipy.stats package. Here is a quick example of using the KDE on this data ( Figure 4-40 ): In [ 10 ]: from scipy.stats import gaussian_kde # fit an array of size [Ndim, Nsamples] data = np . vstack ([ x , y ]) kde = gaussian_kde ( data ) # evaluate on a regular grid xgrid = np . linspace ( - 3.5 , 3.5 , 40 ) ygrid = np . linspace ( - 6 , 6 , 40 ) Xgrid , Ygrid = np . meshgrid ( xgrid , ygrid ) Z = kde . evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) 248 | Chapter 4: Visualization with Matplotlib # Plot the result as an image plt . imshow ( Z . reshape ( Xgrid",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_240"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". evaluate ( np . vstack ([ Xgrid . ravel (), Ygrid . ravel ()])) 248 | Chapter 4: Visualization with Matplotlib # Plot the result as an image plt . imshow ( Z . reshape ( Xgrid . shape ), origin = 'lower' , aspect = 'auto' , extent = [ - 3.5 , 3.5 , - 6 , 6 ], cmap = 'Blues' ) cb = plt . colorbar () cb . set_label ( \"density\" ) Figure 4-40. A kernel density representation of a distribution KDE has a smoothing length that effectively slides the knob between detail and smoothness (one example of the ubiquitous bias–variance trade-off). The literature on choosing an appropriate smoothing length is vast: gaussian_kde uses a rule of thumb to attempt to find a nearly optimal smoothing length for the input data. Other KDE implementations are available within the SciPy ecosystem, each with its own various strengths and weaknesses; see, for example, sklearn.neighbors.Kernel Density and statsmodels.nonparametric.kernel_density.KDEMultivariate . For visualizations based on KDE, using Matplotlib tends to be overly verbose. The Sea‐ born library, discussed in “Visualization with Seaborn” on page 311 , provides a much more terse API for creating KDE-based visualizations. Customizing Plot Legends Plot legends give meaning to a visualization, assigning labels to the various plot ele‐ ments. We previously saw how to create a simple legend; here we’ll take a look at cus‐ tomizing the placement and aesthetics of the legend in Matplotlib. The simplest legend can be created with the plt.legend() command, which auto‐ matically creates a legend for any labeled plot elements ( Figure 4-41 ): In [ 1 ]: import matplotlib.pyplot as plt plt . style . use ( 'classic' ) Customizing Plot Legends | 249 In [ 2 ]: % matplotlib inline import numpy as np In [ 3 ]: x = np . linspace ( 0 , 10 , 1000 ) fig , ax = plt . subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); Figure 4-41",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_241"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots () ax . plot ( x , np . sin ( x ), '-b' , label = 'Sine' ) ax . plot ( x , np . cos ( x ), '--r' , label = 'Cosine' ) ax . axis ( 'equal' ) leg = ax . legend (); Figure 4-41. A default plot legend But there are many ways we might want to customize such a legend. For example, we can specify the location and turn off the frame ( Figure 4-42 ): In [ 4 ]: ax . legend ( loc = 'upper left' , frameon = False ) fig Figure 4-42. A customized plot legend We can use the ncol command to specify the number of columns in the legend ( Figure 4-43 ): In [ 5 ]: ax . legend ( frameon = False , loc = 'lower center' , ncol = 2 ) fig 250 | Chapter 4: Visualization with Matplotlib Figure 4-43. A two-column plot legend We can use a rounded box ( fancybox ) or add a shadow, change the transparency (alpha value) of the frame, or change the padding around the text ( Figure 4-44 ): In [ 6 ]: ax . legend ( fancybox = True , framealpha = 1 , shadow = True , borderpad = 1 ) fig Figure 4-44. A fancybox plot legend For more information on available legend options, see the plt.legend docstring. Choosing Elements for the Legend As we’ve already seen, the legend includes all labeled elements by default. If this is not what is desired, we can fine-tune which elements and labels appear in the legend by using the objects returned by plot commands. The plt.plot() command is able to create multiple lines at once, and returns a list of created line instances. Passing any of these to plt.legend() will tell it which to identify, along with the labels we’d like to specify ( Figure 4-45 ): In [ 7 ]: y = np . sin ( x [:, np . newaxis ] + np . pi * np . arange ( 0 , 2 , 0.5 )) lines = plt . plot ( x , y ) Customizing Plot Legends | 251 # lines is a list of plt.Line2D instances plt . legend ( lines [: 2 ], [ 'first' , 'second' ]); Figure 4-45. Customization of legend elements I generally find in practice that it is clearer to use the first method, applying labels to the plot elements you’d like to show on the legend ( Figure 4-46 ): In [ 8 ]: plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_242"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". plot ( x , y [:, 0 ], label = 'first' ) plt . plot ( x , y [:, 1 ], label = 'second' ) plt . plot ( x , y [:, 2 :]) plt . legend ( framealpha = 1 , frameon = True ); Figure 4-46. Alternative method of customizing legend elements Notice that by default, the legend ignores all elements without a label attribute set. Legend for Size of Points Sometimes the legend defaults are not sufficient for the given visualization. For exam‐ ple, perhaps you’re using the size of points to mark certain features of the data, and want to create a legend reflecting this. Here is an example where we’ll use the size of points to indicate populations of California cities. We’d like a legend that specifies the 252 | Chapter 4: Visualization with Matplotlib scale of the sizes of the points, and we’ll accomplish this by plotting some labeled data with no entries ( Figure 4-47 ): In [ 9 ]: import pandas as pd cities = pd . read_csv ( 'data/california_cities.csv' ) # Extract the data we're interested in lat , lon = cities [ 'latd' ], cities [ 'longd' ] population , area = cities [ 'population_total' ], cities [ 'area_total_km2' ] # Scatter the points, using size and color but no label plt . scatter ( lon , lat , label = None , c = np . log10 ( population ), cmap = 'viridis' , s = area , linewidth = 0 , alpha = 0.5 ) plt . axis ( aspect = 'equal' ) plt . xlabel ( 'longitude' ) plt . ylabel ( 'latitude' ) plt . colorbar ( label = 'log$_{10}$(population)' ) plt . clim ( 3 , 7 ) # Here we create a legend: # we'll plot empty lists with the desired size and label for area in [ 100 , 300 , 500 ]: plt . scatter ([], [], c = 'k' , alpha = 0.3 , s = area , label = str ( area ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , title = 'City Area' ) plt . title ( 'California Cities: Area and Population' ); Figure 4-47. Location, geographic size, and population of California cities The legend will always reference some object that is on the plot, so if we’d like to dis‐ play a particular shape we need to plot it",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_243"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Location, geographic size, and population of California cities The legend will always reference some object that is on the plot, so if we’d like to dis‐ play a particular shape we need to plot it. In this case, the objects we want (gray cir‐ cles) are not on the plot, so we fake them by plotting empty lists. Notice too that the legend only lists plot elements that have a label specified. Customizing Plot Legends | 253 By plotting empty lists, we create labeled plot objects that are picked up by the legend, and now our legend tells us some useful information. This strategy can be useful for creating more sophisticated visualizations. Finally, note that for geographic data like this, it would be clearer if we could show state boundaries or other map-specific elements. For this, an excellent choice of tool is Matplotlib’s Basemap add-on toolkit, which we’ll explore in “Geographic Data with Basemap” on page 298 . Multiple Legends Sometimes when designing a plot you’d like to add multiple legends to the same axes. Unfortunately, Matplotlib does not make this easy: via the standard legend interface, it is only possible to create a single legend for the entire plot. If you try to create a second legend using plt.legend() or ax.legend() , it will simply override the first one. We can work around this by creating a new legend artist from scratch, and then using the lower-level ax.add_artist() method to manually add the second artist to the plot ( Figure 4-48 ): In [ 10 ]: fig , ax = plt . subplots () lines = [] styles = [ '-' , '--' , '-.' , ':' ] x = np . linspace ( 0 , 10 , 1000 ) for i in range ( 4 ): lines += ax . plot ( x , np . sin ( x - i * np . pi / 2 ), styles [ i ], color = 'black' ) ax . axis ( 'equal' ) # specify the lines and labels of the first legend ax . legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_244"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". legend ( lines [: 2 ], [ 'line A' , 'line B' ], loc = 'upper right' , frameon = False ) # Create the second legend and add the artist manually. from matplotlib.legend import Legend leg = Legend ( ax , lines [ 2 :], [ 'line C' , 'line D' ], loc = 'lower right' , frameon = False ) ax . add_artist ( leg ); 254 | Chapter 4: Visualization with Matplotlib Figure 4-48. A split plot legend This is a peek into the low-level artist objects that compose any Matplotlib plot. If you examine the source code of ax.legend() (recall that you can do this within the IPy‐ thon notebook using ax.legend?? ) you’ll see that the function simply consists of some logic to create a suitable Legend artist, which is then saved in the legend_ attribute and added to the figure when the plot is drawn. Customizing Colorbars Plot legends identify discrete labels of discrete points. For continuous labels based on the color of points, lines, or regions, a labeled colorbar can be a great tool. In Mat‐ plotlib, a colorbar is a separate axes that can provide a key for the meaning of colors in a plot. Because the book is printed in black and white, this section has an accompa‐ nying online appendix where you can view the figures in full color ( https:// github.com/jakevdp/PythonDataScienceHandbook ). We’ll start by setting up the note‐ book for plotting and importing the functions we will use: In [ 1 ]: import matplotlib.pyplot as plt plt . style . use ( 'classic' ) In [ 2 ]: % matplotlib inline import numpy as np As we have seen several times throughout this section, the simplest colorbar can be created with the plt.colorbar function ( Figure 4-49 ): In [ 3 ]: x = np . linspace ( 0 , 10 , 1000 ) I = np . sin ( x ) * np . cos ( x [:, np . newaxis ]) plt . imshow ( I ) plt . colorbar (); Customizing Colorbars | 255 Figure 4-49. A simple colorbar legend We’ll now discuss a few ideas for customizing these colorbars and using them effec‐ tively in various situations",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_245"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". colorbar (); Customizing Colorbars | 255 Figure 4-49. A simple colorbar legend We’ll now discuss a few ideas for customizing these colorbars and using them effec‐ tively in various situations. Customizing Colorbars We can specify the colormap using the cmap argument to the plotting function that is creating the visualization ( Figure 4-50 ): In [ 4 ]: plt . imshow ( I , cmap = 'gray' ); Figure 4-50. A grayscale colormap All the available colormaps are in the plt.cm namespace; using IPython’s tacompletion feature will give you a full list of built-in possibilities: plt.cm.<TAB> But being able to choose a colormap is just the first step: more important is how to decide among the possibilities! The choice turns out to be much more subtle than you might initially expect. 256 | Chapter 4: Visualization with Matplotlib Choosing the colormap A full treatment of color choice within visualization is beyond the scope of this book, but for entertaining reading on this subject and others, see the article “Ten Simple Rules for Better Figures” . Matplotlib’s online documentation also has an interesting discussion of colormap choice. Broadly, you should be aware of three different categories of colormaps: Sequential colormaps These consist of one continuous sequence of colors (e.g., binary or viridis ). Divergent colormaps These usually contain two distinct colors, which show positive and negative devi‐ ations from a mean (e.g., RdBu or PuOr ). Qualitative colormaps These mix colors with no particular sequence (e.g., rainbow or jet ). The jet colormap, which was the default in Matplotlib prior to version 2.0, is an example of a qualitative colormap. Its status as the default was quite unfortunate, because qualitative maps are often a poor choice for representing quantitative data. Among the problems is the fact that qualitative maps usually do not display any uni‐ form progression in brightness as the scale increases",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_246"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Among the problems is the fact that qualitative maps usually do not display any uni‐ form progression in brightness as the scale increases. We can see this by converting the jet colorbar into black and white ( Figure 4-51 ): In [ 5 ]: from matplotlib.colors import LinearSegmentedColormap def grayscale_cmap ( cmap ): \"\"\"Return a grayscale version of the given colormap\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) # convert RGBA to perceived grayscale luminance # cf. http://alienryderflex.com/hsp.html RGB_weight = [ 0.299 , 0.587 , 0.114 ] luminance = np . sqrt ( np . dot ( colors [:, : 3 ] ** 2 , RGB_weight )) colors [:, : 3 ] = luminance [:, np . newaxis ] return LinearSegmentedColormap . from_list ( cmap . name + \"_gray\" , colors , cmap . N ) def view_colormap ( cmap ): \"\"\"Plot a colormap with its grayscale equivalent\"\"\" cmap = plt . cm . get_cmap ( cmap ) colors = cmap ( np . arange ( cmap . N )) cmap = grayscale_cmap ( cmap ) grayscale = cmap ( np . arange ( cmap . N )) Customizing Colorbars | 257 fig , ax = plt . subplots ( 2 , figsize = ( 6 , 2 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ([ colors ], extent = [ 0 , 10 , 0 , 1 ]) ax [ 1 ] . imshow ([ grayscale ], extent = [ 0 , 10 , 0 , 1 ]) In [ 6 ]: view_colormap ( 'jet' ) Figure 4-51. The jet colormap and its uneven luminance scale Notice the bright stripes in the grayscale image. Even in full color, this uneven bright‐ ness means that the eye will be drawn to certain portions of the color range, which will potentially emphasize unimportant parts of the dataset. It’s better to use a color‐ map such as viridis (the default as of Matplotlib 2.0), which is specifically construc‐ ted to have an even brightness variation across the range. Thus, it not only plays well with our color perception, but also will translate well to grayscale printing ( Figure 4-52 ): In [ 7 ]: view_colormap ( 'viridis' ) Figure 4-52",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_247"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Thus, it not only plays well with our color perception, but also will translate well to grayscale printing ( Figure 4-52 ): In [ 7 ]: view_colormap ( 'viridis' ) Figure 4-52. The viridis colormap and its even luminance scale If you favor rainbow schemes, another good option for continuous data is the cubehelix colormap ( Figure 4-53 ): In [ 8 ]: view_colormap ( 'cubehelix' ) Figure 4-53. The cubehelix colormap and its luminance For other situations, such as showing positive and negative deviations from some mean, dual-color colorbars such as RdBu (short for Red-Blue ) can be useful. However, 258 | Chapter 4: Visualization with Matplotlib as you can see in Figure 4-54 , it’s important to note that the positive-negative infor‐ mation will be lost upon translation to grayscale! In [ 9 ]: view_colormap ( 'RdBu' ) Figure 4-54. The RdBu (Red-Blue) colormap and its luminance We’ll see examples of using some of these color maps as we continue. There are a large number of colormaps available in Matplotlib; to see a list of them, you can use IPython to explore the plt.cm submodule. For a more principled approach to colors in Python, you can refer to the tools and documentation within the Seaborn library (see “Visualization with Seaborn” on page 311 ). Color limits and extensions Matplotlib allows for a large range of colorbar customization. The colorbar itself is simply an instance of plt.Axes , so all of the axes and tick formatting tricks we’ve learned are applicable. The colorbar has some interesting flexibility; for example, we can narrow the color limits and indicate the out-of-bounds values with a triangular arrow at the top and bottom by setting the extend property. This might come in handy, for example, if you’re displaying an image that is subject to noise ( Figure 4-55 ): In [ 10 ]: # make noise in 1% of the image pixels speckles = ( np . random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_248"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . random ( I . shape ) < 0.01 ) I [ speckles ] = np . random . normal ( 0 , 3 , np . count_nonzero ( speckles )) plt . figure ( figsize = ( 10 , 3.5 )) plt . subplot ( 1 , 2 , 1 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar () plt . subplot ( 1 , 2 , 2 ) plt . imshow ( I , cmap = 'RdBu' ) plt . colorbar ( extend = 'both' ) plt . clim ( - 1 , 1 ); Customizing Colorbars | 259 Figure 4-55. Specifying colormap extensions Notice that in the left panel, the default color limits respond to the noisy pixels, and the range of the noise completely washes out the pattern we are interested in. In the right panel, we manually set the color limits, and add extensions to indicate values that are above or below those limits. The result is a much more useful visualization of our data. Discrete colorbars Colormaps are by default continuous, but sometimes you’d like to represent discrete values. The easiest way to do this is to use the plt.cm.get_cmap() function, and pass the name of a suitable colormap along with the number of desired bins ( Figure 4-56 ): In [ 11 ]: plt . imshow ( I , cmap = plt . cm . get_cmap ( 'Blues' , 6 )) plt . colorbar () plt . clim ( - 1 , 1 ); Figure 4-56. A discretized colormap The discrete version of a colormap can be used just like any other colormap. 260 | Chapter 4: Visualization with Matplotlib Example: Handwritten Digits For an example of where this might be useful, let’s look at an interesting visualization of some handwritten digits data. This data is included in Scikit-Learn, and consists of nearly 2,000 8×8 thumbnails showing various handwritten digits. For now, let’s start by downloading the digits data and visualizing several of the exam‐ ple images with plt.imshow() ( Figure 4-57 ): In [ 12 ]: # load images of the digits 0 through 5 and visualize several of them from sklearn.datasets import load_digits digits = load_digits ( n_class = 6 ) fig , ax = plt . subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_249"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( 8 , 8 , figsize = ( 6 , 6 )) for i , axi in enumerate ( ax . flat ): axi . imshow ( digits . images [ i ], cmap = 'binary' ) axi . set ( xticks = [], yticks = []) Figure 4-57. Sample of handwritten digit data Because each digit is defined by the hue of its 64 pixels, we can consider each digit to be a point lying in 64-dimensional space: each dimension represents the brightness of one pixel. But visualizing relationships in such high-dimensional spaces can be extremely difficult. One way to approach this is to use a dimensionality reduction technique such as manifold learning to reduce the dimensionality of the data while maintaining the relationships of interest. Dimensionality reduction is an example of unsupervised machine learning, and we will discuss it in more detail in “What Is Machine Learning?” on page 332 . Deferring the discussion of these details, let’s take a look at a two-dimensional mani‐ fold learning projection of this digits data (see “In-Depth: Manifold Learning” on page 445 for details): Customizing Colorbars | 261 In [ 13 ]: # project the digits into 2 dimensions using IsoMap from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) projection = iso . fit_transform ( digits . data ) We’ll use our discrete colormap to view the results, setting the ticks and clim to improve the aesthetics of the resulting colorbar ( Figure 4-58 ): In [ 14 ]: # plot the results plt . scatter ( projection [:, 0 ], projection [:, 1 ], lw = 0.1 , c = digits . target , cmap = plt . cm . get_cmap ( 'cubehelix' , 6 )) plt . colorbar ( ticks = range ( 6 ), label = 'digit value' ) plt . clim ( - 0.5 , 5.5 ) Figure 4-58. Manifold embedding of handwritten digit pixels The projection also gives us some interesting insights on the relationships within the dataset: for example, the ranges of 5 and 3 nearly overlap in this projection, indicating that some handwritten fives and threes are difficult to distinguish, and therefore more likely to be confused by an automated classification algorithm",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_250"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Other values, like 0 and 1, are more distantly separated, and therefore much less likely to be con‐ fused. This observation agrees with our intuition, because 5 and 3 look much more similar than do 0 and 1. We’ll return to manifold learning and digit classification in Chapter 5 . Multiple Subplots Sometimes it is helpful to compare different views of data side by side. To this end, Matplotlib has the concept of subplots : groups of smaller axes that can exist together within a single figure. These subplots might be insets, grids of plots, or other more complicated layouts. In this section, we’ll explore four routines for creating subplots in Matplotlib. We’ll start by setting up the notebook for plotting and importing the functions we will use: 262 | Chapter 4: Visualization with Matplotlib In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-white' ) import numpy as np plt.axes: Subplots by Hand The most basic method of creating an axes is to use the plt.axes function. As we’ve seen previously, by default this creates a standard axes object that fills the entire fig‐ ure. plt.axes also takes an optional argument that is a list of four numbers in the figure coordinate system. These numbers represent [ bottom , left , width , height ] in the figure coordinate system, which ranges from 0 at the bottom left of the figure to 1 at the top right of the figure. For example, we might create an inset axes at the top-right corner of another axes by setting the x and y position to 0.65 (that is, starting at 65% of the width and 65% of the height of the figure) and the x and y extents to 0.2 (that is, the size of the axes is 20% of the width and 20% of the height of the figure). Figure 4-59 shows the result of this code: In [ 2 ]: ax1 = plt . axes () # standard axes ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) Figure 4-59. Example of an inset axes The equivalent of this command within the object-oriented interface is fig.add_axes()",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_251"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". axes () # standard axes ax2 = plt . axes ([ 0.65 , 0.65 , 0.2 , 0.2 ]) Figure 4-59. Example of an inset axes The equivalent of this command within the object-oriented interface is fig.add_axes() . Let’s use this to create two vertically stacked axes ( Figure 4-60 ): In [ 3 ]: fig = plt . figure () ax1 = fig . add_axes ([ 0.1 , 0.5 , 0.8 , 0.4 ], xticklabels = [], ylim = ( - 1.2 , 1.2 )) ax2 = fig . add_axes ([ 0.1 , 0.1 , 0.8 , 0.4 ], ylim = ( - 1.2 , 1.2 )) x = np . linspace ( 0 , 10 ) ax1 . plot ( np . sin ( x )) ax2 . plot ( np . cos ( x )); Multiple Subplots | 263 Figure 4-60. Vertically stacked axes example We now have two axes (the top with no tick labels) that are just touching: the bottom of the upper panel (at position 0.5) matches the top of the lower panel (at position 0.1 + 0.4). plt.subplot: Simple Grids of Subplots Aligned columns or rows of subplots are a common enough need that Matplotlib has several convenience routines that make them easy to create. The lowest level of these is plt.subplot() , which creates a single subplot within a grid. As you can see, this command takes three integer arguments—the number of rows, the number of col‐ umns, and the index of the plot to be created in this scheme, which runs from the upper left to the bottom right ( Figure 4-61 ): In [ 4 ]: for i in range ( 1 , 7 ): plt . subplot ( 2 , 3 , i ) plt . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) Figure 4-61. A plt.subplot() example 264 | Chapter 4: Visualization with Matplotlib The command plt.subplots_adjust can be used to adjust the spacing between these plots. The following code (the result of which is shown in Figure 4-62 ) uses the equivalent object-oriented command, fig.add_subplot() : In [ 5 ]: fig = plt . figure () fig . subplots_adjust ( hspace = 0.4 , wspace = 0.4 ) for i in range ( 1 , 7 ): ax = fig . add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) Figure 4-62",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_252"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". add_subplot ( 2 , 3 , i ) ax . text ( 0.5 , 0.5 , str (( 2 , 3 , i )), fontsize = 18 , ha = 'center' ) Figure 4-62. plt.subplot() with adjusted margins We’ve used the hspace and wspace arguments of plt.subplots_adjust , which spec‐ ify the spacing along the height and width of the figure, in units of the subplot size (in this case, the space is 40% of the subplot width and height). plt.subplots: The Whole Grid in One Go The approach just described can become quite tedious when you’re creating a large grid of subplots, especially if you’d like to hide the and y-axis labels on the inner plots. For this purpose, plt.subplots() is the easier tool to use (note the s at the end of subplots ). Rather than creating a single subplot, this function creates a full grid of subplots in a single line, returning them in a NumPy array. The arguments are the number of rows and number of columns, along with optional keywords sharex and sharey , which allow you to specify the relationships between different axes. Here we’ll create a 2×3 grid of subplots, where all axes in the same row share their y-axis scale, and all axes in the same column share their x-axis scale ( Figure 4-63 ): In [ 6 ]: fig , ax = plt . subplots ( 2 , 3 , sharex = 'col' , sharey = 'row' ) Multiple Subplots | 265 Figure 4-63. Shared x and y axis in plt.subplots() Note that by specifying sharex and sharey , we’ve automatically removed inner labels on the grid to make the plot cleaner. The resulting grid of axes instances is returned within a NumPy array, allowing for convenient specification of the desired axes using standard array indexing notation ( Figure 4-64 ): In [ 7 ]: # axes are in a two-dimensional array, indexed by [row, col] for i in range ( 2 ): for j in range ( 3 ): ax [ i , j ] . text ( 0.5 , 0.5 , str (( i , j )), fontsize = 18 , ha = 'center' ) fig Figure 4-64. Identifying plots in a subplot grid In comparison to plt.subplot() , plt.subplots() is more consistent with Python’s conventional 0-based indexing",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_253"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Identifying plots in a subplot grid In comparison to plt.subplot() , plt.subplots() is more consistent with Python’s conventional 0-based indexing. plt.GridSpec: More Complicated Arrangements To go beyond a regular grid to subplots that span multiple rows and columns, plt.GridSpec() is the best tool. The plt.GridSpec() object does not create a plot by 266 | Chapter 4: Visualization with Matplotlib itself; it is simply a convenient interface that is recognized by the plt.subplot() command. For example, a gridspec for a grid of two rows and three columns with some specified width and height space looks like this: In [ 8 ]: grid = plt . GridSpec ( 2 , 3 , wspace = 0.4 , hspace = 0.3 ) From this we can specify subplot locations and extents using the familiar Python slic‐ ing syntax ( Figure 4-65 ): In [ 9 ]: plt . subplot ( grid [ 0 , 0 ]) plt . subplot ( grid [ 0 , 1 :]) plt . subplot ( grid [ 1 , : 2 ]) plt . subplot ( grid [ 1 , 2 ]); Figure 4-65. Irregular subplots with plt.GridSpec This type of flexible grid alignment has a wide range of uses. I most often use it when creating multi-axes histogram plots like the one shown here ( Figure 4-66 ): In [ 10 ]: # Create some normally distributed data mean = [ 0 , 0 ] cov = [[ 1 , 1 ], [ 1 , 2 ]] x , y = np . random . multivariate_normal ( mean , cov , 3000 ) . T # Set up the axes with gridspec fig = plt . figure ( figsize = ( 6 , 6 )) grid = plt . GridSpec ( 4 , 4 , hspace = 0.2 , wspace = 0.2 ) main_ax = fig . add_subplot ( grid [: - 1 , 1 :]) y_hist = fig . add_subplot ( grid [: - 1 , 0 ], xticklabels = [], sharey = main_ax ) x_hist = fig . add_subplot ( grid [ - 1 , 1 :], yticklabels = [], sharex = main_ax ) # scatter points on the main axes main_ax . plot ( x , y , 'ok' , markersize = 3 , alpha = 0.2 ) # histogram on the attached axes x_hist . hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () Multiple Subplots | 267 y_hist",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_254"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". hist ( x , 40 , histtype = 'stepfilled' , orientation = 'vertical' , color = 'gray' ) x_hist . invert_yaxis () Multiple Subplots | 267 y_hist . hist ( y , 40 , histtype = 'stepfilled' , orientation = 'horizontal' , color = 'gray' ) y_hist . invert_xaxis () Figure 4-66. Visualizing multidimensional distributions with plt.GridSpec This type of distribution plotted alongside its margins is common enough that it has its own plotting API in the Seaborn package; see “Visualization with Seaborn” on page 311 for more details. Text and Annotation Creating a good visualization involves guiding the reader so that the figure tells a story. In some cases, this story can be told in an entirely visual manner, without the need for added text, but in others, small textual cues and labels are necessary. Perhaps the most basic types of annotations you will use are axes labels and titles, but the options go beyond this. Let’s take a look at some data and how we might visualize and annotate it to help convey interesting information. We’ll start by setting up the note‐ book for plotting and importing the functions we will use: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import matplotlib as mpl plt . style . use ( 'seaborn-whitegrid' ) import numpy as np import pandas as pd 268 | Chapter 4: Visualization with Matplotlib Example: Effect of Holidays on US Births Let’s return to some data we worked with earlier in “Example: Birthrate Data” on page 174 , where we generated a plot of average births over the course of the calendar year; as already mentioned, this data can be downloaded at https://raw.githubusercon tent.com/jakevdp/data-CDCbirths/master/births.csv . We’ll start with the same cleaning procedure we used there, and plot the results ( Figure 4-67 ): In [ 2 ]: births = pd . read_csv ( 'births.csv' ) quartiles = np . percentile ( births [ 'births' ], [ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_255"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". read_csv ( 'births.csv' ) quartiles = np . percentile ( births [ 'births' ], [ 25 , 50 , 75 ]) mu , sig = quartiles [ 1 ], 0.74 * ( quartiles [ 2 ] - quartiles [ 0 ]) births = births . query ( '(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)' ) births [ 'day' ] = births [ 'day' ] . astype ( int ) births . index = pd . to_datetime ( 10000 * births . year + 100 * births . month + births . day , format = ' %Y%m%d ' ) births_by_date = births . pivot_table ( 'births' , [ births . index . month , births . index . day ]) births_by_date . index = [ pd . datetime ( 2012 , month , day ) for ( month , day ) in births_by_date . index ] In [ 3 ]: fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ); Figure 4-67. Average daily births by date When we’re communicating data like this, it is often useful to annotate certain fea‐ tures of the plot to draw the reader’s attention. This can be done manually with the plt.text / ax.text command, which will place text at a particular x / y value ( Figure 4-68 ): Text and Annotation | 269 In [ 4 ]: fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot style = dict ( size = 10 , color = 'gray' ) ax . text ( '2012-1-1' , 3950 , \"New Year's Day\" , ** style ) ax . text ( '2012-7-4' , 4250 , \"Independence Day\" , ha = 'center' , ** style ) ax . text ( '2012-9-4' , 4850 , \"Labor Day\" , ha = 'center' , ** style ) ax . text ( '2012-10-31' , 4600 , \"Halloween\" , ha = 'right' , ** style ) ax . text ( '2012-11-25' , 4450 , \"Thanksgiving\" , ha = 'center' , ** style ) ax . text ( '2012-12-25' , 3850 , \"Christmas \" , ha = 'right' , ** style ) # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_256"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); Figure 4-68. Annotated average daily births by date The ax.text method takes an x position, a y position, a string, and then optional key‐ words specifying the color, size, style, alignment, and other properties of the text. Here we used ha='right' and ha='center' , where ha is short for horizonal align‐ ment . See the docstring of plt.text() and of mpl.text.Text() for more information on available options. Transforms and Text Position In the previous example, we anchored our text annotations to data locations. Some‐ times it’s preferable to anchor the text to a position on the axes or figure, independent of the data. In Matplotlib, we do this by modifying the transform . 270 | Chapter 4: Visualization with Matplotlib Any graphics display framework needs some scheme for translating between coordi‐ nate systems. For example, a data point at x , y = 1, 1 needs to somehow be repre‐ sented at a certain location on the figure, which in turn needs to be represented in pixels on the screen. Mathematically, such coordinate transformations are relatively straightforward, and Matplotlib has a well-developed set of tools that it uses inter‐ nally to perform them (the tools can be explored in the matplotlib.transforms sub‐ module). The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_257"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The average user rarely needs to worry about the details of these transforms, but it is helpful knowledge to have when considering the placement of text on a figure. There are three predefined transforms that can be useful in this situation: ax.transData Transform associated with data coordinates ax.transAxes Transform associated with the axes (in units of axes dimensions) fig.transFigure Transform associated with the figure (in units of figure dimensions) Here let’s look at an example of drawing text at various locations using these trans‐ forms ( Figure 4-69 ): In [ 5 ]: fig , ax = plt . subplots ( facecolor = 'lightgray' ) ax . axis ([ 0 , 10 , 0 , 10 ]) # transform=ax.transData is the default, but we'll specify it anyway ax . text ( 1 , 5 , \". Data: (1, 5)\" , transform = ax . transData ) ax . text ( 0.5 , 0.1 , \". Axes: (0.5, 0.1)\" , transform = ax . transAxes ) ax . text ( 0.2 , 0.2 , \". Figure: (0.2, 0.2)\" , transform = fig . transFigure ); Figure 4-69. Comparing Matplotlib’s coordinate systems Text and Annotation | 271 Note that by default, the text is aligned above and to the left of the specified coordi‐ nates; here the “.” at the beginning of each string will approximately mark the given coordinate location. The transData coordinates give the usual data coordinates associated with the and y-axis labels. The transAxes coordinates give the location from the bottom-left cor‐ ner of the axes (here the white box) as a fraction of the axes size. The transFigure coordinates are similar, but specify the position from the bottom left of the figure (here the gray box) as a fraction of the figure size. Notice now that if we change the axes limits, it is only the transData coordinates that will be affected, while the others remain stationary ( Figure 4-70 ): In [ 6 ]: ax . set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig Figure 4-70",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_258"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". set_xlim ( 0 , 2 ) ax . set_ylim ( - 6 , 6 ) fig Figure 4-70. Comparing Matplotlib’s coordinate systems You can see this behavior more clearly by changing the axes limits interactively; if you are executing this code in a notebook, you can make that happen by changing %mat plotlib inline to %matplotlib notebook and using each plot’s menu to interact with the plot. Arrows and Annotation Along with tick marks and text, another useful annotation mark is the simple arrow. Drawing arrows in Matplotlib is often much harder than you might hope. While there is a plt.arrow() function available, I wouldn’t suggest using it; the arrows it creates are SVG objects that will be subject to the varying aspect ratio of your plots, and the result is rarely what the user intended. Instead, I’d suggest using the plt.anno tate() function. This function creates some text and an arrow, and the arrows can be very flexibly specified. 272 | Chapter 4: Visualization with Matplotlib Here we’ll use annotate with several of its options ( Figure 4-71 ): In [ 7 ]: % matplotlib inline fig , ax = plt . subplots () x = np . linspace ( 0 , 20 , 1000 ) ax . plot ( x , np . cos ( x )) ax . axis ( 'equal' ) ax . annotate ( 'local maximum' , xy = ( 6.28 , 1 ), xytext = ( 10 , 4 ), arrowprops = dict ( facecolor = 'black' , shrink = 0.05 )) ax . annotate ( 'local minimum' , xy = ( 5 * np . pi , - 1 ), xytext = ( 2 , - 6 ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )); Figure 4-71. Annotation examples The arrow style is controlled through the arrowprops dictionary, which has numer‐ ous options available. These options are fairly well documented in Matplotlib’s online documentation, so rather than repeating them here I’ll quickly show some of the pos‐ sibilities. Let’s demonstrate several of the possible options using the birthrate plot from before ( Figure 4-72 ): In [ 8 ]: fig , ax = plt . subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_259"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( figsize = ( 12 , 4 )) births_by_date . plot ( ax = ax ) # Add labels to the plot ax . annotate ( \"New Year's Day\" , xy = ( '2012-1-1' , 4100 ), xycoords = 'data' , xytext = ( 50 , - 30 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"arc3,rad=-0.2\" )) ax . annotate ( \"Independence Day\" , xy = ( '2012-7-4' , 4250 ), xycoords = 'data' , bbox = dict ( boxstyle = \"round\" , fc = \"none\" , ec = \"gray\" ), Text and Annotation | 273 xytext = ( 10 , - 40 ), textcoords = 'offset points' , ha = 'center' , arrowprops = dict ( arrowstyle = \"->\" )) ax . annotate ( 'Labor Day' , xy = ( '2012-9-4' , 4850 ), xycoords = 'data' , ha = 'center' , xytext = ( 0 , - 20 ), textcoords = 'offset points' ) ax . annotate ( '' , xy = ( '2012-9-1' , 4850 ), xytext = ( '2012-9-7' , 4850 ), xycoords = 'data' , textcoords = 'data' , arrowprops = { 'arrowstyle' : '|-|,widthA=0.2,widthB=0.2' , }) ax . annotate ( 'Halloween' , xy = ( '2012-10-31' , 4600 ), xycoords = 'data' , xytext = ( - 80 , - 40 ), textcoords = 'offset points' , arrowprops = dict ( arrowstyle = \"fancy\" , fc = \"0.6\" , ec = \"none\" , connectionstyle = \"angle3,angleA=0,angleB=-90\" )) ax . annotate ( 'Thanksgiving' , xy = ( '2012-11-25' , 4500 ), xycoords = 'data' , xytext = ( - 120 , - 60 ), textcoords = 'offset points' , bbox = dict ( boxstyle = \"round4,pad=.5\" , fc = \"0.9\" ), arrowprops = dict ( arrowstyle = \"->\" , connectionstyle = \"angle,angleA=0,angleB=80,rad=20\" )) ax . annotate ( 'Christmas' , xy = ( '2012-12-25' , 3850 ), xycoords = 'data' , xytext = ( - 30 , 0 ), textcoords = 'offset points' , size = 13 , ha = 'right' , va = \"center\" , bbox = dict ( boxstyle = \"round\" , alpha = 0.1 ), arrowprops = dict ( arrowstyle = \"wedge,tail_width=0.5\" , alpha = 0.1 )); # Label the axes ax . set ( title = 'USA births by day of year (1969-1988)' , ylabel = 'average daily births' ) # Format the x axis with centered month labels ax . xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_260"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". xaxis . set_major_locator ( mpl . dates . MonthLocator ()) ax . xaxis . set_minor_locator ( mpl . dates . MonthLocator ( bymonthday = 15 )) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) ax . xaxis . set_minor_formatter ( mpl . dates . DateFormatter ( '%h' )); ax . set_ylim ( 3600 , 5400 ); 274 | Chapter 4: Visualization with Matplotlib Figure 4-72. Annotated average birth rates by day You’ll notice that the specifications of the arrows and text boxes are very detailed: this gives you the power to create nearly any arrow style you wish. Unfortunately, it also means that these sorts of features often must be manually tweaked, a process that can be very time-consuming when one is producing publication-quality graphics! Finally, I’ll note that the preceding mix of styles is by no means best practice for presenting data, but rather included as a demonstration of some of the available options. More discussion and examples of available arrow and annotation styles can be found in the Matplotlib gallery, in particular http://matplotlib.org/examples/pylab_examples/ annotation_demo2.html . Customizing Ticks Matplotlib’s default tick locators and formatters are designed to be generally sufficient in many common situations, but are in no way optimal for every plot. This section will give several examples of adjusting the tick locations and formatting for the par‐ ticular plot type you’re interested in. Before we go into examples, it will be best for us to understand further the object hierarchy of Matplotlib plots. Matplotlib aims to have a Python object representing everything that appears on the plot: for example, recall that the figure is the bound‐ ing box within which plot elements appear. Each Matplotlib object can also act as a container of sub-objects; for example, each figure can contain one or more axes objects, each of which in turn contain other objects representing plot contents. The tick marks are no exception",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_261"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The tick marks are no exception. Each axes has attributes xaxis and yaxis , which in turn have attributes that contain all the properties of the lines, ticks, and labels that make up the axes. Customizing Ticks | 275 Major and Minor Ticks Within each axis, there is the concept of a major tick mark and a minor tick mark. As the names would imply, major ticks are usually bigger or more pronounced, while minor ticks are usually smaller. By default, Matplotlib rarely makes use of minor ticks, but one place you can see them is within logarithmic plots ( Figure 4-73 ): In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt plt . style . use ( 'seaborn-whitegrid' ) import numpy as np In [ 2 ]: ax = plt . axes ( xscale = 'log' , yscale = 'log' ) Figure 4-73. Example of logarithmic scales and labels We see here that each major tick shows a large tick mark and a label, while each minor tick shows a smaller tick mark with no label. We can customize these tick properties—that is, locations and labels—by setting the formatter and locator objects of each axis. Let’s examine these for the x axis of the plot just shown: In [ 3 ]: print ( ax . xaxis . get_major_locator ()) print ( ax . xaxis . get_minor_locator ()) <matplotlib.ticker.LogLocator object at 0x107530cc0> <matplotlib.ticker.LogLocator object at 0x107530198> In [ 4 ]: print ( ax . xaxis . get_major_formatter ()) print ( ax . xaxis . get_minor_formatter ()) <matplotlib.ticker.LogFormatterMathtext object at 0x107512780> <matplotlib.ticker.NullFormatter object at 0x10752dc18> We see that both major and minor tick labels have their locations specified by a LogLocator (which makes sense for a logarithmic plot). Minor ticks, though, have their labels formatted by a NullFormatter ; this says that no labels will be shown. 276 | Chapter 4: Visualization with Matplotlib We’ll now show a few examples of setting these locators and formatters for various plots",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_262"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 276 | Chapter 4: Visualization with Matplotlib We’ll now show a few examples of setting these locators and formatters for various plots. Hiding Ticks or Labels Perhaps the most common tick/label formatting operation is the act of hiding ticks or labels. We can do this using plt.NullLocator() and plt.NullFormatter() , as shown here ( Figure 4-74 ): In [ 5 ]: ax = plt . axes () ax . plot ( np . random . rand ( 50 )) ax . yaxis . set_major_locator ( plt . NullLocator ()) ax . xaxis . set_major_formatter ( plt . NullFormatter ()) Figure 4-74. Plot with hidden tick labels (x-axis) and hidden ticks (y-axis) Notice that we’ve removed the labels (but kept the ticks/gridlines) from the x axis, and removed the ticks (and thus the labels as well) from the y axis. Having no ticks at all can be useful in many situations—for example, when you want to show a grid of images. For instance, consider Figure 4-75 , which includes images of different faces, an example often used in supervised machine learning problems (for more informa‐ tion, see “In-Depth: Support Vector Machines” on page 405 ): In [ 6 ]: fig , ax = plt . subplots ( 5 , 5 , figsize = ( 5 , 5 )) fig . subplots_adjust ( hspace = 0 , wspace = 0 ) # Get some face data from scikit-learn from sklearn.datasets import fetch_olivetti_faces faces = fetch_olivetti_faces () . images for i in range ( 5 ): for j in range ( 5 ): ax [ i , j ] . xaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . yaxis . set_major_locator ( plt . NullLocator ()) ax [ i , j ] . imshow ( faces [ 10 * i + j ], cmap = \"bone\" ) Customizing Ticks | 277 Figure 4-75. Hiding ticks within image plots Notice that each image has its own axes, and we’ve set the locators to null because the tick values (pixel number in this case) do not convey relevant information for this particular visualization. Reducing or Increasing the Number of Ticks One common problem with the default settings is that smaller subplots can end up with crowded labels",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_263"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Reducing or Increasing the Number of Ticks One common problem with the default settings is that smaller subplots can end up with crowded labels. We can see this in the plot grid shown in Figure 4-76 : In [ 7 ]: fig , ax = plt . subplots ( 4 , 4 , sharex = True , sharey = True ) Figure 4-76. A default plot with crowded ticks Particularly for the x ticks, the numbers nearly overlap, making them quite difficult to decipher. We can fix this with the plt.MaxNLocator() , which allows us to specify the maximum number of ticks that will be displayed. Given this maximum number, Mat‐ plotlib will use internal logic to choose the particular tick locations ( Figure 4-77 ): 278 | Chapter 4: Visualization with Matplotlib In [ 8 ]: # For every axis, set the x and y major locator for axi in ax . flat : axi . xaxis . set_major_locator ( plt . MaxNLocator ( 3 )) axi . yaxis . set_major_locator ( plt . MaxNLocator ( 3 )) fig Figure 4-77. Customizing the number of ticks This makes things much cleaner. If you want even more control over the locations of regularly spaced ticks, you might also use plt.MultipleLocator , which we’ll discuss in the following section. Fancy Tick Formats Matplotlib’s default tick formatting can leave a lot to be desired; it works well as a broad default, but sometimes you’d like to do something more. Consider the plot shown in Figure 4-78 , a sine and a cosine: In [ 9 ]: # Plot a sine and cosine curve fig , ax = plt . subplots () x = np . linspace ( 0 , 3 * np . pi , 1000 ) ax . plot ( x , np . sin ( x ), lw = 3 , label = 'Sine' ) ax . plot ( x , np . cos ( x ), lw = 3 , label = 'Cosine' ) # Set up grid, legend, and limits ax . grid ( True ) ax . legend ( frameon = False ) ax . axis ( 'equal' ) ax . set_xlim ( 0 , 3 * np . pi ); Customizing Ticks | 279 Figure 4-78. A default plot with integer ticks There are a couple changes we might like to make. First, it’s more natural for this data to space the ticks and grid lines in multiples of π",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_264"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A default plot with integer ticks There are a couple changes we might like to make. First, it’s more natural for this data to space the ticks and grid lines in multiples of π . We can do this by setting a Multi pleLocator , which locates ticks at a multiple of the number you provide. For good measure, we’ll add both major and minor ticks in multiples of π /4 ( Figure 4-79 ): In [ 10 ]: ax . xaxis . set_major_locator ( plt . MultipleLocator ( np . pi / 2 )) ax . xaxis . set_minor_locator ( plt . MultipleLocator ( np . pi / 4 )) fig Figure 4-79. Ticks at multiples of pi/2 But now these tick labels look a little bit silly: we can see that they are multiples of π , but the decimal representation does not immediately convey this. To fix this, we can change the tick formatter. There’s no built-in formatter for what we want to do, so we’ll instead use plt.FuncFormatter , which accepts a user-defined function giving fine-grained control over the tick outputs ( Figure 4-80 ): In [ 11 ]: def format_func ( value , tick_number ): # find number of multiples of pi/2 N = int ( np . round ( 2 * value / np . pi )) if N == 0 : return \"0\" 280 | Chapter 4: Visualization with Matplotlib elif N == 1 : return r \"$\\pi/2$\" elif N == 2 : return r \"$\\pi$\" elif N % 2 > 0 : return r \"${0}\\pi/2$\" . format ( N ) else : return r \"${0}\\pi$\" . format ( N // 2 ) ax . xaxis . set_major_formatter ( plt . FuncFormatter ( format_func )) fig Figure 4-80. Ticks with custom labels This is much better! Notice that we’ve made use of Matplotlib’s LaTeX support, speci‐ fied by enclosing the string within dollar signs. This is very convenient for display of mathematical symbols and formulae; in this case, \"$\\pi$\" is rendered as the Greek character π . The plt.FuncFormatter() offers extremely fine-grained control over the appearance of your plot ticks, and comes in very handy when you’re preparing plots for presenta‐ tion or publication. Summary of Formatters and Locators We’ve mentioned a couple of the available formatters and locators",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_265"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Summary of Formatters and Locators We’ve mentioned a couple of the available formatters and locators. We’ll conclude this section by briefly listing all the built-in locator and formatter options. For more information on any of these, refer to the docstrings or to the Matplotlib online docu‐ mentation. Each of the following is available in the plt namespace: Locator class Description NullLocator No ticks FixedLocator Tick locations are fixed IndexLocator Locator for index plots (e.g., where x = range(len(y))) Customizing Ticks | 281 Locator class Description LinearLocator Evenly spaced ticks from min to max LogLocator Logarithmically ticks from min to max MultipleLocator Ticks and range are a multiple of base MaxNLocator Finds up to a max number of ticks at nice locations AutoLocator (Default) MaxNLocator with simple defaults AutoMinorLocator Locator for minor ticks Formatter class Description NullFormatter No labels on the ticks IndexFormatter Set the strings from a list of labels FixedFormatter Set the strings manually for the labels FuncFormatter User-defined function sets the labels FormatStrFormatter Use a format string for each value ScalarFormatter (Default) Formatter for scalar values LogFormatter Default formatter for log axes We’ll see additional examples of these throughout the remainder of the book. Customizing Matplotlib: Configurations and Stylesheets Matplotlib’s default plot settings are often the subject of complaint among its users. While much is slated to change in the 2.0 Matplotlib release, the ability to customize default settings helps bring the package in line with your own aesthetic preferences. Here we’ll walk through some of Matplotlib’s runtime configuration ( rc ) options, and take a look at the newer stylesheets feature, which contains some nice sets of default configurations. Plot Customization by Hand Throughout this chapter, we’ve seen how it is possible to tweak individual plot set‐ tings to end up with something that looks a little bit nicer than the default",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_266"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Plot Customization by Hand Throughout this chapter, we’ve seen how it is possible to tweak individual plot set‐ tings to end up with something that looks a little bit nicer than the default. It’s possi‐ ble to do these customizations for each individual plot. For example, here is a fairly drab default histogram ( Figure 4-81 ): In [ 1 ]: import matplotlib.pyplot as plt plt . style . use ( 'classic' ) import numpy as np % matplotlib inline 282 | Chapter 4: Visualization with Matplotlib In [ 2 ]: x = np . random . randn ( 1000 ) plt . hist ( x ); Figure 4-81. A histogram in Matplotlib’s default style We can adjust this by hand to make it a much more visually pleasing plot, shown in Figure 4-82 : In [ 3 ]: # use a gray background ax = plt . axes ( axisbg = '#E6E6E6' ) ax . set_axisbelow ( True ) # draw solid white grid lines plt . grid ( color = 'w' , linestyle = 'solid' ) # hide axis spines for spine in ax . spines . values (): spine . set_visible ( False ) # hide top and right ticks ax . xaxis . tick_bottom () ax . yaxis . tick_left () # lighten ticks and labels ax . tick_params ( colors = 'gray' , direction = 'out' ) for tick in ax . get_xticklabels (): tick . set_color ( 'gray' ) for tick in ax . get_yticklabels (): tick . set_color ( 'gray' ) # control face and edge color of histogram ax . hist ( x , edgecolor = '#E6E6E6' , color = '#EE6666' ); Customizing Matplotlib: Configurations and Stylesheets | 283 Figure 4-82. A histogram with manual customizations This looks better, and you may recognize the look as inspired by the look of the R language’s ggplot visualization package. But this took a whole lot of effort! We defi‐ nitely do not want to have to do all that tweaking each time we create a plot. Fortu‐ nately, there is a way to adjust these defaults once in a way that will work for all plots. Changing the Defaults: rcParams Each time Matplotlib loads, it defines a runtime configuration ( rc ) containing the default styles for every plot element you create",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_267"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Changing the Defaults: rcParams Each time Matplotlib loads, it defines a runtime configuration ( rc ) containing the default styles for every plot element you create. You can adjust this configuration at any time using the plt.rc convenience routine. Let’s see what it looks like to modify the rc parameters so that our default plot will look similar to what we did before. We’ll start by saving a copy of the current rcParams dictionary, so we can easily reset these changes in the current session: In [ 4 ]: IPython_default = plt . rcParams . copy () Now we can use the plt.rc function to change some of these settings: In [ 5 ]: from matplotlib import cycler colors = cycler ( 'color' , [ '#EE6666' , '#3388BB' , '#9988DD' , '#EECC55' , '#88BB44' , '#FFBBBB' ]) plt . rc ( 'axes' , facecolor = '#E6E6E6' , edgecolor = 'none' , axisbelow = True , grid = True , prop_cycle = colors ) plt . rc ( 'grid' , color = 'w' , linestyle = 'solid' ) plt . rc ( 'xtick' , direction = 'out' , color = 'gray' ) plt . rc ( 'ytick' , direction = 'out' , color = 'gray' ) plt . rc ( 'patch' , edgecolor = '#E6E6E6' ) plt . rc ( 'lines' , linewidth = 2 ) With these settings defined, we can now create a plot and see our settings in action ( Figure 4-83 ): In [ 6 ]: plt . hist ( x ); 284 | Chapter 4: Visualization with Matplotlib Figure 4-83. A customized histogram using rc settings Let’s see what simple line plots look like with these rc parameters ( Figure 4-84 ): In [ 7 ]: for i in range ( 4 ): plt . plot ( np . random . rand ( 10 )) Figure 4-84. A line plot with customized styles I find this much more aesthetically pleasing than the default styling. If you disagree with my aesthetic sense, the good news is that you can adjust the rc parameters to suit your own tastes! These settings can be saved in a .matplotlibrc file, which you can read about in the Matplotlib documentation . That said, I prefer to customize Mat‐ plotlib using its stylesheets instead",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_268"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That said, I prefer to customize Mat‐ plotlib using its stylesheets instead. Stylesheets The version 1.4 release of Matplotlib in August 2014 added a very convenient style module, which includes a number of new default stylesheets, as well as the ability to create and package your own styles. These stylesheets are formatted similarly to the .matplotlibrc files mentioned earlier, but must be named with a .mplstyle extension. Customizing Matplotlib: Configurations and Stylesheets | 285 Even if you don’t create your own style, the stylesheets included by default are extremely useful. The available styles are listed in plt.style.available —here I’ll list only the first five for brevity: In [ 8 ]: plt . style . available [: 5 ] Out[8]: ['fivethirtyeight', 'seaborn-pastel', 'seaborn-whitegrid', 'ggplot', 'grayscale'] The basic way to switch to a stylesheet is to call: plt . style . use ( 'stylename' ) But keep in mind that this will change the style for the rest of the session! Alterna‐ tively, you can use the style context manager, which sets a style temporarily: with plt . style . context ( 'stylename' ): make_a_plot () Let’s create a function that will make two basic types of plot: In [ 9 ]: def hist_and_lines (): np . random . seed ( 0 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 11 , 4 )) ax [ 0 ] . hist ( np . random . randn ( 1000 )) for i in range ( 3 ): ax [ 1 ] . plot ( np . random . rand ( 10 )) ax [ 1 ] . legend ([ 'a' , 'b' , 'c' ], loc = 'lower left' ) We’ll use this to explore how these plots look using the various built-in styles. Default style The default style is what we’ve been seeing so far throughout the book; we’ll start with that. First, let’s reset our runtime configuration to the notebook default: In [ 10 ]: # reset rcParams plt . rcParams . update ( IPython_default ); Now let’s see how it looks ( Figure 4-85 ): In [ 11 ]: hist_and_lines () 286 | Chapter 4: Visualization with Matplotlib Figure 4-85",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_269"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". rcParams . update ( IPython_default ); Now let’s see how it looks ( Figure 4-85 ): In [ 11 ]: hist_and_lines () 286 | Chapter 4: Visualization with Matplotlib Figure 4-85. Matplotlib’s default style FiveThirtyEight style The FiveThirtyEight style mimics the graphics found on the popular FiveThirtyEight website . As you can see in Figure 4-86 , it is typified by bold colors, thick lines, and transparent axes. In [ 12 ]: with plt . style . context ( 'fivethirtyeight' ): hist_and_lines () Figure 4-86. The FiveThirtyEight style ggplot The ggplot package in the R language is a very popular visualization tool. Matplot‐ lib’s ggplot style mimics the default styles from that package ( Figure 4-87 ): In [ 13 ]: with plt . style . context ( 'ggplot' ): hist_and_lines () Customizing Matplotlib: Configurations and Stylesheets | 287 Figure 4-87. The ggplot style Bayesian Methods for Hackers style There is a very nice short online book called Probabilistic Programming and Bayesian Methods for Hackers ; it features figures created with Matplotlib, and uses a nice set of rc parameters to create a consistent and visually appealing style throughout the book. This style is reproduced in the bmh stylesheet ( Figure 4-88 ): In [ 14 ]: with plt . style . context ( 'bmh' ): hist_and_lines () Figure 4-88. The bmh style Dark background For figures used within presentations, it is often useful to have a dark rather than light background. The dark_background style provides this ( Figure 4-89 ): In [ 15 ]: with plt . style . context ( 'dark_background' ): hist_and_lines () 288 | Chapter 4: Visualization with Matplotlib Figure 4-89. The dark_background style Grayscale Sometimes you might find yourself preparing figures for a print publication that does not accept color figures. For this, the grayscale style, shown in Figure 4-90 , can be very useful: In [ 16 ]: with plt . style . context ( 'grayscale' ): hist_and_lines () Figure 4-90",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_270"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For this, the grayscale style, shown in Figure 4-90 , can be very useful: In [ 16 ]: with plt . style . context ( 'grayscale' ): hist_and_lines () Figure 4-90. The grayscale style Seaborn style Matplotlib also has stylesheets inspired by the Seaborn library (discussed more fully in “Visualization with Seaborn” on page 311 ). As we will see, these styles are loaded automatically when Seaborn is imported into a notebook. I’ve found these settings to be very nice, and tend to use them as defaults in my own data exploration (see Figure 4-91 ): In [ 17 ]: import seaborn hist_and_lines () Customizing Matplotlib: Configurations and Stylesheets | 289 Figure 4-91. Seaborn’s plotting style With all of these built-in options for various plot styles, Matplotlib becomes much more useful for both interactive visualization and creation of figures for publication. Throughout this book, I will generally use one or more of these style conventions when creating plots. Three-Dimensional Plotting in Matplotlib Matplotlib was initially designed with only two-dimensional plotting in mind. Around the time of the 1.0 release, some three-dimensional plotting utilities were built on top of Matplotlib’s two-dimensional display, and the result is a convenient (if somewhat limited) set of tools for three-dimensional data visualization. We enable three-dimensional plots by importing the mplot3d toolkit, included with the main Matplotlib installation ( Figure 4-92 ): In [ 1 ]: from mpl_toolkits import mplot3d Once this submodule is imported, we can create a three-dimensional axes by passing the keyword projection='3d' to any of the normal axes creation routines: In [ 2 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt In [ 3 ]: fig = plt . figure () ax = plt . axes ( projection = '3d' ) 290 | Chapter 4: Visualization with Matplotlib Figure 4-92. An empty three-dimensional axes With this 3D axes enabled, we can now plot a variety of three-dimensional plot types",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_271"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". An empty three-dimensional axes With this 3D axes enabled, we can now plot a variety of three-dimensional plot types. Three-dimensional plotting is one of the functionalities that benefits immensely from viewing figures interactively rather than statically in the notebook; recall that to use interactive figures, you can use %matplotlib notebook rather than %matplotlib inline when running this code. Three-Dimensional Points and Lines The most basic three-dimensional plot is a line or scatter plot created from sets of (x, y, z) triples. In analogy with the more common two-dimensional plots discussed ear‐ lier, we can create these using the ax.plot3D and ax.scatter3D functions. The call signature for these is nearly identical to that of their two-dimensional counterparts, so you can refer to “Simple Line Plots” on page 224 and “Simple Scatter Plots” on page 233 for more information on controlling the output. Here we’ll plot a trigono‐ metric spiral, along with some points drawn randomly near the line ( Figure 4-93 ): In [ 4 ]: ax = plt . axes ( projection = '3d' ) # Data for a three-dimensional line zline = np . linspace ( 0 , 15 , 1000 ) xline = np . sin ( zline ) yline = np . cos ( zline ) ax . plot3D ( xline , yline , zline , 'gray' ) # Data for three-dimensional scattered points zdata = 15 * np . random . random ( 100 ) xdata = np . sin ( zdata ) + 0.1 * np . random . randn ( 100 ) ydata = np . cos ( zdata ) + 0.1 * np . random . randn ( 100 ) ax . scatter3D ( xdata , ydata , zdata , c = zdata , cmap = 'Greens' ); Three-Dimensional Plotting in Matplotlib | 291 Figure 4-93. Points and lines in three dimensions Notice that by default, the scatter points have their transparency adjusted to give a sense of depth on the page. While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_272"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While the three-dimensional effect is sometimes difficult to see within a static image, an interactive view can lead to some nice intuition about the layout of the points. Three-Dimensional Contour Plots Analogous to the contour plots we explored in “Density and Contour Plots” on page 241 , mplot3d contains tools to create three-dimensional relief plots using the same inputs. Like two-dimensional ax.contour plots, ax.contour3D requires all the input data to be in the form of two-dimensional regular grids, with the Z data evaluated at each point. Here we’ll show a three-dimensional contour diagram of a thredimensional sinusoidal function ( Figure 4-94 ): In [ 5 ]: def f ( x , y ): return np . sin ( np . sqrt ( x ** 2 + y ** 2 )) x = np . linspace ( - 6 , 6 , 30 ) y = np . linspace ( - 6 , 6 , 30 ) X , Y = np . meshgrid ( x , y ) Z = f ( X , Y ) In [ 6 ]: fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . contour3D ( X , Y , Z , 50 , cmap = 'binary' ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'z' ); 292 | Chapter 4: Visualization with Matplotlib Figure 4-94. A three-dimensional contour plot Sometimes the default viewing angle is not optimal, in which case we can use the view_init method to set the elevation and azimuthal angles. In this example (the result of which is shown in Figure 4-95 ), we’ll use an elevation of 60 degrees (that is, 60 degrees above the x - y plane) and an azimuth of 35 degrees (that is, rotated 35 degrees counter-clockwise about the z -axis): In [ 7 ]: ax . view_init ( 60 , 35 ) fig Figure 4-95. Adjusting the view angle for a three-dimensional plot Again, note that we can accomplish this type of rotation interactively by clicking and dragging when using one of Matplotlib’s interactive backends. Wireframes and Surface Plots Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_273"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Wireframes and Surface Plots Two other types of three-dimensional plots that work on gridded data are wireframes and surface plots. These take a grid of values and project it onto the specified thredimensional surface, and can make the resulting three-dimensional forms quite easy to visualize. Here’s an example using a wireframe ( Figure 4-96 ): Three-Dimensional Plotting in Matplotlib | 293 In [ 8 ]: fig = plt . figure () ax = plt . axes ( projection = '3d' ) ax . plot_wireframe ( X , Y , Z , color = 'black' ) ax . set_title ( 'wireframe' ); Figure 4-96. A wireframe plot A surface plot is like a wireframe plot, but each face of the wireframe is a filled poly‐ gon. Adding a colormap to the filled polygons can aid perception of the topology of the surface being visualized ( Figure 4-97 ): In [ 9 ]: ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ) ax . set_title ( 'surface' ); Figure 4-97. A three-dimensional surface plot Note that though the grid of values for a surface plot needs to be two-dimensional, it need not be rectilinear. Here is an example of creating a partial polar grid, which when used with the surface3D plot can give us a slice into the function we’re visualiz‐ ing ( Figure 4-98 ): 294 | Chapter 4: Visualization with Matplotlib In [ 10 ]: r = np . linspace ( 0 , 6 , 20 ) theta = np . linspace ( - 0.9 * np . pi , 0.8 * np . pi , 40 ) r , theta = np . meshgrid ( r , theta ) X = r * np . sin ( theta ) Y = r * np . cos ( theta ) Z = f ( X , Y ) ax = plt . axes ( projection = '3d' ) ax . plot_surface ( X , Y , Z , rstride = 1 , cstride = 1 , cmap = 'viridis' , edgecolor = 'none' ); Figure 4-98. A polar surface plot Surface Triangulations For some applications, the evenly sampled grids required by the preceding routines are overly restrictive and inconvenient. In these situations, the triangulation-based plots can be very useful",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_274"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In these situations, the triangulation-based plots can be very useful. What if rather than an even draw from a Cartesian or a polar grid, we instead have a set of random draws? In [ 11 ]: theta = 2 * np . pi * np . random . random ( 1000 ) r = 6 * np . random . random ( 1000 ) x = np . ravel ( r * np . sin ( theta )) y = np . ravel ( r * np . cos ( theta )) z = f ( x , y ) We could create a scatter plot of the points to get an idea of the surface we’re sampling from ( Figure 4-99 ): In [ 12 ]: ax = plt . axes ( projection = '3d' ) ax . scatter ( x , y , z , c = z , cmap = 'viridis' , linewidth = 0.5 ); Three-Dimensional Plotting in Matplotlib | 295 Figure 4-99. A three-dimensional sampled surface This leaves a lot to be desired. The function that will help us in this case is ax.plot_trisurf , which creates a surface by first finding a set of triangles formed between adjacent points (the result is shown in Figure 4-100 ; remember that x , y , and z here are one-dimensional arrays): In [ 13 ]: ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , cmap = 'viridis' , edgecolor = 'none' ); Figure 4-100. A triangulated surface plot The result is certainly not as clean as when it is plotted with a grid, but the flexibility of such a triangulation allows for some really interesting three-dimensional plots. For example, it is actually possible to plot a three-dimensional Möbius strip using this, as we’ll see next. Example: Visualizing a Möbius strip A Möbius strip is similar to a strip of paper glued into a loop with a half-twist. Topo‐ logically, it’s quite interesting because despite appearances it has only a single side! Here we will visualize such an object using Matplotlib’s three-dimensional tools. The key to creating the Möbius strip is to think about its parameterization: it’s a tw296 | Chapter 4: Visualization with Matplotlib dimensional strip, so we need two intrinsic dimensions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_275"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The key to creating the Möbius strip is to think about its parameterization: it’s a tw296 | Chapter 4: Visualization with Matplotlib dimensional strip, so we need two intrinsic dimensions. Let’s call them θ , which ranges from 0 to 2 π around the loop, and w which ranges from –1 to 1 across the width of the strip: In [ 14 ]: theta = np . linspace ( 0 , 2 * np . pi , 30 ) w = np . linspace ( - 0.25 , 0.25 , 8 ) w , theta = np . meshgrid ( w , theta ) Now from this parameterization, we must determine the (x, y, z) positions of the embedded strip. Thinking about it, we might realize that there are two rotations happening: one is the position of the loop about its center (what we’ve called θ ), while the other is the twist‐ ing of the strip about its axis (we’ll call this φ ). For a Möbius strip, we must have the strip make half a twist during a full loop, or Δφ = Δθ /2. In [ 15 ]: phi = 0.5 * theta Now we use our recollection of trigonometry to derive the three-dimensional embed‐ ding. We’ll define r , the distance of each point from the center, and use this to find the embedded x , y , z coordinates: In [ 16 ]: # radius in x-y plane r = 1 + w * np . cos ( phi ) x = np . ravel ( r * np . cos ( theta )) y = np . ravel ( r * np . sin ( theta )) z = np . ravel ( w * np . sin ( phi )) Finally, to plot the object, we must make sure the triangulation is correct. The best way to do this is to define the triangulation within the underlying parameterization , and then let Matplotlib project this triangulation into the three-dimensional space of the Möbius strip. This can be accomplished as follows ( Figure 4-101 ): In [ 17 ]: # triangulate in the underlying parameterization from matplotlib.tri import Triangulation tri = Triangulation ( np . ravel ( w ), np . ravel ( theta )) ax = plt . axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidths = 0.2 ); ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_276"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". axes ( projection = '3d' ) ax . plot_trisurf ( x , y , z , triangles = tri . triangles , cmap = 'viridis' , linewidths = 0.2 ); ax . set_xlim ( - 1 , 1 ); ax . set_ylim ( - 1 , 1 ); ax . set_zlim ( - 1 , 1 ); Three-Dimensional Plotting in Matplotlib | 297 Figure 4-101. Visualizing a Möbius strip Combining all of these techniques, it is possible to create and display a wide variety of three-dimensional objects and patterns in Matplotlib. Geographic Data with Basemap One common type of visualization in data science is that of geographic data. Matplot‐ lib’s main tool for this type of visualization is the Basemap toolkit, which is one of several Matplotlib toolkits that live under the mpl_toolkits namespace. Admittedly, Basemap feels a bit clunky to use, and often even simple visualizations take much longer to render than you might hope. More modern solutions, such as leaflet or the Google Maps API, may be a better choice for more intensive map visualizations. Still, Basemap is a useful tool for Python users to have in their virtual toolbelts. In this sec‐ tion, we’ll show several examples of the type of map visualization that is possible with this toolkit. Installation of Basemap is straightforward; if you’re using conda you can type this and the package will be downloaded: $ conda install basemap We add just a single new import to our standard boilerplate: In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap Once you have the Basemap toolkit installed and imported, geographic plots are just a few lines away (the graphics in Figure 4-102 also require the PIL package in Python 2, or the pillow package in Python 3): In [ 2 ]: plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 =- 100 ) m . bluemarble ( scale = 0.5 ); 298 | Chapter 4: Visualization with Matplotlib Figure 4-102. A “bluemarble” projection of the Earth The meaning of the arguments to Basemap will be discussed momentarily",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_277"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". bluemarble ( scale = 0.5 ); 298 | Chapter 4: Visualization with Matplotlib Figure 4-102. A “bluemarble” projection of the Earth The meaning of the arguments to Basemap will be discussed momentarily. The useful thing is that the globe shown here is not a mere image; it is a fully func‐ tioning Matplotlib axes that understands spherical coordinates and allows us to easily over-plot data on the map! For example, we can use a different map projection, zoom in to North America, and plot the location of Seattle. We’ll use an etopo image (which shows topographical features both on land and under the ocean) as the map back‐ ground ( Figure 4-103 ): In [ 3 ]: fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . etopo ( scale = 0.5 , alpha = 0.5 ) # Map (long, lat) to (x, y) for plotting x , y = m ( - 122.3 , 47.6 ) plt . plot ( x , y , 'ok' , markersize = 5 ) plt . text ( x , y , ' Seattle' , fontsize = 12 ); Geographic Data with Basemap | 299 Figure 4-103. Plotting data and labels on the map This gives you a brief glimpse into the sort of geographic visualizations that are possi‐ ble with just a few lines of Python. We’ll now discuss the features of Basemap in more depth, and provide several examples of visualizing map data. Using these brief exam‐ ples as building blocks, you should be able to create nearly any map visualization that you desire. Map Projections The first thing to decide when you are using maps is which projection to use. You’re probably familiar with the fact that it is impossible to project a spherical map, such as that of the Earth, onto a flat surface without somehow distorting it or breaking its continuity. These projections have been developed over the course of human history, and there are a lot of choices! Depending on the intended use of the map projection, there are certain map features (e.g., direction, area, distance, shape, or other consider‐ ations) that are useful to maintain",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_278"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The Basemap package implements several dozen such projections, all referenced by a short format code. Here we’ll briefly demonstrate some of the more common ones. We’ll start by defining a convenience routine to draw our world map along with the longitude and latitude lines: 300 | Chapter 4: Visualization with Matplotlib In [ 4 ]: from itertools import chain def draw_map ( m , scale = 0.2 ): # draw a shaded-relief image m . shadedrelief ( scale = scale ) # lats and longs are returned as a dictionary lats = m . drawparallels ( np . linspace ( - 90 , 90 , 13 )) lons = m . drawmeridians ( np . linspace ( - 180 , 180 , 13 )) # keys contain the plt.Line2D instances lat_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lats . items ())) lon_lines = chain ( * ( tup [ 1 ][ 0 ] for tup in lons . items ())) all_lines = chain ( lat_lines , lon_lines ) # cycle through these lines and set the desired style for line in all_lines : line . set ( linestyle = '-' , alpha = 0.3 , color = 'w' ) Cylindrical projections The simplest of map projections are cylindrical projections, in which lines of constant latitude and longitude are mapped to horizontal and vertical lines, respectively. This type of mapping represents equatorial regions quite well, but results in extreme dis‐ tortions near the poles. The spacing of latitude lines varies between different cylindri‐ cal projections, leading to different conservation properties, and different distortion near the poles. In Figure 4-104 , we show an example of the equidistant cylindrical pro‐ jection , which chooses a latitude scaling that preserves distances along meridians. Other cylindrical projections are the Mercator ( projection='merc' ) and the cylin‐ drical equal-area ( projection='cea' ) projections. In [ 5 ]: fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'cyl' , resolution = None , llcrnrlat =- 90 , urcrnrlat = 90 , llcrnrlon =- 180 , urcrnrlon = 180 , ) draw_map ( m ) Figure 4-104",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_279"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Cylindrical equal-area projection Geographic Data with Basemap | 301 The additional arguments to Basemap for this view specify the latitude ( lat ) and lon‐ gitude ( lon ) of the lower-left corner ( llcrnr ) and upper-right corner ( urcrnr ) for the desired map, in units of degrees. Pseudo-cylindrical projections Pseudo-cylindrical projections relax the requirement that meridians (lines of constant longitude) remain vertical; this can give better properties near the poles of the projec‐ tion. The Mollweide projection ( projection='moll' ) is one common example of this, in which all meridians are elliptical arcs ( Figure 4-105 ). It is constructed so as to preserve area across the map: though there are distortions near the poles, the area of small patches reflects the true area. Other pseudo-cylindrical projections are the sinusoidal ( projection='sinu' ) and Robinson ( projection='robin' ) projections. In [ 6 ]: fig = plt . figure ( figsize = ( 8 , 6 ), edgecolor = 'w' ) m = Basemap ( projection = 'moll' , resolution = None , lat_0 = 0 , lon_0 = 0 ) draw_map ( m ) Figure 4-105. The Molleweide projection The extra arguments to Basemap here refer to the central latitude ( lat_0 ) and longi‐ tude ( lon_0 ) for the desired map. Perspective projections Perspective projections are constructed using a particular choice of perspective point, similar to if you photographed the Earth from a particular point in space (a point which, for some projections, technically lies within the Earth!). One common exam‐ ple is the orthographic projection ( projection='ortho' ), which shows one side of the globe as seen from a viewer at a very long distance. Thus, it can show only half the globe at a time. Other perspective-based projections include the gnomonic projection ( projection='gnom' ) and stereographic projection ( projection='stere' ). These are often the most useful for showing small portions of the map",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_280"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These are often the most useful for showing small portions of the map. 302 | Chapter 4: Visualization with Matplotlib Here is an example of the orthographic projection ( Figure 4-106 ): In [ 7 ]: fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'ortho' , resolution = None , lat_0 = 50 , lon_0 = 0 ) draw_map ( m ); Figure 4-106. The orthographic projection Conic projections A conic projection projects the map onto a single cone, which is then unrolled. This can lead to very good local properties, but regions far from the focus point of the cone may become very distorted. One example of this is the Lambert conformal conic projection ( projection='lcc' ), which we saw earlier in the map of North America. It projects the map onto a cone arranged in such a way that two standard parallels (specified in Basemap by lat_1 and lat_2 ) have well-represented distances, with scale decreasing between them and increasing outside of them. Other useful conic projec‐ tions are the equidistant conic ( projection='eqdc' ) and the Albers equal-area ( pro jection='aea' ) projection ( Figure 4-107 ). Conic projections, like perspective projections, tend to be good choices for representing small to medium patches of the globe. In [ 8 ]: fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = None , lon_0 = 0 , lat_0 = 50 , lat_1 = 45 , lat_2 = 55 , Geographic Data with Basemap | 303 width = 1.6E7 , height = 1.2E7 ) draw_map ( m ) Figure 4-107. The Albers equal-area projection Other projections If you’re going to do much with map-based visualizations, I encourage you to read up on other available projections, along with their properties, advantages, and disadvan‐ tages. Most likely, they are available in the Basemap package",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_281"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Most likely, they are available in the Basemap package . If you dig deep enough into this topic, you’ll find an incredible subculture of geo-viz geeks who will be ready to argue fervently in support of their favorite projection for any given application! Drawing a Map Background Earlier we saw the bluemarble() and shadedrelief() methods for projecting global images on the map, as well as the drawparallels() and drawmeridians() methods for drawing lines of constant latitude and longitude. The Basemap package contains a range of useful functions for drawing borders of physical features like continents, oceans, lakes, and rivers, as well as political boundaries such as countries and US states and counties. The following are some of the available drawing functions that you may wish to explore using IPython’s help features: • Physical boundaries and bodies of water drawcoastlines() Draw continental coast lines drawlsmask() Draw a mask between the land and sea, for use with projecting images on one or the other 304 | Chapter 4: Visualization with Matplotlib drawmapboundary() Draw the map boundary, including the fill color for oceans drawrivers() Draw rivers on the map fillcontinents() Fill the continents with a given color; optionally fill lakes with another color • Political boundaries drawcountries() Draw country boundaries drawstates() Draw US state boundaries drawcounties() Draw US county boundaries • Map features drawgreatcircle() Draw a great circle between two points drawparallels() Draw lines of constant latitude drawmeridians() Draw lines of constant longitude drawmapscale() Draw a linear scale on the map • Whole-globe images bluemarble() Project NASA’s blue marble image onto the map shadedrelief() Project a shaded relief image onto the map etopo() Draw an etopo relief image onto the map warpimage() Project a user-provided image onto the map Geographic Data with Basemap | 305 For the boundary-based features, you must set the desired resolution when creating a Basemap image",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_282"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The resolution argument of the Basemap class sets the level of detail in boundaries, either 'c' (crude), 'l' (low), 'i' (intermediate), 'h' (high), 'f' (full), or None if no boundaries will be used. This choice is important: setting higresolution boundaries on a global map, for example, can be very slow. Here’s an example of drawing land/sea boundaries, and the effect of the resolution parameter. We’ll create both a loand high-resolution map of Scotland’s beautiful Isle of Skye. It’s located at 57.3°N, 6.2°W, and a map of 90,000×120,000 kilometers shows it well ( Figure 4-108 ): In [ 9 ]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 8 )) for i , res in enumerate ([ 'l' , 'h' ]): m = Basemap ( projection = 'gnom' , lat_0 = 57.3 , lon_0 =- 6.2 , width = 90000 , height = 120000 , resolution = res , ax = ax [ i ]) m . fillcontinents ( color = \"#FFDDCC\" , lake_color = '#DDEEFF' ) m . drawmapboundary ( fill_color = \"#DDEEFF\" ) m . drawcoastlines () ax [ i ] . set_title ( \"resolution='{0}'\" . format ( res )); Figure 4-108. Map boundaries at low and high resolution Notice that the low-resolution coastlines are not suitable for this level of zoom, while high-resolution works just fine. The low level would work just fine for a global view, however, and would be much faster than loading the high-resolution border data for the entire globe! It might require some experimentation to find the correct resolution 306 | Chapter 4: Visualization with Matplotlib parameter for a given view; the best route is to start with a fast, low-resolution plot and increase the resolution as needed. Plotting Data on Maps Perhaps the most useful piece of the Basemap toolkit is the ability to over-plot a vari‐ ety of data onto a map background. For simple plotting and text, any plt function works on the map; you can use the Basemap instance to project latitude and longitude coordinates to (x, y) coordinates for plotting with plt , as we saw earlier in the Seat‐ tle example",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_283"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In addition to this, there are many map-specific functions available as methods of the Basemap instance. These work very similarly to their standard Matplotlib counter‐ parts, but have an additional Boolean argument latlon , which if set to True allows you to pass raw latitudes and longitudes to the method, rather than projected (x, y) coordinates. Some of these map-specific methods are: contour() / contourf() Draw contour lines or filled contours imshow() Draw an image pcolor() / pcolormesh() Draw a pseudocolor plot for irregular/regular meshes plot() Draw lines and/or markers scatter() Draw points with markers quiver() Draw vectors barbs() Draw wind barbs drawgreatcircle() Draw a great circle We’ll see examples of a few of these as we continue. For more information on these functions, including several example plots, see the online Basemap documentation . Geographic Data with Basemap | 307 Example: California Cities Recall that in “Customizing Plot Legends” on page 249 , we demonstrated the use of size and color in a scatter plot to convey information about the location, size, and population of California cities. Here, we’ll create this plot again, but using Basemap to put the data in context. We start with loading the data, as we did before: In [ 10 ]: import pandas as pd cities = pd . read_csv ( 'data/california_cities.csv' ) # Extract the data we're interested in lat = cities [ 'latd' ] . values lon = cities [ 'longd' ] . values population = cities [ 'population_total' ] . values area = cities [ 'area_total_km2' ] . values Next, we set up the map projection, scatter the data, and then create a colorbar and legend ( Figure 4-109 ): In [ 11 ]: # 1. Draw the map background fig = plt . figure ( figsize = ( 8 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'h' , lat_0 = 37.5 , lon_0 =- 119 , width = 1E6 , height = 1.2E6 ) m . shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_284"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". shadedrelief () m . drawcoastlines ( color = 'gray' ) m . drawcountries ( color = 'gray' ) m . drawstates ( color = 'gray' ) # 2. scatter city data, with color reflecting population # and size reflecting area m . scatter ( lon , lat , latlon = True , c = np . log10 ( population ), s = area , cmap = 'Reds' , alpha = 0.5 ) # 3. create colorbar and legend plt . colorbar ( label = r '$\\log_{10}({\\rm population})$' ) plt . clim ( 3 , 7 ) # make legend with dummy points for a in [ 100 , 300 , 500 ]: plt . scatter ([], [], c = 'k' , alpha = 0.5 , s = a , label = str ( a ) + ' km$^2$' ) plt . legend ( scatterpoints = 1 , frameon = False , labelspacing = 1 , loc = 'lower left' ); 308 | Chapter 4: Visualization with Matplotlib Figure 4-109. Scatter plot over a map background This shows us roughly where larger populations of people have settled in California: they are clustered near the coast in the Los Angeles and San Francisco areas, stretched along the highways in the flat central valley, and avoiding almost completely the mountainous regions along the borders of the state. Example: Surface Temperature Data As an example of visualizing some more continuous geographic data, let’s consider the “polar vortex” that hit the eastern half of the United States in January 2014. A great source for any sort of climatic data is NASA’s Goddard Institute for Space Stud‐ ies . Here we’ll use the GIS 250 temperature data, which we can download using shell commands (these commands may have to be modified on Windows machines). The data used here was downloaded on 6/12/2016, and the file size is approximately 9 MB: In [ 12 ]: # !curl -O http://data.giss.nasa.gov/pub/gistemp/gistemp250.nc.gz # !gunzip gistemp250.nc.gz The data comes in NetCDF format, which can be read in Python by the netCDF4 library",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_285"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You can install this library as shown here: $ conda install netcdf4 Geographic Data with Basemap | 309 We read the data as follows: In [ 13 ]: from netCDF4 import Dataset data = Dataset ( 'gistemp250.nc' ) The file contains many global temperature readings on a variety of dates; we need to select the index of the date we’re interested in—in this case, January 15, 2014: In [ 14 ]: from netCDF4 import date2index from datetime import datetime timeindex = date2index ( datetime ( 2014 , 1 , 15 ), data . variables [ 'time' ]) Now we can load the latitude and longitude data, as well as the temperature anomaly for this index: In [ 15 ]: lat = data . variables [ 'lat' ][:] lon = data . variables [ 'lon' ][:] lon , lat = np . meshgrid ( lon , lat ) temp_anomaly = data . variables [ 'tempanomaly' ][ timeindex ] Finally, we’ll use the pcolormesh() method to draw a color mesh of the data. We’ll look at North America, and use a shaded relief map in the background. Note that for this data we specifically chose a divergent colormap, which has a neutral color at zero and two contrasting colors at negative and positive values ( Figure 4-110 ). We’ll also lightly draw the coastlines over the colors for reference: In [ 16 ]: fig = plt . figure ( figsize = ( 10 , 8 )) m = Basemap ( projection = 'lcc' , resolution = 'c' , width = 8E6 , height = 8E6 , lat_0 = 45 , lon_0 =- 100 ,) m . shadedrelief ( scale = 0.5 ) m . pcolormesh ( lon , lat , temp_anomaly , latlon = True , cmap = 'RdBu_r' ) plt . clim ( - 8 , 8 ) m . drawcoastlines ( color = 'lightgray' ) plt . title ( 'January 2014 Temperature Anomaly' ) plt . colorbar ( label = 'temperature anomaly (°C)' ); The data paints a picture of the localized, extreme temperature anomalies that hap‐ pened during that month. The eastern half of the United States was much colder than normal, while the western half and Alaska were much warmer. Regions with no recorded temperature show the map background. 310 | Chapter 4: Visualization with Matplotlib Figure 4-110",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_286"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Regions with no recorded temperature show the map background. 310 | Chapter 4: Visualization with Matplotlib Figure 4-110. The temperature anomaly in January 2014 Visualization with Seaborn Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up: • Prior to version 2.0, Matplotlib’s defaults are not exactly the best choices. It was based off of MATLAB circa 1999, and this often shows. • Matplotlib’s API is relatively low level. Doing sophisticated statistical visualiza‐ tion is possible, but often requires a lot of boilerplate code. • Matplotlib predated Pandas by more than a decade, and thus is not designed for use with Pandas DataFrame s. In order to visualize data from a Pandas DataFrame , you must extract each Series and often concatenate them together into the right format. It would be nicer to have a plotting library that can intelligently use the DataFrame labels in a plot. An answer to these problems is Seaborn . Seaborn provides an API on top of Matplot‐ lib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality pro‐ vided by Pandas DataFrame s. Visualization with Seaborn | 311 To be fair, the Matplotlib team is addressing this: it has recently added the plt.style tools (discussed in “Customizing Matplotlib: Configurations and Stylesheets” on page 282 ), and is starting to handle Pandas data more seamlessly. The 2.0 release of the library will include a new default stylesheet that will improve on the current status quo. But for all the reasons just discussed, Seaborn remains an extremely useful add-on. Seaborn Versus Matplotlib Here is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. We start with the typical imports: In [ 1 ]: import matplotlib.pyplot as plt plt . style",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_287"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We start with the typical imports: In [ 1 ]: import matplotlib.pyplot as plt plt . style . use ( 'classic' ) % matplotlib inline import numpy as np import pandas as pd Now we create some random walk data: In [ 2 ]: # Create some data rng = np . random . RandomState ( 0 ) x = np . linspace ( 0 , 10 , 500 ) y = np . cumsum ( rng . randn ( 500 , 6 ), 0 ) And do a simple plot ( Figure 4-111 ): In [ 3 ]: # Plot the data with Matplotlib defaults plt . plot ( x , y ) plt . legend ( 'ABCDEF' , ncol = 2 , loc = 'upper left' ); Figure 4-111. Data in Matplotlib’s default style Although the result contains all the information we’d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21st-century data visualization. 312 | Chapter 4: Visualization with Matplotlib Now let’s take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines, but it can also overwrite Matplotlib’s default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling Seaborn’s set() method. By convention, Sea‐ born is imported as sns : In [ 4 ]: import seaborn as sns sns . set () Now let’s rerun the same two lines as before ( Figure 4-112 ): In [ 5 ]: # same plotting code as above! plt . plot ( x , y ) plt . legend ( 'ABCDEF' , ncol = 2 , loc = 'upper left' ); Figure 4-112. Data in Seaborn’s default style Ah, much better! Exploring Seaborn Plots The main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting. Let’s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_288"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood), but the Seaborn API is much more convenient. Visualization with Seaborn | 313 Histograms, KDE, and densities Often in statistical data visualization, all you want is to plot histograms and joint dis‐ tributions of variables. We have seen that this is relatively straightforward in Matplot‐ lib ( Figure 4-113 ): In [ 6 ]: data = np . random . multivariate_normal ([ 0 , 0 ], [[ 5 , 2 ], [ 2 , 2 ]], size = 2000 ) data = pd . DataFrame ( data , columns = [ 'x' , 'y' ]) for col in 'xy' : plt . hist ( data [ col ], normed = True , alpha = 0.5 ) Figure 4-113. Histograms for visualizing distributions Rather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot ( Figure 4-114 ): In [ 7 ]: for col in 'xy' : sns . kdeplot ( data [ col ], shade = True ) 314 | Chapter 4: Visualization with Matplotlib Figure 4-114. Kernel density estimates for visualizing distributions Histograms and KDE can be combined using distplot ( Figure 4-115 ): In [ 8 ]: sns . distplot ( data [ 'x' ]) sns . distplot ( data [ 'y' ]); Figure 4-115. Kernel density and histograms plotted together If we pass the full two-dimensional dataset to kdeplot , we will get a two-dimensional visualization of the data ( Figure 4-116 ): In [ 9 ]: sns . kdeplot ( data ); Visualization with Seaborn | 315 Figure 4-116. A two-dimensional kernel density plot We can see the joint distribution and the marginal distributions together using sns.jointplot . For this plot, we’ll set the style to a white background ( Figure 4-117 ): In [ 10 ]: with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'kde' ); Figure 4-117",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_289"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For this plot, we’ll set the style to a white background ( Figure 4-117 ): In [ 10 ]: with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'kde' ); Figure 4-117. A joint distribution plot with a two-dimensional kernel density estimate 316 | Chapter 4: Visualization with Matplotlib There are other parameters that can be passed to jointplot —for example, we can use a hexagonally based histogram instead ( Figure 4-118 ): In [ 11 ]: with sns . axes_style ( 'white' ): sns . jointplot ( \"x\" , \"y\" , data , kind = 'hex' ) Figure 4-118. A joint distribution plot with a hexagonal bin representation Pair plots When you generalize joint plots to datasets of larger dimensions, you end up with pair plots . This is very useful for exploring correlations between multidimensional data, when you’d like to plot all pairs of values against each other. We’ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species: In [ 12 ]: iris = sns . load_dataset ( \"iris\" ) iris . head () Out[12]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Visualization with Seaborn | 317 Visualizing the multidimensional relationships among the samples is as easy as call‐ ing sns.pairplot ( Figure 4-119 ): In [ 13 ]: sns . pairplot ( iris , hue = 'species' , size = 2.5 ); Figure 4-119. A pair plot showing the relationships between four variables Faceted histograms Sometimes the best way to view data is via histograms of subsets. Seaborn’s FacetGrid makes this extremely simple. We’ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data ( Figure 4-120 ): In [ 14 ]: tips = sns . load_dataset ( 'tips' ) tips",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_290"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We’ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data ( Figure 4-120 ): In [ 14 ]: tips = sns . load_dataset ( 'tips' ) tips . head () Out[14]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 318 | Chapter 4: Visualization with Matplotlib 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 In [ 15 ]: tips [ 'tip_pct' ] = 100 * tips [ 'tip' ] / tips [ 'total_bill' ] grid = sns . FacetGrid ( tips , row = \"sex\" , col = \"time\" , margin_titles = True ) grid . map ( plt . hist , \"tip_pct\" , bins = np . linspace ( 0 , 40 , 15 )); Figure 4-120. An example of a faceted histogram Factor plots Factor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter ( Figure 4-121 ): In [ 16 ]: with sns . axes_style ( style = 'ticks' ): g = sns . factorplot ( \"day\" , \"total_bill\" , \"sex\" , data = tips , kind = \"box\" ) g . set_axis_labels ( \"Day\" , \"Total Bill\" ); Visualization with Seaborn | 319 Figure 4-121. An example of a factor plot, comparing distributions given various discrete factors Joint distributions Similar to the pair plot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distribu‐ tions ( Figure 4-122 ): In [ 17 ]: with sns . axes_style ( 'white' ): sns . jointplot ( \"total_bill\" , \"tip\" , data = tips , kind = 'hex' ) Figure 4-122. A joint distribution plot 320 | Chapter 4: Visualization with Matplotlib The joint plot can even do some automatic kernel density estimation and regression ( Figure 4-123 ): In [ 18 ]: sns . jointplot ( \"total_bill\" , \"tip\" , data = tips , kind = 'reg' ); Figure 4-123. A joint distribution plot with a regression fit Bar plots Time series can be plotted with sns.factorplot",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_291"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". jointplot ( \"total_bill\" , \"tip\" , data = tips , kind = 'reg' ); Figure 4-123. A joint distribution plot with a regression fit Bar plots Time series can be plotted with sns.factorplot . In the following example (visualized in Figure 4-124 ), we’ll use the Planets data that we first saw in “Aggregation and Grouping” on page 158 : In [ 19 ]: planets = sns . load_dataset ( 'planets' ) planets . head () Out[19]: method number orbital_period mass distance year 0 Radial Velocity 1 269.300 7.10 77.40 2006 1 Radial Velocity 1 874.774 2.21 56.95 2008 2 Radial Velocity 1 763.000 2.60 19.84 2011 3 Radial Velocity 1 326.030 19.40 110.62 2007 4 Radial Velocity 1 516.220 10.50 119.47 2009 In [ 20 ]: with sns . axes_style ( 'white' ): g = sns . factorplot ( \"year\" , data = planets , aspect = 2 , kind = \"count\" , color = 'steelblue' ) g . set_xticklabels ( step = 5 ) Visualization with Seaborn | 321 Figure 4-124. A histogram as a special case of a factor plot We can learn more by looking at the method of discovery of each of these planets, as illustrated in Figure 4-125 : In [ 21 ]: with sns . axes_style ( 'white' ): g = sns . factorplot ( \"year\" , data = planets , aspect = 4.0 , kind = 'count' , hue = 'method' , order = range ( 2001 , 2015 )) g . set_ylabels ( 'Number of Planets Discovered' ) Figure 4-125. Number of planets discovered by year and type (see the online appendix for a full-scale figure) For more information on plotting with Seaborn, see the Seaborn documentation , a tutorial , and the Seaborn gallery . Example: Exploring Marathon Finishing Times Here we’ll look at using Seaborn to help visualize and understand finishing results from a marathon. I’ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloa‐ ded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_292"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will start by downloading the data from the Web, and loading it into Pandas: 322 | Chapter 4: Visualization with Matplotlib In [ 22 ]: # !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/ # master/marathon-data.csv In [ 23 ]: data = pd . read_csv ( 'marathon-data.csv' ) data . head () Out[23]: age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 By default, Pandas loaded the time columns as Python strings (type object ); we can see this by looking at the dtypes attribute of the DataFrame : In [ 24 ]: data . dtypes Out[24]: age int64 gender object split object final object dtype: object Let’s fix this by providing a converter for the times: In [ 25 ]: def convert_time ( s ): h , m , s = map ( int , s . split ( ':' )) return pd . datetools . timedelta ( hours = h , minutes = m , seconds = s ) data = pd . read_csv ( 'marathon-data.csv' , converters = { 'split' : convert_time , 'final' : convert_time }) data . head () Out[25]: age gender split final 0 33 M 01:05:38 02:08:51 1 32 M 01:06:26 02:09:28 2 31 M 01:06:49 02:10:42 3 38 M 01:06:16 02:13:45 4 31 M 01:06:32 02:13:59 In [ 26 ]: data . dtypes Out[26]: age int64 gender object split timedelta64[ns] final timedelta64[ns] dtype: object That looks much better. For the purpose of our Seaborn plotting utilities, let’s next add columns that give the times in seconds: In [ 27 ]: data [ 'split_sec' ] = data [ 'split' ] . astype ( int ) / 1E9 data [ 'final_sec' ] = data [ 'final' ] . astype ( int ) / 1E9 data . head () Visualization with Seaborn | 323 Out[27]: age gender split final split_sec final_sec 0 33 M 01:05:38 02:08:51 3938.0 7731.0 1 32 M 01:06:26 02:09:28 3986.0 7768.0 2 31 M 01:06:49 02:10:42 4009.0 7842.0 3 38 M 01:06:16 02:13:45 3976.0 8025.0 4 31 M 01:06:32 02:13:59 3992.0 8039.0 To get an idea of what the data looks like, we can plot a jointplot over the data ( Figure 4-126 ): In [ 28 ]: with sns . axes_style ( 'white' ): g = sns",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_293"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". axes_style ( 'white' ): g = sns . jointplot ( \"split_sec\" , \"final_sec\" , data , kind = 'hex' ) g . ax_joint . plot ( np . linspace ( 4000 , 16000 ), np . linspace ( 8000 , 32000 ), ':k' ) Figure 4-126. The relationship between the split for the first half-marathon and the fin‐ ishing time for the full marathon The dotted line shows where someone’s time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you’ll know that those who do the opposite—run faster dur‐ ing the second half of the race—are said to have “negative-split” the race. Let’s create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race: In [ 29 ]: data [ 'split_frac' ] = 1 - 2 * data [ 'split_sec' ] / data [ 'final_sec' ] data . head () 324 | Chapter 4: Visualization with Matplotlib Out[29]: age gender split final split_sec final_sec split_frac 0 33 M 01:05:38 02:08:51 3938.0 7731.0 -0.018756 1 32 M 01:06:26 02:09:28 3986.0 7768.0 -0.026262 2 31 M 01:06:49 02:10:42 4009.0 7842.0 -0.022443 3 38 M 01:06:16 02:13:45 3976.0 8025.0 0.009097 4 31 M 01:06:32 02:13:59 3992.0 8039.0 0.006842 Where this split difference is less than zero, the person negative-split the race by that fraction. Let’s do a distribution plot of this split fraction ( Figure 4-127 ): In [ 30 ]: sns . distplot ( data [ 'split_frac' ], kde = False ); plt . axvline ( 0 , color = \"k\" , linestyle = \"--\" ); Figure 4-127. The distribution of split fractions; 0.0 indicates a runner who completed the first and second halves in identical times In [ 31 ]: sum ( data . split_frac < 0 ) Out[31]: 251 Out of nearly 40,000 participants, there were only 250 people who negative-split their marathon. Let’s see whether there is any correlation between this split fraction and other vari‐ ables",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_294"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Let’s see whether there is any correlation between this split fraction and other vari‐ ables. We’ll do this using a pairgrid , which draws plots of all these correlations ( Figure 4-128 ): In [ 32 ]: g = sns . PairGrid ( data , vars = [ 'age' , 'split_sec' , 'final_sec' , 'split_frac' ], hue = 'gender' , palette = 'RdBu_r' ) g . map ( plt . scatter , alpha = 0.8 ) g . add_legend (); Visualization with Seaborn | 325 Figure 4-128. The relationship between quantities within the marathon dataset It looks like the split fraction does not correlate particularly with age, but does corre‐ late with the final time: faster runners tend to have closer to even splits on their mara‐ thon time. (We see here that Seaborn is no panacea for Matplotlib’s ills when it comes to plot styles: in particular, the x -axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in “Customizing Ticks” on page 275 can be used to adjust such things if desired.) The difference between men and women here is interesting. Let’s look at the histo‐ gram of split fractions for these two groups ( Figure 4-129 ): In [ 33 ]: sns . kdeplot ( data . split_frac [ data . gender == 'M' ], label = 'men' , shade = True ) sns . kdeplot ( data . split_frac [ data . gender == 'W' ], label = 'women' , shade = True ) plt . xlabel ( 'split_frac' ); 326 | Chapter 4: Visualization with Matplotlib Figure 4-129. The distribution of split fractions by gender The interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribu‐ tion among the men and women. Let’s see if we can suss out what’s going on by look‐ ing at the distributions as a function of age. A nice way to compare distributions is to use a violin plot ( Figure 4-130 ): In [ 34 ]: sns . violinplot ( \"gender\" , \"split_frac\" , data = data , palette = [ \"lightblue\" , \"lightpink\" ]); Figure 4-130",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_295"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". violinplot ( \"gender\" , \"split_frac\" , data = data , palette = [ \"lightblue\" , \"lightpink\" ]); Figure 4-130. A violin plot showing the split fraction by gender Visualization with Seaborn | 327 This is yet another way to compare the distributions between men and women. Let’s look a little deeper, and compare these violin plots as a function of age. We’ll start by creating a new column in the array that specifies the decade of age that each person is in ( Figure 4-131 ): In [ 35 ]: data [ 'age_dec' ] = data . age . map ( lambda age : 10 * ( age // 10 )) data . head () Out[35]: age gender split final split_sec final_sec split_frac age_dec 0 33 M 01:05:38 02:08:51 3938.0 7731.0 -0.018756 30 1 32 M 01:06:26 02:09:28 3986.0 7768.0 -0.026262 30 2 31 M 01:06:49 02:10:42 4009.0 7842.0 -0.022443 30 3 38 M 01:06:16 02:13:45 3976.0 8025.0 0.009097 30 4 31 M 01:06:32 02:13:59 3992.0 8039.0 0.006842 30 In [ 36 ]: men = ( data . gender == 'M' ) women = ( data . gender == 'W' ) with sns . axes_style ( style = None ): sns . violinplot ( \"age_dec\" , \"split_frac\" , hue = \"gender\" , data = data , split = True , inner = \"quartile\" , palette = [ \"lightblue\" , \"lightpink\" ]); Figure 4-131. A violin plot showing the split fraction by gender and age Looking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter). 328 | Chapter 4: Visualization with Matplotlib Also surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we’re estimating the distribution from small numbers, as there are only a handful of runners in that range: In [ 38 ]: ( data . age > 80 ) . sum () Out[38]: 7 Back to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_296"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". age > 80 ) . sum () Out[38]: 7 Back to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We’ll use regplot , which will automatically fit a linear regression to the data ( Figure 4-132 ): In [ 37 ]: g = sns . lmplot ( 'final_sec' , 'split_frac' , col = 'gender' , data = data , markers = \".\" , scatter_kws = dict ( color = 'c' )) g . map ( plt . axhline , y = 0.1 , color = \"k\" , ls = \":\" ); Figure 4-132. Split fraction versus finishing time by gender Apparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split. Further Resources Matplotlib Resources A single chapter in a book can never hope to cover all the available features and plot types available in Matplotlib. As with other packages we’ve seen, liberal use of IPy‐ thon’s tab-completion and help functions (see “Help and Documentation in IPython” on page 3 ) can be very helpful when you’re exploring Matplotlib’s API. In addition, Matplotlib’s online documentation can be a helpful reference. See in particular the Further Resources | 329 Matplotlib gallery linked on that page: it shows thumbnails of hundreds of different plot types, each one linked to a page with the Python code snippet used to generate it. In this way, you can visually inspect and learn about a wide range of different plotting styles and visualization techniques. For a book-length treatment of Matplotlib, I would recommend Interactive Applica‐ tions Using Matplotlib , written by Matplotlib core developer Ben Root. Other Python Graphics Libraries Although Matplotlib is the most prominent Python visualization library, there are other more modern tools that are worth exploring as well",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_297"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Other Python Graphics Libraries Although Matplotlib is the most prominent Python visualization library, there are other more modern tools that are worth exploring as well. I’ll mention a few of them briefly here: • Bokeh is a JavaScript visualization library with a Python frontend that creates highly interactive visualizations capable of handling very large and/or streaming datasets. The Python frontend outputs a JSON data structure that can be inter‐ preted by the Bokeh JS engine. • Plotly is the eponymous open source product of the Plotly company, and is simi‐ lar in spirit to Bokeh. Because Plotly is the main product of a startup, it is receiv‐ ing a high level of development effort. Use of the library is entirely free. • Vispy is an actively developed project focused on dynamic visualizations of very large datasets. Because it is built to target OpenGL and make use of efficient graphics processors in your computer, it is able to render some quite large and stunning visualizations. • Vega and Vega-Lite are declarative graphics representations, and are the product of years of research into the fundamental language of data visualization. The ref‐ erence rendering implementation is JavaScript, but the API is language agnostic. There is a Python API under development in the Altair package . Though it’s not mature yet, I’m quite excited for the possibilities of this project to provide a com‐ mon reference point for visualization in Python and other languages. The visualization space in the Python community is very dynamic, and I fully expect this list to be out of date as soon as it is published. Keep an eye out for what’s coming in the future! 330 | Chapter 4: Visualization with Matplotlib CHAPTER 5 Machine Learning In many ways, machine learning is the primary means by which data science mani‐ fests itself to the broader world",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_298"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Machine learning is where these computational and algorithmic skills of data science meet the statistical thinking of data science, and the result is a collection of approaches to inference and data exploration that are not about effective theory so much as effective computation. The term “machine learning” is sometimes thrown around as if it is some kind of magic pill: apply machine learning to your data, and all your problems will be solved! As you might expect, the reality is rarely this simple. While these methods can be incredibly powerful, to be effective they must be approached with a firm grasp of the strengths and weaknesses of each method, as well as a grasp of general concepts such as bias and variance, overfitting and underfitting, and more. This chapter will dive into practical aspects of machine learning, primarily using Python’s Scikit-Learn package. This is not meant to be a comprehensive introduction to the field of machine learning; that is a large subject and necessitates a more techni‐ cal approach than we take here. Nor is it meant to be a comprehensive manual for the use of the Scikit-Learn package (for this, see “Further Machine Learning Resources” on page 514 ). Rather, the goals of this chapter are: • To introduce the fundamental vocabulary and concepts of machine learning. • To introduce the Scikit-Learn API and show some examples of its use. • To take a deeper dive into the details of several of the most important machine learning approaches, and develop an intuition into how they work and when and where they are applicable. Much of this material is drawn from the Scikit-Learn tutorials and workshops I have given on several occasions at PyCon, SciPy, PyData, and other conferences",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_299"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Much of this material is drawn from the Scikit-Learn tutorials and workshops I have given on several occasions at PyCon, SciPy, PyData, and other conferences. Any 331 clarity in the following pages is likely due to the many workshop participants and cinstructors who have given me valuable feedback on this material over the years! Finally, if you are seeking a more comprehensive or technical treatment of any of these subjects, I’ve listed several resources and references in “Further Machine Learn‐ ing Resources” on page 514 . What Is Machine Learning? Before we take a look at the details of various machine learning methods, let’s start by looking at what machine learning is, and what it isn’t. Machine learning is often cate‐ gorized as a subfield of artificial intelligence, but I find that categorization can often be misleading at first brush. The study of machine learning certainly arose from research in this context, but in the data science application of machine learning meth‐ ods, it’s more helpful to think of machine learning as a means of building models of data . Fundamentally, machine learning involves building mathematical models to help understand data. “Learning” enters the fray when we give these models tunable parameters that can be adapted to observed data; in this way the program can be con‐ sidered to be “learning” from the data. Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data. I’ll leave to the reader the more philosophical digression regarding the extent to which this type of mathematical, model-based “learning” is similar to the “learning” exhibited by the human brain. Understanding the problem setting in machine learning is essential to using these tools effectively, and so we will start with some broad categorizations of the types of approaches we’ll discuss here",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_300"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Categories of Machine Learning At the most fundamental level, machine learning can be categorized into two main types: supervised learning and unsupervised learning. Supervised learning involves somehow modeling the relationship between measured features of data and some label associated with the data; once this model is deter‐ mined, it can be used to apply labels to new, unknown data. This is further subdivi‐ ded into classification tasks and regression tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities. We will see examples of both types of supervised learning in the following section. Unsupervised learning involves modeling the features of a dataset without reference to any label, and is often described as “letting the dataset speak for itself.” These models include tasks such as clustering and dimensionality reduction. Clustering algorithms 332 | Chapter 5: Machine Learning identify distinct groups of data, while dimensionality reduction algorithms search for more succinct representations of the data. We will see examples of both types of unsupervised learning in the following section. In addition, there are so-called semi-supervised learning methods, which fall some‐ where between supervised learning and unsupervised learning. Semi-supervised learning methods are often useful when only incomplete labels are available. Qualitative Examples of Machine Learning Applications To make these ideas more concrete, let’s take a look at a few very simple examples of a machine learning task. These examples are meant to give an intuitive, noquantitative overview of the types of machine learning tasks we will be looking at in this chapter. In later sections, we will go into more depth regarding the particular models and how they are used. For a preview of these more technical aspects, you can find the Python source that generates the figures in the online appendix",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_301"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For a preview of these more technical aspects, you can find the Python source that generates the figures in the online appendix . Classification: Predicting discrete labels We will first take a look at a simple classification task, in which you are given a set of labeled points and want to use these to classify some unlabeled points. Imagine that we have the data shown in Figure 5-1 (the code used to generate this figure, and all figures in this section, is available in the online appendix). Figure 5-1. A simple data set for classification What Is Machine Learning? | 333 Here we have two-dimensional data; that is, we have two features for each point, rep‐ resented by the (x,y) positions of the points on the plane. In addition, we have one of two class labels for each point, here represented by the colors of the points. From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled “blue” or “red.” There are a number of possible models for such a classification task, but here we will use an extremely simple one. We will make the assumption that the two groups can be separated by drawing a straight line through the plane between them, such that points on each side of the line fall in the same group. Here the model is a quantitative version of the statement “a straight line separates the classes,” while the model param‐ eters are the particular numbers describing the location and orientation of that line for our data. The optimal values for these model parameters are learned from the data (this is the “learning” in machine learning), which is often called training the model . Figure 5-2 is a visual representation of what the trained model looks like for this data. Figure 5-2. A simple classification model Now that this model has been trained, it can be generalized to new, unlabeled data. In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_302"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model. This stage is usually called predic‐ tion . See Figure 5-3 . 334 | Chapter 5: Machine Learning Figure 5-3. Applying a classification model to new data This is the basic idea of a classification task in machine learning, where “classifica‐ tion” indicates that the data has discrete class labels. At first glance this may look fairly trivial: it would be relatively easy to simply look at this data and draw such a discriminatory line to accomplish this classification. A benefit of the machine learn‐ ing approach, however, is that it can generalize to much larger datasets in many more dimensions. For example, this is similar to the task of automated spam detection for email; in this case, we might use the following features and labels: • feature 1 , feature 2 , etc. normalized counts of important words or phrases (“Viagra,” “Nigerian prince,” etc.) • label “spam” or “not spam” For the training set, these labels might be determined by individual inspection of a small representative sample of emails; for the remaining emails, the label would be determined using the model. For a suitably trained classification algorithm with enough well-constructed features (typically thousands or millions of words or phrases), this type of approach can be very effective. We will see an example of such text-based classification in “In Depth: Naive Bayes Classification” on page 382 . Some important classification algorithms that we will discuss in more detail are Gaus‐ sian naive Bayes (see “In Depth: Naive Bayes Classification” on page 382 ), support vector machines (see “In-Depth: Support Vector Machines” on page 405 ), and ran‐ dom forest classification (see “In-Depth: Decision Trees and Random Forests” on page 421 )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_303"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Regression: Predicting continuous labels In contrast with the discrete labels of a classification algorithm, we will next look at a simple regression task in which the labels are continuous quantities. What Is Machine Learning? | 335 Consider the data shown in Figure 5-4 , which consists of a set of points, each with a continuous label. Figure 5-4. A simple dataset for regression As with the classification example, we have two-dimensional data; that is, there are two features describing each data point. The color of each point represents the con‐ tinuous label for that point. There are a number of possible regression models we might use for this type of data, but here we will use a simple linear regression to predict the points. This simple linear regression model assumes that if we treat the label as a third spatial dimension, we can fit a plane to the data. This is a higher-level generalization of the well-known problem of fitting a line to data with two coordinates. We can visualize this setup as shown in Figure 5-5 . 336 | Chapter 5: Machine Learning Figure 5-5. A three-dimensional view of the regression data Notice that the feature 1–feature 2 plane here is the same as in the two-dimensional plot from before; in this case, however, we have represented the labels by both color and three-dimensional axis position. From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow us to predict the expected label for any set of input parameters. Returning to the two-dimensional projection, when we fit such a plane we get the result shown in Figure 5-6 . Figure 5-6. A representation of the regression model What Is Machine Learning? | 337 This plane of fit gives us what we need to predict labels for new points. Visually, we find the results shown in Figure 5-7 . Figure 5-7. Applying the regression model to new data As with the classification example, this may seem rather trivial in a low number of dimensions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_304"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 5-7. Applying the regression model to new data As with the classification example, this may seem rather trivial in a low number of dimensions. But the power of these methods is that they can be straightforwardly applied and evaluated in the case of data with many, many features. For example, this is similar to the task of computing the distance to galaxies observed through a telescope—in this case, we might use the following features and labels: • feature 1 , feature 2 , etc. brightness of each galaxy at one of several wavelengths or colors • label distance or redshift of the galaxy The distances for a small number of these galaxies might be determined through an independent set of (typically more expensive) observations. We could then estimate distances to remaining galaxies using a suitable regression model, without the need to employ the more expensive observation across the entire set. In astronomy circles, this is known as the “photometric redshift” problem. Some important regression algorithms that we will discuss are linear regression (see “In Depth: Linear Regression” on page 390 ), support vector machines (see “In-Depth: Support Vector Machines” on page 405 ), and random forest regression (see “IDepth: Decision Trees and Random Forests” on page 421 ). Clustering: Inferring labels on unlabeled data The classification and regression illustrations we just looked at are examples of super‐ vised learning algorithms, in which we are trying to build a model that will predict labels for new data. Unsupervised learning involves models that describe data without reference to any known labels. 338 | Chapter 5: Machine Learning One common case of unsupervised learning is “clustering,” in which data is automati‐ cally assigned to some number of discrete groups. For example, we might have some two-dimensional data like that shown in Figure 5-8 . Figure 5-8. Example data for clustering By eye, it is clear that each of these points is part of a distinct group",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_305"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, we might have some two-dimensional data like that shown in Figure 5-8 . Figure 5-8. Example data for clustering By eye, it is clear that each of these points is part of a distinct group. Given this input, a clustering model will use the intrinsic structure of the data to determine which points are related. Using the very fast and intuitive k -means algorithm (see “In Depth: k-Means Clustering” on page 462 ), we find the clusters shown in Figure 5-9 . k -means fits a model consisting of k cluster centers; the optimal centers are assumed to be those that minimize the distance of each point from its assigned center. Again, this might seem like a trivial exercise in two dimensions, but as our data becomes larger and more complex, such clustering algorithms can be employed to extract use‐ ful information from the dataset. We will discuss the k -means algorithm in more depth in “In Depth: k-Means Cluster‐ ing” on page 462 . Other important clustering algorithms include Gaussian mixture models (see “In Depth: Gaussian Mixture Models” on page 476 ) and spectral cluster‐ ing (see Scikit-Learn’s clustering documentation ). What Is Machine Learning? | 339 Figure 5-9. Data labeled with a k-means clustering model Dimensionality reduction: Inferring structure of unlabeled data Dimensionality reduction is another example of an unsupervised algorithm, in which labels or other information are inferred from the structure of the dataset itself. Dimensionality reduction is a bit more abstract than the examples we looked at before, but generally it seeks to pull out some low-dimensional representation of data that in some way preserves relevant qualities of the full dataset. Different dimension‐ ality reduction routines measure these relevant qualities in different ways, as we will see in “In-Depth: Manifold Learning” on page 445 . As an example of this, consider the data shown in Figure 5-10",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_306"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As an example of this, consider the data shown in Figure 5-10 . Visually, it is clear that there is some structure in this data: it is drawn from a ondimensional line that is arranged in a spiral within this two-dimensional space. In a sense, you could say that this data is “intrinsically” only one dimensional, though this one-dimensional data is embedded in higher-dimensional space. A suitable dimen‐ sionality reduction model in this case would be sensitive to this nonlinear embedded structure, and be able to pull out this lower-dimensionality representation. 340 | Chapter 5: Machine Learning Figure 5-10. Example data for dimensionality reduction Figure 5-11 presents a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this. Figure 5-11. Data with a label learned via dimensionality reduction Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm did in fact detect the structure we saw by eye. As with the previous examples, the power of What Is Machine Learning? | 341 dimensionality reduction algorithms becomes clearer in higher-dimensional cases. For example, we might wish to visualize important relationships within a dataset that has 100 or 1,000 features. Visualizing 1,000-dimensional data is a challenge, and one way we can make this more manageable is to use a dimensionality reduction techni‐ que to reduce the data to two or three dimensions. Some important dimensionality reduction algorithms that we will discuss are princi‐ pal component analysis (see “In Depth: Principal Component Analysis” on page 433 ) and various manifold learning algorithms, including Isomap and locally linear embedding (see “In-Depth: Manifold Learning” on page 445 ). Summary Here we have seen a few simple examples of some of the basic types of machine learn‐ ing approaches",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_307"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Summary Here we have seen a few simple examples of some of the basic types of machine learn‐ ing approaches. Needless to say, there are a number of important practical details that we have glossed over, but I hope this section was enough to give you a basic idea of what types of problems machine learning approaches can solve. In short, we saw the following: Supervised learning Models that can predict labels based on labeled training data Classification Models that predict labels as two or more discrete categories Regression Models that predict continuous labels Unsupervised learning Models that identify structure in unlabeled data Clustering Models that detect and identify distinct groups in the data Dimensionality reduction Models that detect and identify lower-dimensional structure in highedimensional data In the following sections we will go into much greater depth within these categories, and see some more interesting examples of where these concepts can be useful. All of the figures in the preceding discussion are generated based on actual machine learning computations; the code behind them can be found in the online appendix . 342 | Chapter 5: Machine Learning Introducing Scikit-Learn There are several Python libraries that provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn , a package that provides efficient versions of a large number of common algorithms. Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you under‐ stand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward. This section provides an overview of the Scikit-Learn API; a solid understanding of these API elements will form the foundation for understanding the deeper practical discussion of machine learning algorithms and approaches in the following chapters",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_308"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will start by covering data representation in Scikit-Learn, followed by covering the Estimator API, and finally go through a more interesting example of using these tools for exploring a set of images of handwritten digits. Data Representation in Scikit-Learn Machine learning is about creating models from data: for that reason, we’ll start by discussing how data can be represented in order to be understood by the computer. The best way to think about data within Scikit-Learn is in terms of tables of data. Data as table A basic table is a two-dimensional grid of data, in which the rows represent individ‐ ual elements of the dataset, and the columns represent quantities related to each of these elements. For example, consider the Iris dataset , famously analyzed by Ronald Fisher in 1936. We can download this dataset in the form of a Pandas DataFrame using the Seaborn library : In [ 1 ]: import seaborn as sns iris = sns . load_dataset ( 'iris' ) iris . head () Out[1]: sepal_length sepal_width petal_length petal_width species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Here each row of the data refers to a single observed flower, and the number of rows is the total number of flowers in the dataset. In general, we will refer to the rows of the matrix as samples , and the number of rows as n_samples . Introducing Scikit-Learn | 343 Likewise, each column of the data refers to a particular quantitative piece of informa‐ tion that describes each sample. In general, we will refer to the columns of the matrix as features , and the number of columns as n_features . Features matrix This table layout makes clear that the information can be thought of as a twdimensional numerical array or matrix, which we will call the features matrix . By con‐ vention, this features matrix is often stored in a variable named X",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_309"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". By con‐ vention, this features matrix is often stored in a variable named X . The features matrix is assumed to be two-dimensional, with shape [n_samples, n_features] , and is most often contained in a NumPy array or a Pandas DataFrame , though some ScikiLearn models also accept SciPy sparse matrices. The samples (i.e., rows) always refer to the individual objects described by the dataset. For example, the sample might be a flower, a person, a document, an image, a sound file, a video, an astronomical object, or anything else you can describe with a set of quantitative measurements. The features (i.e., columns) always refer to the distinct observations that describe each sample in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases. Target array In addition to the feature matrix X , we also generally work with a label or target array, which by convention we will usually call y . The target array is usually one dimen‐ sional, with length n_samples , and is generally contained in a NumPy array or Pan‐ das Series . The target array may have continuous numerical values, or discrete classes/labels. While some Scikit-Learn estimators do handle multiple target values in the form of a two-dimensional [n_samples, n_targets] target array, we will pri‐ marily be working with the common case of a one-dimensional target array. Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to predict from the data : in statistical terms, it is the dependent variable. For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the species column would be considered the feature",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_310"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 344 | Chapter 5: Machine Learning With this target array in mind, we can use Seaborn (discussed earlier in “Visualiza‐ tion with Seaborn” on page 311 ) to conveniently visualize the data (see Figure 5-12 ): In [ 2 ]: % matplotlib inline import seaborn as sns ; sns . set () sns . pairplot ( iris , hue = 'species' , size = 1.5 ); Figure 5-12. A visualization of the Iris dataset For use in Scikit-Learn, we will extract the features matrix and target array from the DataFrame , which we can do using some of the Pandas DataFrame operations dis‐ cussed in Chapter 3 : In [ 3 ]: X_iris = iris . drop ( 'species' , axis = 1 ) X_iris . shape Out[3]: (150, 4) In [ 4 ]: y_iris = iris [ 'species' ] y_iris . shape Out[4]: (150,) To summarize, the expected layout of features and target values is visualized in Figure 5-13 . Introducing Scikit-Learn | 345 Figure 5-13. Scikit-Learn’s data layout With this data properly formatted, we can move on to consider the estimator API of Scikit-Learn. Scikit-Learn’s Estimator API The Scikit-Learn API is designed with the following guiding principles in mind, as outlined in the Scikit-Learn API paper : Consistency All objects share a common interface drawn from a limited set of methods, with consistent documentation. Inspection All specified parameter values are exposed as public attributes. Limited object hierarchy Only algorithms are represented by Python classes; datasets are represented in standard formats (NumPy arrays, Pandas DataFrame s, SciPy sparse matrices) and parameter names use standard Python strings. Composition Many machine learning tasks can be expressed as sequences of more fundamen‐ tal algorithms, and Scikit-Learn makes use of this wherever possible. Sensible defaults When models require user-specified parameters, the library defines an appropri‐ ate default value. 346 | Chapter 5: Machine Learning In practice, these principles make Scikit-Learn very easy to use, once the basic princi‐ ples are understood",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_311"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 346 | Chapter 5: Machine Learning In practice, these principles make Scikit-Learn very easy to use, once the basic princi‐ ples are understood. Every machine learning algorithm in Scikit-Learn is imple‐ mented via the Estimator API, which provides a consistent interface for a wide range of machine learning applications. Basics of the API Most commonly, the steps in using the Scikit-Learn estimator API are as follows (we will step through a handful of detailed examples in the sections that follow): 1. Choose a class of model by importing the appropriate estimator class from ScikiLearn. 2. Choose model hyperparameters by instantiating this class with desired values. 3. Arrange data into a features matrix and target vector following the discussion from before. 4. Fit the model to your data by calling the fit() method of the model instance. 5. Apply the model to new data: • For supervised learning, often we predict labels for unknown data using the predict() method. • For unsupervised learning, we often transform or infer properties of the data using the transform() or predict() method. We will now step through several simple examples of applying supervised and unsu‐ pervised learning methods. Supervised learning example: Simple linear regression As an example of this process, let’s consider a simple linear regression—that is, the common case of fitting a line to x , y data. We will use the following simple data for our regression example ( Figure 5-14 ): In [ 5 ]: import matplotlib.pyplot as plt import numpy as np rng = np . random . RandomState ( 42 ) x = 10 * rng . rand ( 50 ) y = 2 * x - 1 + rng . randn ( 50 ) plt . scatter ( x , y ); Introducing Scikit-Learn | 347 Figure 5-14. Data for linear regression With this data in place, we can use the recipe outlined earlier. Let’s walk through the process: 1. Choose a class of model. In Scikit-Learn, every class of model is represented by a Python class",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_312"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Let’s walk through the process: 1. Choose a class of model. In Scikit-Learn, every class of model is represented by a Python class. So, for example, if we would like to compute a simple linear regression model, we can import the linear regression class: In [ 6 ]: from sklearn.linear_model import LinearRegression Note that other, more general linear regression models exist as well; you can read more about them in the sklearn.linear_model module documentation . 2. Choose model hyperparameters. An important point is that a class of model is not the same as an instance of a model . Once we have decided on our model class, there are still some options open to us. Depending on the model class we are working with, we might need to answer one or more questions like the following: • Would we like to fit for the offset (i.e., intercept)? • Would we like the model to be normalized? • Would we like to preprocess our features to add model flexibility? • What degree of regularization would we like to use in our model? • How many model components would we like to use? 348 | Chapter 5: Machine Learning These are examples of the important choices that must be made once the model class is selected . These choices are often represented as hyperparameters , or parameters that must be set before the model is fit to data. In Scikit-Learn, we choose hyperparameters by passing values at model instantiation. We will explore how you can quantitatively motivate the choice of hyperparameters in “Hyperparameters and Model Validation” on page 359 . For our linear regression example, we can instantiate the LinearRegression class and specify that we would like to fit the intercept using the fit_inter cept hyperparameter: In [ 7 ]: model = LinearRegression ( fit_intercept = True ) model Out[7]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) Keep in mind that when the model is instantiated, the only action is the storing of these hyperparameter values",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_313"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, we have not yet applied the model to any data: the Scikit-Learn API makes very clear the distinction between choice of model and application of model to data . 3. Arrange data into a features matrix and target vector. Previously we detailed the Scikit-Learn data representation, which requires a two-dimensional features matrix and a one-dimensional target array. Here our target variable y is already in the correct form (a lengtn_samples array), but we need to massage the data x to make it a matrix of size [n_samples, n_features] . In this case, this amounts to a simple reshaping of the one-dimensional array: In [ 8 ]: X = x [:, np . newaxis ] X . shape Out[8]: (50, 1) 4. Fit the model to your data. Now it is time to apply our model to data. This can be done with the fit() method of the model: In [ 9 ]: model . fit ( X , y ) Out[9]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) This fit() command causes a number of model-dependent internal computa‐ tions to take place, and the results of these computations are stored in modespecific attributes that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the fit() process have trailing underscores; for example, in this linear model, we have the following: Introducing Scikit-Learn | 349 In [ 10 ]: model . coef_ Out[10]: array([ 1.9776566]) In [ 11 ]: model . intercept_ Out[11]: -0.90331072553111635 These two parameters represent the slope and intercept of the simple linear fit to the data. Comparing to the data definition, we see that they are very close to the input slope of 2 and intercept of –1. One question that frequently comes up regards the uncertainty in such internal model parameters. In general, Scikit-Learn does not provide tools to draw con‐ clusions from internal model parameters themselves: interpreting model parame‐ ters is much more a statistical modeling question than a machine learning question. Machine learning rather focuses on what the model predicts",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_314"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Machine learning rather focuses on what the model predicts . If you would like to dive into the meaning of fit parameters within the model, other tools are available, including the StatsModels Python package . 5. Predict labels for unknown data. Once the model is trained, the main task of supervised machine learning is to evaluate it based on what it says about new data that was not part of the training set. In Scikit-Learn, we can do this using the predict() method. For the sake of this example, our “new data” will be a grid of x values, and we will ask what y values the model predicts: In [ 12 ]: xfit = np . linspace ( - 1 , 11 ) As before, we need to coerce these x values into a [n_samples, n_features] features matrix, after which we can feed it to the model: In [ 13 ]: Xfit = xfit [:, np . newaxis ] yfit = model . predict ( Xfit ) Finally, let’s visualize the results by plotting first the raw data, and then this model fit ( Figure 5-15 ): In [ 14 ]: plt . scatter ( x , y ) plt . plot ( xfit , yfit ); Typically one evaluates the efficacy of the model by comparing its results to some known baseline, as we will see in the next example. 350 | Chapter 5: Machine Learning Figure 5-15. A simple linear regression fit to the data Supervised learning example: Iris classification Let’s take a look at another example of this process, using the Iris dataset we discussed earlier. Our question will be this: given a model trained on a portion of the Iris data, how well can we predict the remaining labels? For this task, we will use an extremely simple generative model known as Gaussian naive Bayes, which proceeds by assuming each class is drawn from an axis-aligned Gaussian distribution (see “In Depth: Naive Bayes Classification” on page 382 for more details). Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes is often a good model to use as a baseline classification, before you explore whether improvements can be found through more sophisticated models",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_315"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We would like to evaluate the model on data it has not seen before, and so we will split the data into a training set and a testing set . This could be done by hand, but it is more convenient to use the train_test_split utility function: In [ 15 ]: from sklearn.cross_validation import train_test_split Xtrain , Xtest , ytrain , ytest = train_test_split ( X_iris , y_iris , random_state = 1 ) With the data arranged, we can follow our recipe to predict the labels: In [ 16 ]: from sklearn.naive_bayes import GaussianNB # 1. choose model class model = GaussianNB () # 2. instantiate model model . fit ( Xtrain , ytrain ) # 3. fit model to data y_model = model . predict ( Xtest ) # 4. predict on new data Finally, we can use the accuracy_score utility to see the fraction of predicted labels that match their true value: Introducing Scikit-Learn | 351 In [ 17 ]: from sklearn.metrics import accuracy_score accuracy_score ( ytest , y_model ) Out[17]: 0.97368421052631582 With an accuracy topping 97%, we see that even this very naive classification algo‐ rithm is effective for this particular dataset! Unsupervised learning example: Iris dimensionality As an example of an unsupervised learning problem, let’s take a look at reducing the dimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data is four dimensional: there are four features recorded for each sample. The task of dimensionality reduction is to ask whether there is a suitable lowedimensional representation that retains the essential features of the data. Often dimensionality reduction is used as an aid to visualizing data; after all, it is much eas‐ ier to plot data in two dimensions than in four dimensions or higher! Here we will use principal component analysis (PCA; see “In Depth: Principal Com‐ ponent Analysis” on page 433 ), which is a fast linear dimensionality reduction techni‐ que. We will ask the model to return two components—that is, a two-dimensional representation of the data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_316"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will ask the model to return two components—that is, a two-dimensional representation of the data. Following the sequence of steps outlined earlier, we have: In [ 18 ]: from sklearn.decomposition import PCA # 1. Choose the model class model = PCA ( n_components = 2 ) # 2. Instantiate the model with hyperparameters model . fit ( X_iris ) # 3. Fit to data. Notice y is not specified! X_2D = model . transform ( X_iris ) # 4. Transform the data to two dimensions Now let’s plot the results. A quick way to do this is to insert the results into the origi‐ nal Iris DataFrame , and use Seaborn’s lmplot to show the results ( Figure 5-16 ): In [ 19 ]: iris [ 'PCA1' ] = X_2D [:, 0 ] iris [ 'PCA2' ] = X_2D [:, 1 ] sns . lmplot ( \"PCA1\" , \"PCA2\" , hue = 'species' , data = iris , fit_reg = False ); We see that in the two-dimensional representation, the species are fairly well separa‐ ted, even though the PCA algorithm had no knowledge of the species labels! This indicates to us that a relatively straightforward classification will probably be effective on the dataset, as we saw before. 352 | Chapter 5: Machine Learning Figure 5-16. The Iris data projected to two dimensions Unsupervised learning: Iris clustering Let’s next look at applying clustering to the Iris data. A clustering algorithm attempts to find distinct groups of data without reference to any labels. Here we will use a powerful clustering method called a Gaussian mixture model (GMM), discussed in more detail in “In Depth: Gaussian Mixture Models” on page 476 . A GMM attempts to model the data as a collection of Gaussian blobs. We can fit the Gaussian mixture model as follows: In [ 20 ]: from sklearn.mixture import GMM # 1. Choose the model class model = GMM ( n_components = 3 , covariance_type = 'full' ) # 2. Instantiate the model w/ hyperparameters model . fit ( X_iris ) # 3. Fit to data. Notice y is not specified! y_gmm = model . predict ( X_iris ) # 4",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_317"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Instantiate the model w/ hyperparameters model . fit ( X_iris ) # 3. Fit to data. Notice y is not specified! y_gmm = model . predict ( X_iris ) # 4. Determine cluster labels As before, we will add the cluster label to the Iris DataFrame and use Seaborn to plot the results ( Figure 5-17 ): In [ 21 ]: iris [ 'cluster' ] = y_gmm sns . lmplot ( \"PCA1\" , \"PCA2\" , data = iris , hue = 'species' , col = 'cluster' , fit_reg = False ); By splitting the data by cluster number, we see exactly how well the GMM algorithm has recovered the underlying label: the setosa species is separated perfectly within cluster 0, while there remains a small amount of mixing between versicolor and vir‐ ginica . This means that even without an expert to tell us the species labels of the indi‐ vidual flowers, the measurements of these flowers are distinct enough that we could automatically identify the presence of these different groups of species with a simple Introducing Scikit-Learn | 353 clustering algorithm! This sort of algorithm might further give experts in the field clues as to the relationship between the samples they are observing. Figure 5-17. k-means clusters within the Iris data Application: Exploring Handwritten Digits To demonstrate these principles on a more interesting problem, let’s consider one piece of the optical character recognition problem: the identification of handwritten digits. In the wild, this problem involves both locating and identifying characters in an image. Here we’ll take a shortcut and use Scikit-Learn’s set of preformatted digits, which is built into the library. Loading and visualizing the digits data We’ll use Scikit-Learn’s data access interface and take a look at this data: In [ 22 ]: from sklearn.datasets import load_digits digits = load_digits () digits . images . shape Out[22]: (1797, 8, 8) The images data is a three-dimensional array: 1,797 samples, each consisting of an 8×8 grid of pixels",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_318"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". images . shape Out[22]: (1797, 8, 8) The images data is a three-dimensional array: 1,797 samples, each consisting of an 8×8 grid of pixels. Let’s visualize the first hundred of these ( Figure 5-18 ): In [ 23 ]: import matplotlib.pyplot as plt fig , axes = plt . subplots ( 10 , 10 , figsize = ( 8 , 8 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i , ax in enumerate ( axes . flat ): ax . imshow ( digits . images [ i ], cmap = 'binary' , interpolation = 'nearest' ) ax . text ( 0.05 , 0.05 , str ( digits . target [ i ]), transform = ax . transAxes , color = 'green' ) 354 | Chapter 5: Machine Learning Figure 5-18. The handwritten digits data; each sample is represented by one 8×8 grid of pixels In order to work with this data within Scikit-Learn, we need a two-dimensional, [n_samples, n_features] representation. We can accomplish this by treating each pixel in the image as a feature—that is, by flattening out the pixel arrays so that we have a length-64 array of pixel values representing each digit. Additionally, we need the target array, which gives the previously determined label for each digit. These two quantities are built into the digits dataset under the data and target attributes, respectively: In [ 24 ]: X = digits . data X . shape Out[24]: (1797, 64) In [ 25 ]: y = digits . target y . shape Out[25]: (1797,) We see here that there are 1,797 samples and 64 features. Unsupervised learning: Dimensionality reduction We’d like to visualize our points within the 64-dimensional parameter space, but it’s difficult to effectively visualize points in such a high-dimensional space. Instead we’ll reduce the dimensions to 2, using an unsupervised method. Here, we’ll make use of a Introducing Scikit-Learn | 355 manifold learning algorithm called Isomap (see “In-Depth: Manifold Learning” on page 445 ), and transform the data to two dimensions: In [ 26 ]: from sklearn.manifold import Isomap iso = Isomap ( n_components = 2 ) iso . fit ( digits",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_319"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( digits . data ) data_projected = iso . transform ( digits . data ) data_projected . shape Out[26]: (1797, 2) We see that the projected data is now two-dimensional. Let’s plot this data to see if we can learn anything from its structure ( Figure 5-19 ): In [ 27 ]: plt . scatter ( data_projected [:, 0 ], data_projected [:, 1 ], c = digits . target , edgecolor = 'none' , alpha = 0.5 , cmap = plt . cm . get_cmap ( 'spectral' , 10 )) plt . colorbar ( label = 'digit label' , ticks = range ( 10 )) plt . clim ( - 0.5 , 9.5 ); Figure 5-19. An Isomap embedding of the digits data This plot gives us some good intuition into how well various numbers are separated in the larger 64-dimensional space. For example, zeros (in black) and ones (in purple) have very little overlap in parameter space. Intuitively, this makes sense: a zero is empty in the middle of the image, while a one will generally have ink in the middle. On the other hand, there seems to be a more or less continuous spectrum between ones and fours: we can understand this by realizing that some people draw ones with “hats” on them, which cause them to look similar to fours. Overall, however, the different groups appear to be fairly well separated in the param‐ eter space: this tells us that even a very straightforward supervised classification algo‐ rithm should perform suitably on this data. Let’s give it a try. 356 | Chapter 5: Machine Learning Classification on digits Let’s apply a classification algorithm to the digits. As with the Iris data previously, we will split the data into a training and test set, and fit a Gaussian naive Bayes model: In [ 28 ]: Xtrain , Xtest , ytrain , ytest = train_test_split ( X , y , random_state = 0 ) In [ 29 ]: from sklearn.naive_bayes import GaussianNB model = GaussianNB () model . fit ( Xtrain , ytrain ) y_model = model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_320"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( Xtrain , ytrain ) y_model = model . predict ( Xtest ) Now that we have predicted our model, we can gauge its accuracy by comparing the true values of the test set to the predictions: In [ 30 ]: from sklearn.metrics import accuracy_score accuracy_score ( ytest , y_model ) Out[30]: 0.83333333333333337 With even this extremely simple model, we find about 80% accuracy for classification of the digits! However, this single number doesn’t tell us where we’ve gone wrong— one nice way to do this is to use the confusion matrix , which we can compute with Scikit-Learn and plot with Seaborn ( Figure 5-20 ): In [ 31 ]: from sklearn.metrics import confusion_matrix mat = confusion_matrix ( ytest , y_model ) sns . heatmap ( mat , square = True , annot = True , cbar = False ) plt . xlabel ( 'predicted value' ) plt . ylabel ( 'true value' ); Figure 5-20. A confusion matrix showing the frequency of misclassifications by our classifier Introducing Scikit-Learn | 357 This shows us where the mislabeled points tend to be: for example, a large number of twos here are misclassified as either ones or eights. Another way to gain intuition into the characteristics of the model is to plot the inputs again, with their predicted labels. We’ll use green for correct labels, and red for incorrect labels ( Figure 5-21 ): In [ 32 ]: fig , axes = plt . subplots ( 10 , 10 , figsize = ( 8 , 8 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i , ax in enumerate ( axes . flat ): ax . imshow ( digits . images [ i ], cmap = 'binary' , interpolation = 'nearest' ) ax . text ( 0.05 , 0.05 , str ( y_model [ i ]), transform = ax . transAxes , color = 'green' if ( ytest [ i ] == y_model [ i ]) else 'red' ) Figure 5-21. Data showing correct (green) and incorrect (red) labels; for a color version of this plot, see the online appendix Examining this subset of the data, we can gain insight regarding where the algorithm might not be performing optimally",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_321"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To go beyond our 80% classification rate, we might move to a more sophisticated algorithm, such as support vector machines (see “In-Depth: Support Vector Machines” on page 405 ) or random forests (see “IDepth: Decision Trees and Random Forests” on page 421 ), or another classification approach. 358 | Chapter 5: Machine Learning Summary In this section we have covered the essential features of the Scikit-Learn data repre‐ sentation, and the estimator API. Regardless of the type of estimator, the same import/instantiate/fit/predict pattern holds. Armed with this information about the estimator API, you can explore the Scikit-Learn documentation and begin trying out various models on your data. In the next section, we will explore perhaps the most important topic in machine learning: how to select and validate your model. Hyperparameters and Model Validation In the previous section, we saw the basic recipe for applying a supervised machine learning model: 1. Choose a class of model 2. Choose model hyperparameters 3. Fit the model to the training data 4. Use the model to predict labels for new data The first two pieces of this—the choice of model and choice of hyperparameters—are perhaps the most important part of using these tools and techniques effectively. In order to make an informed choice, we need a way to validate that our model and our hyperparameters are a good fit to the data. While this may sound simple, there are some pitfalls that you must avoid to do this effectively. Thinking About Model Validation In principle, model validation is very simple: after choosing a model and its hyper‐ parameters, we can estimate how effective it is by applying it to some of the training data and comparing the prediction to the known value. The following sections first show a naive approach to model validation and why it fails, before exploring the use of holdout sets and cross-validation for more robust model evaluation",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_322"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The following sections first show a naive approach to model validation and why it fails, before exploring the use of holdout sets and cross-validation for more robust model evaluation. Model validation the wrong way Let’s demonstrate the naive approach to validation using the Iris data, which we saw in the previous section. We will start by loading the data: In [ 1 ]: from sklearn.datasets import load_iris iris = load_iris () Hyperparameters and Model Validation | 359 X = iris . data y = iris . target Next we choose a model and hyperparameters. Here we’ll use a k -neighbors classifier with n_neighbors=1 . This is a very simple and intuitive model that says “the label of an unknown point is the same as the label of its closest training point”: In [ 2 ]: from sklearn.neighbors import KNeighborsClassifier model = KNeighborsClassifier ( n_neighbors = 1 ) Then we train the model, and use it to predict labels for data we already know: In [ 3 ]: model . fit ( X , y ) y_model = model . predict ( X ) Finally, we compute the fraction of correctly labeled points: In [ 4 ]: from sklearn.metrics import accuracy_score accuracy_score ( y , y_model ) Out[4]: 1.0 We see an accuracy score of 1.0, which indicates that 100% of points were correctly labeled by our model! But is this truly measuring the expected accuracy? Have we really come upon a model that we expect to be correct 100% of the time? As you may have gathered, the answer is no. In fact, this approach contains a funda‐ mental flaw: it trains and evaluates the model on the same data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_323"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In fact, this approach contains a funda‐ mental flaw: it trains and evaluates the model on the same data . Furthermore, the nearest neighbor model is an instance-based estimator that simply stores the training data, and predicts labels by comparing new data to these stored points; except in con‐ trived cases, it will get 100% accuracy every time! Model validation the right way: Holdout sets So what can be done? We can get a better sense of a model’s performance using what’s known as a holdout set ; that is, we hold back some subset of the data from the training of the model, and then use this holdout set to check the model performance. We can do this splitting using the train_test_split utility in Scikit-Learn: In [ 5 ]: from sklearn.cross_validation import train_test_split # split the data with 50% in each set X1 , X2 , y1 , y2 = train_test_split ( X , y , random_state = 0 , train_size = 0.5 ) # fit the model on one set of data model . fit ( X1 , y1 ) # evaluate the model on the second set of data y2_model = model . predict ( X2 ) accuracy_score ( y2 , y2_model ) Out[5]: 0.90666666666666662 360 | Chapter 5: Machine Learning We see here a more reasonable result: the nearest-neighbor classifier is about 90% accurate on this holdout set. The holdout set is similar to unknown data, because the model has not “seen” it before. Model validation via cross-validation One disadvantage of using a holdout set for model validation is that we have lost a portion of our data to the model training. In the previous case, half the dataset does not contribute to the training of the model! This is not optimal, and can cause prob‐ lems—especially if the initial set of training data is small. One way to address this is to use cross-validation —that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set. Visually, it might look something like Figure 5-22 . Figure 5-22",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_324"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Visually, it might look something like Figure 5-22 . Figure 5-22. Visualization of two-fold cross-validation Here we do two validation trials, alternately using each half of the data as a holdout set. Using the split data from before, we could implement it like this: In [ 6 ]: y2_model = model . fit ( X1 , y1 ) . predict ( X2 ) y1_model = model . fit ( X2 , y2 ) . predict ( X1 ) accuracy_score ( y1 , y1_model ), accuracy_score ( y2 , y2_model ) Out[6]: (0.95999999999999996, 0.90666666666666662) What comes out are two accuracy scores, which we could combine (by, say, taking the mean) to get a better measure of the global model performance. This particular form of cross-validation is a two-fold cross-validation —one in which we have split the data into two sets and used each in turn as a validation set. We could expand on this idea to use even more trials, and more folds in the data—for example, Figure 5-23 is a visual depiction of five-fold cross-validation. Hyperparameters and Model Validation | 361 Figure 5-23. Visualization of five-fold cross-validation Here we split the data into five groups, and use each of them in turn to evaluate the model fit on the other 4/5 of the data. This would be rather tedious to do by hand, and so we can use Scikit-Learn’s cross_val_score convenience routine to do it succinctly: In [ 7 ]: from sklearn.cross_validation import cross_val_score cross_val_score ( model , X , y , cv = 5 ) Out[7]: array([ 0.96666667, 0.96666667, 0.93333333, 0.93333333, 1. ]) Repeating the validation across different subsets of the data gives us an even better idea of the performance of the algorithm. Scikit-Learn implements a number of cross-validation schemes that are useful in par‐ ticular situations; these are implemented via iterators in the cross_validation mod‐ ule. For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points; that is, we train on all points but one in each trial",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_325"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, we might wish to go to the extreme case in which our number of folds is equal to the number of data points; that is, we train on all points but one in each trial. This type of cross-validation is known as leave-one-out cross-validation, and can be used as follows: In [ 8 ]: from sklearn.cross_validation import LeaveOneOut scores = cross_val_score ( model , X , y , cv = LeaveOneOut ( len ( X ))) scores Out[8]: array([ 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 362 | Chapter 5: Machine Learning 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) Because we have 150 samples, the leave-one-out cross-validation yields scores for 150 trials, and the score indicates either successful (1.0) or unsuccessful (0.0) prediction. Taking the mean of these gives an estimate of the error rate: In [ 9 ]: scores . mean () Out[9]: 0.95999999999999996 Other cross-validation schemes can be used similarly. For a description of what is available in Scikit-Learn, use IPython to explore the sklearn.cross_validation sub‐ module, or take a look at Scikit-Learn’s online cross-validation documentation . Selecting the Best Model Now that we’ve seen the basics of validation and cross-validation, we will go into a little more depth regarding model selection and selection of hyperparameters. These issues are some of the most important aspects of the practice of machine learning, and I find that this information is often glossed over in introductory machine learn‐ ing tutorials",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_326"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These issues are some of the most important aspects of the practice of machine learning, and I find that this information is often glossed over in introductory machine learn‐ ing tutorials. Of core importance is the following question: if our estimator is underperforming, how should we move forward? There are several possible answers: • Use a more complicated/more flexible model • Use a less complicated/less flexible model • Gather more training samples • Gather more data to add features to each sample The answer to this question is often counterintuitive. In particular, sometimes using a more complicated model will give worse results, and adding more training samples may not improve your results! The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful. Hyperparameters and Model Validation | 363 The bias–variance trade-off Fundamentally, the question of “the best model” is about finding a sweet spot in the trade-off between bias and variance . Consider Figure 5-24 , which presents two regression fits to the same dataset. Figure 5-24. A high-bias and high-variance regression model It is clear that neither of these models is a particularly good fit to the data, but they fail in different ways. The model on the left attempts to find a straight-line fit through the data. Because the data are intrinsically more complicated than a straight line, the straight-line model will never be able to describe this dataset well. Such a model is said to underfit the data; that is, it does not have enough model flexibility to suitably account for all the features in the data. Another way of saying this is that the model has high bias . The model on the right attempts to fit a high-order polynomial through the data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_327"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Another way of saying this is that the model has high bias . The model on the right attempts to fit a high-order polynomial through the data. Here the model fit has enough flexibility to nearly perfectly account for the fine fea‐ tures in the data, but even though it very accurately describes the training data, its precise form seems to be more reflective of the particular noise properties of the data rather than the intrinsic properties of whatever process generated that data. Such a model is said to overfit the data; that is, it has so much model flexibility that the model ends up accounting for random errors as well as the underlying data distribu‐ tion. Another way of saying this is that the model has high variance . To look at this in another light, consider what happens if we use these two models to predict the y -value for some new data. In diagrams in Figure 5-25 , the red/lighter points indicate data that is omitted from the training set. 364 | Chapter 5: Machine Learning Figure 5-25. Training and validation scores in high-bias and high-variance models The score here is the R 2 score, or coefficient of determination , which measures how well a model performs relative to a simple mean of the target values. R 2 = 1 indicates a perfect match, R 2 = 0 indicates the model does no better than simply taking the mean of the data, and negative values mean even worse models. From the scores asso‐ ciated with these two models, we can make an observation that holds more generally: • For high-bias models, the performance of the model on the validation set is simi‐ lar to the performance on the training set. • For high-variance models, the performance of the model on the validation set is far worse than the performance on the training set. If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in Figure 5-26",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_328"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". If we imagine that we have some ability to tune the model complexity, we would expect the training score and validation score to behave as illustrated in Figure 5-26 . The diagram shown in Figure 5-26 is often called a validation curve , and we see the following essential features: • The training score is everywhere higher than the validation score. This is gener‐ ally the case: the model will be a better fit to data it has seen than to data it has not seen. • For very low model complexity (a high-bias model), the training data is underfit, which means that the model is a poor predictor both for the training data and for any previously unseen data. • For very high model complexity (a high-variance model), the training data is overfit, which means that the model predicts the training data very well, but fails for any previously unseen data. • For some intermediate value, the validation curve has a maximum. This level of complexity indicates a suitable trade-off between bias and variance. Hyperparameters and Model Validation | 365 Figure 5-26. A schematic of the relationship between model complexity, training score, and validation score The means of tuning the model complexity varies from model to model; when we discuss individual models in depth in later sections, we will see how each model allows for such tuning. Validation curves in Scikit-Learn Let’s look at an example of using cross-validation to compute the validation curve for a class of models. Here we will use a polynomial regression model: this is a generalized linear model in which the degree of the polynomial is a tunable parameter. For example, a degree-1 polynomial fits a straight line to the data; for model parameters a and b : y = ax + b A degree-3 polynomial fits a cubic curve to the data; for model parameters a , b , c , d : y = ax 3 + bx 2 + cx + d We can generalize this to any number of polynomial features. In Scikit-Learn, we can implement this with a simple linear regression combined with the polynomial pre‐ processor",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_329"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Scikit-Learn, we can implement this with a simple linear regression combined with the polynomial pre‐ processor. We will use a pipeline to string these operations together (we will discuss polynomial features and pipelines more fully in “Feature Engineering” on page 375 ): 366 | Chapter 5: Machine Learning In [ 10 ]: from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import make_pipeline def PolynomialRegression ( degree = 2 , ** kwargs ): return make_pipeline ( PolynomialFeatures ( degree ), LinearRegression ( ** kwargs )) Now let’s create some data to which we will fit our model: In [ 11 ]: import numpy as np def make_data ( N , err = 1.0 , rseed = 1 ): # randomly sample the data rng = np . random . RandomState ( rseed ) X = rng . rand ( N , 1 ) ** 2 y = 10 - 1. / ( X . ravel () + 0.1 ) if err > 0 : y += err * rng . randn ( N ) return X , y X , y = make_data ( 40 ) We can now visualize our data, along with polynomial fits of several degrees ( Figure 5-27 ): In [ 12 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn ; seaborn . set () # plot formatting X_test = np . linspace ( - 0.1 , 1.1 , 500 )[:, None ] plt . scatter ( X . ravel (), y , color = 'black' ) axis = plt . axis () for degree in [ 1 , 3 , 5 ]: y_test = PolynomialRegression ( degree ) . fit ( X , y ) . predict ( X_test ) plt . plot ( X_test . ravel (), y_test , label = 'degree={0}' . format ( degree )) plt . xlim ( - 0.1 , 1.0 ) plt . ylim ( - 2 , 12 ) plt . legend ( loc = 'best' ); The knob controlling model complexity in this case is the degree of the polynomial, which can be any non-negative integer. A useful question to answer is this: what degree of polynomial provides a suitable trade-off between bias (underfitting) and variance (overfitting)? Hyperparameters and Model Validation | 367 Figure 5-27",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_330"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Three different polynomial models fit to a dataset We can make progress in this by visualizing the validation curve for this particular data and model; we can do this straightforwardly using the validation_curve conve‐ nience routine provided by Scikit-Learn. Given a model, data, parameter name, and a range to explore, this function will automatically compute both the training score and validation score across the range ( Figure 5-28 ): In [ 13 ]: from sklearn.learning_curve import validation_curve degree = np . arange ( 0 , 21 ) train_score , val_score = validation_curve ( PolynomialRegression (), X , y , 'polynomialfeatures__degree' , degree , cv = 7 ) plt . plot ( degree , np . median ( train_score , 1 ), color = 'blue' , label = 'training score' ) plt . plot ( degree , np . median ( val_score , 1 ), color = 'red' , label = 'validation score' ) plt . legend ( loc = 'best' ) plt . ylim ( 0 , 1 ) plt . xlabel ( 'degree' ) plt . ylabel ( 'score' ); This shows precisely the qualitative behavior we expect: the training score is every‐ where higher than the validation score; the training score is monotonically improving with increased model complexity; and the validation score reaches a maximum before dropping off as the model becomes overfit. 368 | Chapter 5: Machine Learning Figure 5-28. The validation curves for the data in Figure 5-27 (cf. Figure 5-26 ) From the validation curve, we can read off that the optimal trade-off between bias and variance is found for a third-order polynomial; we can compute and display this fit over the original data as follows ( Figure 5-29 ): In [ 14 ]: plt . scatter ( X . ravel (), y ) lim = plt . axis () y_test = PolynomialRegression ( 3 ) . fit ( X , y ) . predict ( X_test ) plt . plot ( X_test . ravel (), y_test ); plt . axis ( lim ); Figure 5-29",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_331"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". scatter ( X . ravel (), y ) lim = plt . axis () y_test = PolynomialRegression ( 3 ) . fit ( X , y ) . predict ( X_test ) plt . plot ( X_test . ravel (), y_test ); plt . axis ( lim ); Figure 5-29. The cross-validated optimal model for the data in Figure 5-27 Hyperparameters and Model Validation | 369 Notice that finding this optimal model did not actually require us to compute the training score, but examining the relationship between the training score and valida‐ tion score can give us useful insight into the performance of the model. Learning Curves One important aspect of model complexity is that the optimal model will generally depend on the size of your training data. For example, let’s generate a new dataset with a factor of five more points ( Figure 5-30 ): In [ 15 ]: X2 , y2 = make_data ( 200 ) plt . scatter ( X2 . ravel (), y2 ); Figure 5-30. Data to demonstrate learning curves We will duplicate the preceding code to plot the validation curve for this larger data‐ set; for reference let’s over-plot the previous results as well ( Figure 5-31 ): In [ 16 ]: degree = np . arange ( 21 ) train_score2 , val_score2 = validation_curve ( PolynomialRegression (), X2 , y2 , 'polynomialfeatures__degree' , degree , cv = 7 ) plt . plot ( degree , np . median ( train_score2 , 1 ), color = 'blue' , label = 'training score' ) plt . plot ( degree , np . median ( val_score2 , 1 ), color = 'red' , label = 'validation score' ) plt . plot ( degree , np . median ( train_score , 1 ), color = 'blue' , alpha = 0.3 , linestyle = 'dashed' ) plt . plot ( degree , np . median ( val_score , 1 ), color = 'red' , alpha = 0.3 , linestyle = 'dashed' ) plt . legend ( loc = 'lower center' ) plt . ylim ( 0 , 1 ) 370 | Chapter 5: Machine Learning plt . xlabel ( 'degree' ) plt . ylabel ( 'score' ); Figure 5-31. Learning curves for the polynomial model fit to data in Figure 5-30 The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_332"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Learning curves for the polynomial model fit to data in Figure 5-30 The solid lines show the new results, while the fainter dashed lines show the results of the previous smaller dataset. It is clear from the validation curve that the larger data‐ set can support a much more complicated model: the peak here is probably around a degree of 6, but even a degree-20 model is not seriously overfitting the data—the vali‐ dation and training scores remain very close. Thus we see that the behavior of the validation curve has not one, but two, important inputs: the model complexity and the number of training points. It is often useful to explore the behavior of the model as a function of the number of training points, which we can do by using increasingly larger subsets of the data to fit our model. A plot of the training/validation score with respect to the size of the training set is known as a learning curve. The general behavior we would expect from a learning curve is this: • A model of a given complexity will overfit a small dataset: this means the training score will be relatively high, while the validation score will be relatively low. • A model of a given complexity will underfit a large dataset: this means that the training score will decrease, but the validation score will increase. • A model will never, except by chance, give a better score to the validation set than the training set: this means the curves should keep getting closer together but never cross. Hyperparameters and Model Validation | 371 With these features in mind, we would expect a learning curve to look qualitatively like that shown in Figure 5-32 . Figure 5-32. Schematic showing the typical interpretation of learning curves The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_333"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Schematic showing the typical interpretation of learning curves The notable feature of the learning curve is the convergence to a particular score as the number of training samples grows. In particular, once you have enough points that a particular model has converged, adding more training data will not help you! The only way to increase model performance in this case is to use another (often more complex) model. Learning curves in Scikit-Learn Scikit-Learn offers a convenient utility for computing such learning curves from your models; here we will compute a learning curve for our original dataset with a seconorder polynomial model and a ninth-order polynomial ( Figure 5-33 ): In [ 17 ]: from sklearn.learning_curve import learning_curve fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) fig . subplots_adjust ( left = 0.0625 , right = 0.95 , wspace = 0.1 ) for i , degree in enumerate ([ 2 , 9 ]): N , train_lc , val_lc = learning_curve ( PolynomialRegression ( degree ), X , y , cv = 7 , train_sizes = np . linspace ( 0.3 , 1 , 25 )) ax [ i ] . plot ( N , np . mean ( train_lc , 1 ), color = 'blue' , label = 'training score' ) ax [ i ] . plot ( N , np . mean ( val_lc , 1 ), color = 'red' , label = 'validation score' ) ax [ i ] . hlines ( np . mean ([ train_lc [ - 1 ], val_lc [ - 1 ]]), N [ 0 ], N [ - 1 ], color = 'gray' , linestyle = 'dashed' ) 372 | Chapter 5: Machine Learning ax [ i ] . set_ylim ( 0 , 1 ) ax [ i ] . set_xlim ( N [ 0 ], N [ - 1 ]) ax [ i ] . set_xlabel ( 'training size' ) ax [ i ] . set_ylabel ( 'score' ) ax [ i ] . set_title ( 'degree = {0}' . format ( degree ), size = 14 ) ax [ i ] . legend ( loc = 'best' ) Figure 5-33. Learning curves for a low-complexity model (left) and a high-complexity model (right) This is a valuable diagnostic, because it gives us a visual depiction of how our model responds to increasing training data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_334"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, when your learning curve has already converged (i.e., when the training and validation curves are already close to each other), adding more training data will not significantly improve the fit! This situa‐ tion is seen in the left panel, with the learning curve for the degree-2 model. The only way to increase the converged score is to use a different (usually more com‐ plicated) model. We see this in the right panel: by moving to a much more compli‐ cated model, we increase the score of convergence (indicated by the dashed line), but at the expense of higher model variance (indicated by the difference between the training and validation scores). If we were to add even more data points, the learning curve for the more complicated model would eventually converge. Plotting a learning curve for your particular choice of model and dataset can help you to make this type of decision about how to move forward in improving your analysis. Validation in Practice: Grid Search The preceding discussion is meant to give you some intuition into the trade-off between bias and variance, and its dependence on model complexity and training set size. In practice, models generally have more than one knob to turn, and thus plots of validation and learning curves change from lines to multidimensional surfaces. In these cases, such visualizations are difficult and we would rather simply find the par‐ ticular model that maximizes the validation score. Hyperparameters and Model Validation | 373 Scikit-Learn provides automated tools to do this in the grid_search module. Here is an example of using grid search to find the optimal polynomial model. We will explore a three-dimensional grid of model features—namely, the polynomial degree, the flag telling us whether to fit the intercept, and the flag telling us whether to nor‐ malize the problem. We can set this up using Scikit-Learn’s GridSearchCV metestimator: In [ 18 ]: from sklearn.grid_search import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_335"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We can set this up using Scikit-Learn’s GridSearchCV metestimator: In [ 18 ]: from sklearn.grid_search import GridSearchCV param_grid = { 'polynomialfeatures__degree' : np . arange ( 21 ), 'linearregression__fit_intercept' : [ True , False ], 'linearregression__normalize' : [ True , False ]} grid = GridSearchCV ( PolynomialRegression (), param_grid , cv = 7 ) Notice that like a normal estimator, this has not yet been applied to any data. Calling the fit() method will fit the model at each grid point, keeping track of the scores along the way: In [ 19 ]: grid . fit ( X , y ); Now that this is fit, we can ask for the best parameters as follows: In [ 20 ]: grid . best_params_ Out[20]: {'linearregression__fit_intercept': False, 'linearregression__normalize': True, 'polynomialfeatures__degree': 4} Finally, if we wish, we can use the best model and show the fit to our data using code from before ( Figure 5-34 ): In [ 21 ]: model = grid . best_estimator_ plt . scatter ( X . ravel (), y ) lim = plt . axis () y_test = model . fit ( X , y ) . predict ( X_test ) plt . plot ( X_test . ravel (), y_test , hold = True ); plt . axis ( lim ); The grid search provides many more options, including the ability to specify a cus‐ tom scoring function, to parallelize the computations, to do randomized searches, and more. For information, see the examples in “In-Depth: Kernel Density Estima‐ tion” on page 491 and “Application: A Face Detection Pipeline” on page 506 , or refer to Scikit-Learn’s grid search documentation . 374 | Chapter 5: Machine Learning Figure 5-34. The best-fit model determined via an automatic grid-search Summary In this section, we have begun to explore the concept of model validation and hyper‐ parameter optimization, focusing on intuitive aspects of the bias–variance trade-off and how it comes into play when fitting models to data. In particular, we found that the use of a validation set or cross-validation approach is vital when tuning parame‐ ters in order to avoid overfitting for more complex/flexible models",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_336"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, we found that the use of a validation set or cross-validation approach is vital when tuning parame‐ ters in order to avoid overfitting for more complex/flexible models. In later sections, we will discuss the details of particularly useful models, and throughout will talk about what tuning is available for these models and how these free parameters affect model complexity. Keep the lessons of this section in mind as you read on and learn about these machine learning approaches! Feature Engineering The previous sections outline the fundamental ideas of machine learning, but all of the examples assume that you have numerical data in a tidy, [n_samples, n_fea tures] format. In the real world, data rarely comes in such a form. With this in mind, one of the more important steps in using machine learning in practice is feature engi‐ neering —that is, taking whatever information you have about your problem and turning it into numbers that you can use to build your feature matrix. In this section, we will cover a few common examples of feature engineering tasks: features for representing categorical data , features for representing text , and features for representing images . Additionally, we will discuss derived features for increasing model complexity and imputation of missing data. Often this process is known as vec‐ torization , as it involves converting arbitrary data into well-behaved vectors. Feature Engineering | 375 Categorical Features One common type of non-numerical data is categorical data. For example, imagine you are exploring some data on housing prices, and along with numerical features like “price” and “rooms,” you also have “neighborhood” information",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_337"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, imagine you are exploring some data on housing prices, and along with numerical features like “price” and “rooms,” you also have “neighborhood” information. For example, your data might look something like this: In [ 1 ]: data = [ { 'price' : 850000 , 'rooms' : 4 , 'neighborhood' : 'Queen Anne' }, { 'price' : 700000 , 'rooms' : 3 , 'neighborhood' : 'Fremont' }, { 'price' : 650000 , 'rooms' : 3 , 'neighborhood' : 'Wallingford' }, { 'price' : 600000 , 'rooms' : 2 , 'neighborhood' : 'Fremont' } ] You might be tempted to encode this data with a straightforward numerical mapping: In [ 2 ]: { 'Queen Anne' : 1 , 'Fremont' : 2 , 'Wallingford' : 3 }; It turns out that this is not generally a useful approach in Scikit-Learn: the package’s models make the fundamental assumption that numerical features reflect algebraic quantities. Thus such a mapping would imply, for example, that Queen Anne < Fre‐ mont < Wallingford , or even that Wallingford - Queen Anne = Fremont , which (niche demographic jokes aside) does not make much sense. In this case, one proven technique is to use one-hot encoding , which effectively creates extra columns indicating the presence or absence of a category with a value of 1 or 0, respectively. When your data comes as a list of dictionaries, Scikit-Learn’s DictVector izer will do this for you: In [ 3 ]: from sklearn.feature_extraction import DictVectorizer vec = DictVectorizer ( sparse = False , dtype = int ) vec . fit_transform ( data ) Out[3]: array([[ 0, 1, 0, 850000, 4], [ 1, 0, 0, 700000, 3], [ 0, 0, 1, 650000, 3], [ 1, 0, 0, 600000, 2]], dtype=int64) Notice that the neighborhood column has been expanded into three separate columns, representing the three neighborhood labels, and that each row has a 1 in the column associated with its neighborhood. With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model. To see the meaning of each column, you can inspect the feature names: In [ 4 ]: vec",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_338"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". With these categorical features thus encoded, you can proceed as normal with fitting a Scikit-Learn model. To see the meaning of each column, you can inspect the feature names: In [ 4 ]: vec . get_feature_names () Out[4]: ['neighborhood=Fremont', 'neighborhood=Queen Anne', 'neighborhood=Wallingford', 'price', 'rooms'] 376 | Chapter 5: Machine Learning There is one clear disadvantage of this approach: if your category has many possible values, this can greatly increase the size of your dataset. However, because the enco‐ ded data contains mostly zeros, a sparse output can be a very efficient solution: In [ 5 ]: vec = DictVectorizer ( sparse = True , dtype = int ) vec . fit_transform ( data ) Out[5]: <4x5 sparse matrix of type '<class 'numpy.int64'>' with 12 stored elements in Compressed Sparse Row format> Many (though not yet all) of the Scikit-Learn estimators accept such sparse inputs when fitting and evaluating models. sklearn.preprocessing.OneHotEncoder and sklearn.feature_extraction.FeatureHasher are two additional tools that ScikiLearn includes to support this type of encoding. Text Features Another common need in feature engineering is to convert text to a set of representa‐ tive numerical values. For example, most automatic mining of social media data relies on some form of encoding the text as numbers. One of the simplest methods of encoding data is by word counts : you take each snippet of text, count the occurrences of each word within it, and put the results in a table. For example, consider the following set of three phrases: In [ 6 ]: sample = [ 'problem of evil' , 'evil queen' , 'horizon problem' ] For a vectorization of this data based on word count, we could construct a column representing the word “problem,” the word “evil,” the word “horizon,” and so on. While doing this by hand would be possible, we can avoid the tedium by using ScikiLearn’s CountVectorizer : In [ 7 ]: from sklearn.feature_extraction.text import CountVectorizer vec = CountVectorizer () X = vec",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_339"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit_transform ( sample ) X Out[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>' with 7 stored elements in Compressed Sparse Row format> The result is a sparse matrix recording the number of times each word appears; it is easier to inspect if we convert this to a DataFrame with labeled columns: In [ 8 ]: import pandas as pd pd . DataFrame ( X . toarray (), columns = vec . get_feature_names ()) Feature Engineering | 377 Out[8]: evil horizon of problem queen 0 1 0 1 1 0 1 1 0 0 0 1 2 0 1 0 1 0 There are some issues with this approach, however: the raw word counts lead to fea‐ tures that put too much weight on words that appear very frequently, and this can be suboptimal in some classification algorithms. One approach to fix this is known as term frequency–inverse document frequency ( TF–IDF ), which weights the word counts by a measure of how often they appear in the documents. The syntax for computing these features is similar to the previous example: In [ 9 ]: from sklearn.feature_extraction.text import TfidfVectorizer vec = TfidfVectorizer () X = vec . fit_transform ( sample ) pd . DataFrame ( X . toarray (), columns = vec . get_feature_names ()) Out[9]: evil horizon of problem queen 0 0.517856 0.000000 0.680919 0.517856 0.000000 1 0.605349 0.000000 0.000000 0.000000 0.795961 2 0.000000 0.795961 0.000000 0.605349 0.000000 For an example of using TF–IDF in a classification problem, see “In Depth: Naive Bayes Classification” on page 382 . Image Features Another common need is to suitably encode images for machine learning analysis. The simplest approach is what we used for the digits data in “Introducing ScikiLearn” on page 343 : simply using the pixel values themselves. But depending on the application, such approaches may not be optimal. A comprehensive summary of feature extraction techniques for images is well beyond the scope of this section, but you can find excellent implementations of many of the standard approaches in the Scikit-Image project",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_340"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For one example of using ScikiLearn and Scikit-Image together, see “Application: A Face Detection Pipeline” on page 506 . Derived Features Another useful type of feature is one that is mathematically derived from some input features. We saw an example of this in “Hyperparameters and Model Validation” on page 359 when we constructed polynomial features from our input data. We saw that we could convert a linear regression into a polynomial regression not by changing the model, but by transforming the input! This is sometimes known as basis function regression , and is explored further in “In Depth: Linear Regression” on page 390 . 378 | Chapter 5: Machine Learning For example, this data clearly cannot be well described by a straight line ( Figure 5-35 ): In [ 10 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt x = np . array ([ 1 , 2 , 3 , 4 , 5 ]) y = np . array ([ 4 , 2 , 1 , 3 , 7 ]) plt . scatter ( x , y ); Figure 5-35. Data that is not well described by a straight line Still, we can fit a line to the data using LinearRegression and get the optimal result ( Figure 5-36 ): In [ 11 ]: from sklearn.linear_model import LinearRegression X = x [:, np . newaxis ] model = LinearRegression () . fit ( X , y ) yfit = model . predict ( X ) plt . scatter ( x , y ) plt . plot ( x , yfit ); Figure 5-36. A poor straight-line fit Feature Engineering | 379 It’s clear that we need a more sophisticated model to describe the relationship between x and y . We can do this by transforming the data, adding extra columns of features to drive more flexibility in the model. For example, we can add polynomial features to the data this way: In [ 12 ]: from sklearn.preprocessing import PolynomialFeatures poly = PolynomialFeatures ( degree = 3 , include_bias = False ) X2 = poly . fit_transform ( X ) print ( X2 ) [[ 1. 1. 1.] [ 2. 4. 8.] [ 3. 9. 27.] [ 4. 16. 64.] [ 5. 25. 125.]] The derived feature matrix has one column representing x , and a second column rep‐ resenting x 2 , and a third column representing x 3",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_341"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 1. 1.] [ 2. 4. 8.] [ 3. 9. 27.] [ 4. 16. 64.] [ 5. 25. 125.]] The derived feature matrix has one column representing x , and a second column rep‐ resenting x 2 , and a third column representing x 3 . Computing a linear regression on this expanded input gives a much closer fit to our data ( Figure 5-37 ): In [ 13 ]: model = LinearRegression () . fit ( X2 , y ) yfit = model . predict ( X2 ) plt . scatter ( x , y ) plt . plot ( x , yfit ); Figure 5-37. A linear fit to polynomial features derived from the data This idea of improving a model not by changing the model, but by transforming the inputs, is fundamental to many of the more powerful machine learning methods. We explore this idea further in “In Depth: Linear Regression” on page 390 in the context of basis function regression . More generally, this is one motivational path to the pow‐ erful set of techniques known as kernel methods , which we will explore in “In-Depth: Support Vector Machines” on page 405 . 380 | Chapter 5: Machine Learning Imputation of Missing Data Another common need in feature engineering is handling missing data. We discussed the handling of missing data in DataFrame s in “Handling Missing Data” on page 119 , and saw that often the NaN value is used to mark missing values. For example, we might have a dataset that looks like this: In [ 14 ]: from numpy import nan X = np . array ([[ nan , 0 , 3 ], [ 3 , 7 , 9 ], [ 3 , 5 , 2 ], [ 4 , nan , 6 ], [ 8 , 8 , 1 ]]) y = np . array ([ 14 , 16 , - 1 , 8 , - 5 ]) When applying a typical machine learning model to such data, we will need to first replace such missing data with some appropriate fill value. This is known as imputa‐ tion of missing values, and strategies range from simple (e.g., replacing missing values with the mean of the column) to sophisticated (e.g., using matrix completion or a robust model to handle such data). The sophisticated approaches tend to be very application-specific, and we won’t dive into them here",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_342"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The sophisticated approaches tend to be very application-specific, and we won’t dive into them here. For a baseline imputation approach, using the mean, median, or most frequent value, Scikit-Learn provides the Imputer class: In [ 15 ]: from sklearn.preprocessing import Imputer imp = Imputer ( strategy = 'mean' ) X2 = imp . fit_transform ( X ) X2 Out[15]: array([[ 4.5, 0. , 3. ], [ 3. , 7. , 9. ], [ 3. , 5. , 2. ], [ 4. , 5. , 6. ], [ 8. , 8. , 1. ]]) We see that in the resulting data, the two missing values have been replaced with the mean of the remaining values in the column. This imputed data can then be fed directly into, for example, a LinearRegression estimator: In [ 16 ]: model = LinearRegression () . fit ( X2 , y ) model . predict ( X2 ) Out[16]: array([ 13.14869292, 14.3784627 , -1.15539732, 10.96606197, -5.33782027]) Feature Pipelines With any of the preceding examples, it can quickly become tedious to do the transfor‐ mations by hand, especially if you wish to string together multiple steps. For example, we might want a processing pipeline that looks something like this: Feature Engineering | 381 1. Impute missing values using the mean 2. Transform features to quadratic 3. Fit a linear regression To streamline this type of processing pipeline, Scikit-Learn provides a pipeline object, which can be used as follows: In [ 17 ]: from sklearn.pipeline import make_pipeline model = make_pipeline ( Imputer ( strategy = 'mean' ), PolynomialFeatures ( degree = 2 ), LinearRegression ()) This pipeline looks and acts like a standard Scikit-Learn object, and will apply all the specified steps to any input data. In [ 18 ]: model . fit ( X , y ) # X with missing values, from above print ( y ) print ( model . predict ( X )) [14 16 -1 8 -5] [ 14. 16. -1. 8. -5.] All the steps of the model are applied automatically",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_343"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( X , y ) # X with missing values, from above print ( y ) print ( model . predict ( X )) [14 16 -1 8 -5] [ 14. 16. -1. 8. -5.] All the steps of the model are applied automatically. Notice that for the simplicity of this demonstration, we’ve applied the model to the data it was trained on; this is why it was able to perfectly predict the result (refer back to “Hyperparameters and Model Validation” on page 359 for further discussion of this). For some examples of Scikit-Learn pipelines in action, see the following section on naive Bayes classification as well as “In Depth: Linear Regression” on page 390 and “In-Depth: Support Vector Machines” on page 405 . In Depth: Naive Bayes Classification The previous four sections have given a general overview of the concepts of machine learning. In this section and the ones that follow, we will be taking a closer look at several specific algorithms for supervised and unsupervised learning, starting here with naive Bayes classification. Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets. Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-andirty baseline for a classification problem. This section will focus on an intuitive explanation of how naive Bayes classifiers work, followed by a couple examples of them in action on some datasets. 382 | Chapter 5: Machine Learning Bayesian Classification Naive Bayes classifiers are built on Bayesian classification methods. These rely on Bayes’s theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities. In Bayesian classification, we’re interested in finding the probability of a label given some observed features, which we can write as P L features",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_344"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Bayesian classification, we’re interested in finding the probability of a label given some observed features, which we can write as P L features . Bayes’s theorem tells us how to express this in terms of quantities we can compute more directly: P L features = P features L P L P features If we are trying to decide between two labels—let’s call them L 1 and L 2 —then one way to make this decision is to compute the ratio of the posterior probabilities for each label: P L 1 features P L 1 features = P features L 1 P features L 2 P L 1 P L 2 All we need now is some model by which we can compute P features L i for each label. Such a model is called a generative model because it specifies the hypothetical random process that generates the data. Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier. The general ver‐ sion of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model. This is where the “naive” in “naive Bayes” comes in: if we make very naive assump‐ tions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification. Different types of naive Bayes classifiers rest on different naive assumptions about the data, and we will examine a few of these in the following sections. We begin with the standard imports: In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt import seaborn as sns ; sns . set () Gaussian Naive Bayes Perhaps the easiest naive Bayes classifier to understand is Gaussian naive Bayes. In this classifier, the assumption is that data from each label is drawn from a simple Gaus‐ sian distribution",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_345"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In this classifier, the assumption is that data from each label is drawn from a simple Gaus‐ sian distribution . Imagine that you have the following data ( Figure 5-38 ): In Depth: Naive Bayes Classification | 383 In [ 2 ]: from sklearn.datasets import make_blobs X , y = make_blobs ( 100 , 2 , centers = 2 , random_state = 2 , cluster_std = 1.5 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'RdBu' ); Figure 5-38. Data for Gaussian naive Bayes classification One extremely fast way to create a simple model is to assume that the data is described by a Gaussian distribution with no covariance between dimensions. We can fit this model by simply finding the mean and standard deviation of the points within each label, which is all you need to define such a distribution. The result of this naive Gaussian assumption is shown in Figure 5-39 . Figure 5-39. Visualization of the Gaussian naive Bayes model 384 | Chapter 5: Machine Learning The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses. With this generative model in place for each class, we have a simple recipe to compute the likelihood P features L 1 for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point. This procedure is implemented in Scikit-Learn’s sklearn.naive_bayes.GaussianNB estimator: In [ 3 ]: from sklearn.naive_bayes import GaussianNB model = GaussianNB () model . fit ( X , y ); Now let’s generate some new data and predict the label: In [ 4 ]: rng = np . random . RandomState ( 0 ) Xnew = [ - 6 , - 14 ] + [ 14 , 18 ] * rng . rand ( 2000 , 2 ) ynew = model . predict ( Xnew ) Now we can plot this new data to get an idea of where the decision boundary is ( Figure 5-40 ): In [ 5 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'RdBu' ) lim = plt . axis () plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ], c = ynew , s = 20 , cmap = 'RdBu' , alpha = 0.1 ) plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_346"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'RdBu' ) lim = plt . axis () plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ], c = ynew , s = 20 , cmap = 'RdBu' , alpha = 0.1 ) plt . axis ( lim ); Figure 5-40. Visualization of the Gaussian naive Bayes classification We see a slightly curved boundary in the classifications—in general, the boundary in Gaussian naive Bayes is quadratic. A nice piece of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the predict_proba method: In Depth: Naive Bayes Classification | 385 In [ 6 ]: yprob = model . predict_proba ( Xnew ) yprob [ - 8 :] . round ( 2 ) Out[6]: array([[ 0.89, 0.11], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 1. , 0. ], [ 0. , 1. ], [ 0.15, 0.85]]) The columns give the posterior probabilities of the first and second label, respectively. If you are looking for estimates of uncertainty in your classification, Bayesian approaches like this can be a useful approach. Of course, the final classification will only be as good as the model assumptions that lead to it, which is why Gaussian naive Bayes often does not produce very good results. Still, in many cases—especially as the number of features becomes large—this assumption is not detrimental enough to prevent Gaussian naive Bayes from being a useful method. Multinomial Naive Bayes The Gaussian assumption just described is by no means the only simple assumption that could be used to specify the generative distribution for each label. Another useful example is multinomial naive Bayes, where the features are assumed to be generated from a simple multinomial distribution. The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_347"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The idea is precisely the same as before, except that instead of modeling the data dis‐ tribution with the best-fit Gaussian, we model the data distribution with a best-fit multinomial distribution. Example: Classifying text One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified. We discussed the extraction of such features from text in “Feature Engi‐ neering” on page 375 ; here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories. Let’s download the data and take a look at the target names: In [ 7 ]: from sklearn.datasets import fetch_20newsgroups 386 | Chapter 5: Machine Learning data = fetch_20newsgroups () data . target_names Out[7]: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] For simplicity, we will select just a few of these categories, and download the training and testing set: In [ 8 ]: categories = [ 'talk.religion.misc' , 'soc.religion.christian' , 'sci.space' , 'comp.graphics' ] train = fetch_20newsgroups ( subset = 'train' , categories = categories ) test = fetch_20newsgroups ( subset = 'test' , categories = categories ) Here is a representative entry from the data: In [ 9 ]: print ( train",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_348"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". data [ 5 ]) From: dmcgee@uluhe.soest.hawaii.edu (Don McGee) Subject: Federal Hearing Originator: dmcgee@uluhe Organization: School of Ocean and Earth Science and Technology Distribution: usa Lines: 10 Fact or rumor? Madalyn Murray O'Hare an atheist who eliminated the use of the bible reading and prayer in public schools 15 years ago is now going to appear before the FCC with a petition to stop the reading of the Gospel on the airways of America. And she is also campaigning to remove Christmas programs, songs, etc from the public schools. If it is true then mail to Federal Communications Commission 1919 H Street Washington DC 20054 expressing your opposition to her request. Reference Petition number 2493. In Depth: Naive Bayes Classification | 387 In order to use this data for machine learning, we need to be able to convert the con‐ tent of each string into a vector of numbers. For this we will use the TF–IDF vector‐ izer (discussed in “Feature Engineering” on page 375 ), and create a pipeline that attaches it to a multinomial naive Bayes classifier: In [ 10 ]: from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import make_pipeline model = make_pipeline ( TfidfVectorizer (), MultinomialNB ()) With this pipeline, we can apply the model to the training data, and predict labels for the test data: In [ 11 ]: model . fit ( train . data , train . target ) labels = model . predict ( test . data ) Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. For example, here is the confusion matrix between the true and predicted labels for the test data ( Figure 5-41 ): In [ 12 ]: from sklearn.metrics import confusion_matrix mat = confusion_matrix ( test . target , labels ) sns . heatmap ( mat . T , square = True , annot = True , fmt = 'd' , cbar = False , xticklabels = train . target_names , yticklabels = train . target_names ) plt . xlabel ( 'true label' ) plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_349"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". heatmap ( mat . T , square = True , annot = True , fmt = 'd' , cbar = False , xticklabels = train . target_names , yticklabels = train . target_names ) plt . xlabel ( 'true label' ) plt . ylabel ( 'predicted label' ); Figure 5-41. Confusion matrix for the multinomial naive Bayes text classifier 388 | Chapter 5: Machine Learning Evidently, even this very simple classifier can successfully separate space talk from computer talk, but it gets confused between talk about religion and talk about Chris‐ tianity. This is perhaps an expected area of confusion! The very cool thing here is that we now have the tools to determine the category for any string, using the predict() method of this pipeline. Here’s a quick utility func‐ tion that will return the prediction for a single string: In [ 13 ]: def predict_category ( s , train = train , model = model ): pred = model . predict ([ s ]) return train . target_names [ pred [ 0 ]] Let’s try it out: In [ 14 ]: predict_category ( 'sending a payload to the ISS' ) Out[14]: 'sci.space' In [ 15 ]: predict_category ( 'discussing islam vs atheism' ) Out[15]: 'soc.religion.christian' In [ 16 ]: predict_category ( 'determining the screen resolution' ) Out[16]: 'comp.graphics' Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking. Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective. When to Use Naive Bayes Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_350"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". When to Use Naive Bayes Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model. That said, they have several advantages: • They are extremely fast for both training and prediction • They provide straightforward probabilistic prediction • They are often very easily interpretable • They have very few (if any) tunable parameters These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification. If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem. If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform. Naive Bayes classifiers tend to perform especially well in one of the following situations: In Depth: Naive Bayes Classification | 389 • When the naive assumptions actually match the data (very rare in practice) • For very well-separated categories, when model complexity is less important • For very high-dimensional data, when model complexity is less important The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in every single dimension to be close overall). This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information. For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful. In Depth: Linear Regression Just as naive Bayes (discussed earlier in “In Depth: Naive Bayes Classification” on page 382 ) is a good starting point for classification tasks, linear regression models are a good starting point for regression tasks",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_351"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Such models are popular because they can be fit very quickly, and are very interpretable. You are probably familiar with the sim‐ plest form of a linear regression model (i.e., fitting a straight line to data), but such models can be extended to model more complicated data behavior. In this section we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before moving on to see how linear models can be generalized to account for more complicated patterns in data. We begin with the stan‐ dard imports: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () import numpy as np Simple Linear Regression We will start with the most familiar linear regression, a straight-line fit to data. A straight-line fit is a model of the form y = ax + b where a is commonly known as the slope , and b is commonly known as the intercept . Consider the following data, which is scattered about a line with a slope of 2 and an intercept of –5 ( Figure 5-42 ): In [ 2 ]: rng = np . random . RandomState ( 1 ) x = 10 * rng . rand ( 50 ) y = 2 * x - 5 + rng . randn ( 50 ) plt . scatter ( x , y ); 390 | Chapter 5: Machine Learning Figure 5-42. Data for linear regression We can use Scikit-Learn’s LinearRegression estimator to fit this data and construct the best-fit line ( Figure 5-43 ): In [ 3 ]: from sklearn.linear_model import LinearRegression model = LinearRegression ( fit_intercept = True ) model . fit ( x [:, np . newaxis ], y ) xfit = np . linspace ( 0 , 10 , 1000 ) yfit = model . predict ( xfit [:, np . newaxis ]) plt . scatter ( x , y ) plt . plot ( xfit , yfit ); Figure 5-43. A linear regression model In Depth: Linear Regression | 391 The slope and intercept of the data are contained in the model’s fit parameters, which in Scikit-Learn are always marked by a trailing underscore. Here the relevant parame‐ ters are coef_ and intercept_ : In [ 4 ]: print ( \"Model slope: \" , model . coef_ [ 0 ]) print ( \"Model intercept:\" , model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_352"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Here the relevant parame‐ ters are coef_ and intercept_ : In [ 4 ]: print ( \"Model slope: \" , model . coef_ [ 0 ]) print ( \"Model intercept:\" , model . intercept_ ) Model slope: 2.02720881036 Model intercept: -4.99857708555 We see that the results are very close to the inputs, as we might hope. The LinearRegression estimator is much more capable than this, however—in addi‐ tion to simple straight-line fits, it can also handle multidimensional linear models of the form: y = a 0 + a 1 x 1 + a 2 x 2 + ⋯ where there are multiple x values. Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions. The multidimensional nature of such regressions makes them more difficult to visu‐ alize, but we can see one of these fits in action by building some example data, using NumPy’s matrix multiplication operator: In [ 5 ]: rng = np . random . RandomState ( 1 ) X = 10 * rng . rand ( 100 , 3 ) y = 0.5 + np . dot ( X , [ 1.5 , - 2. , 1. ]) model . fit ( X , y ) print ( model . intercept_ ) print ( model . coef_ ) 0.5 [ 1.5 -2. 1. ] Here the y data is constructed from three random x values, and the linear regression recovers the coefficients used to construct the data. In this way, we can use the single LinearRegression estimator to fit lines, planes, or hyperplanes to our data. It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well. Basis Function Regression One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to basis functions . We have seen one ver‐ sion of this before, in the PolynomialRegression pipeline used in “Hyperparameters 392 | Chapter 5: Machine Learning and Model Validation” on page 359 and “Feature Engineering” on page 375",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_353"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The idea is to take our multidimensional linear model: y = a 0 + a 1 x 1 + a 2 x 2 + a 3 x 3 + ⋯ and build the x 1 , x 2 , x 3 , and so on from our single-dimensional input x . That is, we let x n = f n x , where f n is some function that transforms our data. For example, if f n x = x n , our model becomes a polynomial regression: y = a 0 + a 1 x + a 2 x 2 + a 3 x 3 + ⋯ Notice that this is still a linear model —the linearity refers to the fact that the coeffi‐ cients a n never multiply or divide each other. What we have effectively done is taken our one-dimensional x values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between x and y . Polynomial basis functions This polynomial projection is useful enough that it is built into Scikit-Learn, using the PolynomialFeatures transformer: In [ 6 ]: from sklearn.preprocessing import PolynomialFeatures x = np . array ([ 2 , 3 , 4 ]) poly = PolynomialFeatures ( 3 , include_bias = False ) poly . fit_transform ( x [:, None ]) Out[6]: array([[ 2., 4., 8.], [ 3., 9., 27.], [ 4., 16., 64.]]) We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value. This new, highedimensional data representation can then be plugged into a linear regression. As we saw in “Feature Engineering” on page 375 , the cleanest way to accomplish this is to use a pipeline. Let’s make a 7th-degree polynomial model in this way: In [ 7 ]: from sklearn.pipeline import make_pipeline poly_model = make_pipeline ( PolynomialFeatures ( 7 ), LinearRegression ()) With this transform in place, we can use the linear model to fit much more compli‐ cated relationships between x and y . For example, here is a sine wave with noise ( Figure 5-44 ): In Depth: Linear Regression | 393 In [ 8 ]: rng = np . random . RandomState ( 1 ) x = 10 * rng . rand ( 50 ) y = np . sin ( x ) + 0.1 * rng . randn ( 50 ) poly_model . fit ( x [:, np . newaxis ], y ) yfit = poly_model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_354"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". random . RandomState ( 1 ) x = 10 * rng . rand ( 50 ) y = np . sin ( x ) + 0.1 * rng . randn ( 50 ) poly_model . fit ( x [:, np . newaxis ], y ) yfit = poly_model . predict ( xfit [:, np . newaxis ]) plt . scatter ( x , y ) plt . plot ( xfit , yfit ); Figure 5-44. A linear polynomial fit to nonlinear training data Our linear model, through the use of 7th-order polynomial basis functions, can pro‐ vide an excellent fit to this nonlinear data! Gaussian basis functions Of course, other basis functions are possible. For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases. The result might look something like Figure 5-45 . 394 | Chapter 5: Machine Learning Figure 5-45. A Gaussian basis function fit to nonlinear data The shaded regions in the plot shown in Figure 5-45 are the scaled basis functions, and when added together they reproduce the smooth curve through the data. These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in Figure 5-46 (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn’s source is a good way to see how they can be created): In [ 9 ]: from sklearn.base import BaseEstimator , TransformerMixin class GaussianFeatures ( BaseEstimator , TransformerMixin ): \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\" def __init__ ( self , N , width_factor = 2.0 ): self . N = N self . width_factor = width_factor @staticmethod def _gauss_basis ( x , y , width , axis = None ): arg = ( x - y ) / width return np . exp ( - 0.5 * np . sum ( arg ** 2 , axis )) def fit ( self , X , y = None ): # create N centers spread along the data range self . centers_ = np . linspace ( X . min (), X . max (), self . N ) self . width_ = self . width_factor * ( self . centers_ [ 1 ] - self . centers_ [ 0 ]) return self def transform ( self , X ): In Depth: Linear Regression | 395 return self . _gauss_basis ( X [:, :, np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_355"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". width_ = self . width_factor * ( self . centers_ [ 1 ] - self . centers_ [ 0 ]) return self def transform ( self , X ): In Depth: Linear Regression | 395 return self . _gauss_basis ( X [:, :, np . newaxis ], self . centers_ , self . width_ , axis = 1 ) gauss_model = make_pipeline ( GaussianFeatures ( 20 ), LinearRegression ()) gauss_model . fit ( x [:, np . newaxis ], y ) yfit = gauss_model . predict ( xfit [:, np . newaxis ]) plt . scatter ( x , y ) plt . plot ( xfit , yfit ) plt . xlim ( 0 , 10 ); Figure 5-46. A Gaussian basis function fit computed with a custom transformer We put this example here just to make clear that there is nothing magic about poly‐ nomial basis functions: if you have some sort of intuition into the generating process of your data that makes you think one basis or another might be appropriate, you can use them as well. Regularization The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to overfitting (refer back to “Hyper‐ parameters and Model Validation” on page 359 for a discussion of this). For example, if we choose too many Gaussian basis functions, we end up with results that don’t look so good ( Figure 5-47 ): In [ 10 ]: model = make_pipeline ( GaussianFeatures ( 30 ), LinearRegression ()) model . fit ( x [:, np . newaxis ], y ) plt . scatter ( x , y ) plt . plot ( xfit , model . predict ( xfit [:, np . newaxis ])) 396 | Chapter 5: Machine Learning plt . xlim ( 0 , 10 ) plt . ylim ( - 1.5 , 1.5 ); Figure 5-47. An overly complex basis function model that overfits the data With the data projected to the 30-dimensional basis, the model has far too much flex‐ ibility and goes to extreme values between locations where it is constrained by data. We can see the reason for this if we plot the coefficients of the Gaussian bases with respect to their locations ( Figure 5-48 ): In [ 11 ]: def basis_plot ( model , title = None ): fig , ax = plt . subplots ( 2 , sharex = True ) model . fit ( x [:, np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_356"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( 2 , sharex = True ) model . fit ( x [:, np . newaxis ], y ) ax [ 0 ] . scatter ( x , y ) ax [ 0 ] . plot ( xfit , model . predict ( xfit [:, np . newaxis ])) ax [ 0 ] . set ( xlabel = 'x' , ylabel = 'y' , ylim = ( - 1.5 , 1.5 )) if title : ax [ 0 ] . set_title ( title ) ax [ 1 ] . plot ( model . steps [ 0 ][ 1 ] . centers_ , model . steps [ 1 ][ 1 ] . coef_ ) ax [ 1 ] . set ( xlabel = 'basis location' , ylabel = 'coefficient' , xlim = ( 0 , 10 )) model = make_pipeline ( GaussianFeatures ( 30 ), LinearRegression ()) basis_plot ( model ) In Depth: Linear Regression | 397 Figure 5-48. The coefficients of the Gaussian bases in the overly complex model The lower panel in Figure 5-48 shows the amplitude of the basis function at each location. This is typical overfitting behavior when basis functions overlap: the coeffi‐ cients of adjacent basis functions blow up and cancel each other out. We know that such behavior is problematic, and it would be nice if we could limit such spikes explicitly in the model by penalizing large values of the model parameters. Such a penalty is known as regularization , and comes in several forms. Ridge regression ( L 2 regularization) Perhaps the most common form of regularization is known as ridge regression or L 2 regularization , sometimes also called Tikhonov regularization . This proceeds by penal‐ izing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be: P = α ∑ n = 1 N θ n 2 where α is a free parameter that controls the strength of the penalty. This type of penalized model is built into Scikit-Learn with the Ridge estimator ( Figure 5-49 ): In [ 12 ]: from sklearn.linear_model import Ridge model = make_pipeline ( GaussianFeatures ( 30 ), Ridge ( alpha = 0.1 )) basis_plot ( model , title = 'Ridge Regression' ) 398 | Chapter 5: Machine Learning Figure 5-49",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_357"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Ridge ( L 2 ) regularization applied to the overly complex model (compare to Figure 5-48 ) The α parameter is essentially a knob controlling the complexity of the resulting model. In the limit α 0, we recover the standard linear regression result; in the limit α ∞, all model responses will be suppressed. One advantage of ridge regres‐ sion in particular is that it can be computed very efficiently—at hardly more compu‐ tational cost than the original linear regression model. Lasso regularization ( L 1 ) Another very common type of regularization is known as lasso, and involves penaliz‐ ing the sum of absolute values (1-norms) of regression coefficients: P = α ∑ n = 1 N θ n Though this is conceptually very similar to ridge regression, the results can differ sur‐ prisingly: for example, due to geometric reasons lasso regression tends to favor sparse models where possible; that is, it preferentially sets model coefficients to exactly zero. We can see this behavior in duplicating the plot shown in Figure 5-49 , but using L1- normalized coefficients ( Figure 5-50 ): In [ 13 ]: from sklearn.linear_model import Lasso model = make_pipeline ( GaussianFeatures ( 30 ), Lasso ( alpha = 0.001 )) basis_plot ( model , title = 'Lasso Regression' ) In Depth: Linear Regression | 399 Figure 5-50. Lasso ( L 1 ) regularization applied to the overly complex model (compare to Figure 5-48 ) With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions. As with ridge regularization, the α parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation (refer back to “Hyperparameters and Model Validation” on page 359 for a discussion of this). Example: Predicting Bicycle Traffic As an example, let’s take a look at whether we can predict the number of bicycle trips across Seattle’s Fremont Bridge based on weather, season, and other factors",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_358"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Example: Predicting Bicycle Traffic As an example, let’s take a look at whether we can predict the number of bicycle trips across Seattle’s Fremont Bridge based on weather, season, and other factors. We have seen this data already in “Working with Time Series” on page 188 . In this section, we will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor. Fortunately, the NOAA makes available their daily weather station data (I used station ID USW00024233) and we can easily use Pandas to join the two data sources. We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day. In particular, this is an example of how the tools of Scikit-Learn can be used in a stat‐ istical modeling framework, in which the parameters of the model are assumed to have interpretable meaning. As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models. 400 | Chapter 5: Machine Learning Let’s start by loading the two datasets, indexing by date: In [ 14 ]: import pandas as pd counts = pd . read_csv ( 'fremont_hourly.csv' , index_col = 'Date' , parse_dates = True ) weather = pd . read_csv ( '599021.csv' , index_col = 'DATE' , parse_dates = True ) Next we will compute the total daily bicycle traffic, and put this in its own DataFrame : In [ 15 ]: daily = counts . resample ( 'd' , how = 'sum' ) daily [ 'Total' ] = daily",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_359"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". resample ( 'd' , how = 'sum' ) daily [ 'Total' ] = daily . sum ( axis = 1 ) daily = daily [[ 'Total' ]] # remove other columns We saw previously that the patterns of use generally vary from day to day; let’s account for this in our data by adding binary columns that indicate the day of the week: In [ 16 ]: days = [ 'Mon' , 'Tue' , 'Wed' , 'Thu' , 'Fri' , 'Sat' , 'Sun' ] for i in range ( 7 ): daily [ days [ i ]] = ( daily . index . dayofweek == i ) . astype ( float ) Similarly, we might expect riders to behave differently on holidays; let’s add an indica‐ tor of this as well: In [ 17 ]: from pandas.tseries.holiday import USFederalHolidayCalendar cal = USFederalHolidayCalendar () holidays = cal . holidays ( '2012' , '2016' ) daily = daily . join ( pd . Series ( 1 , index = holidays , name = 'holiday' )) daily [ 'holiday' ] . fillna ( 0 , inplace = True ) We also might suspect that the hours of daylight would affect how many people ride; let’s use the standard astronomical calculation to add this information ( Figure 5-51 ): In [ 18 ]: def hours_of_daylight ( date , axis = 23.44 , latitude = 47.61 ): \"\"\"Compute the hours of daylight for the given date\"\"\" days = ( date - pd . datetime ( 2000 , 12 , 21 )) . days m = ( 1. - np . tan ( np . radians ( latitude )) * np . tan ( np . radians ( axis ) * np . cos ( days * 2 * np . pi / 365.25 ))) return 24. * np . degrees ( np . arccos ( 1 - np . clip ( m , 0 , 2 ))) / 180. daily [ 'daylight_hrs' ] = list ( map ( hours_of_daylight , daily . index )) daily [[ 'daylight_hrs' ]] . plot (); In Depth: Linear Regression | 401 Figure 5-51. Visualization of hours of daylight in Seattle We can also add the average temperature and total precipitation to the data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_360"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". plot (); In Depth: Linear Regression | 401 Figure 5-51. Visualization of hours of daylight in Seattle We can also add the average temperature and total precipitation to the data. In addi‐ tion to the inches of precipitation, let’s add a flag that indicates whether a day is dry (has zero precipitation): In [ 19 ]: # temperatures are in 1/10 deg C; convert to C weather [ 'TMIN' ] /= 10 weather [ 'TMAX' ] /= 10 weather [ 'Temp (C)' ] = 0.5 * ( weather [ 'TMIN' ] + weather [ 'TMAX' ]) # precip is in 1/10 mm; convert to inches weather [ 'PRCP' ] /= 254 weather [ 'dry day' ] = ( weather [ 'PRCP' ] == 0 ) . astype ( int ) daily = daily . join ( weather [[ 'PRCP' , 'Temp (C)' , 'dry day' ]]) Finally, let’s add a counter that increases from day 1, and measures how many years have passed. This will let us measure any observed annual increase or decrease in daily crossings: In [ 20 ]: daily [ 'annual' ] = ( daily . index - daily . index [ 0 ]) . days / 365. Now our data is in order, and we can take a look at it: In [ 21 ]: daily . head () Out[21]: Total Mon Tue Wed Thu Fri Sat Sun holiday daylight_hrs \\\\ Date 2012-10-03 3521 0 0 1 0 0 0 0 0 11.277359 2012-10-04 3475 0 0 0 1 0 0 0 0 11.219142 2012-10-05 3148 0 0 0 0 1 0 0 0 11.161038 2012-10-06 2006 0 0 0 0 0 1 0 0 11.103056 402 | Chapter 5: Machine Learning 2012-10-07 2142 0 0 0 0 0 0 1 0 11.045208 PRCP Temp (C) dry day annual Date 2012-10-03 0 13.35 1 0.000000 2012-10-04 0 13.60 1 0.002740 2012-10-05 0 15.30 1 0.005479 2012-10-06 0 15.85 1 0.008219 2012-10-07 0 15.85 1 0.010959 With this in place, we can choose the columns to use, and fit a linear regression model to our data. We will set fit_intercept = False , because the daily flags essen‐ tially operate as their own day-specific intercepts: In [ 22 ]: column_names = [ 'Mon' , 'Tue' , 'Wed' , 'Thu' , 'Fri' , 'Sat' , 'Sun' , 'holiday' , 'daylight_hrs' , 'PRCP' , 'dry day' , 'Temp (C)' , 'annual' ] X = daily [ column_names ] y = daily [ 'Total' ] model = LinearRegression ( fit_intercept = False ) model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_361"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( X , y ) daily [ 'predicted' ] = model . predict ( X ) Finally, we can compare the total and predicted bicycle traffic visually ( Figure 5-52 ): In [ 23 ]: daily [[ 'Total' , 'predicted' ]] . plot ( alpha = 0.5 ); Figure 5-52. Our model’s prediction of bicycle traffic It is evident that we have missed some key features, especially during the summer time. Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have In Depth: Linear Regression | 403 failed to take into account (e.g., perhaps people ride less at both high and low temper‐ atures). Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count: In [ 24 ]: params = pd . Series ( model . coef_ , index = X . columns ) params Out[24]: Mon 503.797330 Tue 612.088879 Wed 591.611292 Thu 481.250377 Fri 176.838999 Sat -1104.321406 Sun -1134.610322 holiday -1187.212688 daylight_hrs 128.873251 PRCP -665.185105 dry day 546.185613 Temp (C) 65.194390 annual 27.865349 dtype: float64 These numbers are difficult to interpret without some measure of their uncertainty. We can compute these uncertainties quickly using bootstrap resamplings of the data: In [ 25 ]: from sklearn.utils import resample np . random . seed ( 1 ) err = np . std ([ model . fit ( * resample ( X , y )) . coef_ for i in range ( 1000 )], 0 ) With these errors estimated, let’s again look at the results: In [ 26 ]: print ( pd . DataFrame ({ 'effect' : params . round ( 0 ), 'error' : err . round ( 0 )})) effect error Mon 504 85 Tue 612 82 Wed 592 82 Thu 481 85 Fri 177 81 Sat -1104 79 Sun -1135 82 holiday -1187 164 daylight_hrs 129 9 PRCP -665 62 dry day 546 33 Temp (C) 65 4 annual 28 18 We first see that there is a relatively stable trend in the weekly baseline: there are many more riders on weekdays than on weekends and holidays",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_362"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We see that for each 404 | Chapter 5: Machine Learning additional hour of daylight, 129 ± 9 more people choose to ride; a temperature increase of one degree Celsius encourages 65 ± 4 people to grab their bicycle; a dry day means an average of 546 ± 33 more riders; and each inch of precipitation means 665 ± 62 more people leave their bike at home. Once all these effects are accounted for, we see a modest increase of 28 ± 18 new daily riders each year. Our model is almost certainly missing some relevant information. For example, non‐ linear effects (such as effects of precipitation and cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model. Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday’s numbers, or the effect of an unexpected sunny day after a streak of rainy days). These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish! In-Depth: Support Vector Machines Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in classification problems. We begin with the standard imports: In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt from scipy import stats # use Seaborn plotting defaults import seaborn as sns ; sns . set () Motivating Support Vector Machines As part of our discussion of Bayesian classification (see “In Depth: Naive Bayes Clas‐ sification” on page 382 ), we learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically deter‐ mine labels for new points",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_363"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That was an example of generative classification ; here we will consider instead discriminative classification : rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other. As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated ( Figure 5-53 ): In-Depth: Support Vector Machines | 405 In [ 2 ]: from sklearn.datasets.samples_generator import make_blobs X , y = make_blobs ( n_samples = 50 , centers = 2 , random_state = 0 , cluster_std = 0.60 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ); Figure 5-53. Simple data for classification A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. For two-dimensional data like that shown here, this is a task we could do by hand. But immediately we see a problem: there is more than one possible dividing line that can perfectly discrimi‐ nate between the two classes! We can draw them as follows ( Figure 5-54 ): In [ 3 ]: xfit = np . linspace ( - 1 , 3.5 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) plt . plot ([ 0.6 ], [ 2.1 ], 'x' , color = 'red' , markeredgewidth = 2 , markersize = 10 ) for m , b in [( 1 , 0.65 ), ( 0.5 , 1.6 ), ( - 0.2 , 2.9 )]: plt . plot ( xfit , m * xfit + b , '-k' ) plt . xlim ( - 1 , 3.5 ); 406 | Chapter 5: Machine Learning Figure 5-54. Three perfect linear discriminative classifiers for our data These are three very different separators that, nevertheless, perfectly discriminate between these samples. Depending on which you choose, a new data point (e.g., the one marked by the “X” in Figure 5-54 ) will be assigned a different label! Evidently our simple intuition of “drawing a line between classes” is not enough, and we need to think a bit deeper",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_364"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Support Vector Machines: Maximizing the Margin Support vector machines offer one way to improve on this. The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a margin of some width, up to the nearest point. Here is an example of how this might look ( Figure 5-55 ): In [ 4 ]: xfit = np . linspace ( - 1 , 3.5 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) for m , b , d in [( 1 , 0.65 , 0.33 ), ( 0.5 , 1.6 , 0.55 ), ( - 0.2 , 2.9 , 0.2 )]: yfit = m * xfit + b plt . plot ( xfit , yfit , '-k' ) plt . fill_between ( xfit , yfit - d , yfit + d , edgecolor = 'none' , color = '#AAAAAA' , alpha = 0.4 ) plt . xlim ( - 1 , 3.5 ); In-Depth: Support Vector Machines | 407 Figure 5-55. Visualization of “margins” within discriminative classifiers In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model. Support vector machines are an example of such a max‐ imum margin estimator. Fitting a support vector machine Let’s see the result of an actual fit to this data: we will use Scikit-Learn’s support vector classifier to train an SVM model on this data. For the time being, we will use a linear kernel and set the C parameter to a very large number (we’ll discuss the meaning of these in more depth momentarily): In [ 5 ]: from sklearn.svm import SVC # \"Support vector classifier\" model = SVC ( kernel = 'linear' , C = 1E10 ) model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_365"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( X , y ) Out[5]: SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) To better visualize what’s happening here, let’s create a quick convenience function that will plot SVM decision boundaries for us ( Figure 5-56 ): In [ 6 ]: def plot_svc_decision_function ( model , ax = None , plot_support = True ): \"\"\"Plot the decision function for a two-dimensional SVC\"\"\" if ax is None : ax = plt . gca () xlim = ax . get_xlim () ylim = ax . get_ylim () # create grid to evaluate model x = np . linspace ( xlim [ 0 ], xlim [ 1 ], 30 ) 408 | Chapter 5: Machine Learning y = np . linspace ( ylim [ 0 ], ylim [ 1 ], 30 ) Y , X = np . meshgrid ( y , x ) xy = np . vstack ([ X . ravel (), Y . ravel ()]) . T P = model . decision_function ( xy ) . reshape ( X . shape ) # plot decision boundary and margins ax . contour ( X , Y , P , colors = 'k' , levels = [ - 1 , 0 , 1 ], alpha = 0.5 , linestyles = [ '--' , '-' , '--' ]) # plot support vectors if plot_support : ax . scatter ( model . support_vectors_ [:, 0 ], model . support_vectors_ [:, 1 ], s = 300 , linewidth = 1 , facecolors = 'none' ); ax . set_xlim ( xlim ) ax . set_ylim ( ylim ) In [ 7 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) plot_svc_decision_function ( model ); Figure 5-56. A support vector machine classifier fit to the data, with margins (dashed lines) and support vectors (circles) shown This is the dividing line that maximizes the margin between the two sets of points. Notice that a few of the training points just touch the margin; they are indicated by the black circles in Figure 5-56 . These points are the pivotal elements of this fit, and are known as the support vectors , and give the algorithm its name. In Scikit-Learn, the identity of these points is stored in the support_vectors_ attribute of the classifier: In [ 8 ]: model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_366"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Scikit-Learn, the identity of these points is stored in the support_vectors_ attribute of the classifier: In [ 8 ]: model . support_vectors_ Out[8]: array([[ 0.44359863, 3.11530945], [ 2.33812285, 3.43116792], [ 2.06156753, 1.96918596]]) In-Depth: Support Vector Machines | 409 A key to this classifier’s success is that for the fit, only the position of the support vec‐ tors matters; any points further from the margin that are on the correct side do not modify the fit! Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin. We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset ( Figure 5-57 ): In [ 9 ]: def plot_svm ( N = 10 , ax = None ): X , y = make_blobs ( n_samples = 200 , centers = 2 , random_state = 0 , cluster_std = 0.60 ) X = X [: N ] y = y [: N ] model = SVC ( kernel = 'linear' , C = 1E10 ) model . fit ( X , y ) ax = ax or plt . gca () ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) ax . set_xlim ( - 1 , 4 ) ax . set_ylim ( - 1 , 6 ) plot_svc_decision_function ( model , ax ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) fig . subplots_adjust ( left = 0.0625 , right = 0.95 , wspace = 0.1 ) for axi , N in zip ( ax , [ 60 , 120 ]): plot_svm ( N , axi ) axi . set_title ( 'N = {0}' . format ( N )) Figure 5-57. The influence of new training points on the SVM model In the left panel, we see the model and the support vectors for 60 training points. In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel. This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_367"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model. 410 | Chapter 5: Machine Learning If you are running this notebook live, you can use IPython’s interactive widgets to view this feature of the SVM model interactively ( Figure 5-58 ): In [ 10 ]: from ipywidgets import interact , fixed interact ( plot_svm , N = [ 10 , 200 ], ax = fixed ( None )); Figure 5-58. The first frame of the interactive SVM visualization (see the online appen‐ dix for the full version) Beyond linear boundaries: Kernel SVM Where SVM becomes extremely powerful is when it is combined with kernels . We have seen a version of kernels before, in the basis function regressions of “In Depth: Linear Regression” on page 390 . There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier. In SVM models, we can use a version of the same idea. To motivate the need for ker‐ nels, let’s look at some data that is not linearly separable ( Figure 5-59 ): In [ 11 ]: from sklearn.datasets.samples_generator import make_circles X , y = make_circles ( 100 , factor =. 1 , noise =. 1 ) clf = SVC ( kernel = 'linear' ) . fit ( X , y ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) plot_svc_decision_function ( clf , plot_support = False ); In-Depth: Support Vector Machines | 411 Figure 5-59. A linear classifier performs poorly for nonlinear boundaries It is clear that no linear discrimination will ever be able to separate this data. But we can draw a lesson from the basis function regressions in “In Depth: Linear Regres‐ sion” on page 390 , and think about how we might project the data into a higher dimension such that a linear separator would be sufficient. For example, one simple projection we could use would be to compute a radial basis function centered on the middle clump: In [ 12 ]: r = np . exp ( - ( X ** 2 )",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_368"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, one simple projection we could use would be to compute a radial basis function centered on the middle clump: In [ 12 ]: r = np . exp ( - ( X ** 2 ) . sum ( 1 )) We can visualize this extra data dimension using a three-dimensional plot—if you are running this notebook live, you will be able to use the sliders to rotate the plot ( Figure 5-60 ): In [ 13 ]: from mpl_toolkits import mplot3d def plot_3D ( elev = 30 , azim = 30 , X = X , y = y ): ax = plt . subplot ( projection = '3d' ) ax . scatter3D ( X [:, 0 ], X [:, 1 ], r , c = y , s = 50 , cmap = 'autumn' ) ax . view_init ( elev = elev , azim = azim ) ax . set_xlabel ( 'x' ) ax . set_ylabel ( 'y' ) ax . set_zlabel ( 'r' ) interact ( plot_3D , elev = [ - 90 , 90 ], azip = ( - 180 , 180 ), X = fixed ( X ), y = fixed ( y )); 412 | Chapter 5: Machine Learning Figure 5-60. A third dimension added to the data allows for linear separation We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, r =0.7. Here we had to choose and carefully tune our projection; if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results. In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use. One strategy to this end is to compute a basis function centered at every point in the dataset, and let the SVM algorithm sift through the results. This type of basis function transformation is known as a kernel transformation , as it is based on a similarity rela‐ tionship (or kernel) between each pair of points. A potential problem with this strategy—projecting N points into N dimensions—is that it might become very computationally intensive as N grows large",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_369"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A potential problem with this strategy—projecting N points into N dimensions—is that it might become very computationally intensive as N grows large. However, because of a neat little procedure known as the kernel trick , a fit on kernetransformed data can be done implicitly—that is, without ever building the full N - dimensional representation of the kernel projection! This kernel trick is built into the SVM, and is one of the reasons the method is so powerful. In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the kernel model hyperparameter ( Figure 5-61 ): In [ 14 ]: clf = SVC ( kernel = 'rbf' , C = 1E6 ) clf . fit ( X , y ) Out[14]: SVC(C=1000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) In-Depth: Support Vector Machines | 413 In [ 15 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) plot_svc_decision_function ( clf ) plt . scatter ( clf . support_vectors_ [:, 0 ], clf . support_vectors_ [:, 1 ], s = 300 , lw = 1 , facecolors = 'none' ); Figure 5-61. Kernel SVM fit to the data Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary. This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used. Tuning the SVM: Softening margins Our discussion so far has centered on very clean datasets, in which a perfect decision boundary exists. But what if your data has some amount of overlap? For example, you may have data like this ( Figure 5-62 ): In [ 16 ]: X , y = make_blobs ( n_samples = 100 , centers = 2 , random_state = 0 , cluster_std = 1.2 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ); 414 | Chapter 5: Machine Learning Figure 5-62",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_370"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ); 414 | Chapter 5: Machine Learning Figure 5-62. Data with some level of overlap To handle this case, the SVM implementation has a bit of a fudge-factor that “softens” the margin; that is, it allows some of the points to creep into the margin if that allows a better fit. The hardness of the margin is controlled by a tuning parameter, most often known as C . For very large C , the margin is hard, and points cannot lie in it. For smaller C , the margin is softer, and can grow to encompass some points. The plot shown in Figure 5-63 gives a visual picture of how a changing C parameter affects the final fit, via the softening of the margin: In [ 17 ]: X , y = make_blobs ( n_samples = 100 , centers = 2 , random_state = 0 , cluster_std = 0.8 ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) fig . subplots_adjust ( left = 0.0625 , right = 0.95 , wspace = 0.1 ) for axi , C in zip ( ax , [ 10.0 , 0.1 ]): model = SVC ( kernel = 'linear' , C = C ) . fit ( X , y ) axi . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'autumn' ) plot_svc_decision_function ( model , axi ) axi . scatter ( model . support_vectors_ [:, 0 ], model . support_vectors_ [:, 1 ], s = 300 , lw = 1 , facecolors = 'none' ); axi . set_title ( 'C = {0:.1f}' . format ( C ), size = 14 ) In-Depth: Support Vector Machines | 415 Figure 5-63. The effect of the C parameter on the support vector fit The optimal value of the C parameter will depend on your dataset, and should be tuned via cross-validation or a similar procedure (refer back to “Hyperparameters and Model Validation” on page 359 for further information). Example: Face Recognition As an example of support vector machines in action, let’s take a look at the facial rec‐ ognition problem. We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_371"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures. A fetcher for the dataset is built into Scikit-Learn: In [ 18 ]: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people ( min_faces_per_person = 60 ) print ( faces . target_names ) print ( faces . images . shape ) ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair'] (1348, 62, 47) Let’s plot a few of these faces to see what we’re working with ( Figure 5-64 ): In [ 19 ]: fig , ax = plt . subplots ( 3 , 5 ) for i , axi in enumerate ( ax . flat ): axi . imshow ( faces . images [ i ], cmap = 'bone' ) axi . set ( xticks = [], yticks = [], xlabel = faces . target_names [ faces . target [ i ]]) 416 | Chapter 5: Machine Learning Figure 5-64. Examples from the Labeled Faces in the Wild dataset Each image contains [62×47] or nearly 3,000 pixels. We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use a principal com‐ ponent analysis (see “In Depth: Principal Component Analysis” on page 433 ) to extract 150 fundamental components to feed into our support vector machine classi‐ fier. We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline: In [ 20 ]: from sklearn.svm import SVC from sklearn.decomposition import RandomizedPCA from sklearn.pipeline import make_pipeline pca = RandomizedPCA ( n_components = 150 , whiten = True , random_state = 42 ) svc = SVC ( kernel = 'rbf' , class_weight = 'balanced' ) model = make_pipeline ( pca , svc ) For the sake of testing our classifier output, we will split the data into a training and testing set: In [ 21 ]: from sklearn.cross_validation import train_test_split Xtrain , Xtest , ytrain , ytest = train_test_split ( faces . data , faces",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_372"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". data , faces . target , random_state = 42 ) Finally, we can use a grid search cross-validation to explore combinations of parame‐ ters. Here we will adjust C (which controls the margin hardness) and gamma (which controls the size of the radial basis function kernel), and determine the best model: In [ 22 ]: from sklearn.grid_search import GridSearchCV param_grid = { 'svc__C' : [ 1 , 5 , 10 , 50 ], 'svc__gamma' : [ 0.0001 , 0.0005 , 0.001 , 0.005 ]} grid = GridSearchCV ( model , param_grid ) In-Depth: Support Vector Machines | 417 % time grid . fit ( Xtrain , ytrain ) print ( grid . best_params_ ) CPU times: user 47.8 s, sys: 4.08 s, total: 51.8 s Wall time: 26 s {'svc__gamma': 0.001, 'svc__C': 10} The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum. Now with this cross-validated model, we can predict the labels for the test data, which the model has not yet seen: In [ 23 ]: model = grid . best_estimator_ yfit = model . predict ( Xtest ) Let’s take a look at a few of the test images along with their predicted values ( Figure 5-65 ): In [ 24 ]: fig , ax = plt . subplots ( 4 , 6 ) for i , axi in enumerate ( ax . flat ): axi . imshow ( Xtest [ i ] . reshape ( 62 , 47 ), cmap = 'bone' ) axi . set ( xticks = [], yticks = []) axi . set_ylabel ( faces . target_names [ yfit [ i ]] . split ()[ - 1 ], color = 'black' if yfit [ i ] == ytest [ i ] else 'red' ) fig . suptitle ( 'Predicted Names; Incorrect Labels in Red' , size = 14 ); Figure 5-65. Labels predicted by our model Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s face in the bottom row was mislabeled as Blair). We can get a better sense of our esti‐ mator’s performance using the classification report, which lists recovery statistics label by label: 418 | Chapter 5: Machine Learning In [ 25 ]: from sklearn.metrics import classification_report print ( classification_report ( ytest , yfit , target_names = faces",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_373"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". target_names )) precision recall f1-score support Ariel Sharon 0.65 0.73 0.69 15 Colin Powell 0.81 0.87 0.84 68 Donald Rumsfeld 0.75 0.87 0.81 31 George W Bush 0.93 0.83 0.88 126 Gerhard Schroeder 0.86 0.78 0.82 23 Hugo Chavez 0.93 0.70 0.80 20 Junichiro Koizumi 0.80 1.00 0.89 12 Tony Blair 0.83 0.93 0.88 42 avg / total 0.85 0.85 0.85 337 We might also display the confusion matrix between these classes ( Figure 5-66 ): In [ 26 ]: from sklearn.metrics import confusion_matrix mat = confusion_matrix ( ytest , yfit ) sns . heatmap ( mat . T , square = True , annot = True , fmt = 'd' , cbar = False , xticklabels = faces . target_names , yticklabels = faces . target_names ) plt . xlabel ( 'true label' ) plt . ylabel ( 'predicted label' ); Figure 5-66. Confusion matrix for the faces data This helps us get a sense of which labels are likely to be confused by the estimator. In-Depth: Support Vector Machines | 419 For a real-world facial recognition task, in which the photos do not come precropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation. For this kind of application, one good option is to make use of OpenCV , which among other things, includes pre‐ trained implementations of state-of-the-art feature extraction tools for images in gen‐ eral and faces in particular. Support Vector Machine Summary We have seen here a brief intuitive introduction to the principals behind support vec‐ tor machines. These methods are a powerful classification method for a number of reasons: • Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory. • Once the model is trained, the prediction phase is very fast",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_374"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Once the model is trained, the prediction phase is very fast. • Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is a challenging regime for other algorithms. • Their integration with kernel methods makes them very versatile, able to adapt to many types of data. However, SVMs have several disadvantages as well: • The scaling with the number of samples N is � N 3 at worst, or � N 2 for effi‐ cient implementations. For large numbers of training samples, this computa‐ tional cost can be prohibitive. • The results are strongly dependent on a suitable choice for the softening parame‐ ter C . This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size. • The results do not have a direct probabilistic interpretation. This can be estima‐ ted via an internal cross-validation (see the probability parameter of SVC ), but this extra estimation is costly. With those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs. Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results. 420 | Chapter 5: Machine Learning In-Depth: Decision Trees and Random Forests Previously we have looked in depth at a simple generative classifier (naive Bayes; see “In Depth: Naive Bayes Classification” on page 382 ) and a powerful discriminative classifier (support vector machines; see “In-Depth: Support Vector Machines” on page 405 ). Here we’ll take a look at motivating another powerful algorithm—a non‐ parametric algorithm called random forests . Random forests are an example of an ensemble method, a method that relies on aggregating the results of an ensemble of simpler estimators",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_375"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Random forests are an example of an ensemble method, a method that relies on aggregating the results of an ensemble of simpler estimators. The somewhat surprising result with such ensemble methods is that the sum can be greater than the parts; that is, a majority vote among a number of estimators can end up being better than any of the individual estimators doing the voting! We will see examples of this in the following sections. We begin with the stan‐ dard imports: In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt import seaborn as sns ; sns . set () Motivating Random Forests: Decision Trees Random forests are an example of an ensemble learner built on decision trees. For this reason we’ll start by discussing decision trees themselves. Decision trees are extremely intuitive ways to classify or label objects: you simply ask a series of questions designed to zero in on the classification. For example, if you wanted to build a decision tree to classify an animal you come across while on a hike, you might construct the one shown in Figure 5-67 . Figure 5-67. An example of a binary decision tree In-Depth: Decision Trees and Random Forests | 421 The binary splitting makes this extremely efficient: in a well-constructed tree, each question will cut the number of options by approximately half, very quickly narrow‐ ing the options even among a large number of classes. The trick, of course, comes in deciding which questions to ask at each step. In machine learning implementations of decision trees, the questions generally take the form of axis-aligned splits in the data; that is, each node in the tree splits the data into two groups using a cutoff value within one of the features. Let’s now take a look at an example. Creating a decision tree Consider the following two-dimensional data, which has one of four class labels ( Figure 5-68 ): In [ 2 ]: from sklearn.datasets import make_blobs X , y = make_blobs ( n_samples = 300 , centers = 4 , random_state = 0 , cluster_std = 1.0 ) plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_376"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 50 , cmap = 'rainbow' ); Figure 5-68. Data for the decision tree classifier A simple decision tree built on this data will iteratively split the data along one or the other axis according to some quantitative criterion, and at each level assign the label of the new region according to a majority vote of points within it. Figure 5-69 presents a visualization of the first four levels of a decision tree classifier for this data. 422 | Chapter 5: Machine Learning Figure 5-69. Visualization of how the decision tree splits the data Notice that after the first split, every point in the upper branch remains unchanged, so there is no need to further subdivide this branch. Except for nodes that contain all of one color, at each level every region is again split along one of the two features. This process of fitting a decision tree to our data can be done in Scikit-Learn with the DecisionTreeClassifier estimator: In [ 3 ]: from sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier () . fit ( X , y ) Let’s write a quick utility function to help us visualize the output of the classifier: In [ 4 ]: def visualize_classifier ( model , X , y , ax = None , cmap = 'rainbow' ): ax = ax or plt . gca () # Plot the training points ax . scatter ( X [:, 0 ], X [:, 1 ], c = y , s = 30 , cmap = cmap , clim = ( y . min (), y . max ()), zorder = 3 ) ax . axis ( 'tight' ) ax . axis ( 'off' ) xlim = ax . get_xlim () ylim = ax . get_ylim () # fit the estimator model . fit ( X , y ) xx , yy = np . meshgrid ( np . linspace ( * xlim , num = 200 ), np . linspace ( * ylim , num = 200 )) Z = model . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) . reshape ( xx . shape ) # Create a color plot with the results n_classes = len ( np . unique ( y )) contours = ax . contourf ( xx , yy , Z , alpha = 0.3 , levels = np . arange ( n_classes + 1 ) - 0.5 , cmap = cmap , clim = ( y . min (), y . max ()), zorder = 1 ) ax",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_377"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". unique ( y )) contours = ax . contourf ( xx , yy , Z , alpha = 0.3 , levels = np . arange ( n_classes + 1 ) - 0.5 , cmap = cmap , clim = ( y . min (), y . max ()), zorder = 1 ) ax . set ( xlim = xlim , ylim = ylim ) Now we can examine what the decision tree classification looks like ( Figure 5-70 ): In [ 5 ]: visualize_classifier ( DecisionTreeClassifier (), X , y ) In-Depth: Decision Trees and Random Forests | 423 Figure 5-70. Visualization of a decision tree classification If you’re running this notebook live, you can use the helpers script included in the online appendix to bring up an interactive visualization of the decision tree building process ( Figure 5-71 ): In [ 6 ]: # helpers_05_08 is found in the online appendix # (https://github.com/jakevdp/PythonDataScienceHandbook) import helpers_05_08 helpers_05_08 . plot_tree_interactive ( X , y ); Figure 5-71. First frame of the interactive decision tree widget; for the full version, see the online appendix Notice that as the depth increases, we tend to get very strangely shaped classification regions; for example, at a depth of five, there is a tall and skinny purple region 424 | Chapter 5: Machine Learning between the yellow and blue regions. It’s clear that this is less a result of the true, intrinsic data distribution, and more a result of the particular sampling or noise prop‐ erties of the data. That is, this decision tree, even at only five levels deep, is clearly overfitting our data. Decision trees and overfitting Such overfitting turns out to be a general property of decision trees; it is very easy to go too deep in the tree, and thus to fit details of the particular data rather than the overall properties of the distributions they are drawn from. Another way to see this overfitting is to look at models trained on different subsets of the data—for example, in Figure 5-72 we train two different trees, each on half of the original data. Figure 5-72",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_378"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 5-72. An example of two randomized decision trees It is clear that in some places, the two trees produce consistent results (e.g., in the four corners), while in other places, the two trees give very different classifications (e.g., in the regions between any two clusters). The key observation is that the incon‐ sistencies tend to happen where the classification is less certain, and thus by using information from both of these trees, we might come up with a better result! If you are running this notebook live, the following function will allow you to interac‐ tively display the fits of trees trained on a random subset of the data ( Figure 5-73 ): In [ 7 ]: # helpers_05_08 is found in the online appendix # (https://github.com/jakevdp/PythonDataScienceHandbook) import helpers_05_08 helpers_05_08 . randomized_tree_interactive ( X , y ) In-Depth: Decision Trees and Random Forests | 425 Figure 5-73. First frame of the interactive randomized decision tree widget; for the full version, see the online appendix Just as using information from two trees improves our results, we might expect that using information from many trees would improve our results even further. Ensembles of Estimators: Random Forests This notion—that multiple overfitting estimators can be combined to reduce the effect of this overfitting—is what underlies an ensemble method called bagging . Bag‐ ging makes use of an ensemble (a grab bag, perhaps) of parallel estimators, each of which overfits the data, and averages the results to find a better classification. An ensemble of randomized decision trees is known as a random forest . We can do this type of bagging classification manually using Scikit-Learn’s Bagging Classifier meta-estimator as shown here ( Figure 5-74 ): In [ 8 ]: from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import BaggingClassifier tree = DecisionTreeClassifier () bag = BaggingClassifier ( tree , n_estimators = 100 , max_samples = 0.8 , random_state = 1 ) bag",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_379"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit ( X , y ) visualize_classifier ( bag , X , y ) 426 | Chapter 5: Machine Learning Figure 5-74. Decision boundaries for an ensemble of random decision trees In this example, we have randomized the data by fitting each estimator with a ran‐ dom subset of 80% of the training points. In practice, decision trees are more effec‐ tively randomized when some stochasticity is injected in how the splits are chosen; this way, all the data contributes to the fit each time, but the results of the fit still have the desired randomness. For example, when determining which feature to split on, the randomized tree might select from among the top several features. You can read more technical details about these randomization strategies in the Scikit-Learn docu‐ mentation and references within. In Scikit-Learn, such an optimized ensemble of randomized decision trees is imple‐ mented in the RandomForestClassifier estimator, which takes care of all the ran‐ domization automatically. All you need to do is select a number of estimators, and it will very quickly (in parallel, if desired) fit the ensemble of trees ( Figure 5-75 ): In [ 9 ]: from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier ( n_estimators = 100 , random_state = 0 ) visualize_classifier ( model , X , y ); In-Depth: Decision Trees and Random Forests | 427 Figure 5-75. Decision boundaries for a random forest, which is an optimized ensemble of decision trees We see that by averaging over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the parameter space should be split. Random Forest Regression In the previous section we considered random forests within the context of classifica‐ tion. Random forests can also be made to work in the case of regression (that is, con‐ tinuous rather than categorical variables). The estimator to use for this is the RandomForestRegressor , and the syntax is very similar to what we saw earlier",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_380"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The estimator to use for this is the RandomForestRegressor , and the syntax is very similar to what we saw earlier. Consider the following data, drawn from the combination of a fast and slow oscilla‐ tion ( Figure 5-76 ): In [ 10 ]: rng = np . random . RandomState ( 42 ) x = 10 * rng . rand ( 200 ) def model ( x , sigma = 0.3 ): fast_oscillation = np . sin ( 5 * x ) slow_oscillation = np . sin ( 0.5 * x ) noise = sigma * rng . randn ( len ( x )) return slow_oscillation + fast_oscillation + noise y = model ( x ) plt . errorbar ( x , y , 0.3 , fmt = 'o' ); 428 | Chapter 5: Machine Learning Figure 5-76. Data for random forest regression Using the random forest regressor, we can find the best-fit curve as follows ( Figure 5-77 ): In [ 11 ]: from sklearn.ensemble import RandomForestRegressor forest = RandomForestRegressor ( 200 ) forest . fit ( x [:, None ], y ) xfit = np . linspace ( 0 , 10 , 1000 ) yfit = forest . predict ( xfit [:, None ]) ytrue = model ( xfit , sigma = 0 ) plt . errorbar ( x , y , 0.3 , fmt = 'o' , alpha = 0.5 ) plt . plot ( xfit , yfit , '-r' ); plt . plot ( xfit , ytrue , '-k' , alpha = 0.5 ); Here the true model is shown by the smooth curve, while the random forest model is shown by the jagged curve. As you can see, the nonparametric random forest model is flexible enough to fit the multiperiod data, without us needing to specify a multi‐ period model! In-Depth: Decision Trees and Random Forests | 429 Figure 5-77. Random forest model fit to the data Example: Random Forest for Classifying Digits Earlier we took a quick look at the handwritten digits data (see “Introducing ScikiLearn” on page 343 ). Let’s use that again here to see how the random forest classifier can be used in this context. In [ 12 ]: from sklearn.datasets import load_digits digits = load_digits () digits . keys () Out[12]: dict_keys(['target', 'data', 'target_names', 'DESCR', 'images']) To remind us what we’re looking at, we’ll visualize the first few data points ( Figure 5-78 ): In [ 13 ]: # set up the figure fig = plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_381"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". figure ( figsize = ( 6 , 6 )) # figure size in inches fig . subplots_adjust ( left = 0 , right = 1 , bottom = 0 , top = 1 , hspace = 0.05 , wspace = 0.05 ) # plot the digits: each image is 8x8 pixels for i in range ( 64 ): ax = fig . add_subplot ( 8 , 8 , i + 1 , xticks = [], yticks = []) ax . imshow ( digits . images [ i ], cmap = plt . cm . binary , interpolation = 'nearest' ) # label the image with the target value ax . text ( 0 , 7 , str ( digits . target [ i ])) 430 | Chapter 5: Machine Learning Figure 5-78. Representation of the digits data We can quickly classify the digits using a random forest as follows ( Figure 5-79 ): In [ 14 ]: from sklearn.cross_validation import train_test_split Xtrain , Xtest , ytrain , ytest = train_test_split ( digits . data , digits . target , random_state = 0 ) model = RandomForestClassifier ( n_estimators = 1000 ) model . fit ( Xtrain , ytrain ) ypred = model . predict ( Xtest ) We can take a look at the classification report for this classifier: In [ 15 ]: from sklearn import metrics print ( metrics . classification_report ( ypred , ytest )) precision recall f1-score support 0 1.00 0.97 0.99 38 1 1.00 0.98 0.99 44 2 0.95 1.00 0.98 42 3 0.98 0.96 0.97 46 4 0.97 1.00 0.99 37 5 0.98 0.96 0.97 49 6 1.00 1.00 1.00 52 7 1.00 0.96 0.98 50 8 0.94 0.98 0.96 46 9 0.96 0.98 0.97 46 avg / total 0.98 0.98 0.98 450 In-Depth: Decision Trees and Random Forests | 431 And for good measure, plot the confusion matrix ( Figure 5-79 ): In [ 16 ]: from sklearn.metrics import confusion_matrix mat = confusion_matrix ( ytest , ypred ) sns . heatmap ( mat . T , square = True , annot = True , fmt = 'd' , cbar = False ) plt . xlabel ( 'true label' ) plt . ylabel ( 'predicted label' ); Figure 5-79. Confusion matrix for digit classification with random forests We find that a simple, untuned random forest results in a very accurate classification of the digits data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_382"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Confusion matrix for digit classification with random forests We find that a simple, untuned random forest results in a very accurate classification of the digits data. Summary of Random Forests This section contained a brief introduction to the concept of ensemble estimators , and in particular the random forest model—an ensemble of randomized decision trees. Random forests are a powerful method with several advantages: • Both training and prediction are very fast, because of the simplicity of the under‐ lying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities. • The multiple trees allow for a probabilistic classification: a majority vote among estimators gives an estimate of the probability (accessed in Scikit-Learn with the predict_proba() method). • The nonparametric model is extremely flexible, and can thus perform well on tasks that are underfit by other estimators. 432 | Chapter 5: Machine Learning A primary disadvantage of random forests is that the results are not easily interpreta‐ ble; that is, if you would like to draw conclusions about the meaning of the classifica‐ tion model, random forests may not be the best choice. In Depth: Principal Component Analysis Up until now, we have been looking in depth at supervised learning estimators: those estimators that predict labels based on labeled training data. Here we begin looking at several unsupervised estimators, which can highlight interesting aspects of the data without reference to any known labels. In this section, we explore what is perhaps one of the most broadly used of unsuper‐ vised algorithms, principal component analysis (PCA). PCA is fundamentally a dimensionality reduction algorithm, but it can also be useful as a tool for visualiza‐ tion, for noise filtering, for feature extraction and engineering, and much more. After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_383"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". After a brief conceptual discussion of the PCA algorithm, we will see a couple examples of these further applications. We begin with the standard imports: In [ 1 ]: % matplotlib inline import numpy as np import matplotlib.pyplot as plt import seaborn as sns ; sns . set () Introducing Principal Component Analysis Principal component analysis is a fast and flexible unsupervised method for dimen‐ sionality reduction in data, which we saw briefly in “Introducing Scikit-Learn” on page 343 . Its behavior is easiest to visualize by looking at a two-dimensional dataset. Consider the following 200 points ( Figure 5-80 ): In [ 2 ]: rng = np . random . RandomState ( 1 ) X = np . dot ( rng . rand ( 2 , 2 ), rng . randn ( 2 , 200 )) . T plt . scatter ( X [:, 0 ], X [:, 1 ]) plt . axis ( 'equal' ); By eye, it is clear that there is a nearly linear relationship between the x and y vari‐ ables. This is reminiscent of the linear regression data we explored in “In Depth: Lin‐ ear Regression” on page 390 , but the problem setting here is slightly different: rather than attempting to predict the y values from the x values, the unsupervised learning problem attempts to learn about the relationship between the x and y values. In Depth: Principal Component Analysis | 433 Figure 5-80. Data for demonstration of PCA In principal component analysis, one quantifies this relationship by finding a list of the principal axes in the data, and using those axes to describe the dataset. Using Scikit-Learn’s PCA estimator, we can compute this as follows: In [ 3 ]: from sklearn.decomposition import PCA pca = PCA ( n_components = 2 ) pca . fit ( X ) Out[3]: PCA(copy=True, n_components=2, whiten=False) The fit learns some quantities from the data, most importantly the “components” and “explained variance”: In [ 4 ]: print ( pca . components_ ) [[ 0.94446029 0.32862557] [ 0.32862557 -0.94446029]] In [ 5 ]: print ( pca",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_384"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". components_ ) [[ 0.94446029 0.32862557] [ 0.32862557 -0.94446029]] In [ 5 ]: print ( pca . explained_variance_ ) [ 0.75871884 0.01838551] To see what these numbers mean, let’s visualize them as vectors over the input data, using the “components” to define the direction of the vector, and the “explained var‐ iance” to define the squared-length of the vector ( Figure 5-81 ): In [ 6 ]: def draw_vector ( v0 , v1 , ax = None ): ax = ax or plt . gca () arrowprops = dict ( arrowstyle = '->' , linewidth = 2 , shrinkA = 0 , shrinkB = 0 ) ax . annotate ( '' , v1 , v0 , arrowprops = arrowprops ) # plot data 434 | Chapter 5: Machine Learning plt . scatter ( X [:, 0 ], X [:, 1 ], alpha = 0.2 ) for length , vector in zip ( pca . explained_variance_ , pca . components_ ): v = vector * 3 * np . sqrt ( length ) draw_vector ( pca . mean_ , pca . mean_ + v ) plt . axis ( 'equal' ); Figure 5-81. Visualization of the principal axes in the data These vectors represent the principal axes of the data, and the length shown in Figure 5-81 is an indication of how “important” that axis is in describing the distribu‐ tion of the data—more precisely, it is a measure of the variance of the data when pro‐ jected onto that axis. The projection of each data point onto the principal axes are the “principal components” of the data. If we plot these principal components beside the original data, we see the plots shown in Figure 5-82 . Figure 5-82. Transformed principal axes in the data In Depth: Principal Component Analysis | 435 This transformation from data axes to principal axes is as an affine transformation , which basically means it is composed of a translation, rotation, and uniform scaling. While this algorithm to find principal components may seem like just a mathematical curiosity, it turns out to have very far-reaching applications in the world of machine learning and data exploration",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_385"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". PCA as dimensionality reduction Using PCA for dimensionality reduction involves zeroing out one or more of the smallest principal components, resulting in a lower-dimensional projection of the data that preserves the maximal data variance. Here is an example of using PCA as a dimensionality reduction transform: In [ 7 ]: pca = PCA ( n_components = 1 ) pca . fit ( X ) X_pca = pca . transform ( X ) print ( \"original shape: \" , X . shape ) print ( \"transformed shape:\" , X_pca . shape ) original shape: (200, 2) transformed shape: (200, 1) The transformed data has been reduced to a single dimension. To understand the effect of this dimensionality reduction, we can perform the inverse transform of this reduced data and plot it along with the original data ( Figure 5-83 ): In [ 8 ]: X_new = pca . inverse_transform ( X_pca ) plt . scatter ( X [:, 0 ], X [:, 1 ], alpha = 0.2 ) plt . scatter ( X_new [:, 0 ], X_new [:, 1 ], alpha = 0.8 ) plt . axis ( 'equal' ); Figure 5-83. Visualization of PCA as dimensionality reduction 436 | Chapter 5: Machine Learning The light points are the original data, while the dark points are the projected version. This makes clear what a PCA dimensionality reduction means: the information along the least important principal axis or axes is removed, leaving only the component(s) of the data with the highest variance. The fraction of variance that is cut out (propor‐ tional to the spread of points about the line formed in Figure 5-83 ) is roughly a meas‐ ure of how much “information” is discarded in this reduction of dimensionality. This reduced-dimension dataset is in some senses “good enough” to encode the most important relationships between the points: despite reducing the dimension of the data by 50%, the overall relationship between the data points is mostly preserved. PCA for visualization: Handwritten digits The usefulness of the dimensionality reduction may not be entirely apparent in only two dimensions, but becomes much more clear when we look at high-dimensional data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_386"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To see this, let’s take a quick look at the application of PCA to the digits data we saw in “In-Depth: Decision Trees and Random Forests” on page 421 . We start by loading the data: In [ 9 ]: from sklearn.datasets import load_digits digits = load_digits () digits . data . shape Out[9]: (1797, 64) Recall that the data consists of 8×8 pixel images, meaning that they are 64- dimensional. To gain some intuition into the relationships between these points, we can use PCA to project them to a more manageable number of dimensions, say two: In [ 10 ]: pca = PCA ( 2 ) # project from 64 to 2 dimensions projected = pca . fit_transform ( digits . data ) print ( digits . data . shape ) print ( projected . shape ) (1797, 64) (1797, 2) We can now plot the first two principal components of each point to learn about the data ( Figure 5-84 ): In [ 11 ]: plt . scatter ( projected [:, 0 ], projected [:, 1 ], c = digits . target , edgecolor = 'none' , alpha = 0.5 , cmap = plt . cm . get_cmap ( 'spectral' , 10 )) plt . xlabel ( 'component 1' ) plt . ylabel ( 'component 2' ) plt . colorbar (); In Depth: Principal Component Analysis | 437 Figure 5-84. PCA applied to the handwritten digits data Recall what these components mean: the full data is a 64-dimensional point cloud, and these points are the projection of each data point along the directions with the largest variance. Essentially, we have found the optimal stretch and rotation in 64- dimensional space that allows us to see the layout of the digits in two dimensions, and have done this in an unsupervised manner—that is, without reference to the labels. What do the components mean? We can go a bit further here, and begin to ask what the reduced dimensions mean . This meaning can be understood in terms of combinations of basis vectors. For example, each image in the training set is defined by a collection of 64 pixel values, which we will call the vector x : x = x 1 , x 2 , x 3 ⋯ x 64 One way we can think about this is in terms of a pixel basis",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_387"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That is, to construct the image, we multiply each element of the vector by the pixel it describes, and then add the results together to build the image: image x = x 1 · pixel 1 + x 2 · pixel 2 + x 3 · pixel 3 ⋯ x 64 · pixel 64 One way we might imagine reducing the dimension of this data is to zero out all but a few of these basis vectors. For example, if we use only the first eight pixels, we get an eight-dimensional projection of the data ( Figure 5-85 ), but it is not very reflective of the whole image: we’ve thrown out nearly 90% of the pixels! 438 | Chapter 5: Machine Learning Figure 5-85. A naive dimensionality reduction achieved by discarding pixels The upper row of panels shows the individual pixels, and the lower row shows the cumulative contribution of these pixels to the construction of the image. Using only eight of the pixel-basis components, we can only construct a small portion of the 64- pixel image. Were we to continue this sequence and use all 64 pixels, we would recover the original image. But the pixel-wise representation is not the only choice of basis. We can also use other basis functions, which each contain some predefined contribution from each pixel, and write something like: image x = mean + x 1 · basis 1 + x 2 · basis 2 + x 3 · basis 3 ⋯ PCA can be thought of as a process of choosing optimal basis functions, such that adding together just the first few of them is enough to suitably reconstruct the bulk of the elements in the dataset. The principal components, which act as the lodimensional representation of our data, are simply the coefficients that multiply each of the elements in this series. Figure 5-86 is a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions. Figure 5-86",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_388"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 5-86 is a similar depiction of reconstructing this digit using the mean plus the first eight PCA basis functions. Figure 5-86. A more sophisticated dimensionality reduction achieved by discarding the least important principal components (compare to Figure 5-85 ) Unlike the pixel basis, the PCA basis allows us to recover the salient features of the input image with just a mean plus eight components! The amount of each pixel in each component is the corollary of the orientation of the vector in our twdimensional example. This is the sense in which PCA provides a low-dimensional representation of the data: it discovers a set of basis functions that are more efficient than the native pixel-basis of the input data. In Depth: Principal Component Analysis | 439 Choosing the number of components A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data. We can determine this by looking at the cumulative explained variance ratio as a function of the number of components ( Figure 5-87 ): In [ 12 ]: pca = PCA () . fit ( digits . data ) plt . plot ( np . cumsum ( pca . explained_variance_ratio_ )) plt . xlabel ( 'number of components' ) plt . ylabel ( 'cumulative explained variance' ); Figure 5-87. The cumulative explained variance, which measures how well PCA pre‐ serves the content of the data This curve quantifies how much of the total, 64-dimensional variance is contained within the first N components. For example, we see that with the digits the first 10 components contain approximately 75% of the variance, while you need around 50 components to describe close to 100% of the variance. Here we see that our two-dimensional projection loses a lot of information (as meas‐ ured by the explained variance) and that we’d need about 20 components to retain 90% of the variance. Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_389"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Looking at this plot for a high-dimensional dataset can help you understand the level of redundancy present in multiple observations. PCA as Noise Filtering PCA can also be used as a filtering approach for noisy data. The idea is this: any com‐ ponents with variance much larger than the effect of the noise should be relatively unaffected by the noise. So if you reconstruct the data using just the largest subset of principal components, you should be preferentially keeping the signal and throwing out the noise. 440 | Chapter 5: Machine Learning Let’s see how this looks with the digits data. First we will plot several of the input noise-free data ( Figure 5-88 ): In [ 13 ]: def plot_digits ( data ): fig , axes = plt . subplots ( 4 , 10 , figsize = ( 10 , 4 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i , ax in enumerate ( axes . flat ): ax . imshow ( data [ i ] . reshape ( 8 , 8 ), cmap = 'binary' , interpolation = 'nearest' , clim = ( 0 , 16 )) plot_digits ( digits . data ) Figure 5-88. Digits without noise Now let’s add some random noise to create a noisy dataset, and replot it ( Figure 5-89 ): In [ 14 ]: np . random . seed ( 42 ) noisy = np . random . normal ( digits . data , 4 ) plot_digits ( noisy ) Figure 5-89. Digits with Gaussian random noise added It’s clear by eye that the images are noisy, and contain spurious pixels. Let’s train a PCA on the noisy data, requesting that the projection preserve 50% of the variance: In Depth: Principal Component Analysis | 441 In [ 15 ]: pca = PCA ( 0.50 ) . fit ( noisy ) pca . n_components_ Out[15]: 12 Here 50% of the variance amounts to 12 principal components. Now we compute these components, and then use the inverse of the transform to reconstruct the fil‐ tered digits ( Figure 5-90 ): In [ 16 ]: components = pca . transform ( noisy ) filtered = pca . inverse_transform ( components ) plot_digits ( filtered ) Figure 5-90",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_390"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". transform ( noisy ) filtered = pca . inverse_transform ( components ) plot_digits ( filtered ) Figure 5-90. Digits “denoised” using PCA This signal preserving/noise filtering property makes PCA a very useful feature selec‐ tion routine—for example, rather than training a classifier on very high-dimensional data, you might instead train the classifier on the lower-dimensional representation, which will automatically serve to filter out random noise in the inputs. Example: Eigenfaces Earlier we explored an example of using a PCA projection as a feature selector for facial recognition with a support vector machine ( “In-Depth: Support Vector Machines” on page 405 ). Here we will take a look back and explore a bit more of what went into that. Recall that we were using the Labeled Faces in the Wild dataset made available through Scikit-Learn: In [ 17 ]: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people ( min_faces_per_person = 60 ) print ( faces . target_names ) print ( faces . images . shape ) ['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush' 'Gerhard Schroeder' 'Hugo Chavez' 'Junichiro Koizumi' 'Tony Blair'] (1348, 62, 47) Let’s take a look at the principal axes that span this dataset. Because this is a large dataset, we will use RandomizedPCA —it contains a randomized method to approxi‐ 442 | Chapter 5: Machine Learning mate the first N principal components much more quickly than the standard PCA esti‐ mator, and thus is very useful for high-dimensional data (here, a dimensionality of nearly 3,000). We will take a look at the first 150 components: In [ 18 ]: from sklearn.decomposition import RandomizedPCA pca = RandomizedPCA ( 150 ) pca . fit ( faces",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_391"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will take a look at the first 150 components: In [ 18 ]: from sklearn.decomposition import RandomizedPCA pca = RandomizedPCA ( 150 ) pca . fit ( faces . data ) Out[18]: RandomizedPCA(copy=True, iterated_power=3, n_components=150, random_state=None, whiten=False) In this case, it can be interesting to visualize the images associated with the first sev‐ eral principal components (these components are technically known as “eigenvec‐ tors,” so these types of images are often called “eigenfaces”). As you can see in Figure 5-91 , they are as creepy as they sound: In [ 19 ]: fig , axes = plt . subplots ( 3 , 8 , figsize = ( 9 , 4 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i , ax in enumerate ( axes . flat ): ax . imshow ( pca . components_ [ i ] . reshape ( 62 , 47 ), cmap = 'bone' ) Figure 5-91. A visualization of eigenfaces learned from the LFW dataset The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces (from the top left) seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. Let’s take a look at the cumulative variance of these components to see how much of the data information the projection is preserv‐ ing ( Figure 5-92 ): In [ 20 ]: plt . plot ( np . cumsum ( pca . explained_variance_ratio_ )) plt . xlabel ( 'number of components' ) plt . ylabel ( 'cumulative explained variance' ); In Depth: Principal Component Analysis | 443 Figure 5-92. Cumulative explained variance for the LFW data We see that these 150 components account for just over 90% of the variance. That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_392"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That would lead us to believe that using these 150 components, we would recover most of the essential characteristics of the data. To make this more concrete, we can compare the input images with the images reconstructed from these 150 components ( Figure 5-93 ): In [ 21 ]: # Compute the components and projected faces pca = RandomizedPCA ( 150 ) . fit ( faces . data ) components = pca . transform ( faces . data ) projected = pca . inverse_transform ( components ) In [ 22 ]: # Plot the results fig , ax = plt . subplots ( 2 , 10 , figsize = ( 10 , 2.5 ), subplot_kw = { 'xticks' :[], 'yticks' :[]}, gridspec_kw = dict ( hspace = 0.1 , wspace = 0.1 )) for i in range ( 10 ): ax [ 0 , i ] . imshow ( faces . data [ i ] . reshape ( 62 , 47 ), cmap = 'binary_r' ) ax [ 1 , i ] . imshow ( projected [ i ] . reshape ( 62 , 47 ), cmap = 'binary_r' ) ax [ 0 , 0 ] . set_ylabel ( 'full-dim \\n input' ) ax [ 1 , 0 ] . set_ylabel ( '150-dim \\n reconstruction' ); Figure 5-93. 150-dimensional PCA reconstruction of the LFW data 444 | Chapter 5: Machine Learning The top row here shows the input images, while the bottom row shows the recon‐ struction of the images from just 150 of the ~3,000 initial features. This visualization makes clear why the PCA feature selection used in “In-Depth: Support Vector Machines” on page 405 was so successful: although it reduces the dimensionality of the data by nearly a factor of 20, the projected images contain enough information that we might, by eye, recognize the individuals in the image. What this means is that our classification algorithm needs to be trained on 150-dimensional data rather than 3,000-dimensional data, which depending on the particular algorithm we choose, can lead to a much more efficient classification. Principal Component Analysis Summary In this section we have discussed the use of principal component analysis for dimen‐ sionality reduction, for visualization of high-dimensional data, for noise filtering, and for feature selection within high-dimensional data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_393"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Because of the versatility and interpretability of PCA, it has been shown to be effective in a wide variety of contexts and disciplines. Given any high-dimensional dataset, I tend to start with PCA in order to visualize the relationship between points (as we did with the digits), to understand the main variance in the data (as we did with the eigenfaces), and to understand the intrinsic dimensionality (by plotting the explained variance ratio). Certainly PCA is not useful for every high-dimensional dataset, but it offers a straightforward and efficient path to gaining insight into high-dimensional data. PCA’s main weakness is that it tends to be highly affected by outliers in the data. For this reason, many robust variants of PCA have been developed, many of which act to iteratively discard data points that are poorly described by the initial components. Scikit-Learn contains a couple interesting variants on PCA, including RandomizedPCA and SparsePCA , both also in the sklearn.decomposition submodule. Randomi zedPCA , which we saw earlier, uses a nondeterministic method to quickly approxi‐ mate the first few principal components in very high-dimensional data, while SparsePCA introduces a regularization term (see “In Depth: Linear Regression” on page 390 ) that serves to enforce sparsity of the components. In the following sections, we will look at other unsupervised learning methods that build on some of the ideas of PCA. In-Depth: Manifold Learning We have seen how principal component analysis can be used in the dimensionality reduction task—reducing the number of features of a dataset while maintaining the essential relationships between the points. While PCA is flexible, fast, and easily interpretable, it does not perform so well when there are nonlinear relationships within the data; we will see some examples of these below",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_394"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While PCA is flexible, fast, and easily interpretable, it does not perform so well when there are nonlinear relationships within the data; we will see some examples of these below. In-Depth: Manifold Learning | 445 To address this deficiency, we can turn to a class of methods known as manifold learn‐ ing —a class of unsupervised estimators that seeks to describe datasets as lodimensional manifolds embedded in high-dimensional spaces. When you think of a manifold, I’d suggest imagining a sheet of paper: this is a two-dimensional object that lives in our familiar three-dimensional world, and can be bent or rolled in two dimensions. In the parlance of manifold learning, we can think of this sheet as a twdimensional manifold embedded in three-dimensional space. Rotating, reorienting, or stretching the piece of paper in three-dimensional space doesn’t change the flat geometry of the paper: such operations are akin to linear embeddings. If you bend, curl, or crumple the paper, it is still a two-dimensional manifold, but the embedding into the three-dimensional space is no longer linear. Manifold learning algorithms would seek to learn about the fundamental twdimensional nature of the paper, even as it is contorted to fill the three-dimensional space. Here we will demonstrate a number of manifold methods, going most deeply into a couple techniques: multidimensional scaling (MDS), locally linear embedding (LLE), and isometric mapping (Isomap). We begin with the standard imports: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () import numpy as np Manifold Learning: “HELLO” To make these concepts more clear, let’s start by generating some two-dimensional data that we can use to define a manifold. Here is a function that will create data in the shape of the word “HELLO”: In [ 2 ]: def make_hello ( N = 1000 , rseed = 42 ): # Make a plot with \"HELLO\" text; save as PNG fig , ax = plt . subplots ( figsize = ( 4 , 1 )) fig",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_395"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( figsize = ( 4 , 1 )) fig . subplots_adjust ( left = 0 , right = 1 , bottom = 0 , top = 1 ) ax . axis ( 'off' ) ax . text ( 0.5 , 0.4 , 'HELLO' , va = 'center' , ha = 'center' , weight = 'bold' , size = 85 ) fig . savefig ( 'hello.png' ) plt . close ( fig ) # Open this PNG and draw random points from it from matplotlib.image import imread data = imread ( 'hello.png' )[:: - 1 , :, 0 ] . T rng = np . random . RandomState ( rseed ) X = rng . rand ( 4 * N , 2 ) i , j = ( X * data . shape ) . astype ( int ) . T mask = ( data [ i , j ] < 1 ) X = X [ mask ] 446 | Chapter 5: Machine Learning X [:, 0 ] *= ( data . shape [ 0 ] / data . shape [ 1 ]) X = X [: N ] return X [ np . argsort ( X [:, 0 ])] Let’s call the function and visualize the resulting data ( Figure 5-94 ): In [ 3 ]: X = make_hello ( 1000 ) colorize = dict ( c = X [:, 0 ], cmap = plt . cm . get_cmap ( 'rainbow' , 5 )) plt . scatter ( X [:, 0 ], X [:, 1 ], ** colorize ) plt . axis ( 'equal' ); Figure 5-94. Data for use with manifold learning The output is two dimensional, and consists of points drawn in the shape of the word “HELLO”. This data form will help us to see visually what these algorithms are doing. Multidimensional Scaling (MDS) Looking at data like this, we can see that the particular choice of x and y values of the dataset are not the most fundamental description of the data: we can scale, shrink, or rotate the data, and the “HELLO” will still be apparent. For example, if we use a rota‐ tion matrix to rotate the data, the x and y values change, but the data is still funda‐ mentally the same ( Figure 5-95 ): In [ 4 ]: def rotate ( X , angle ): theta = np . deg2rad ( angle ) R = [[ np . cos ( theta ), np . sin ( theta )], [ - np . sin ( theta ), np . cos ( theta )]] return np . dot ( X , R ) X2 = rotate ( X , 20 ) + 5 plt . scatter ( X2 [:, 0 ], X2 [:, 1 ], ** colorize ) plt . axis ( 'equal' ); In-Depth: Manifold Learning | 447 Figure 5-95",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_396"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". cos ( theta )]] return np . dot ( X , R ) X2 = rotate ( X , 20 ) + 5 plt . scatter ( X2 [:, 0 ], X2 [:, 1 ], ** colorize ) plt . axis ( 'equal' ); In-Depth: Manifold Learning | 447 Figure 5-95. Rotated dataset This tells us that the x and y values are not necessarily fundamental to the relation‐ ships in the data. What is fundamental, in this case, is the distance between each point and the other points in the dataset. A common way to represent this is to use a dis‐ tance matrix: for N points, we construct an N × N array such that entry i , j contains the distance between point i and point j . Let’s use Scikit-Learn’s efficient pair wise_distances function to do this for our original data: In [ 5 ]: from sklearn.metrics import pairwise_distances D = pairwise_distances ( X ) D . shape Out[5]: (1000, 1000) As promised, for our N =1,000 points, we obtain a 1,000×1,000 matrix, which can be visualized as shown in Figure 5-96 : In [ 6 ]: plt . imshow ( D , zorder = 2 , cmap = 'Blues' , interpolation = 'nearest' ) plt . colorbar (); 448 | Chapter 5: Machine Learning Figure 5-96. Visualization of the pairwise distances between points If we similarly construct a distance matrix for our rotated and translated data, we see that it is the same: In [ 7 ]: D2 = pairwise_distances ( X2 ) np . allclose ( D , D2 ) Out[7]: True This distance matrix gives us a representation of our data that is invariant to rotations and translations, but the visualization of the matrix is not entirely intuitive. In the representation presented in Figure 5-96 , we have lost any visible sign of the interest‐ ing structure in the data: the “HELLO” that we saw before. Further, while computing this distance matrix from the (x, y) coordinates is straight‐ forward, transforming the distances back into x and y coordinates is rather difficult. This is exactly what the multidimensional scaling algorithm aims to do: given a dis‐ tance matrix between points, it recovers a D -dimensional coordinate representation of the data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_397"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is exactly what the multidimensional scaling algorithm aims to do: given a dis‐ tance matrix between points, it recovers a D -dimensional coordinate representation of the data. Let’s see how it works for our distance matrix, using the precomputed dissimilarity to specify that we are passing a distance matrix ( Figure 5-97 ): In [ 8 ]: from sklearn.manifold import MDS model = MDS ( n_components = 2 , dissimilarity = 'precomputed' , random_state = 1 ) out = model . fit_transform ( D ) plt . scatter ( out [:, 0 ], out [:, 1 ], ** colorize ) plt . axis ( 'equal' ); In-Depth: Manifold Learning | 449 Figure 5-97. An MDS embedding computed from the pairwise distances The MDS algorithm recovers one of the possible two-dimensional coordinate repre‐ sentations of our data, using only the N × N distance matrix describing the relation‐ ship between the data points. MDS as Manifold Learning The usefulness of this becomes more apparent when we consider the fact that dis‐ tance matrices can be computed from data in any dimension. So, for example, instead of simply rotating the data in the two-dimensional plane, we can project it into three dimensions using the following function (essentially a three-dimensional generaliza‐ tion of the rotation matrix used earlier): In [ 9 ]: def random_projection ( X , dimension = 3 , rseed = 42 ): assert dimension >= X . shape [ 1 ] rng = np . random . RandomState ( rseed ) C = rng . randn ( dimension , dimension ) e , V = np . linalg . eigh ( np . dot ( C , C . T )) return np . dot ( X , V [: X . shape [ 1 ]]) X3 = random_projection ( X , 3 ) X3 . shape Out[9]: (1000, 3) Let’s visualize these points to see what we’re working with ( Figure 5-98 ): In [ 10 ]: from mpl_toolkits import mplot3d ax = plt . axes ( projection = '3d' ) ax . scatter3D ( X3 [:, 0 ], X3 [:, 1 ], X3 [:, 2 ], ** colorize ) ax . view_init ( azim = 70 , elev = 50 ) 450 | Chapter 5: Machine Learning Figure 5-98",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_398"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". axes ( projection = '3d' ) ax . scatter3D ( X3 [:, 0 ], X3 [:, 1 ], X3 [:, 2 ], ** colorize ) ax . view_init ( azim = 70 , elev = 50 ) 450 | Chapter 5: Machine Learning Figure 5-98. Data embedded linearly into three dimensions We can now ask the MDS estimator to input this three-dimensional data, compute the distance matrix, and then determine the optimal two-dimensional embedding for this distance matrix. The result recovers a representation of the original data ( Figure 5-99 ): In [ 11 ]: model = MDS ( n_components = 2 , random_state = 1 ) out3 = model . fit_transform ( X3 ) plt . scatter ( out3 [:, 0 ], out3 [:, 1 ], ** colorize ) plt . axis ( 'equal' ); Figure 5-99. The MDS embedding of the three-dimensional data recovers the input up to a rotation and reflection This is essentially the goal of a manifold learning estimator: given high-dimensional embedded data, it seeks a low-dimensional representation of the data that preserves In-Depth: Manifold Learning | 451 certain relationships within the data. In the case of MDS, the quantity preserved is the distance between every pair of points. Nonlinear Embeddings: Where MDS Fails Our discussion so far has considered linear embeddings, which essentially consist of rotations, translations, and scalings of data into higher-dimensional spaces. Where MDS breaks down is when the embedding is nonlinear—that is, when it goes beyond this simple set of operations. Consider the following embedding, which takes the input and contorts it into an “S” shape in three dimensions: In [ 12 ]: def make_hello_s_curve ( X ): t = ( X [:, 0 ] - 2 ) * 0.75 * np . pi x = np . sin ( t ) y = X [:, 1 ] z = np . sign ( t ) * ( np . cos ( t ) - 1 ) return np . vstack (( x , y , z )) . T XS = make_hello_s_curve ( X ) This is again three-dimensional data, but we can see that the embedding is much more complicated ( Figure 5-100 ): In [ 13 ]: from mpl_toolkits import mplot3d ax = plt . axes ( projection = '3d' ) ax . scatter3D ( XS [:, 0 ], XS [:, 1 ], XS [:, 2 ], ** colorize ); Figure 5-100",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_399"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". axes ( projection = '3d' ) ax . scatter3D ( XS [:, 0 ], XS [:, 1 ], XS [:, 2 ], ** colorize ); Figure 5-100. Data embedded nonlinearly into three dimensions The fundamental relationships between the data points are still there, but this time the data has been transformed in a nonlinear way: it has been wrapped up into the shape of an “S.” 452 | Chapter 5: Machine Learning If we try a simple MDS algorithm on this data, it is not able to “unwrap” this nonlin‐ ear embedding, and we lose track of the fundamental relationships in the embedded manifold ( Figure 5-101 ): In [ 14 ]: from sklearn.manifold import MDS model = MDS ( n_components = 2 , random_state = 2 ) outS = model . fit_transform ( XS ) plt . scatter ( outS [:, 0 ], outS [:, 1 ], ** colorize ) plt . axis ( 'equal' ); Figure 5-101. The MDS algorithm applied to the nonlinear data; it fails to recover the underlying structure The best two-dimensional linear embedding does not unwrap the S-curve, but instead throws out the original y-axis. Nonlinear Manifolds: Locally Linear Embedding How can we move forward here? Stepping back, we can see that the source of the problem is that MDS tries to preserve distances between faraway points when con‐ structing the embedding. But what if we instead modified the algorithm such that it only preserves distances between nearby points? The resulting embedding would be closer to what we want. Visually, we can think of it as illustrated in Figure 5-102 . In-Depth: Manifold Learning | 453 Figure 5-102. Representation of linkages between points within MDS and LLE Here each faint line represents a distance that should be preserved in the embedding. On the left is a representation of the model used by MDS: it tries to preserve the dis‐ tances between each pair of points in the dataset",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_400"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". On the left is a representation of the model used by MDS: it tries to preserve the dis‐ tances between each pair of points in the dataset. On the right is a representation of the model used by a manifold learning algorithm called locally linear embedding (LLE): rather than preserving all distances, it instead tries to preserve only the distan‐ ces between neighboring points : in this case, the nearest 100 neighbors of each point. Thinking about the left panel, we can see why MDS fails: there is no way to flatten this data while adequately preserving the length of every line drawn between the two points. For the right panel, on the other hand, things look a bit more optimistic. We could imagine unrolling the data in a way that keeps the lengths of the lines approxi‐ mately the same. This is precisely what LLE does, through a global optimization of a cost function reflecting this logic. LLE comes in a number of flavors; here we will use the modified LLE algorithm to recover the embedded two-dimensional manifold. In general, modified LLE does bet‐ ter than other flavors of the algorithm at recovering well-defined manifolds with very little distortion ( Figure 5-103 ): In [ 15 ]: from sklearn.manifold import LocallyLinearEmbedding model = LocallyLinearEmbedding ( n_neighbors = 100 , n_components = 2 , method = 'modified' , eigen_solver = 'dense' ) out = model . fit_transform ( XS ) fig , ax = plt . subplots () ax . scatter ( out [:, 0 ], out [:, 1 ], ** colorize ) ax . set_ylim ( 0.15 , - 0.15 ); 454 | Chapter 5: Machine Learning Figure 5-103. Locally linear embedding can recover the underlying data from a nonli‐ nearly embedded input The result remains somewhat distorted compared to our original manifold, but cap‐ tures the essential relationships in the data! Some Thoughts on Manifold Methods Though this story and motivation is compelling, in practice manifold learning tech‐ niques tend to be finicky enough that they are rarely used for anything more than simple qualitative visualization of high-dimensional data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_401"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The following are some of the particular challenges of manifold learning, which all contrast poorly with PCA: • In manifold learning, there is no good framework for handling missing data. In contrast, there are straightforward iterative approaches for missing data in PCA. • In manifold learning, the presence of noise in the data can “short-circuit” the manifold and drastically change the embedding. In contrast, PCA naturally filters noise from the most important components. • The manifold embedding result is generally highly dependent on the number of neighbors chosen, and there is generally no solid quantitative way to choose an optimal number of neighbors. In contrast, PCA does not involve such a choice. • In manifold learning, the globally optimal number of output dimensions is diffi‐ cult to determine. In contrast, PCA lets you find the output dimension based on the explained variance. • In manifold learning, the meaning of the embedded dimensions is not always clear. In PCA, the principal components have a very clear meaning. In-Depth: Manifold Learning | 455 • In manifold learning the computational expense of manifold methods scales as O[N 2 ] or O[N 3 ]. For PCA, there exist randomized approaches that are generally much faster (though see the megaman package for some more scalable imple‐ mentations of manifold learning). With all that on the table, the only clear advantage of manifold learning methods over PCA is their ability to preserve nonlinear relationships in the data; for that reason I tend to explore data with manifold methods only after first exploring them with PCA. Scikit-Learn implements several common variants of manifold learning beyond Iso‐ map and LLE: the Scikit-Learn documentation has a nice discussion and comparison of them . Based on my own experience, I would give the following recommendations: • For toy problems such as the S-curve we saw before, locally linear embedding (LLE) and its variants (especially modified LLE ), perform very well",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_402"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is imple‐ mented in sklearn.manifold.LocallyLinearEmbedding . • For high-dimensional data from real-world sources, LLE often produces poor results, and isometric mapping (Isomap) seems to generally lead to more mean‐ ingful embeddings. This is implemented in sklearn.manifold.Isomap . • For data that is highly clustered, t-distributed stochastic neighbor embedding (SNE) seems to work very well, though can be very slow compared to other meth‐ ods. This is implemented in sklearn.manifold.TSNE . If you’re interested in getting a feel for how these work, I’d suggest running each of the methods on the data in this section. Example: Isomap on Faces One place manifold learning is often used is in understanding the relationship between high-dimensional data points. A common case of high-dimensional data is images; for example, a set of images with 1,000 pixels each can be thought of as col‐ lection of points in 1,000 dimensions—the brightness of each pixel in each image defines the coordinate in that dimension. Here let’s apply Isomap on some faces data. We will use the Labeled Faces in the Wild dataset, which we previously saw in “In-Depth: Support Vector Machines” on page 405 and “In Depth: Principal Component Analysis” on page 433 . Running this com‐ mand will download the data and cache it in your home directory for later use: In [ 16 ]: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people ( min_faces_per_person = 30 ) faces . data . shape Out[16]: (2370, 2914) 456 | Chapter 5: Machine Learning We have 2,370 images, each with 2,914 pixels. In other words, the images can be thought of as data points in a 2,914-dimensional space! Let’s quickly visualize several of these images to see what we’re working with ( Figure 5-104 ): In [ 17 ]: fig , ax = plt . subplots ( 4 , 8 , subplot_kw = dict ( xticks = [], yticks = [])) for i , axi in enumerate ( ax . flat ): axi . imshow ( faces . images [ i ], cmap = 'gray' ) Figure 5-104",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_403"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". subplots ( 4 , 8 , subplot_kw = dict ( xticks = [], yticks = [])) for i , axi in enumerate ( ax . flat ): axi . imshow ( faces . images [ i ], cmap = 'gray' ) Figure 5-104. Examples of the input faces We would like to plot a low-dimensional embedding of the 2,914-dimensional data to learn the fundamental relationships between the images. One useful way to start is to compute a PCA, and examine the explained variance ratio, which will give us an idea of how many linear features are required to describe the data ( Figure 5-105 ): In [ 18 ]: from sklearn.decomposition import RandomizedPCA model = RandomizedPCA ( 100 ) . fit ( faces . data ) plt . plot ( np . cumsum ( model . explained_variance_ratio_ )) plt . xlabel ( 'n components' ) plt . ylabel ( 'cumulative variance' ); We see that for this data, nearly 100 components are required to preserve 90% of the variance. This tells us that the data is intrinsically very high dimensional—it can’t be described linearly with just a few components. In-Depth: Manifold Learning | 457 Figure 5-105. Cumulative variance from the PCA projection When this is the case, nonlinear manifold embeddings like LLE and Isomap can be helpful. We can compute an Isomap embedding on these faces using the same pattern shown before: In [ 19 ]: from sklearn.manifold import Isomap model = Isomap ( n_components = 2 ) proj = model . fit_transform ( faces . data ) proj . shape Out[19]: (2370, 2) The output is a two-dimensional projection of all the input images. To get a better idea of what the projection tells us, let’s define a function that will output image thumbnails at the locations of the projections: In [ 20 ]: from matplotlib import offsetbox def plot_components ( data , model , images = None , ax = None , thumb_frac = 0.05 , cmap = 'gray' ): ax = ax or plt . gca () proj = model . fit_transform ( data ) ax . plot ( proj [:, 0 ], proj [:, 1 ], '.k' ) if images is not None : min_dist_2 = ( thumb_frac * max ( proj . max ( 0 ) - proj . min ( 0 ))) ** 2 shown_images = np . array ([ 2 * proj",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_404"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". plot ( proj [:, 0 ], proj [:, 1 ], '.k' ) if images is not None : min_dist_2 = ( thumb_frac * max ( proj . max ( 0 ) - proj . min ( 0 ))) ** 2 shown_images = np . array ([ 2 * proj . max ( 0 )]) for i in range ( data . shape [ 0 ]): dist = np . sum (( proj [ i ] - shown_images ) ** 2 , 1 ) if np . min ( dist ) < min_dist_2 : # don't show points that are too close continue 458 | Chapter 5: Machine Learning shown_images = np . vstack ([ shown_images , proj [ i ]]) imagebox = offsetbox . AnnotationBbox ( offsetbox . OffsetImage ( images [ i ], cmap = cmap ), proj [ i ]) ax . add_artist ( imagebox ) Calling this function now, we see the result ( Figure 5-106 ): In [ 21 ]: fig , ax = plt . subplots ( figsize = ( 10 , 10 )) plot_components ( faces . data , model = Isomap ( n_components = 2 ), images = faces . images [:, :: 2 , :: 2 ]) Figure 5-106. Isomap embedding of the faces data The result is interesting: the first two Isomap dimensions seem to describe global image features: the overall darkness or lightness of the image from left to right, and the general orientation of the face from bottom to top. This gives us a nice visual indication of some of the fundamental features in our data. In-Depth: Manifold Learning | 459 We could then go on to classify this data, perhaps using manifold features as inputs to the classification algorithm as we did in “In-Depth: Support Vector Machines” on page 405 . Example: Visualizing Structure in Digits As another example of using manifold learning for visualization, let’s take a look at the MNIST handwritten digits set. This data is similar to the digits we saw in “IDepth: Decision Trees and Random Forests” on page 421 , but with many more pixels per image. It can be downloaded from http://mldata.org/ with the Scikit-Learn utility: In [ 22 ]: from sklearn.datasets import fetch_mldata mnist = fetch_mldata ( 'MNIST original' ) mnist . data . shape Out[22]: (70000, 784) This consists of 70,000 images, each with 784 pixels (i.e., the images are 28×28)",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_405"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". data . shape Out[22]: (70000, 784) This consists of 70,000 images, each with 784 pixels (i.e., the images are 28×28). As before, we can take a look at the first few images ( Figure 5-107 ): In [ 23 ]: fig , ax = plt . subplots ( 6 , 8 , subplot_kw = dict ( xticks = [], yticks = [])) for i , axi in enumerate ( ax . flat ): axi . imshow ( mnist . data [ 1250 * i ] . reshape ( 28 , 28 ), cmap = 'gray_r' ) Figure 5-107. Examples of the MNIST digits This gives us an idea of the variety of handwriting styles in the dataset. Let’s compute a manifold learning projection across the data, illustrated in Figure 5-108 . For speed here, we’ll only use 1/30 of the data, which is about ~2,000 points (because of the relatively poor scaling of manifold learning, I find that a few thousand samples is a good number to start with for relatively quick exploration before moving to a full calculation): 460 | Chapter 5: Machine Learning In [ 24 ]: # use only 1/30 of the data: full dataset takes a long time! data = mnist . data [:: 30 ] target = mnist . target [:: 30 ] model = Isomap ( n_components = 2 ) proj = model . fit_transform ( data ) plt . scatter ( proj [:, 0 ], proj [:, 1 ], c = target , cmap = plt . cm . get_cmap ( 'jet' , 10 )) plt . colorbar ( ticks = range ( 10 )) plt . clim ( - 0.5 , 9.5 ); Figure 5-108. Isomap embedding of the MNIST digit data The resulting scatter plot shows some of the relationships between the data points, but is a bit crowded. We can gain more insight by looking at just a single number at a time ( Figure 5-109 ): In [ 25 ]: from sklearn.manifold import Isomap # Choose 1/4 of the \"1\" digits to project data = mnist . data [ mnist . target == 1 ][:: 4 ] fig , ax = plt . subplots ( figsize = ( 10 , 10 )) model = Isomap ( n_neighbors = 5 , n_components = 2 , eigen_solver = 'dense' ) plot_components ( data , model , images = data . reshape (( - 1 , 28 , 28 )), ax = ax , thumb_frac = 0.05 , cmap = 'gray_r' ) In-Depth: Manifold Learning | 461 Figure 5-109",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_406"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". reshape (( - 1 , 28 , 28 )), ax = ax , thumb_frac = 0.05 , cmap = 'gray_r' ) In-Depth: Manifold Learning | 461 Figure 5-109. Isomap embedding of only the 1s within the digits data The result gives you an idea of the variety of forms that the number “1” can take within the dataset. The data lies along a broad curve in the projected space, which appears to trace the orientation of the digit. As you move up the plot, you find ones that have hats and/or bases, though these are very sparse within the dataset. The pro‐ jection lets us identify outliers that have data issues (i.e., pieces of the neighboring digits that snuck into the extracted images). Now, this in itself may not be useful for the task of classifying digits, but it does help us get an understanding of the data, and may give us ideas about how to move for‐ ward, such as how we might want to preprocess the data before building a classifica‐ tion pipeline. In Depth: k-Means Clustering In the previous few sections, we have explored one category of unsupervised machine learning models: dimensionality reduction. Here we will move on to another class of unsupervised machine learning models: clustering algorithms. Clustering algorithms 462 | Chapter 5: Machine Learning seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points. Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering , which is implemented in sklearn.cluster.KMeans . We begin with the standard imports: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () # for plot styling import numpy as np Introducing k-Means The k -means algorithm searches for a predetermined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like: • The “cluster center” is the arithmetic mean of all the points belonging to the cluster",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_407"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It accomplishes this using a simple conception of what the optimal clustering looks like: • The “cluster center” is the arithmetic mean of all the points belonging to the cluster. • Each point is closer to its own cluster center than to other cluster centers. Those two assumptions are the basis of the k -means model. We will soon dive into exactly how the algorithm reaches this solution, but for now let’s take a look at a sim‐ ple dataset and see the k -means result. First, let’s generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization ( Figure 5-110 ): In [ 2 ]: from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 300 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], s = 50 ); By eye, it is relatively easy to pick out the four clusters. The k -means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API: In [ 3 ]: from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 4 ) kmeans . fit ( X ) y_kmeans = kmeans . predict ( X ) In Depth: k-Means Clustering | 463 Figure 5-110. Data for demonstration of clustering Let’s visualize the results by plotting the data colored by these labels. We will also plot the cluster centers as determined by the k -means estimator ( Figure 5-111 ): In [ 4 ]: plt . scatter ( X [:, 0 ], X [:, 1 ], c = y_kmeans , s = 50 , cmap = 'viridis' ) centers = kmeans . cluster_centers_ plt . scatter ( centers [:, 0 ], centers [:, 1 ], c = 'black' , s = 200 , alpha = 0.5 ); Figure 5-111. k-means cluster centers with clusters indicated by color The good news is that the k -means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_408"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data 464 | Chapter 5: Machine Learning points—an exhaustive search would be very, very costly. Fortunately for us, such an exhaustive search is not necessary; instead, the typical approach to k -means involves an intuitive iterative approach known as expectation–maximization . k-Means Algorithm: Expectation–Maximization Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k -means is a particularly simple and easy-tunderstand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach consists of the following procedure: 1. Guess some cluster centers 2. Repeat until converged a. E-Step : assign points to the nearest cluster center b. M-Step : set the cluster centers to the mean Here the “E-step” or “Expectation step” is so named because it involves updating our expectation of which cluster each point belongs to. The “M-step” or “Maximization step” is so named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster. The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics. We can visualize the algorithm as shown in Figure 5-112 . For the particular initialization shown here, the clusters converge in just three itera‐ tions. For an interactive version of this figure, refer to the code in the online appen‐ dix . Figure 5-112. Visualization of the E–M algorithm for k-means The k -means algorithm is simple enough that we can write it in a few lines of code",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_409"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 5-112. Visualization of the E–M algorithm for k-means The k -means algorithm is simple enough that we can write it in a few lines of code. The following is a very basic implementation ( Figure 5-113 ): In Depth: k-Means Clustering | 465 In [ 5 ]: from sklearn.metrics import pairwise_distances_argmin def find_clusters ( X , n_clusters , rseed = 2 ): # 1. Randomly choose clusters rng = np . random . RandomState ( rseed ) i = rng . permutation ( X . shape [ 0 ])[: n_clusters ] centers = X [ i ] while True : # 2a. Assign labels based on closest center labels = pairwise_distances_argmin ( X , centers ) # 2b. Find new centers from means of points new_centers = np . array ([ X [ labels == i ] . mean ( 0 ) for i in range ( n_clusters )]) # 2c. Check for convergence if np . all ( centers == new_centers ): break centers = new_centers return centers , labels centers , labels = find_clusters ( X , 4 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 50 , cmap = 'viridis' ); Figure 5-113. Data labeled with k-means Most well-tested implementations will do a bit more than this under the hood, but the preceding function gives the gist of the expectation–maximization approach. 466 | Chapter 5: Machine Learning Caveats of expectation–maximization There are a few issues to be aware of when using the expectation–maximization algorithm. The globally optimal result may not be achieved First, although the E–M procedure is guaranteed to improve the result in each step, there is no assurance that it will lead to the global best solution. For exam‐ ple, if we use a different random seed in our simple procedure, the particular starting guesses lead to poor results ( Figure 5-114 ): In [ 6 ]: centers , labels = find_clusters ( X , 4 , rseed = 0 ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 50 , cmap = 'viridis' ); Figure 5-114. An example of poor convergence in k-means Here the E–M approach has converged, but has not converged to a globally opti‐ mal configuration",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_410"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". An example of poor convergence in k-means Here the E–M approach has converged, but has not converged to a globally opti‐ mal configuration. For this reason, it is common for the algorithm to be run for multiple starting guesses, as indeed Scikit-Learn does by default (set by the n_init parameter, which defaults to 10). The number of clusters must be selected beforehand Another common challenge with k -means is that you must tell it how many clus‐ ters you expect: it cannot learn the number of clusters from the data. For exam‐ ple, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters ( Figure 5-115 ): In [ 7 ]: labels = KMeans ( 6 , random_state = 0 ) . fit_predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 50 , cmap = 'viridis' ); In Depth: k-Means Clustering | 467 Figure 5-115. An example where the number of clusters is chosen poorly Whether the result is meaningful is a question that is difficult to answer defini‐ tively; one approach that is rather intuitive, but that we won’t discuss further here, is called silhouette analysis . Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models; see “In Depth: Gaussian Mixture Models” on page 476 ) or which can choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the sklearn.cluster submodule). k-means is limited to linear cluster boundaries The fundamental model assumptions of k -means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffec‐ tive if the clusters have complicated geometries. In particular, the boundaries between k -means clusters will always be linear, which means that it will fail for more complicated boundaries",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_411"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, the boundaries between k -means clusters will always be linear, which means that it will fail for more complicated boundaries. Consider the fol‐ lowing data, along with the cluster labels found by the typical k -means approach ( Figure 5-116 ): In [ 8 ]: from sklearn.datasets import make_moons X , y = make_moons ( 200 , noise =. 05 , random_state = 0 ) In [ 9 ]: labels = KMeans ( 2 , random_state = 0 ) . fit_predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 50 , cmap = 'viridis' ); 468 | Chapter 5: Machine Learning Figure 5-116. Failure of k-means with nonlinear boundaries This situation is reminiscent of the discussion in “In-Depth: Support Vector Machines” on page 405 , where we used a kernel transformation to project the data into a higher dimension where a linear separation is possible. We might imagine using the same trick to allow k -means to discover nonlinear boundaries. One version of this kernelized k -means is implemented in Scikit-Learn within the SpectralClustering estimator. It uses the graph of nearest neighbors to com‐ pute a higher-dimensional representation of the data, and then assigns labels using a k -means algorithm ( Figure 5-117 ): In [ 10 ]: from sklearn.cluster import SpectralClustering model = SpectralClustering ( n_clusters = 2 , affinity = 'nearest_neighbors' , assign_labels = 'kmeans' ) labels = model . fit_predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 50 , cmap = 'viridis' ); We see that with this kernel transform approach, the kernelized k -means is able to find the more complicated nonlinear boundaries between clusters. In Depth: k-Means Clustering | 469 Figure 5-117. Nonlinear boundaries learned by SpectralClustering k-means can be slow for large numbers of samples Because each iteration of k -means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_412"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". You might wonder if this requirement to use all data at each iteration can be relaxed; for example, you might just use a subset of the data to update the cluster centers at each step. This is the idea behind batch-based k -means algorithms, one form of which is implemented in sklearn.cluster.MiniBatchKMeans . The interface for this is the same as for standard KMeans ; we will see an example of its use as we continue our discussion. Examples Being careful about these limitations of the algorithm, we can use k -means to our advantage in a wide variety of situations. We’ll now take a look at a couple examples. Example 1: k-Means on digits To start, let’s take a look at applying k -means on the same simple digits data that we saw in “In-Depth: Decision Trees and Random Forests” on page 421 and “In Depth: Principal Component Analysis” on page 433 . Here we will attempt to use k -means to try to identify similar digits without using the original label information ; this might be similar to a first step in extracting meaning from a new dataset about which you don’t have any a priori label information. We will start by loading the digits and then finding the KMeans clusters. Recall that the digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image: 470 | Chapter 5: Machine Learning In [ 11 ]: from sklearn.datasets import load_digits digits = load_digits () digits . data . shape Out[11]: (1797, 64) The clustering can be performed as we did before: In [ 12 ]: kmeans = KMeans ( n_clusters = 10 , random_state = 0 ) clusters = kmeans . fit_predict ( digits . data ) kmeans . cluster_centers_ . shape Out[12]: (10, 64) The result is 10 clusters in 64 dimensions. Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the “typical” digit within the cluster. Let’s see what these cluster centers look like ( Figure 5-118 ): In [ 13 ]: fig , ax = plt . subplots ( 2 , 5 , figsize = ( 8 , 3 )) centers = kmeans",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_413"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Let’s see what these cluster centers look like ( Figure 5-118 ): In [ 13 ]: fig , ax = plt . subplots ( 2 , 5 , figsize = ( 8 , 3 )) centers = kmeans . cluster_centers_ . reshape ( 10 , 8 , 8 ) for axi , center in zip ( ax . flat , centers ): axi . set ( xticks = [], yticks = []) axi . imshow ( center , interpolation = 'nearest' , cmap = plt . cm . binary ) Figure 5-118. Cluster centers learned by k-means We see that even without the labels , KMeans is able to find clusters whose centers are recognizable digits, with perhaps the exception of 1 and 8. Because k -means knows nothing about the identity of the cluster, the 0–9 labels may be permuted. We can fix this by matching each learned cluster label with the true labels found in them: In [ 14 ]: from scipy.stats import mode labels = np . zeros_like ( clusters ) for i in range ( 10 ): mask = ( clusters == i ) labels [ mask ] = mode ( digits . target [ mask ])[ 0 ] Now we can check how accurate our unsupervised clustering was in finding similar digits within the data: In Depth: k-Means Clustering | 471 In [ 15 ]: from sklearn.metrics import accuracy_score accuracy_score ( digits . target , labels ) Out[15]: 0.79354479688369506 With just a simple k -means algorithm, we discovered the correct grouping for 80% of the input digits! Let’s check the confusion matrix for this ( Figure 5-119 ): In [ 16 ]: from sklearn.metrics import confusion_matrix mat = confusion_matrix ( digits . target , labels ) sns . heatmap ( mat . T , square = True , annot = True , fmt = 'd' , cbar = False , xticklabels = digits . target_names , yticklabels = digits . target_names ) plt . xlabel ( 'true label' ) plt . ylabel ( 'predicted label' ); Figure 5-119. Confusion matrix for the k-means classifier As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_414"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Confusion matrix for the k-means classifier As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones. But this still shows that using k -means, we can essentially build a digit classifier without reference to any known labels ! Just for fun, let’s try to push this even further. We can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm (mentioned in “In-Depth: Manifold Learn‐ ing” on page 445 ) to preprocess the data before performing k -means. t-SNE is a non‐ linear embedding algorithm that is particularly adept at preserving points within clusters. Let’s see how it does: In [ 17 ]: from sklearn.manifold import TSNE # Project the data: this step will take several seconds tsne = TSNE ( n_components = 2 , init = 'pca' , random_state = 0 ) digits_proj = tsne . fit_transform ( digits . data ) 472 | Chapter 5: Machine Learning # Compute the clusters kmeans = KMeans ( n_clusters = 10 , random_state = 0 ) clusters = kmeans . fit_predict ( digits_proj ) # Permute the labels labels = np . zeros_like ( clusters ) for i in range ( 10 ): mask = ( clusters == i ) labels [ mask ] = mode ( digits . target [ mask ])[ 0 ] # Compute the accuracy accuracy_score ( digits . target , labels ) Out[17]: 0.93356149137451305 That’s nearly 94% classification accuracy without using the labels . This is the power of unsupervised learning when used carefully: it can extract information from the data‐ set that it might be difficult to do by hand or by eye. Example 2: k-means for color compression One interesting application of clustering is in color compression within images. For example, imagine you have an image with millions of colors. In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_415"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors. For example, consider the image shown in Figure 5-120 , which is from Scikit-Learn’s datasets module (for this to work, you’ll have to have the pillow Python package installed): In [ 18 ]: # Note: this requires the pillow package to be installed from sklearn.datasets import load_sample_image china = load_sample_image ( \"china.jpg\" ) ax = plt . axes ( xticks = [], yticks = []) ax . imshow ( china ); The image itself is stored in a three-dimensional array of size (height, width, RGB) , containing red/blue/green contributions as integers from 0 to 255: In [ 19 ]: china . shape Out[19]: (427, 640, 3) In Depth: k-Means Clustering | 473 Figure 5-120. The input image One way we can view this set of pixels is as a cloud of points in a three-dimensional color space. We will reshape the data to [n_samples x n_features] , and rescale the colors so that they lie between 0 and 1: In [ 20 ]: data = china / 255.0 # use 01 scale data = data . reshape ( 427 * 640 , 3 ) data . shape Out[20]: (273280, 3) We can visualize these pixels in this color space, using a subset of 10,000 pixels for efficiency ( Figure 5-121 ): In [ 21 ]: def plot_pixels ( data , title , colors = None , N = 10000 ): if colors is None : colors = data # choose a random subset rng = np . random . RandomState ( 0 ) i = rng . permutation ( data . shape [ 0 ])[: N ] colors = colors [ i ] R , G , B = data [ i ] . T fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 )) ax [ 0 ] . scatter ( R , G , color = colors , marker = '.' ) ax [ 0 ] . set ( xlabel = 'Red' , ylabel = 'Green' , xlim = ( 0 , 1 ), ylim = ( 0 , 1 )) ax [ 1 ] . scatter ( R , B , color = colors , marker = '.' ) ax [ 1 ] . set ( xlabel = 'Red' , ylabel = 'Blue' , xlim = ( 0 , 1 ), ylim = ( 0 , 1 )) fig",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_416"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". scatter ( R , B , color = colors , marker = '.' ) ax [ 1 ] . set ( xlabel = 'Red' , ylabel = 'Blue' , xlim = ( 0 , 1 ), ylim = ( 0 , 1 )) fig . suptitle ( title , size = 20 ); In [ 22 ]: plot_pixels ( data , title = 'Input color space: 16 million possible colors' ) 474 | Chapter 5: Machine Learning Figure 5-121. The distribution of the pixels in RGB color space Now let’s reduce these 16 million colors to just 16 colors, using a k -means clustering across the pixel space. Because we are dealing with a very large dataset, we will use the mini batch k -means, which operates on subsets of the data to compute the result much more quickly than the standard k -means algorithm ( Figure 5-122 ): In [ 23 ]: from sklearn.cluster import MiniBatchKMeans kmeans = MiniBatchKMeans ( 16 ) kmeans . fit ( data ) new_colors = kmeans . cluster_centers_ [ kmeans . predict ( data )] plot_pixels ( data , colors = new_colors , title = \"Reduced color space: 16 colors\" ) Figure 5-122. 16 clusters in RGB color space In Depth: k-Means Clustering | 475 The result is a recoloring of the original pixels, where each pixel is assigned the color of its closest cluster center. Plotting these new colors in the image space rather than the pixel space shows us the effect of this ( Figure 5-123 ): In [ 24 ]: china_recolored = new_colors . reshape ( china . shape ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 6 ), subplot_kw = dict ( xticks = [], yticks = [])) fig . subplots_adjust ( wspace = 0.05 ) ax [ 0 ] . imshow ( china ) ax [ 0 ] . set_title ( 'Original Image' , size = 16 ) ax [ 1 ] . imshow ( china_recolored ) ax [ 1 ] . set_title ( '16-color Image' , size = 16 ); Figure 5-123. A comparison of the full-color image (left) and the 16-color image (right) Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_417"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A comparison of the full-color image (left) and the 16-color image (right) Some detail is certainly lost in the rightmost panel, but the overall image is still easily recognizable. This image on the right achieves a compression factor of around 1 mil‐ lion! While this is an interesting application of k -means, there are certainly better way to compress information in images. But the example shows the power of thinking outside of the box with unsupervised methods like k -means. In Depth: Gaussian Mixture Models The k -means clustering model explored in the previous section is simple and rela‐ tively easy to understand, but its simplicity leads to practical challenges in its applica‐ tion. In particular, the nonprobabilistic nature of k -means and its use of simple distance-from-cluster-center to assign cluster membership leads to poor performance for many real-world situations. In this section we will take a look at Gaussian mixture models, which can be viewed as an extension of the ideas behind k -means, but can also be a powerful tool for estimation beyond simple clustering. We begin with the standard imports: 476 | Chapter 5: Machine Learning In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () import numpy as np Motivating GMM: Weaknesses of k-Means Let’s take a look at some of the weaknesses of k -means and think about how we might improve the cluster model. As we saw in the previous section, given simple, welseparated data, k -means finds suitable clustering results",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_418"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As we saw in the previous section, given simple, welseparated data, k -means finds suitable clustering results. For example, if we have simple blobs of data, the k -means algorithm can quickly label those clusters in a way that closely matches what we might do by eye ( Figure 5-124 ): In [ 2 ]: # Generate some data from sklearn.datasets.samples_generator import make_blobs X , y_true = make_blobs ( n_samples = 400 , centers = 4 , cluster_std = 0.60 , random_state = 0 ) X = X [:, :: - 1 ] # flip axes for better plotting In [ 3 ]: # Plot the data with k-means labels from sklearn.cluster import KMeans kmeans = KMeans ( 4 , random_state = 0 ) labels = kmeans . fit ( X ) . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' ); Figure 5-124. k-means labels for simple data From an intuitive standpoint, we might expect that the clustering assignment for some points is more certain than others; for example, there appears to be a very slight overlap between the two middle clusters, such that we might not have complete confi‐ dence in the cluster assignment of points between them. Unfortunately, the k -means model has no intrinsic measure of probability or uncertainty of cluster assignments In Depth: Gaussian Mixture Models | 477 (although it may be possible to use a bootstrap approach to estimate this uncertainty). For this, we must think about generalizing the model. One way to think about the k -means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assign‐ ment within the training set: any point outside this circle is not considered a member of the cluster. We can visualize this cluster model with the following function ( Figure 5-125 ): In [ 4 ]: from sklearn.cluster import KMeans from scipy.spatial.distance import cdist def plot_kmeans ( kmeans , X , n_clusters = 4 , rseed = 0 , ax = None ): labels = kmeans",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_419"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". fit_predict ( X ) # plot the input data ax = ax or plt . gca () ax . axis ( 'equal' ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 ) # plot the representation of the k-means model centers = kmeans . cluster_centers_ radii = [ cdist ( X [ labels == i ], [ center ]) . max () for i , center in enumerate ( centers )] for c , r in zip ( centers , radii ): ax . add_patch ( plt . Circle ( c , r , fc = '#CCCCCC' , lw = 3 , alpha = 0.5 , zorder = 1 )) In [ 5 ]: kmeans = KMeans ( n_clusters = 4 , random_state = 0 ) plot_kmeans ( kmeans , X ) Figure 5-125. The circular clusters implied by the k-means model 478 | Chapter 5: Machine Learning An important observation for k -means is that these cluster models must be circular : k - means has no built-in way of accounting for oblong or elliptical clusters. So, for example, if we take the same data and transform it, the cluster assignments end up becoming muddled ( Figure 5-126 ): In [ 6 ]: rng = np . random . RandomState ( 13 ) X_stretched = np . dot ( X , rng . randn ( 2 , 2 )) kmeans = KMeans ( n_clusters = 4 , random_state = 0 ) plot_kmeans ( kmeans , X_stretched ) Figure 5-126. Poor performance of k-means for noncircular clusters By eye, we recognize that these transformed clusters are noncircular, and thus circu‐ lar clusters would be a poor fit. Nevertheless, k -means is not flexible enough to account for this, and tries to force-fit the data into four circular clusters. This results in a mixing of cluster assignments where the resulting circles overlap: see especially the bottom right of this plot. One might imagine addressing this particular situation by preprocessing the data with PCA (see “In Depth: Principal Component Analysis” on page 433 ), but in practice there is no guarantee that such a global operation will circularize the individual data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_420"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These two disadvantages of k -means—its lack of flexibility in cluster shape and lack of probabilistic cluster assignment—mean that for many datasets (especially lodimensional datasets) it may not perform as well as you might hope. You might imagine addressing these weaknesses by generalizing the k -means model: for example, you could measure uncertainty in cluster assignment by comparing the distances of each point to all cluster centers, rather than focusing on just the closest. You might also imagine allowing the cluster boundaries to be ellipses rather than cir‐ In Depth: Gaussian Mixture Models | 479 cles, so as to account for noncircular clusters. It turns out these are two essential com‐ ponents of a different type of clustering model, Gaussian mixture models. Generalizing E–M: Gaussian Mixture Models A Gaussian mixture model (GMM) attempts to find a mixture of multidimensional Gaussian probability distributions that best model any input dataset. In the simplest case, GMMs can be used for finding clusters in the same manner as k -means ( Figure 5-127 ): In [ 7 ]: from sklearn.mixture import GMM gmm = GMM ( n_components = 4 ) . fit ( X ) labels = gmm . predict ( X ) plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' ); Figure 5-127. Gaussian mixture model labels for the data But because GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments—in Scikit-Learn we do this using the pre dict_proba method. This returns a matrix of size [n_samples, n_clusters] that measures the probability that any point belongs to the given cluster: In [ 8 ]: probs = gmm . predict_proba ( X ) print ( probs [: 5 ] . round ( 3 )) [[ 0. 0. 0.475 0.525] [ 0. 1. 0. 0. ] [ 0. 1. 0. 0. ] [ 0. 0. 0. 1. ] [ 0. 1. 0. 0",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_421"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". predict_proba ( X ) print ( probs [: 5 ] . round ( 3 )) [[ 0. 0. 0.475 0.525] [ 0. 1. 0. 0. ] [ 0. 1. 0. 0. ] [ 0. 0. 0. 1. ] [ 0. 1. 0. 0. ]] We can visualize this uncertainty by, for example, making the size of each point pro‐ portional to the certainty of its prediction; looking at Figure 5-128 , we can see that it 480 | Chapter 5: Machine Learning is precisely the points at the boundaries between clusters that reflect this uncertainty of cluster assignment: In [ 9 ]: size = 50 * probs . max ( 1 ) ** 2 # square emphasizes differences plt . scatter ( X [:, 0 ], X [:, 1 ], c = labels , cmap = 'viridis' , s = size ); Figure 5-128. GMM probablistic labels: probabilities are shown by the size of points Under the hood, a Gaussian mixture model is very similar to k -means: it uses an expectation–maximization approach that qualitatively does the following: 1. Choose starting guesses for the location and shape 2. Repeat until converged: a. E-step : for each point, find weights encoding the probability of membership in each cluster b. M-step : for each cluster, update its location, normalization, and shape based on all data points, making use of the weights The result of this is that each cluster is associated not with a hard-edged sphere, but with a smooth Gaussian model. Just as in the k -means expectation–maximization approach, this algorithm can sometimes miss the globally optimal solution, and thus in practice multiple random initializations are used. Let’s create a function that will help us visualize the locations and shapes of the GMM clusters by drawing ellipses based on the gmm output: In [ 10 ]: from matplotlib.patches import Ellipse def draw_ellipse ( position , covariance , ax = None , ** kwargs ): \"\"\"Draw an ellipse with a given position and covariance\"\"\" In Depth: Gaussian Mixture Models | 481 ax = ax or plt . gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_422"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". gca () # Convert covariance to principal axes if covariance . shape == ( 2 , 2 ): U , s , Vt = np . linalg . svd ( covariance ) angle = np . degrees ( np . arctan2 ( U [ 1 , 0 ], U [ 0 , 0 ])) width , height = 2 * np . sqrt ( s ) else : angle = 0 width , height = 2 * np . sqrt ( covariance ) # Draw the ellipse for nsig in range ( 1 , 4 ): ax . add_patch ( Ellipse ( position , nsig * width , nsig * height , angle , ** kwargs )) def plot_gmm ( gmm , X , label = True , ax = None ): ax = ax or plt . gca () labels = gmm . fit ( X ) . predict ( X ) if label : ax . scatter ( X [:, 0 ], X [:, 1 ], c = labels , s = 40 , cmap = 'viridis' , zorder = 2 ) else : ax . scatter ( X [:, 0 ], X [:, 1 ], s = 40 , zorder = 2 ) ax . axis ( 'equal' ) w_factor = 0.2 / gmm . weights_ . max () for pos , covar , w in zip ( gmm . means_ , gmm . covars_ , gmm . weights_ ): draw_ellipse ( pos , covar , alpha = w * w_factor ) With this in place, we can take a look at what the four-component GMM gives us for our initial data ( Figure 5-129 ): In [ 11 ]: gmm = GMM ( n_components = 4 , random_state = 42 ) plot_gmm ( gmm , X ) 482 | Chapter 5: Machine Learning Figure 5-129. Representation of the four-component GMM in the presence of circular clusters Similarly, we can use the GMM approach to fit our stretched dataset; allowing for a full covariance, the model will fit even very oblong, stretched-out clusters ( Figure 5-130 ): In [ 12 ]: gmm = GMM ( n_components = 4 , covariance_type = 'full' , random_state = 42 ) plot_gmm ( gmm , X_stretched ) Figure 5-130. Representation of the four-component GMM in the presence of noncircu‐ lar clusters This makes clear that GMMs address the two main practical issues with k -means encountered before. In Depth: Gaussian Mixture Models | 483 Choosing the covariance type If you look at the details of the preceding fits, you will see that the covariance_type option was set differently within each",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_423"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Depth: Gaussian Mixture Models | 483 Choosing the covariance type If you look at the details of the preceding fits, you will see that the covariance_type option was set differently within each. This hyperparameter controls the degrees of freedom in the shape of each cluster; it is essential to set this carefully for any given problem. The default is covariance_type=\"diag\" , which means that the size of the cluster along each dimension can be set independently, with the resulting ellipse constrained to align with the axes. A slightly simpler and faster model is cova riance_type=\"spherical\" , which constrains the shape of the cluster such that all dimensions are equal. The resulting clustering will have similar characteristics to that of k -means, though it is not entirely equivalent. A more complicated and computa‐ tionally expensive model (especially as the number of dimensions grows) is to use covariance_type=\"full\" , which allows each cluster to be modeled as an ellipse with arbitrary orientation. We can see a visual representation of these three choices for a single cluster within Figure 5-131 : Figure 5-131. Visualization of GMM covariance types GMM as Density Estimation Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation . That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data. As an example, consider some data generated from Scikit-Learn’s make_moons func‐ tion (visualized in Figure 5-132 ), which we saw in “In Depth: k-Means Clustering” on page 462 : In [ 13 ]: from sklearn.datasets import make_moons Xmoon , ymoon = make_moons ( 200 , noise =. 05 , random_state = 0 ) plt . scatter ( Xmoon [:, 0 ], Xmoon [:, 1 ]); 484 | Chapter 5: Machine Learning Figure 5-132",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_424"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 05 , random_state = 0 ) plt . scatter ( Xmoon [:, 0 ], Xmoon [:, 1 ]); 484 | Chapter 5: Machine Learning Figure 5-132. GMM applied to clusters with nonlinear boundaries If we try to fit this to a two-component GMM viewed as a clustering model, the results are not particularly useful ( Figure 5-133 ): In [ 14 ]: gmm2 = GMM ( n_components = 2 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm2 , Xmoon ) Figure 5-133. Two component GMM fit to nonlinear clusters But if we instead use many more components and ignore the cluster labels, we find a fit that is much closer to the input data ( Figure 5-134 ): In [ 15 ]: gmm16 = GMM ( n_components = 16 , covariance_type = 'full' , random_state = 0 ) plot_gmm ( gmm16 , Xmoon , label = False ) In Depth: Gaussian Mixture Models | 485 Figure 5-134. Using many GMM clusters to model the distribution of points Here the mixture of 16 Gaussians serves not to find separated clusters of data, but rather to model the overall distribution of the input data. This is a generative model of the distribution, meaning that the GMM gives us the recipe to generate new random data distributed similarly to our input. For example, here are 400 new points drawn from this 16-component GMM fit to our original data ( Figure 5-135 ): In [ 16 ]: Xnew = gmm16 . sample ( 400 , random_state = 42 ) plt . scatter ( Xnew [:, 0 ], Xnew [:, 1 ]); Figure 5-135. New data drawn from the 16-component GMM GMM is convenient as a flexible means of modeling an arbitrary multidimensional distribution of data. 486 | Chapter 5: Machine Learning How many components? The fact that GMM is a generative model gives us a natural means of determining the optimal number of components for a given dataset. A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid overfitting",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_425"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A generative model is inherently a probability distribution for the dataset, and so we can simply evaluate the likelihood of the data under the model, using cross-validation to avoid overfitting. Another means of correcting for overfitting is to adjust the model likelihoods using some ana‐ lytic criterion such as the Akaike information criterion (AIC) or the Bayesian infor‐ mation criterion (BIC) . Scikit-Learn’s GMM estimator actually includes built-in methods that compute both of these, and so it is very easy to operate on this approach. Let’s look at the AIC and BIC as a function as the number of GMM components for our moon dataset ( Figure 5-136 ): In [ 17 ]: n_components = np . arange ( 1 , 21 ) models = [ GMM ( n , covariance_type = 'full' , random_state = 0 ) . fit ( Xmoon ) for n in n_components ] plt . plot ( n_components , [ m . bic ( Xmoon ) for m in models ], label = 'BIC' ) plt . plot ( n_components , [ m . aic ( Xmoon ) for m in models ], label = 'AIC' ) plt . legend ( loc = 'best' ) plt . xlabel ( 'n_components' ); Figure 5-136. Visualization of AIC and BIC for choosing the number of GMM components The optimal number of clusters is the value that minimizes the AIC or BIC, depend‐ ing on which approximation we wish to use. The AIC tells us that our choice of 16 components was probably too many: around 8–12 components would have been a In Depth: Gaussian Mixture Models | 487 better choice. As is typical with this sort of problem, the BIC recommends a simpler model. Notice the important point: this choice of number of components measures how well GMM works as a density estimator , not how well it works as a clustering algorithm . I’d encourage you to think of GMM primarily as a density estimator, and use it for clus‐ tering only when warranted within simple datasets. Example: GMM for Generating New Data We just saw a simple example of using GMM as a generative model of data in order to create new samples from the distribution defined by the input data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_426"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Example: GMM for Generating New Data We just saw a simple example of using GMM as a generative model of data in order to create new samples from the distribution defined by the input data. Here we will run with this idea and generate new handwritten digits from the standard digits corpus that we have used before. To start with, let’s load the digits data using Scikit-Learn’s data tools: In [ 18 ]: from sklearn.datasets import load_digits digits = load_digits () digits . data . shape Out[18]: (1797, 64) Next let’s plot the first 100 of these to recall exactly what we’re looking at ( Figure 5-137 ): In [ 19 ]: def plot_digits ( data ): fig , ax = plt . subplots ( 10 , 10 , figsize = ( 8 , 8 ), subplot_kw = dict ( xticks = [], yticks = [])) fig . subplots_adjust ( hspace = 0.05 , wspace = 0.05 ) for i , axi in enumerate ( ax . flat ): im = axi . imshow ( data [ i ] . reshape ( 8 , 8 ), cmap = 'binary' ) im . set_clim ( 0 , 16 ) plot_digits ( digits . data ) We have nearly 1,800 digits in 64 dimensions, and we can build a GMM on top of these to generate more. GMMs can have difficulty converging in such a high dimen‐ sional space, so we will start with an invertible dimensionality reduction algorithm on the data. Here we will use a straightforward PCA, asking it to preserve 99% of the variance in the projected data: In [ 20 ]: from sklearn.decomposition import PCA pca = PCA ( 0.99 , whiten = True ) data = pca . fit_transform ( digits . data ) data . shape Out[20]: (1797, 41) 488 | Chapter 5: Machine Learning Figure 5-137. Handwritten digits input The result is 41 dimensions, a reduction of nearly 1/3 with almost no information loss. Given this projected data, let’s use the AIC to get a gauge for the number of GMM components we should use ( Figure 5-138 ): In [ 21 ]: n_components = np . arange ( 50 , 210 , 10 ) models = [ GMM ( n , covariance_type = 'full' , random_state = 0 ) for n in n_components ] aics = [ model . fit ( data ) . aic ( data ) for model in models ] plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_427"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". arange ( 50 , 210 , 10 ) models = [ GMM ( n , covariance_type = 'full' , random_state = 0 ) for n in n_components ] aics = [ model . fit ( data ) . aic ( data ) for model in models ] plt . plot ( n_components , aics ); It appears that around 110 components minimizes the AIC; we will use this model. Let’s quickly fit this to the data and confirm that it has converged: In [ 22 ]: gmm = GMM ( 110 , covariance_type = 'full' , random_state = 0 ) gmm . fit ( data ) print ( gmm . converged_ ) True Now we can draw samples of 100 new points within this 41-dimensional projected space, using the GMM as a generative model: In [ 23 ]: data_new = gmm . sample ( 100 , random_state = 0 ) data_new . shape Out[23]: (100, 41) In Depth: Gaussian Mixture Models | 489 Figure 5-138. AIC curve for choosing the appropriate number of GMM components Finally, we can use the inverse transform of the PCA object to construct the new dig‐ its ( Figure 5-139 ): In [ 24 ]: digits_new = pca . inverse_transform ( data_new ) plot_digits ( digits_new ) Figure 5-139. “New” digits randomly drawn from the underlying model of the GMM estimator 490 | Chapter 5: Machine Learning The results for the most part look like plausible digits from the dataset! Consider what we’ve done here: given a sampling of handwritten digits, we have modeled the distribution of that data in such a way that we can generate brand new samples of digits from the data: these are “handwritten digits” that do not individu‐ ally appear in the original dataset, but rather capture the general features of the input data as modeled by the mixture model. Such a generative model of digits can prove very useful as a component of a Bayesian generative classifier, as we shall see in the next section. In-Depth: Kernel Density Estimation In the previous section we covered Gaussian mixture models (GMM), which are a kind of hybrid between a clustering estimator and a density estimator",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_428"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In-Depth: Kernel Density Estimation In the previous section we covered Gaussian mixture models (GMM), which are a kind of hybrid between a clustering estimator and a density estimator. Recall that a density estimator is an algorithm that takes a D -dimensional dataset and produces an estimate of the D -dimensional probability distribution which that data is drawn from. The GMM algorithm accomplishes this by representing the density as a weighted sum of Gaussian distributions. Kernel density estimation (KDE) is in some senses an algorithm that takes the mixture-of-Gaussians idea to its logical extreme: it uses a mixture consisting of one Gaussian component per point , resulting in an essentially nonparametric estimator of density. In this section, we will explore the motivation and uses of KDE. We begin with the standard imports: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () import numpy as np Motivating KDE: Histograms As already discussed, a density estimator is an algorithm that seeks to model the probability distribution that generated a dataset. For one-dimensional data, you are probably already familiar with one simple density estimator: the histogram. A histo‐ gram divides the data into discrete bins, counts the number of points that fall in each bin, and then visualizes the results in an intuitive manner. For example, let’s create some data that is drawn from two normal distributions: In [ 2 ]: def make_data ( N , f = 0.3 , rseed = 1 ): rand = np . random . RandomState ( rseed ) x = rand . randn ( N ) x [ int ( f * N ):] += 5 return x x = make_data ( 1000 ) In-Depth: Kernel Density Estimation | 491 We have previously seen that the standard count-based histogram can be created with the plt.hist() function. By specifying the normed parameter of the histogram, we end up with a normalized histogram where the height of the bins does not reflect counts, but instead reflects probability density ( Figure 5-140 ): In [ 3 ]: hist = plt",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_429"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". hist ( x , bins = 30 , normed = True ) Figure 5-140. Data drawn from a combination of normal distributions Notice that for equal binning, this normalization simply changes the scale on the axis, leaving the relative heights essentially the same as in a histogram built from counts. This normalization is chosen so that the total area under the histogram is equal to 1, as we can confirm by looking at the output of the histogram function: In [ 4 ]: density , bins , patches = hist widths = bins [ 1 :] - bins [: - 1 ] ( density * widths ) . sum () Out[4]: 1.0 One of the issues with using a histogram as a density estimator is that the choice of bin size and location can lead to representations that have qualitatively different fea‐ tures. For example, if we look at a version of this data with only 20 points, the choice of how to draw the bins can lead to an entirely different interpretation of the data! Consider this example (visualized in Figure 5-141 ): In [ 5 ]: x = make_data ( 20 ) bins = np . linspace ( - 5 , 10 , 10 ) In [ 6 ]: fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 ), sharex = True , sharey = True , subplot_kw = { 'xlim' :( - 4 , 9 ), 'ylim' :( - 0.02 , 0.3 )}) fig . subplots_adjust ( wspace = 0.05 ) 492 | Chapter 5: Machine Learning for i , offset in enumerate ([ 0.0 , 0.6 ]): ax [ i ] . hist ( x , bins = bins + offset , normed = True ) ax [ i ] . plot ( x , np . full_like ( x , - 0.01 ), '|k' , markeredgewidth = 1 ) Figure 5-141. The problem with histograms: the location of bins can affect interpretation On the left, the histogram makes clear that this is a bimodal distribution. On the right, we see a unimodal distribution with a long tail. Without seeing the preceding code, you would probably not guess that these two histograms were built from the same data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_430"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". On the right, we see a unimodal distribution with a long tail. Without seeing the preceding code, you would probably not guess that these two histograms were built from the same data. With that in mind, how can you trust the intuition that histograms confer? And how might we improve on this? Stepping back, we can think of a histogram as a stack of blocks, where we stack one block within each bin on top of each point in the dataset. Let’s view this directly ( Figure 5-142 ): In [ 7 ]: fig , ax = plt . subplots () bins = np . arange ( - 3 , 8 ) ax . plot ( x , np . full_like ( x , - 0.1 ), '|k' , markeredgewidth = 1 ) for count , edge in zip ( * np . histogram ( x , bins )): for i in range ( count ): ax . add_patch ( plt . Rectangle (( edge , i ), 1 , 1 , alpha = 0.5 )) ax . set_xlim ( - 4 , 8 ) ax . set_ylim ( - 0.2 , 8 ) Out[7]: (-0.2, 8) In-Depth: Kernel Density Estimation | 493 Figure 5-142. Histogram as stack of blocks The problem with our two binnings stems from the fact that the height of the block stack often reflects not on the actual density of points nearby, but on coincidences of how the bins align with the data points. This misalignment between points and their blocks is a potential cause of the poor histogram results seen here. But what if, instead of stacking the blocks aligned with the bins , we were to stack the blocks aligned with the points they represent ? If we do this, the blocks won’t be aligned, but we can add their contributions at each location along the x-axis to find the result. Let’s try this ( Figure 5-143 ): In [ 8 ]: x_d = np . linspace ( - 4 , 8 , 2000 ) density = sum (( abs ( xi - x_d ) < 0.5 ) for xi in x ) plt . fill_between ( x_d , density , alpha = 0.5 ) plt . plot ( x , np . full_like ( x , - 0.1 ), '|k' , markeredgewidth = 1 ) plt . axis ([ - 4 , 8 , - 0.2 , 8 ]); 494 | Chapter 5: Machine Learning Figure 5-143",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_431"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". plot ( x , np . full_like ( x , - 0.1 ), '|k' , markeredgewidth = 1 ) plt . axis ([ - 4 , 8 , - 0.2 , 8 ]); 494 | Chapter 5: Machine Learning Figure 5-143. A “histogram” where blocks center on each individual point; this is an example of a kernel density estimate The result looks a bit messy, but is a much more robust reflection of the actual data characteristics than is the standard histogram. Still, the rough edges are not aestheti‐ cally pleasing, nor are they reflective of any true properties of the data. In order to smooth them out, we might decide to replace the blocks at each location with a smooth function, like a Gaussian. Let’s use a standard normal curve at each point instead of a block ( Figure 5-144 ): In [ 9 ]: from scipy.stats import norm x_d = np . linspace ( - 4 , 8 , 1000 ) density = sum ( norm ( xi ) . pdf ( x_d ) for xi in x ) plt . fill_between ( x_d , density , alpha = 0.5 ) plt . plot ( x , np . full_like ( x , - 0.1 ), '|k' , markeredgewidth = 1 ) plt . axis ([ - 4 , 8 , - 0.2 , 5 ]); In-Depth: Kernel Density Estimation | 495 Figure 5-144. A kernel density estimate with a Gaussian kernel This smoothed-out plot, with a Gaussian distribution contributed at the location of each input point, gives a much more accurate idea of the shape of the data distribu‐ tion, and one that has much less variance (i.e., changes much less in response to dif‐ ferences in sampling). These last two plots are examples of kernel density estimation in one dimension: the first uses a so-called “tophat” kernel and the second uses a Gaussian kernel. We’ll now look at kernel density estimation in more detail. Kernel Density Estimation in Practice The free parameters of kernel density estimation are the kernel , which specifies the shape of the distribution placed at each point, and the kernel bandwidth , which con‐ trols the size of the kernel at each point",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_432"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In practice, there are many kernels you might use for a kernel density estimation: in particular, the Scikit-Learn KDE implementa‐ tion supports one of six kernels, which you can read about in Scikit-Learn’s Density Estimation documentation . While there are several versions of kernel density estimation implemented in Python (notably in the SciPy and StatsModels packages), I prefer to use Scikit-Learn’s version because of its efficiency and flexibility. It is implemented in the sklearn.neigh bors.KernelDensity estimator, which handles KDE in multiple dimensions with one of six kernels and one of a couple dozen distance metrics. Because KDE can be fairly computationally intensive, the Scikit-Learn estimator uses a tree-based algorithm under the hood and can trade off computation time for accuracy using the atol (absolute tolerance) and rtol (relative tolerance) parameters. We can determine the 496 | Chapter 5: Machine Learning kernel bandwidth, which is a free parameter, using Scikit-Learn’s standard crosvalidation tools, as we will soon see. Let’s first see a simple example of replicating the preceding plot using the Scikit-Learn KernelDensity estimator ( Figure 5-145 ): In [ 10 ]: from sklearn.neighbors import KernelDensity # instantiate and fit the KDE model kde = KernelDensity ( bandwidth = 1.0 , kernel = 'gaussian' ) kde . fit ( x [:, None ]) # score_samples returns the log of the probability density logprob = kde . score_samples ( x_d [:, None ]) plt . fill_between ( x_d , np . exp ( logprob ), alpha = 0.5 ) plt . plot ( x , np . full_like ( x , - 0.01 ), '|k' , markeredgewidth = 1 ) plt . ylim ( - 0.02 , 0.22 ) Out[10]: (-0.02, 0.22) Figure 5-145. A kernel density estimate computed with Scikit-Learn The result here is normalized such that the area under the curve is equal to 1",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_433"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". ylim ( - 0.02 , 0.22 ) Out[10]: (-0.02, 0.22) Figure 5-145. A kernel density estimate computed with Scikit-Learn The result here is normalized such that the area under the curve is equal to 1. Selecting the bandwidth via cross-validation The choice of bandwidth within KDE is extremely important to finding a suitable density estimate, and is the knob that controls the bias–variance trade-off in the esti‐ mate of density: too narrow a bandwidth leads to a high-variance estimate (i.e., over‐ fitting), where the presence or absence of a single point makes a large difference. Too wide a bandwidth leads to a high-bias estimate (i.e., underfitting) where the structure in the data is washed out by the wide kernel. In-Depth: Kernel Density Estimation | 497 There is a long history in statistics of methods to quickly estimate the best bandwidth based on rather stringent assumptions about the data: if you look up the KDE imple‐ mentations in the SciPy and StatsModels packages, for example, you will see imple‐ mentations based on some of these rules. In machine learning contexts, we’ve seen that such hyperparameter tuning often is done empirically via a cross-validation approach. With this in mind, the KernelDen sity estimator in Scikit-Learn is designed such that it can be used directly within Scikit-Learn’s standard grid search tools. Here we will use GridSearchCV to optimize the bandwidth for the preceding dataset. Because we are looking at such a small data‐ set, we will use leave-one-out cross-validation, which minimizes the reduction in training set size for each cross-validation trial: In [ 11 ]: from sklearn.grid_search import GridSearchCV from sklearn.cross_validation import LeaveOneOut bandwidths = 10 ** np . linspace ( - 1 , 1 , 100 ) grid = GridSearchCV ( KernelDensity ( kernel = 'gaussian' ), { 'bandwidth' : bandwidths }, cv = LeaveOneOut ( len ( x ))) grid",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_434"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". linspace ( - 1 , 1 , 100 ) grid = GridSearchCV ( KernelDensity ( kernel = 'gaussian' ), { 'bandwidth' : bandwidths }, cv = LeaveOneOut ( len ( x ))) grid . fit ( x [:, None ]); Now we can find the choice of bandwidth that maximizes the score (which in this case defaults to the log-likelihood): In [ 12 ]: grid . best_params_ Out[12]: {'bandwidth': 1.1233240329780276} The optimal bandwidth happens to be very close to what we used in the example plot earlier, where the bandwidth was 1.0 (i.e., the default width of scipy.stats.norm ). Example: KDE on a Sphere Perhaps the most common use of KDE is in graphically representing distributions of points. For example, in the Seaborn visualization library (discussed earlier in “Visual‐ ization with Seaborn” on page 311 ), KDE is built in and automatically used to help visualize points in one and two dimensions. Here we will look at a slightly more sophisticated use of KDE for visualization of dis‐ tributions. We will make use of some geographic data that can be loaded with ScikiLearn: the geographic distributions of recorded observations of two South American mammals, Bradypus variegatus (the brown-throated sloth) and Microryzomys minu‐ tus (the forest small rice rat). With Scikit-Learn, we can fetch this data as follows: In [ 13 ]: from sklearn.datasets import fetch_species_distributions data = fetch_species_distributions () 498 | Chapter 5: Machine Learning # Get matrices/arrays of species IDs and locations latlon = np . vstack ([ data . train [ 'dd lat' ], data . train [ 'dd long' ]]) . T species = np . array ([ d . decode ( 'ascii' ) . startswith ( 'micro' ) for d in data",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_435"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". vstack ([ data . train [ 'dd lat' ], data . train [ 'dd long' ]]) . T species = np . array ([ d . decode ( 'ascii' ) . startswith ( 'micro' ) for d in data . train [ 'species' ]], dtype = 'int' ) With this data loaded, we can use the Basemap toolkit (mentioned previously in “Geographic Data with Basemap” on page 298 ) to plot the observed locations of these two species on the map of South America ( Figure 5-146 ): In [ 14 ]: from mpl_toolkits.basemap import Basemap from sklearn.datasets.species_distributions import construct_grids xgrid , ygrid = construct_grids ( data ) # plot coastlines with Basemap m = Basemap ( projection = 'cyl' , resolution = 'c' , llcrnrlat = ygrid . min (), urcrnrlat = ygrid . max (), llcrnrlon = xgrid . min (), urcrnrlon = xgrid . max ()) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . fillcontinents ( color = '#FFEEDD' ) m . drawcoastlines ( color = 'gray' , zorder = 2 ) m . drawcountries ( color = 'gray' , zorder = 2 ) # plot locations m . scatter ( latlon [:, 1 ], latlon [:, 0 ], zorder = 3 , c = species , cmap = 'rainbow' , latlon = True ); Figure 5-146. Location of species in training data Unfortunately, this doesn’t give a very good idea of the density of the species, because points in the species range may overlap one another. You may not realize it by looking at this plot, but there are over 1,600 points shown here! In-Depth: Kernel Density Estimation | 499 Let’s use kernel density estimation to show this distribution in a more interpretable way: as a smooth indication of density on the map. Because the coordinate system here lies on a spherical surface rather than a flat plane, we will use the haversine distance metric, which will correctly represent distances on a curved surface. There is a bit of boilerplate code here (one of the disadvantages of the Basemap tool‐ kit), but the meaning of each code block should be clear ( Figure 5-147 ): In [ 15 ]: # Set up the data grid for the contour plot X , Y = np",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_436"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". meshgrid ( xgrid [:: 5 ], ygrid [:: 5 ][:: - 1 ]) land_reference = data . coverages [ 6 ][:: 5 , :: 5 ] land_mask = ( land_reference > - 9999 ) . ravel () xy = np . vstack ([ Y . ravel (), X . ravel ()]) . T xy = np . radians ( xy [ land_mask ]) # Create two side-by-side plots fig , ax = plt . subplots ( 1 , 2 ) fig . subplots_adjust ( left = 0.05 , right = 0.95 , wspace = 0.05 ) species_names = [ 'Bradypus Variegatus' , 'Microryzomys Minutus' ] cmaps = [ 'Purples' , 'Reds' ] for i , axi in enumerate ( ax ): axi . set_title ( species_names [ i ]) # plot coastlines with Basemap m = Basemap ( projection = 'cyl' , llcrnrlat = Y . min (), urcrnrlat = Y . max (), llcrnrlon = X . min (), urcrnrlon = X . max (), resolution = 'c' , ax = axi ) m . drawmapboundary ( fill_color = '#DDEEFF' ) m . drawcoastlines () m . drawcountries () # construct a spherical kernel density estimate of the distribution kde = KernelDensity ( bandwidth = 0.03 , metric = 'haversine' ) kde . fit ( np . radians ( latlon [ species == i ])) # evaluate only on the land: -9999 indicates ocean Z = np . full ( land_mask . shape [ 0 ], - 9999.0 ) Z [ land_mask ] = np . exp ( kde . score_samples ( xy )) Z = Z . reshape ( X . shape ) # plot contours of the density levels = np . linspace ( 0 , Z . max (), 25 ) axi . contourf ( X , Y , Z , levels = levels , cmap = cmaps [ i ]) 500 | Chapter 5: Machine Learning Figure 5-147. A kernel density representation of the species distributions Compared to the simple scatter plot we initially used, this visualization paints a much clearer picture of the geographical distribution of observations of these two species. Example: Not-So-Naive Bayes This example looks at Bayesian generative classification with KDE, and demonstrates how to use the Scikit-Learn architecture to create a custom estimator. In “In Depth: Naive Bayes Classification” on page 382 , we took a look at naive Baye‐ sian classification, in which we created a simple generative model for each class, and used these models to build a fast classifier",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_437"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For naive Bayes, the generative model is a simple axis-aligned Gaussian. With a density estimation algorithm like KDE, we can remove the “naive” element and perform the same classification with a more sophisti‐ cated generative model for each class. It’s still Bayesian classification, but it’s no longer naive. The general approach for generative classification is this: 1. Split the training data by label. 2. For each set, fit a KDE to obtain a generative model of the data. This allows you for any observation x and label y to compute a likelihood P x y . 3. From the number of examples of each class in the training set, compute the class prior , P y . 4. For an unknown point x , the posterior probability for each class is P y x ∝ P x y P y . The class that maximizes this posterior is the label assigned to the point. In-Depth: Kernel Density Estimation | 501 The algorithm is straightforward and intuitive to understand; the more difficult piece is couching it within the Scikit-Learn framework in order to make use of the grid search and cross-validation architecture. This is the code that implements the algorithm within the Scikit-Learn framework; we will step through it following the code block: In [ 16 ]: from sklearn.base import BaseEstimator , ClassifierMixin class KDEClassifier ( BaseEstimator , ClassifierMixin ): \"\"\"Bayesian generative classification based on KDE Parameters ---------- bandwidth : float the kernel bandwidth within each class kernel : str the kernel name, passed to KernelDensity \"\"\" def __init__ ( self , bandwidth = 1.0 , kernel = 'gaussian' ): self . bandwidth = bandwidth self . kernel = kernel def fit ( self , X , y ): self . classes_ = np . sort ( np . unique ( y )) training_sets = [ X [ y == yi ] for yi in self . classes_ ] self . models_ = [ KernelDensity ( bandwidth = self . bandwidth , kernel = self . kernel ) . fit ( Xi ) for Xi in training_sets ] self . logpriors_ = [ np . log ( Xi . shape [ 0 ] / X",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_438"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". classes_ ] self . models_ = [ KernelDensity ( bandwidth = self . bandwidth , kernel = self . kernel ) . fit ( Xi ) for Xi in training_sets ] self . logpriors_ = [ np . log ( Xi . shape [ 0 ] / X . shape [ 0 ]) for Xi in training_sets ] return self def predict_proba ( self , X ): logprobs = np . array ([ model . score_samples ( X ) for model in self . models_ ]) . T result = np . exp ( logprobs + self . logpriors_ ) return result / result . sum ( 1 , keepdims = True ) def predict ( self , X ): return self . classes_ [ np . argmax ( self . predict_proba ( X ), 1 )] The anatomy of a custom estimator Let’s step through this code and discuss the essential features: from sklearn.base import BaseEstimator , ClassifierMixin class KDEClassifier ( BaseEstimator , ClassifierMixin ): \"\"\"Bayesian generative classification based on KDE 502 | Chapter 5: Machine Learning Parameters ---------- bandwidth : float the kernel bandwidth within each class kernel : str the kernel name, passed to KernelDensity \"\"\" Each estimator in Scikit-Learn is a class, and it is most convenient for this class to inherit from the BaseEstimator class as well as the appropriate mixin, which pro‐ vides standard functionality. For example, among other things, here the BaseEstima tor contains the logic necessary to clone/copy an estimator for use in a crosvalidation procedure, and ClassifierMixin defines a default score() method used by such routines. We also provide a docstring, which will be captured by IPython’s help functionality (see “Help and Documentation in IPython” on page 3 ). Next comes the class initialization method: def __init__ ( self , bandwidth = 1.0 , kernel = 'gaussian' ): self . bandwidth = bandwidth self . kernel = kernel This is the actual code executed when the object is instantiated with KDEClassi fier() . In Scikit-Learn, it is important that initialization contains no operations other than assigning the passed values by name to self",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_439"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Scikit-Learn, it is important that initialization contains no operations other than assigning the passed values by name to self . This is due to the logic contained in BaseEstimator required for cloning and modifying estimators for crosvalidation, grid search, and other functions. Similarly, all arguments to __init__ should be explicit; that is, *args or **kwargs should be avoided, as they will not be correctly handled within cross-validation routines. Next comes the fit() method, where we handle training data: def fit ( self , X , y ): self . classes_ = np . sort ( np . unique ( y )) training_sets = [ X [ y == yi ] for yi in self . classes_ ] self . models_ = [ KernelDensity ( bandwidth = self . bandwidth , kernel = self . kernel ) . fit ( Xi ) for Xi in training_sets ] self . logpriors_ = [ np . log ( Xi . shape [ 0 ] / X . shape [ 0 ]) for Xi in training_sets ] return self Here we find the unique classes in the training data, train a KernelDensity model for each class, and compute the class priors based on the number of input samples. Finally, fit() should always return self so that we can chain commands. For example: label = model . fit ( X , y ) . predict ( X ) In-Depth: Kernel Density Estimation | 503 Notice that each persistent result of the fit is stored with a trailing underscore (e.g., self.logpriors_ ). This is a convention used in Scikit-Learn so that you can quickly scan the members of an estimator (using IPython’s tab completion) and see exactly which members are fit to training data. Finally, we have the logic for predicting labels on new data: def predict_proba ( self , X ): logprobs = np . vstack ([ model . score_samples ( X ) for model in self . models_ ]) . T result = np . exp ( logprobs + self . logpriors_ ) return result / result . sum ( 1 , keepdims = True ) def predict ( self , X ): return self . classes_ [ np . argmax ( self",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_440"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". models_ ]) . T result = np . exp ( logprobs + self . logpriors_ ) return result / result . sum ( 1 , keepdims = True ) def predict ( self , X ): return self . classes_ [ np . argmax ( self . predict_proba ( X ), 1 )] Because this is a probabilistic classifier, we first implement predict_proba() , which returns an array of class probabilities of shape [n_samples, n_classes] . Entry [i, j] of this array is the posterior probability that sample i is a member of class j , com‐ puted by multiplying the likelihood by the class prior and normalizing. Finally, the predict() method uses these probabilities and simply returns the class with the largest probability. Using our custom estimator Let’s try this custom estimator on a problem we have seen before: the classification of handwritten digits. Here we will load the digits, and compute the cross-validation score for a range of candidate bandwidths using the GridSearchCV meta-estimator (refer back to “Hyperparameters and Model Validation” on page 359 for more infor‐ mation on this): In [ 17 ]: from sklearn.datasets import load_digits from sklearn.grid_search import GridSearchCV digits = load_digits () bandwidths = 10 ** np . linspace ( 0 , 2 , 100 ) grid = GridSearchCV ( KDEClassifier (), { 'bandwidth' : bandwidths }) grid . fit ( digits . data , digits . target ) scores = [ val . mean_validation_score for val in grid . grid_scores_ ] Next we can plot the cross-validation score as a function of bandwidth ( Figure 5-148 ): In [ 18 ]: plt . semilogx ( bandwidths , scores ) plt . xlabel ( 'bandwidth' ) plt . ylabel ( 'accuracy' ) plt . title ( 'KDE Model Performance' ) 504 | Chapter 5: Machine Learning print ( grid . best_params_ ) print ( 'accuracy =' , grid . best_score_ ) {'bandwidth': 7.0548023107186433} accuracy = 0.966611018364 Figure 5-148",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_441"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". best_params_ ) print ( 'accuracy =' , grid . best_score_ ) {'bandwidth': 7.0548023107186433} accuracy = 0.966611018364 Figure 5-148. Validation curve for the KDE-based Bayesian classifier We see that this not-so-naive Bayesian classifier reaches a cross-validation accuracy of just over 96%; this is compared to around 80% for the naive Bayesian classification: In [ 19 ]: from sklearn.naive_bayes import GaussianNB from sklearn.cross_validation import cross_val_score cross_val_score ( GaussianNB (), digits . data , digits . target ) . mean () Out[19]: 0.81860038035501381 One benefit of such a generative classifier is interpretability of results: for each unknown sample, we not only get a probabilistic classification, but a full model of the distribution of points we are comparing it to! If desired, this offers an intuitive win‐ dow into the reasons for a particular classification that algorithms like SVMs and ran‐ dom forests tend to obscure. If you would like to take this further, there are some improvements that could be made to our KDE classifier model: • We could allow the bandwidth in each class to vary independently. • We could optimize these bandwidths not based on their prediction score, but on the likelihood of the training data under the generative model within each class (i.e., use the scores from KernelDensity itself rather than the global prediction accuracy). In-Depth: Kernel Density Estimation | 505 Finally, if you want some practice building your own estimator, you might tackle building a similar Bayesian classifier using Gaussian mixture models instead of KDE. Application: A Face Detection Pipeline This chapter has explored a number of the central concepts and algorithms of machine learning. But moving from these concepts to real-world application can be a challenge. Real-world datasets are noisy and heterogeneous, may have missing fea‐ tures, and may include data in a form that is difficult to map to a clean [n_samples, n_features] matrix",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_442"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Real-world datasets are noisy and heterogeneous, may have missing fea‐ tures, and may include data in a form that is difficult to map to a clean [n_samples, n_features] matrix. Before applying any of the methods discussed here, you must first extract these features from your data; there is no formula for how to do this that applies across all domains, and thus this is where you as a data scientist must exercise your own intuition and expertise. One interesting and compelling application of machine learning is to images, and we have already seen a few examples of this where pixel-level features are used for classi‐ fication. In the real world, data is rarely so uniform and simple pixels will not be suit‐ able, a fact that has led to a large literature on feature extraction methods for image data (see “Feature Engineering” on page 375 ). In this section, we will take a look at one such feature extraction technique, the Histo‐ gram of Oriented Gradients (HOG), which transforms image pixels into a vector rep‐ resentation that is sensitive to broadly informative image features regardless of confounding factors like illumination. We will use these features to develop a simple face detection pipeline, using machine learning algorithms and concepts we’ve seen throughout this chapter. We begin with the standard imports: In [ 1 ]: % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns ; sns . set () import numpy as np HOG Features The Histogram of Gradients is a straightforward feature extraction procedure that was developed in the context of identifying pedestrians within images. HOG involves the following steps: 1. Optionally prenormalize images. This leads to features that resist dependence on variations in illumination. 2. Convolve the image with two filters that are sensitive to horizontal and vertical brightness gradients. These capture edge, contour, and texture information. 3",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_443"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2. Convolve the image with two filters that are sensitive to horizontal and vertical brightness gradients. These capture edge, contour, and texture information. 3. Subdivide the image into cells of a predetermined size, and compute a histogram of the gradient orientations within each cell. 506 | Chapter 5: Machine Learning 4. Normalize the histograms in each cell by comparing to the block of neighboring cells. This further suppresses the effect of illumination across the image. 5. Construct a one-dimensional feature vector from the information in each cell. A fast HOG extractor is built into the Scikit-Image project, and we can try it out rela‐ tively quickly and visualize the oriented gradients within each cell ( Figure 5-149 ): In [ 2 ]: from skimage import data , color , feature import skimage.data image = color . rgb2gray ( data . chelsea ()) hog_vec , hog_vis = feature . hog ( image , visualise = True ) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 6 ), subplot_kw = dict ( xticks = [], yticks = [])) ax [ 0 ] . imshow ( image , cmap = 'gray' ) ax [ 0 ] . set_title ( 'input image' ) ax [ 1 ] . imshow ( hog_vis ) ax [ 1 ] . set_title ( 'visualization of HOG features' ); Figure 5-149. Visualization of HOG features computed from an image HOG in Action: A Simple Face Detector Using these HOG features, we can build up a simple facial detection algorithm with any Scikit-Learn estimator; here we will use a linear support vector machine (refer back to “In-Depth: Support Vector Machines” on page 405 if you need a refresher on this). The steps are as follows: 1. Obtain a set of image thumbnails of faces to constitute “positive” training samples. 2. Obtain a set of image thumbnails of nonfaces to constitute “negative” training samples. 3. Extract HOG features from these training samples. Application: A Face Detection Pipeline | 507 4. Train a linear SVM classifier on these samples. 5",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_444"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3. Extract HOG features from these training samples. Application: A Face Detection Pipeline | 507 4. Train a linear SVM classifier on these samples. 5. For an “unknown” image, pass a sliding window across the image, using the model to evaluate whether that window contains a face or not. 6. If detections overlap, combine them into a single window. Let’s go through these steps and try it out: 1. Obtain a set of positive training samples. Let’s start by finding some positive training samples that show a variety of faces. We have one easy set of data to work with—the Labeled Faces in the Wild dataset, which can be downloaded by Scikit-Learn: In [ 3 ]: from sklearn.datasets import fetch_lfw_people faces = fetch_lfw_people () positive_patches = faces . images positive_patches . shape Out[3]: (13233, 62, 47) This gives us a sample of 13,000 face images to use for training. 2. Obtain a set of negative training samples. Next we need a set of similarly sized thumbnails that do not have a face in them. One way to do this is to take any corpus of input images, and extract thumbnails from them at a variety of scales. Here we can use some of the images shipped with Scikit-Image, along with Scikit-Learn’s PatchExtractor : In [ 4 ]: from skimage import data , transform imgs_to_use = [ 'camera' , 'text' , 'coins' , 'moon' , 'page' , 'clock' , 'immunohistochemistry' , 'chelsea' , 'coffee' , 'hubble_deep_field' ] images = [ color . rgb2gray ( getattr ( data , name )()) for name in imgs_to_use ] In [ 5 ]: from sklearn.feature_extraction.image import PatchExtractor def extract_patches ( img , N , scale = 1.0 , patch_size = positive_patches [ 0 ] . shape ): extracted_patch_size = \\ tuple (( scale * np . array ( patch_size )) . astype ( int )) extractor = PatchExtractor ( patch_size = extracted_patch_size , max_patches = N , random_state = 0 ) patches = extractor . transform ( img [ np . newaxis ]) if scale != 1 : patches = np . array ([ transform",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_445"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". transform ( img [ np . newaxis ]) if scale != 1 : patches = np . array ([ transform . resize ( patch , patch_size ) for patch in patches ]) return patches 508 | Chapter 5: Machine Learning negative_patches = np . vstack ([ extract_patches ( im , 1000 , scale ) for im in images for scale in [ 0.5 , 1.0 , 2.0 ]]) negative_patches . shape Out[5]: (30000, 62, 47) We now have 30,000 suitable image patches that do not contain faces. Let’s take a look at a few of them to get an idea of what they look like ( Figure 5-150 ): In [ 6 ]: fig , ax = plt . subplots ( 6 , 10 ) for i , axi in enumerate ( ax . flat ): axi . imshow ( negative_patches [ 500 * i ], cmap = 'gray' ) axi . axis ( 'off' ) Figure 5-150. Negative image patches, which don’t include faces Our hope is that these would sufficiently cover the space of “nonfaces” that our algorithm is likely to see. 3. Combine sets and extract HOG features. Now that we have these positive samples and negative samples, we can combine them and compute HOG features. This step takes a little while, because the HOG features involve a nontrivial computation for each image: In [ 7 ]: from itertools import chain X_train = np . array ([ feature . hog ( im ) for im in chain ( positive_patches , negative_patches )]) y_train = np . zeros ( X_train . shape [ 0 ]) y_train [: positive_patches . shape [ 0 ]] = 1 In [ 8 ]: X_train . shape Out[8]: (43233, 1215) Application: A Face Detection Pipeline | 509 We are left with 43,000 training samples in 1,215 dimensions, and we now have our data in a form that we can feed into Scikit-Learn! 4. Train a support vector machine. Next we use the tools we have been exploring in this chapter to create a classifier of thumbnail patches. For such a high-dimensional binary classification task, a linear support vector machine is a good choice. We will use Scikit-Learn’s Line arSVC , because in comparison to SVC it often has better scaling for large number of samples",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_446"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We will use Scikit-Learn’s Line arSVC , because in comparison to SVC it often has better scaling for large number of samples. First, though, let’s use a simple Gaussian naive Bayes to get a quick baseline: In [ 9 ]: from sklearn.naive_bayes import GaussianNB from sklearn.cross_validation import cross_val_score cross_val_score ( GaussianNB (), X_train , y_train ) Out[9]: array([ 0.9408785 , 0.8752342 , 0.93976823]) We see that on our training data, even a simple naive Bayes algorithm gets us upward of 90% accuracy. Let’s try the support vector machine, with a grid search over a few choices of the C parameter: In [ 10 ]: from sklearn.svm import LinearSVC from sklearn.grid_search import GridSearchCV grid = GridSearchCV ( LinearSVC (), { 'C' : [ 1.0 , 2.0 , 4.0 , 8.0 ]}) grid . fit ( X_train , y_train ) grid . best_score_ Out[10]: 0.98667684407744083 In [ 11 ]: grid . best_params_ Out[11]: {'C': 4.0} Let’s take the best estimator and retrain it on the full dataset: In [ 12 ]: model = grid . best_estimator_ model . fit ( X_train , y_train ) Out[12]: LinearSVC(C=4.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0) 5. Find faces in a new image. Now that we have this model in place, let’s grab a new image and see how the model does. We will use one portion of the astronaut image for simplicity (see discussion of this in “Caveats and Improvements” on page 512 ), and run a sliding window over it and evaluate each patch ( Figure 5-151 ): 510 | Chapter 5: Machine Learning In [ 13 ]: test_image = skimage . data . astronaut () test_image = skimage . color . rgb2gray ( test_image ) test_image = skimage . transform . rescale ( test_image , 0.5 ) test_image = test_image [: 160 , 40 : 180 ] plt . imshow ( test_image , cmap = 'gray' ) plt . axis ( 'off' ); Figure 5-151",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_447"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". transform . rescale ( test_image , 0.5 ) test_image = test_image [: 160 , 40 : 180 ] plt . imshow ( test_image , cmap = 'gray' ) plt . axis ( 'off' ); Figure 5-151. An image in which we will attempt to locate a face Next, let’s create a window that iterates over patches of this image, and compute HOG features for each patch: In [ 14 ]: def sliding_window ( img , patch_size = positive_patches [ 0 ] . shape , istep = 2 , jstep = 2 , scale = 1.0 ): Ni , Nj = ( int ( scale * s ) for s in patch_size ) for i in range ( 0 , img . shape [ 0 ] - Ni , istep ): for j in range ( 0 , img . shape [ 1 ] - Ni , jstep ): patch = img [ i : i + Ni , j : j + Nj ] if scale != 1 : patch = transform . resize ( patch , patch_size ) yield ( i , j ), patch indices , patches = zip ( * sliding_window ( test_image )) patches_hog = np . array ([ feature . hog ( patch ) for patch in patches ]) patches_hog . shape Out[14]: (1911, 1215) Finally, we can take these HOG-featured patches and use our model to evaluate whether each patch contains a face: In [ 15 ]: labels = model . predict ( patches_hog ) labels . sum () Application: A Face Detection Pipeline | 511 Out[15]: 33.0 We see that out of nearly 2,000 patches, we have found 30 detections. Let’s use the information we have about these patches to show where they lie on our test image, drawing them as rectangles ( Figure 5-152 ): In [ 16 ]: fig , ax = plt . subplots () ax . imshow ( test_image , cmap = 'gray' ) ax . axis ( 'off' ) Ni , Nj = positive_patches [ 0 ] . shape indices = np . array ( indices ) for i , j in indices [ labels == 1 ]: ax . add_patch ( plt . Rectangle (( j , i ), Nj , Ni , edgecolor = 'red' , alpha = 0.3 , lw = 2 , facecolor = 'none' )) Figure 5-152. Windows that were determined to contain a face All of the detected patches overlap and found the face in the image! Not bad for a few lines of Python",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_448"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Windows that were determined to contain a face All of the detected patches overlap and found the face in the image! Not bad for a few lines of Python. Caveats and Improvements If you dig a bit deeper into the preceding code and examples, you’ll see that we still have a bit of work before we can claim a production-ready face detector. There are several issues with what we’ve done, and several improvements that could be made. In particular: 512 | Chapter 5: Machine Learning Our training set, especially for negative features, is not very complete The central issue is that there are many face-like textures that are not in the train‐ ing set, and so our current model is very prone to false positives. You can see this if you try out the preceding algorithm on the full astronaut image: the current model leads to many false detections in other regions of the image. We might imagine addressing this by adding a wider variety of images to the neg‐ ative training set, and this would probably yield some improvement. Another way to address this is to use a more directed approach, such as hard negative min‐ ing . In hard negative mining, we take a new set of images that our classifier has not seen, find all the patches representing false positives, and explicitly add them as negative instances in the training set before retraining the classifier. Our current pipeline searches only at one scale As currently written, our algorithm will miss faces that are not approximately 62×47 pixels. We can straightforwardly address this by using sliding windows of a variety of sizes, and resizing each patch using skimage.transform.resize before feeding it into the model. In fact, the sliding_window() utility used here is already built with this in mind. We should combine overlapped detection patches For a production-ready pipeline, we would prefer not to have 30 detections of the same face, but to somehow reduce overlapping groups of detections down to a single detection",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_449"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This could be done via an unsupervised clustering approach (MeanShift Clustering is one good candidate for this), or via a procedural approach such as nonmaximum suppression , an algorithm common in machine vision. The pipeline should be streamlined Once we address these issues, it would also be nice to create a more streamlined pipeline for ingesting training images and predicting sliding-window outputs. This is where Python as a data science tool really shines: with a bit of work, we could take our prototype code and package it with a well-designed objecoriented API that gives the user the ability to use this easily. I will leave this as a proverbial “exercise for the reader.” More recent advances, such as deep learning, should be considered Finally, I should add that HOG and other procedural feature extraction methods for images are no longer state-of-the-art techniques. Instead, many modern object detection pipelines use variants of deep neural networks. One way to think of neural networks is that they are an estimator that determines optimal feature extraction strategies from the data, rather than relying on the intuition of the user. An intro to these deep neural net methods is conceptually (and compu‐ tationally!) beyond the scope of this section, although open tools like Google’s Application: A Face Detection Pipeline | 513 TensorFlow have recently made deep learning approaches much more accessible than they once were. As of the writing of this book, deep learning in Python is still relatively young, and so I can’t yet point to any definitive resource. That said, the list of references in the following section should provide a useful place to start. Further Machine Learning Resources This chapter has been a quick tour of machine learning in Python, primarily using the tools within the Scikit-Learn library. As long as the chapter is, it is still too short to cover many interesting and important algorithms, approaches, and discussions",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_450"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As long as the chapter is, it is still too short to cover many interesting and important algorithms, approaches, and discussions. Here I want to suggest some resources for those who would like to learn more about machine learning. Machine Learning in Python To learn more about machine learning in Python, I’d suggest some of the following resources: The Scikit-Learn website The Scikit-Learn website has an impressive breadth of documentation and exam‐ ples covering some of the models discussed here, and much, much more. If you want a brief survey of the most important and often used machine learning algo‐ rithms, this website is a good place to start. SciPy, PyCon, and PyData tutorial videos Scikit-Learn and other machine learning topics are perennial favorites in the tutorial tracks of many Python-focused conference series, in particular the PyCon, SciPy, and PyData conferences. You can find the most recent ones via a simple web search. Introduction to Machine Learning with Python Written by Andreas C. Mueller and Sarah Guido, this book includes a fuller treat‐ ment of the topics in this chapter. If you’re interested in reviewing the fundamen‐ tals of machine learning and pushing the Scikit-Learn toolkit to its limits, this is a great resource, written by one of the most prolific developers on the Scikit-Learn team. Python Machine Learning Sebastian Raschka’s book focuses less on Scikit-Learn itself, and more on the breadth of machine learning tools available in Python. In particular, there is some very useful discussion on how to scale Python-based machine learning approaches to large and complex datasets. 514 | Chapter 5: Machine Learning General Machine Learning Of course, machine learning is much broader than just the Python world",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_451"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 514 | Chapter 5: Machine Learning General Machine Learning Of course, machine learning is much broader than just the Python world. There are many good resources to take your knowledge further, and here I highlight a few that I have found useful: Machine Learning Taught by Andrew Ng (Coursera), this is a very clearly taught, free online course covering the basics of machine learning from an algorithmic perspective. It assumes undergraduate-level understanding of mathematics and programming, and steps through detailed considerations of some of the most important machine learning algorithms. Homework assignments, which are algorithmically graded, have you actually implement some of these models yourself. Pattern Recognition and Machine Learning Written by Christopher Bishop, this classic technical text covers the concepts of machine learning discussed in this chapter in detail. If you plan to go further in this subject, you should have this book on your shelf. Machine Learning: A Probabilistic Perspective Written by Kevin Murphy, this is an excellent graduate-level text that explores nearly all important machine learning algorithms from a ground-up, unified probabilistic perspective. These resources are more technical than the material presented in this book, but to really understand the fundamentals of these methods requires a deep dive into the mathematics behind them",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_452"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These resources are more technical than the material presented in this book, but to really understand the fundamentals of these methods requires a deep dive into the mathematics behind them. If you’re up for the challenge and ready to bring your data science to the next level, don’t hesitate to dive in! Further Machine Learning Resources | 515 Index Symbols %automagic, 19 %cpaste, 11 %debug, 22 %history, 16 %lprun, 28 %lsmagic, 13 %magic, 13 %matplotlib, 219 %memit, 29 %mode, 20 - 22 %mprun, 29 %paste, 11 %prun, 27 %run, 12 %time, 25 - 27 %timeit, 12 , 25 - 27 & (ampersand), 77 * (asterisk), 7 : (colon), 44 ? (question mark), 3 ?? (double question mark), 5 _ (underscore) shortcut, 15 | (operator), 77 A absolute value function, 54 aggregate() method, 166 aggregates computed directly from object, 57 multidimensional, 60 summarizing set of values with, 61 aggregation (NumPy), 58 - 63 minimum and maximum, 59 multidimensional aggregates, 60 presidents average height example, 61 summing the values in an array, 59 various functions, 61 aggregation (Pandas), 158 - 170 groupby() operation, 161 - 170 MultiIndex, 140 Planets dataset for, 159 simple aggregation, 159 - 161 Akaike information criterion (AIC), 487 , 489 Albers equal-area projection, 303 algorithmic efficiency big-O notation, 92 dataset size and, 85 ampersand (&), 77 Anaconda, xiv and keyword, 77 annotation of plots, 268 - 275 arrows, 272 - 275 holidays/US births example, 269 transforms and text position, 270 - 272 APIs (see Estimator API) append() method, Pandas vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_453"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Python, 146 apply() method, 167 arithmetic operators, 52 arrays accessing single rows/columns, 45 arithmetic operators, 52 attributes, 42 basics, 42 Boolean, 73 - 75 broadcasting, 63 - 69 centering, 68 computation on, 50 - 58 517 concatenation, 48 , 142 creating copies, 46 creating from Python lists, 39 creating from scratch, 39 data as, 33 DataFrame object as, 102 DataFrame object constructed from, 105 fixed-type, 38 Index object as immutable array, 106 Index object vs., 106 indexing: accessing single elements, 43 reshaping, 47 Series object vs., 99 slicing, 44 slicing multidimensional subarrays, 45 slicing one-dimensional subarrays, 44 sorting, 85 - 96 specifying output to, 56 splitting, 49 standard data types, 41 structured, 92 - 96 subarrays as no-copy views, 46 summing values in, 59 universal functions, 50 - 58 arrows, 272 - 275 asfreq() method, 197 - 199 asterisk (*), 7 automagic function, 19 axes limits, 228 - 230 B bagging, 426 bandwidth (see kernel bandwidth) bar (|) operator, 77 bar plots, 321 Basemap toolkit geographic data with, 298 (see also geographic data) installation, 298 basis function regression, 378 , 392 - 396 Gaussian basis functions, 394 - 396 polynomial basis functions, 393 Bayesian classification, 383 , 501 - 506 (see also naive Bayes classification) Bayesian information criterion (BIC), 487 Bayesian Methods for Hackers stylesheet, 288 Bayess theorem, 383 bias–variance trade-off kernel bandwidth and, 497 model selection and, 364 - 366 bicycle traffic prediction linear regression, 400 time series, 202 - 209 big-O notation, 92 binary ufuncs, 52 binnings, 248 bitwise logic operators, 74 bogosort, 86 Bokeh, 330 Boolean arrays Boolean operators and, 74 counting entries in, 73 working with, 73 - 75 Boolean masks, 70 - 78 Boolean arrays as, 75 - 78 rainfall statistics, 70 working with Boolean arrays, 73 - 75 Boolean operators, 74 broadcasting, 63 - 69 adding two-dimensional array to ondimensional array, 66 basics, 63 - 65 centering an array, 68 defined, 58 , 63 in practice, 68 plotting",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_454"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "arrays, 73 - 75 Boolean operators, 74 broadcasting, 63 - 69 adding two-dimensional array to ondimensional array, 66 basics, 63 - 65 centering an array, 68 defined, 58 , 63 in practice, 68 plotting two-dimensional function, 69 rules, 65 - 68 two compatible arrays, 66 two incompatible arrays, 67 C categorical data, 376 class labels (for data point), 334 classification task defined, 332 machine learning, 333 - 335 clustering, 332 basics, 338 - 339 GMMs, 353 , 476 - 491 k-means, 339 , 462 - 476 code magic commands for determining execu‐ tion time, 12 magic commands for pasting blocks, 11 magic commands for running external, 12 profiling and timing, 25 - 30 timing of snippets, 25 - 27 coefficient of determination, 365 518 | Index colon (:), 44 color compression, 473 - 476 colorbars colormap selection, 256 - 259 customizing, 255 - 262 discrete, 260 handwritten digit example, 261 - 262 colormap, 256 - 259 column(s) accessing single, 45 indexing, 163 MultiIndex for, 133 sorting arrays along, 87 suffixes keyword and overlapping names, 153 column-wise operations, 211 - 213 command history shortcuts, 9 comparison operators, 71 - 73 concatenation datasets, 141 - 146 of arrays, 48 , 142 with pd.concat(), 142 - 146 confusion matrix, 357 conic projections, 303 contour plots, 241 - 245 density and, 241 - 245 three-dimensional function, 241 - 245 three-dimensional plot, 292 Conway, Drew, xi cross-validation, 361 - 370 cubehelix colormap, 258 cylindrical projections, 301 D data as arrays, 33 missing (see missing data) data representation (Scikit-Learn package), 343 - 346 data as table, 343 features matrix, 344 target array, 344 - 345 data science, defining, xi data types, 34 fixed-type arrays, 38 integers, 35 lists in, 37 - 41 NumPy, 41 DataFrame object (Pandas), 102 - 105 as dictionary, 110 - 112 as generalized NumPy array, 102 as specialized dictionary, 103 as two-dimensional array, 112 - 114 constructing, 104 data selection in, 110 defined, 97 index alignment in, 117 masking, 114 multiply indexed, 136 operations between Series",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_455"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "dictionary, 103 as two-dimensional array, 112 - 114 constructing, 104 data selection in, 110 defined, 97 index alignment in, 117 masking, 114 multiply indexed, 136 operations between Series object and, 118 slicing, 114 DataFrame.eval() method, 211 - 213 assignment in, 212 local variables in, 213 DataFrame.query() method, 213 datasets appending, 146 combining (Panda), 141 - 158 concatenation, 141 - 146 merging/joining, 146 - 158 datetime module, 189 datetime64 dtype, 189 dateutil module, 189 debugging, 22 - 24 decision trees, 421 - 426 (see also random forests) creating, 422 - 425 overfitting, 425 deep learning, 513 density estimator GMM, 484 - 488 histogram as, 492 KDE (see kernel density estimation (KDE)) describe() method, 164 development, IPython profiling and timing code, 25 - 30 profiling full scripts, 27 timing of code snippets, 25 - 27 dictionary(-ies) DataFrame as specialization of, 103 DataFrame object constructed from list of, 104 Pandas Series object vs., 100 digits, recognition of (see optical character rec‐ ognition) dimensionality reduction, 261 machine learning, 340 - 342 PCA and, 433 Index | 519 discriminative classification, 405 - 407 documentation, accessing IPython, 3 - 8 , 98 Pandas, 98 double question mark (??), 5 dropna() method, 125 dynamic typing, 34 E eigenfaces, 442 - 445 ensemble estimator/method, 421 (see also random forests) ensemble learner, 421 equidistant cylindrical projection, 301 errors, visualizing basic errorbars, 238 continuous quantities, 239 Matplotlib, 237 - 240 Estimator API, 346 - 359 basics, 347 Iris classification example, 351 Iris clustering example, 353 Iris dimensionality example, 352 simple linear regression example, 347 - 354 eval() function, 210 - 211 DataFrame.eval() method and, 211 - 213 pd.eval() function and, 210 - 211 when to use, 214 exceptions, controlling, 20 - 22 expectation-maximization (E-M) algorithm caveats, 467 - 470 GMM as generalization of, 480 - 484 k-means clustering and, 465 - 476 exponentials, 55 external code, magic commands for running, 12",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_456"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "- 22 expectation-maximization (E-M) algorithm caveats, 467 - 470 GMM as generalization of, 480 - 484 k-means clustering and, 465 - 476 exponentials, 55 external code, magic commands for running, 12 F face recognition HOG, 506 - 514 Isomap, 456 - 460 PCA, 442 - 445 SVMs, 416 - 420 faceted histograms, 318 factor plots, 319 fancy indexing, 78 - 85 basics, 79 binning data, 83 combined with other indexing schemes, 80 modifying values with, 82 selection of random points, 81 feature engineering, 375 - 382 categorical features, 376 derived features, 378 - 380 image features, 378 imputation of missing data, 381 processing pipeline, 381 text features, 377 feature, data point, 334 features matrix, 344 fillna() method, 126 filter() method, 166 FiveThirtyEight stylesheet, 287 fixed-type arrays, 38 G Gaussian basis functions, 394 - 396 Gaussian mixture models (GMMs), 476 - 491 choosing covariance type, 484 clustering with, 353 density estimation algorithm, 484 - 488 E–M generalization, 480 - 484 handwritten data generation example, 488 - 491 k-means weaknesses addressed by, 477 - 480 KDE and, 491 Gaussian naive Bayes classification, 351 , 357 , 383 - 386 , 510 Gaussian process regression (GPR), 239 generative models, 383 geographic data, 298 Basemap toolkit for, 298 California city population example, 308 drawing a map background, 304 - 307 map projections, 300 - 304 plotting data on maps, 307 surface temperature data example, 309 get() operation, 183 get_dummies() method, 183 ggplot stylesheet, 287 graphics libraries, 330 GroupBy aggregation, 170 GroupBy object, 163 - 165 aggregate() method, 166 apply() method, 167 column indexing, 163 dispatch methods, 164 filter() method, 166 520 | Index iteration over groups, 164 transform() method, 167 groupby() operation (Pandas), 161 - 170 GroupBy object and, 163 - 165 grouping example, 169 pivot tables vs., 171 split key specification, 168 split-apply-combine example, 161 - 163 H handwritten digits, recognition of (see optical character recognition) hard negative mining, 513 help",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_457"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "169 pivot tables vs., 171 split key specification, 168 split-apply-combine example, 161 - 163 H handwritten digits, recognition of (see optical character recognition) hard negative mining, 513 help IPython, 3 - 8 magic functions, 13 help() function, 4 hexagonal binnings, 248 hierarchical indexing in one-dimensional Series, 128 - 141 MultiIndex, 128 - 141 , 129 - 131 (see also MultiIndex type) rearranging multi-indices, 137 - 140 unstack() method, 130 with Python tuples as keys, 128 Histogram of Oriented Gradients (HOG) caveats and improvements, 512 - 514 features, 506 for face detection pipeline, 506 - 514 simple face detector, 507 - 512 histograms, 245 - 249 binning data to create, 83 faceted, 318 KDE and, 248 , 491 - 496 manual customization, 282 - 284 plt.hexbin() function, 248 plt.hist2d() function, 247 Seaborn, 314 - 317 simple, 245 - 246 two-dimensional, 247 - 249 holdout sets, 360 Hunter, John, 217 hyperparameters, 349 (see also model validation) I iloc attribute (Pandas), 110 images, encoding for machine learning analy‐ sis, 378 immutable array, Index object as, 106 importing, tab completion for, 7 In objects, IPython, 13 index alignment in DataFrame, 117 in Series, 116 Index object (Pandas), 105 - 107 as immutable array, 106 as ordered set, 106 indexing fancy, 78 - 85 (see also fancy indexing) hierarchical (see hierarchical indexing) NumPy arrays: accessing single elements, 43 Pandas, 107 IndexSlice object, 137 indicator variables, 183 inner join, 153 input/output history, IPython, 13 - 16 In and Out objects, 13 related magic commands, 16 suppressing output, 15 underscore shortcuts and previous outputs, 15 installation, Python, xiv integers, Python, 35 IPython, 1 accessing documentation with ?, 3 accessing source code with ??, 5 command-line commands in shell, 18 controlling exceptions, 20 - 22 debugging, 22 - 24 documentation, 3 - 8 , 34 errors handling, 20 - 24 exploring modules with tab completion, 6 - 7 help and documentation, 3 - 8 input/output history, 13 - 16 keyboard shortcuts in shell, 8",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_458"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "22 - 24 documentation, 3 - 8 , 34 errors handling, 20 - 24 exploring modules with tab completion, 6 - 7 help and documentation, 3 - 8 input/output history, 13 - 16 keyboard shortcuts in shell, 8 launching Jupyter notebook, 2 launching shell, 2 magic commands, 10 - 13 notebook (see Jupyter notebook) plotting from shell, 219 profiling and timing code, 25 - 30 shell commands, 16 - 19 shell-related magic commands, 19 web resources, 30 wildcard matching, 7 Iris dataset Index | 521 as table, 343 classification, 351 clustering, 353 dimensionality, 352 pair plots, 317 scatter plots, 236 visualization of, 345 isnull() method, 124 Isomap dimensionality reduction, 341 , 355 face data, 456 - 460 ix attribute (Pandas), 110 J jet colormap, 257 joins, 145 (see also merging) categories of, 147 - 149 datasets, 146 - 158 many-to-one, 148 one-to-one, 147 set arithmetic for, 152 joint distributions, 316 , 320 Jupyter notebook launching, 2 plotting from, 220 K k-means clustering, 339 , 462 - 476 basics, 463 - 465 color compression example, 473 - 476 expectation-maximization algorithm, 465 - 476 GMM as means of addressing weaknesses of, 477 - 480 simple digits data application, 470 - 473 kernel (defined), 496 kernel bandwidth defined, 496 selection via cross-validation, 497 kernel density estimation (KDE), 491 - 506 bandwidth selection via cross-validation, 497 Bayesian generative classification with, 501 - 506 custom estimator, 501 - 506 histograms and, 491 - 496 in practice, 496 - 506 Matplotlib, 248 Seaborn, 314 visualization of geographic distributions, 498 - 501 kernel SVM, 411 - 414 kernel transformation, 413 kernel trick, 413 keyboard shortcuts, IPython shell, 8 command history, 9 navigation, 8 text entry, 9 Knuth, Donald, 25 L labels/labeling classification task, 333 - 335 clustering, 338 - 339 dimensionality reduction and, 340 - 342 regression task, 335 - 338 simple line plots, 230 - 232 Lambert conformal conic projection, 303 lasso regularization (L1 regularization), 399 learning curves, computing, 372 left join, 153",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_459"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "340 - 342 regression task, 335 - 338 simple line plots, 230 - 232 Lambert conformal conic projection, 303 lasso regularization (L1 regularization), 399 learning curves, computing, 372 left join, 153 left_index keyword, 151 - 152 legends, plot choosing elements for, 251 customizing, 249 - 255 multiple legends on same axes, 254 point size, 252 levels, naming, 133 line plots axes limits for, 228 - 230 labeling, 230 - 232 line colors and styles, 226 - 228 Matplotlib, 224 - 232 line-by-line profiling, 28 linear regression (in machine learning), 390 basis function regression, 392 - 396 regularization, 396 - 400 Seattle bicycle traffic prediction example, 400 simple, 390 - 392 lists, Python, 37 - 41 loc attribute (Pandas), 110 locally linear embedding (LLE), 453 - 455 logarithms, 55 M machine learning, 331 522 | Index basics, 331 - 342 categories of, 332 classification task, 333 - 335 clustering, 338 - 339 decision trees and random forests, 421 defined, 332 dimensionality reduction, 340 - 342 educational resources, 514 face detection pipeline, 506 - 514 feature engineering, 375 - 382 GMM (see Gaussian mixture models) hyperparameters and model validation, 359 - 375 KDE (see kernel density estimation) linear regression (see linear regression) manifold learning (see manifold learning) naive Bayes classification, 382 - 390 PCA (see principal component analysis) qualitative examples, 333 - 342 regression task, 335 - 338 Scikit-Learn basics, 343 supervised, 332 SVMs (see support vector machines) unsupervised, 332 magic commands code block pasting, 11 code execution timing, 12 help commands, 13 IPython input/output history, 16 running external code, 12 shell-related, 19 manifold learning, 445 - 462 \"HELLO\" function, 446 advantages/disadvantages, 455 applying Isomap on faces data, 456 - 460 defined, 446 k-means clustering (see k-means clustering) multidimensional scaling, 450 - 452 PCA vs., 455 visualizing structure in digits, 460 - 462 many-to-one joins, 148 map projections, 300 - 304 conic, 303 cylindrical, 301 perspective,",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_460"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "clustering) multidimensional scaling, 450 - 452 PCA vs., 455 visualizing structure in digits, 460 - 462 many-to-one joins, 148 map projections, 300 - 304 conic, 303 cylindrical, 301 perspective, 302 pseudo-cylindrical, 302 maps, geographic (see geographic data) margins, maximizing, 407 - 416 masking, 114 (see also Boolean masks) Boolean arrays, 75 - 78 Boolean masks, 70 - 78 MATLAB-style interface, 222 Matplotlib, 217 , 329 axes limits for line plots, 228 - 230 changing defaults via rcParams, 284 colorbar customization, 255 - 262 configurations and stylesheets, 282 - 290 density and contour plots, 241 - 245 error visualization, 237 - 240 general tips, 218 - 222 geographic data with Basemap toolkit, 298 gotchas, 232 histograms, binnings, and density, 245 - 249 importing, 218 interfaces, 222 labeling simple line plots, 230 - 232 line colors and styles, 226 - 228 MATLAB-style interfaces, 222 multiple subplots, 262 - 268 object hierarchy of plots, 275 object-oriented interfaces, 223 plot customization, 282 - 284 plot display contexts, 218 - 220 plot legend customization, 249 - 255 plotting from a script, 219 plotting from IPython notebook, 220 plotting from IPython shell, 219 resources and documentation for, 329 saving figures to file, 221 Seaborn vs., 311 - 313 setting styles, 218 simple line plots, 224 - 232 stylesheets, 285 - 290 text and annotation, 268 - 275 three-dimensional function visualization, 241 - 245 three-dimensional plotting, 290 - 298 tick customization, 275 - 282 max() function, 59 maximum margin estimator, 408 (see also support vector machines (SVMs)) memory use, profiling, 29 merge key on keyword, 149 specification of, 149 - 152 merging, 146 - 158 (see also joins) Index | 523 key specification, 149 - 152 relational algebra and, 146 US state population data example, 154 - 158 min() function, 59 Miniconda, xiv missing data, 120 - 124 feature engineering and, 381 handling, 119 - 120 NaN and None, 123 operating on null values in Pandas, 124 - 127 Möbius strip, 296 - 298 model (defined), 334 model",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_461"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "xiv missing data, 120 - 124 feature engineering and, 381 handling, 119 - 120 NaN and None, 123 operating on null values in Pandas, 124 - 127 Möbius strip, 296 - 298 model (defined), 334 model parameters (defined), 334 model selection bias–variance trade-off, 364 - 366 validation curves in Scikit-Learn, 366 - 370 model validation, 359 - 375 bias–variance trade-off, 364 - 366 cross-validation, 361 - 370 grid search example, 373 holdout sets, 360 learning curves, 370 - 373 naive approach to, 359 validation curves, 366 - 370 modules, IPython, 6 - 7 Mollweide projection, 302 multi-indexing (see hierarchical indexing) multidimensional scaling (MDS), 450 - 452 basics, 447 - 450 locally linear embedding and, 453 - 455 nonlinear embeddings, 452 MultiIndex type, 129 - 131 creation methods, 131 - 134 data aggregations on, 140 explicit constructors for, 132 extra dimension of data with, 130 for columns, 133 index setting/resetting, 139 indexing and slicing, 134 - 137 keys option, 144 level names, 133 multiply indexed DataFrames, 136 multiply indexed Series, 134 rearranging, 137 - 140 sorted/unsorted indices with, 137 stacking/unstacking indices, 138 multinomial naive Bayes classification, 386 - 389 N naive Bayes classification, 382 - 390 advantages/disadvantages, 389 Bayesian classification and, 383 Gaussian, 383 - 386 multinomial, 386 - 389 text classification example, 386 - 389 NaN value, 104 , 116 , 122 navigation shortcuts, 8 neural networks, 513 noise filter, PCA as, 440 - 442 None object, 121 , 123 nonlinear embeddings, MDS and, 452 notnull() method, 124 np.argsort() function, 86 np.concatenate() function, 48 , 143 np.sort() function, 86 null values, 124 - 127 detecting, 124 dropping, 125 filling, 126 NumPy, 33 aggregations, 58 - 63 array attributes, 42 array basics, 42 array indexing: accessing single elements, 43 array slicing: accessing subarrays, 44 Boolean masks, 70 - 78 broadcasting, 63 - 69 comparison operators as ufuncs, 71 - 73 computation on arrays, 50 - 58 data types in Python, 34 datetime64 dtype, 189",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_462"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "slicing: accessing subarrays, 44 Boolean masks, 70 - 78 broadcasting, 63 - 69 comparison operators as ufuncs, 71 - 73 computation on arrays, 50 - 58 data types in Python, 34 datetime64 dtype, 189 documentation, 34 fancy indexing, 78 - 85 keywords and/or vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_463"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". operators &/|, 77 sorting arrays, 85 - 92 standard data types, 41 structured arrays, 92 - 96 universal functions, 50 - 58 O object-oriented interface, 223 offsets, time series, 196 on keyword, 149 one-hot encoding, 376 one-to-one joins, 147 optical character recognition digit classification, 357 - 358 524 | Index GMMs, 488 - 491 k-means clustering, 470 - 473 loading/visualizing digits data, 354 Matplotlib, 261 - 262 PCA as noise filtering, 440 - 442 PCA for visualization, 437 random forests for classifying digits, 430 - 432 Scikit-Learn application, 354 - 358 visualizing structure in digits, 460 - 462 or keyword, 77 ordered set, Index object as, 106 orthographic projection, 302 Out objects, IPython, 13 outer join, 153 outer products, 58 outliers, PCA and, 445 output, suppressing, 15 overfitting, 371 , 425 P pair plots, 317 Pandas, 97 aggregation and grouping, 158 - 170 and compound expressions, 209 appending datasets, 146 built-in documentation, 98 combining datasets, 141 - 158 concatenation of datasets, 141 - 146 data indexing and selection, 107 data selection in DataFrame, 110 - 215 data selection in Series, 107 - 110 DataFrame object, 102 - 105 eval() and query(), 208 - 209 handling missing data, 119 - 120 hierarchical indexing, 128 - 141 Index object, 105 - 107 installation, 97 merging/joining datasets, 146 - 158 NaN and None in, 123 null values, 124 - 127 objects, 98 - 107 operating on data in, 115 - 127 (see also universal functions) pandas.eval(), 210 - 211 Panel data, 141 pivot tables, 170 - 178 Series object, 99 - 102 time series, 188 - 214 vectorized string operations, 178 - 188 pandas.eval() function, 210 - 211 Panel data, 141 partial slicing, 135 partitioning (partial sorts), 88 pasting code blocks, magic commands for, 11 pd.concat() function catching repeats as error, 144 concatenation with, 142 - 146 concatenation with joins, 145 duplicate indices, 143 ignoring the index, 144 MultiIndex keys, 144 pd.date_range() function, 193 pd.eval() function, 210 - 211 pd.merge() function, 146 - 158 categories",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_464"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "concatenation with joins, 145 duplicate indices, 143 ignoring the index, 144 MultiIndex keys, 144 pd.date_range() function, 193 pd.eval() function, 210 - 211 pd.merge() function, 146 - 158 categories of joins, 147 - 149 keywords, 149 - 152 left_index/right_index keywords, 151 - 152 merge key specification, 149 - 152 relational algebra and, 146 specifying set arithmetic for joins, 152 pdb (Python debugger), 22 Perez, Fernando, 1 , 217 Period type, 193 perspective projections, 302 pipelines, 366 , 381 pivot tables, 170 - 178 groupby() operation vs., 171 multi-level, 172 syntax, 171 - 173 Titanic passengers example, 170 US birthrate data example, 174 - 178 Planets dataset aggregation and grouping, 159 bar plots, 321 plot legends choosing elements for, 251 customizing, 249 - 255 multiple legends on same axes, 254 points size, 252 Plotly, 330 plotting axes limits for simple line plots, 228 - 230 bar plots, 321 changing defaults via rcParams, 284 colorbars, 255 - 262 data on maps, 307 - 329 density and contour plots, 241 - 245 Index | 525 display contexts, 218 - 220 factor plots, 319 from an IPython shell, 219 from script, 219 histograms, binnings, and density, 245 - 249 IPython notebook, 220 joint distributions, 320 labeling simple line plots, 230 - 232 line colors and styles, 226 - 228 manual customization, 282 - 284 Matplotlib, 217 multiple subplots, 262 - 268 of errors, 237 - 240 pair plots, 317 plot legends, 249 - 255 Seaborn, 311 - 313 simple line plots, 224 - 232 simple scatter plots, 233 - 237 stylesheets for, 285 - 290 text and annotation for, 268 - 275 three-dimensional, 290 - 298 three-dimensional function, 241 - 245 ticks, 275 - 282 two-dimensional function, 69 various Python graphics libraries, 330 plt.axes() function, 263 - 264 plt.contour() function, 241 - 244 plt.GridSpec() function, 266 - 268 plt.imshow() function, 243 - 244 plt.legend() command, 249 - 254 plt.plot() function color arguments, 226 plt.scatter vs., 237 scatter plots with, 233 - 235 plt.scatter() function plt.plot vs., 237 simple scatter",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_465"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "function, 243 - 244 plt.legend() command, 249 - 254 plt.plot() function color arguments, 226 plt.scatter vs., 237 scatter plots with, 233 - 235 plt.scatter() function plt.plot vs., 237 simple scatter plots with, 235 - 237 plt.subplot() function, 264 plt.subplots() function, 265 polynomial basis functions, 393 polynomial regression model, 366 pop() method, 111 population data, US, merge and join operations with, 154 - 158 principal axes, 434 - 436 principal component analysis (PCA), 433 - 515 basics, 433 - 442 choosing number of components, 440 eigenfaces example, 442 - 445 facial recognition example, 442 - 445 for dimensionality reduction, 436 handwritten digit example, 437 - 440 , 440 - 442 manifold learning vs., 455 meaning of components, 438 - 439 noise filtering, 440 - 442 strengths/weaknesses, 445 visualization with, 437 profiling full scripts, 27 line-by-line, 28 memory use, 29 projections (see map projections) pseudo-cylindrical projections, 302 Python installation considerations, xiv Python 2.x vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_466"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Python 3, xiii reasons for using, xii Q query() method DataFrame.query() method, 213 when to use, 214 question mark (?), accessing IPython documen‐ tation with, 3 quicksort algorithm, 87 R radial basis function, 412 rainfall statistics, 70 random forests advantages/disadvantages, 432 classifying digits with, 430 - 432 defined, 426 ensembles of estimators, 426 - 428 motivating with decision trees, 421 - 426 regression, 428 RandomizedPCA, 442 rcParams dictionary, changing defaults via, 284 RdBu colormap, 258 record arrays, 96 reduce() method, 57 regression, 428 - 433 (see also specific forms, e.g.: linear regres‐ sion) regression task defined, 332 machine learning, 335 - 338 526 | Index regular expressions, 181 regularization, 396 - 400 lasso regularization, 399 ridge regression, 398 relational algebra, 146 resample() method, 197 - 199 reset_index() method, 139 reshaping, 47 ridge regression (L2 regularization), 398 right join, 153 right_index keyword, 151 - 152 rolling statistics, 201 runtime configuration (rc), 284 S scatter plots (see simple scatter plots) Scikit-Learn package, 331 , 343 - 346 API (see Estimator API) basics, 343 - 359 data as table, 343 data representation in, 343 - 346 Estimator API, 346 - 354 features matrix, 344 handwritten digit application, 354 - 358 support vector classifier, 408 - 411 target array, 344 - 345 scipy.special submodule, 56 script plotting from, 219 profiling, 27 Seaborn bar plots, 321 datasets and plot types, 313 - 329 faceted histograms, 318 factor plots, 319 histograms, KDE, and densities, 314 - 317 joint distributions, 320 marathon finishing times example, 322 - 329 Matplotlib vs., 311 - 313 pair plots, 317 stylesheet, 289 visualization with, 311 - 313 Seattle, bicycle traffic prediction in linear regression, 400 - 405 time series, 202 - 209 Seattle, rainfall statistics in, 70 semi-supervised learning, 333 Series object (Pandas), 99 - 102 as dictionary, 100 , 107 constructing, 101 data indexing/selection in, 107 - 110 DataFrame as dictionary of, 110 - 112 DataFrame",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_467"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "in, 70 semi-supervised learning, 333 Series object (Pandas), 99 - 102 as dictionary, 100 , 107 constructing, 101 data indexing/selection in, 107 - 110 DataFrame as dictionary of, 110 - 112 DataFrame object constructed from, 104 DataFrame object constructed from dictio‐ nary of, 105 generalized NumPy array, 99 hierarchical indexing in, 128 - 141 index alignment in, 116 indexer attributes, 109 multiply indexed, 134 one-dimensional array, 108 operations between DataFrame and, 118 shell, IPython basics, 16 command-line commands, 18 commands, 16 - 19 keyboard shortcuts in, 8 launching, 2 magic commands, 19 passing values to and from, 18 shift() function, 199 - 201 shortcuts accessing previous output, 15 command history, 9 IPython shell, 8 - 31 navigation, 8 text entry, 9 simple histograms, 245 - 246 simple line plots axes limits for, 228 - 230 labeling, 230 - 232 line colors and styles, 226 - 228 Matplotlib, 224 - 232 simple (Matplotlib), 224 - 232 simple linear regression, 390 - 392 simple scatter plots California city populations, 249 - 254 Matplotlib, 233 - 237 plt.plot, 233 - 235 plt.plot vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_468"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". plt.scatter, 237 plt.scatter, 235 - 237 slice() operation, 183 slicing MultiIndex with sorted/unsorted indices, 137 NumPy arrays, 44 - 47 NumPy arrays: accessing subarrays, 44 Index | 527 NumPy arrays: multidimensional subarrays, 45 NumPy arrays: one-dimensional subarrays, 44 NumPy vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_469"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Python, 46 Pandas conventions, 114 sorting arrays, 85 - 92 along rows or columns, 87 basics, 85 fast sorting with np.sort and np.argsort, 86 k-nearest neighbors example, 88 - 92 partitioning, 88 source code, accessing, 5 splitting arrays, 49 string operations (see vectorized string opera‐ tions) structured arrays, 92 - 96 advanced compound types, 95 creating, 94 record arrays, 96 stylesheets Bayesian Methods for Hackers, 288 default style, 286 FiveThirtyEight style, 287 ggplot, 287 Matplotlib, 285 - 290 Seaborn, 289 subarrays as no-copy views, 46 creating copies, 46 slicing multidimensional, 45 slicing one-dimensional, 44 subplots manual customization, 263 - 264 multiple, 262 - 268 plt.axes() for, 263 - 264 plt.GridSpec() for, 266 - 268 plt.subplot() for, 264 plt.subplots() for, 265 subsets, faceted histograms, 318 suffixes keyword, 153 supervised learning, 332 classification task, 333 - 335 regression task, 335 - 338 support vector (defined), 409 support vector classifier, 408 - 411 support vector machines (SVMs), 405 advantages/disadvantages, 420 face recognition example, 416 - 420 fitting, 408 - 411 kernels and, 411 - 414 maximizing the margin, 407 - 416 motivating, 405 - 420 simple face detector, 507 softening margins, 414 - 416 surface plots, three-dimensional, 293 - 298 T t-distributed stochastic neighbor embedding (SNE), 456 , 472 tab completion exploring IPython modules with, 6 - 7 of object contents, 6 when importing, 7 table, data as, 343 target array, 344 - 345 term frequency-inverse document frequency (TF-IDF), 378 text, 377 (see also annotation of plots) transforms and position of, 270 - 272 text entry shortcuts, 9 three-dimensional plotting contour plots, 292 Möbius strip visualization, 296 - 298 points and lines, 291 surface plots, 293 - 298 surface triangulations, 295 - 298 wireframes, 293 with Matplotlib, 290 - 298 ticks (tick marks) customizing, 275 - 282 fancy formats, 279 - 281 formatter/locator options, 281 major and minor, 276 reducing/increasing number of, 278 Tikhonov regularization, 398",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_470"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "290 - 298 ticks (tick marks) customizing, 275 - 282 fancy formats, 279 - 281 formatter/locator options, 281 major and minor, 276 reducing/increasing number of, 278 Tikhonov regularization, 398 time series bar plots, 321 dates and times in Pandas, 191 datetime64, 189 frequency codes, 195 indexing data by timestamps, 192 native Python dates and times, 189 offsets, 196 Pandas, 188 - 209 Pandas data structures for, 192 - 194 pd.date_range(), 193 528 | Index Python vs",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_471"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Pandas, 188 - 192 resampling and converting frequencies, 197 - 199 rolling statistics, 201 Seattle bicycle counts example, 202 - 209 time-shifts, 199 - 201 typed arrays, 189 Timedelta type, 193 Timestamp type, 193 timestamps, indexing data by, 192 timing, of code, 12 , 25 - 27 transform() method, 167 transforms modifying, 270 - 272 text position and, 270 - 272 triangulated surface plots, 295 - 298 trigonometric functions, 54 tshift() function, 199 - 201 two-fold cross-validation, 361 U ufuncs (see universal functions) unary ufuncs, 52 underfitting, 364 , 371 underscore (_) shortcut, 15 universal functions (ufuncs), 50 - 58 absolute value, 54 advanced features, 56 aggregates, 57 array arithmetic, 52 basics, 51 comparison operators as, 71 - 73 exponentials, 55 index alignment, 116 - 118 index preservation, 115 logarithms, 55 operating on data in Pandas, 115 - 127 operations between DataFrame and Series, 118 outer products, 58 slowness of Python loops, 50 specialized ufuncs, 56 specifying output, 56 trigonometric functions, 54 unstack() method, 130 unsupervised learning clustering, 338 - 339 , 353 defined, 332 dimensionality reduction, 261 , 340 - 342 , 352 , 355 PCA (see principal component analysis) V validation (see model validation) validation curves, 366 - 370 variables dynamic typing, 34 passing to and from shell, 18 variance, in bias–variance trade-off, 364 - 366 vectorized operations, 63 vectorized string operations, 178 - 188 basics, 178 indicator variables, 183 methods similar to Python string methods, 180 methods using regular expressions, 181 recipe database example, 184 - 188 tables of, 180 - 184 vectorized item access and slicing, 183 Vega/Vega-Lite, 330 violin plot, 327 viridis colormap, 258 Vispy, 330 visualization software (see Matplotlib) (see Sea‐ born) W Wickham, Hadley, 161 wildcard matching, 7 wireframe plot, 293 word counts, 377 - 378 Index | 529 About the Author Jake VanderPlas is a long-time user and developer of the Python scientific stack",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_472"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". He currently works as an interdisciplinary research director at the University of Wash‐ ington, conducts his own astronomy research, and spends time advising and consult‐ ing with local scientists from a wide range of fields. Colophon The animal on the cover of Python Data Science Handbook is a Mexican beaded lizard ( Heloderma horridum ), a reptile found in Mexico and parts of Guatemala. It and the Gila monster (a close relative) are the only venomous lizards in the world. This ani‐ mal primarily feeds on eggs, however, so the venom is used as a defense mechanism. When it feels threatened, the lizard will bite—and because it cannot release a large quantity of venom at once, it firmly clamps its jaws and uses a chewing motion to move the toxin deeper into the wound. This bite and the aftereffects of the venom are extremely painful, though rarely fatal to humans. The Greek word heloderma translates to “studded skin,” referring to the distinctive beaded texture of the reptile’s skin. These bumps are osteoderms, which each contain a small piece of bone and serve as protective armor. The Mexican beaded lizard is black with yellow patches and bands. It has a broad head and a thick tail that stores fat to help the animal survive during the hot summer months when it is inactive. On average, these lizards are 22–36 inches long, and weigh around 1.8 pounds. As with most snakes and lizards, the tongue of the Mexican beaded lizard is its pri‐ mary sensory organ. It will flick it out repeatedly to gather scent particles from the environment and detect prey (or, during mating season, a potential partner). When the forked tongue is retracted into the mouth, it touches the Jacobson’s organ, a patch of sensory cells that identify various chemicals and pheromones. The beaded lizard’s venom contains enzymes that have been synthesized to help treat diabetes, and further pharmacological research is in progress. It is endangered by loss of habitat, poaching for the pet trade, and being killed by locals who are simply afraid of it",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_473"
  },
  {
    "document_type": "research_paper",
    "title": "Python Data Science Handbook",
    "author": "Jake VanderPlas",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Python Data Science Handbook (Jake VanderPlas).pdf",
    "date_published": "2016-11-16",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It is endangered by loss of habitat, poaching for the pet trade, and being killed by locals who are simply afraid of it. This animal is protected by legislation in both countries where it lives. Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com . The cover image is from Wood’s Animate Creation . The cover fonts are URW Type‐ writer and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.",
    "chunk_id": "Natural_language_processing_python_data_science_handbook.json_chunk_474"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": "Automatic Detection of Generated Text is Easiest when Humans are Fooled Daphne Ippolito † ∗ daphnei@seas.upenn.edu Daniel Duckworth * duckworthd@google.com Chris Callison-Burch † ccb@seas.upenn.edu Douglas Eck deck@google.com Abstract Recent advancements in neural language moelling make it possible to rapidly generate vast amounts of human-sounding text. The cpabilities of humans and automatic discrimnators to detect machine-generated text have been a large source of research interest, but hmans and machines rely on different cues to make their decisions. Here, we perform carful benchmarking and analysis of three poplar sampling-based decoding strategies—tok , nucleus sampling, and untruncated random sampling—and show that improvements in dcoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer ecerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems. 1 Introduction State-of-the-art generative language models are now capable of producing multi-paragraph ecerpts that at a surface level are virtually inditinguishable from human-written content ( Zellers et al. , 2019 ; Radford et al. , 2019 ; Adelani et al. , 2020 ). Often, only subtle logical fallacies or iiosyncrasies of language give away the text as machine-generated, errors that require a close reading and/or domain knowledge for humans to detect. Deceptive text, whether humaor machingenerated, has entered the sphere of public cocern ( Cooke , 2018 ). It propogates quickly ( Vosoughi et al. , 2018 ), sets political agendas ∗ Equal contribution, Google, † University of Pennsylvnia ( Vargo et al",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". It propogates quickly ( Vosoughi et al. , 2018 ), sets political agendas ∗ Equal contribution, Google, † University of Pennsylvnia ( Vargo et al. , 2018 ), influences elections ( Allcott and Gentzkow , 2017 ), and undermines user trust ( Wang et al. , 2012 ; Song et al. , 2015 ). Recently, Adelani et al. ( 2020 ) have shown that automatcally generated reviews are perceived to be as flent as human-written ones. As generative tecnology matures, authors, well-meaning or othewise, will increasingly employ it to augment and accelerate their own writing. It is more impertive now than ever for both humans and automated systems to be able to detect and identify machingenerated texts in the wild. However, there has thus been little inquiry into the textual propeties that cause humans to give generated text high human-like ratings compared to those that cause automatic systems to rate it highly. To speak of texts produced by language moels, we must first consider how these texts are generated. A neural language model encodes a probability distribution over the next word in a sequence given the previous words. 1 A decoing strategy is an algorithm that generates squences from a language model by determining how words should get selected from this distribtion. The field has largely moved toward proabilistic decoding strategies that randomly saple from the output distribution token-by-token. However, when many low-likelihood words cmulatively contain quite a bit of probability mass, choosing one of these words can lead to odd or contradictory phrases and semantic errors. Hmans are quick to notice these types of errors. For this reason, it has become common to moify the language model’s output probability ditribution to increase the chance of sampling tkens with high likelihood according to the laguage model. Tok random sampling, where low-likelihood words are restricted from being 1 Often these ‘words” are actually subword character squences such as BPE tokens ( Sennrich et al. , 2016 )",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Tok random sampling, where low-likelihood words are restricted from being 1 Often these ‘words” are actually subword character squences such as BPE tokens ( Sennrich et al. , 2016 ). arXiv:1911.00650v2 [cs.CL] 7 May 2020 generated, is one such method. A language model that is only permitted to produce high-likelihood words is less likely to make a poor choice and crate the type of mistakes that are easy for humans to detect. Since humans are not proficient at identfying when a model subtly favors some utterances more often than a human author would, they don’t notice the over-representation of high-likelihood words in the generated text. In contrast, automatic systems excel at identifying statistical anomalies and struggle to build deeper semantic understaning. Tok in particular creates text that is easy for machines to detect but very hard for humans. Thus, we observe the general trend: as the nuber of unlikely words available to be chosen is icreased, humans get better at detecting fakes while automatic systems get worse . In this work, we study three popular random decoding strategies—tok , nucleus, and tempeature sampling—applied to GPT-2 ( Radford et al. , 2019 ). We draw a large number of excerpts geneated by each strategy and train a family of BERbased ( Devlin et al. , 2019 ) binary classifiers to label text excerpts as human-written or machingenerated. We find large differences in human rater and classifier accuracy depending on the dcoding strategy employed and length of the geerated sequences. Regardless of strategy, we find human raters achieve significantly lower accuracy than the automatic discriminators. We also show that when a decoding strategy severely modifies the unigram token distribution, as tok does, hmans have trouble detecting the resultant geneated text, but automatic classifiers find it the eaiest to discriminate",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Worryingly, we further find that classifiers are brittle; they generalize poorly when trained to discriminate samples from one strategy and then evaluated on samples from aother. In summary, our contributions are: • A comprehensive study of generated text dtection systems’ sensitivity to model struture, decoding strategy, and excerpt length. • An analysis of human raters’ ability to idetify machine-generated content, and how hman raters differ from automatic detectors. 2 Related Work Generative Language Models With a suffciently large training set and number of trainable parameters, neural language models based on the Transformer architecture ( Vaswani et al. , 2017 ) are capable of generating convincing, human-like excerpts up to several paragraphs in length. GP2 ( Radford et al. , 2019 ), G ROVER ( Zellers et al. , 2019 ), and Transformer-DMCA ( Liu et al. , 2018 ) are a few examples of large, publicly available models with this ability. G ROVER , in particular, has been shown to generate fake news that is more trustworthy than human-written fake news accoring to human raters. Human Detection The task of trying to guess whether text is coming from a robot or a fellow human was made famous by the Turing Test ( Tuing , 1950 ). It continues to be used is chatbot evauation ( Lowe et al. , 2017 ). The related (but not identical) task of asking human raters to judge the quality of machine-generated excerpts remains the gold-standard for evaluating open-domain genertion systems ( van der Lee et al. , 2019 ). Kreps et al. ( 2020 ), Gehrmann et al. ( 2019 ), and others have stressed the importance of humans being able to identify fake content on the web. Automatic Detection The rise of machingenerated content has led to the development of automated systems to identify it. G ROVER was designed to not only generate convincing news ecerpts but to also identify them using a fine-tuned version of the generative model itself ( Zellers et al. , 2019 )",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". G ROVER was designed to not only generate convincing news ecerpts but to also identify them using a fine-tuned version of the generative model itself ( Zellers et al. , 2019 ). GLTR, expecting attackers to use sampling methods that favor high-likelihood tkens, aims to make machine-generated text dtectable by computing histograms over per-token log likelihoods ( Gehrmann et al. , 2019 ). Bakhtin et al. ( 2019 ) frame human-text detection as a raning task and evaluate their models’ cross-domain and cross-model generalization, finding signifcant loss in quality when training on one dmain and evaluating on another. Schuster et al. ( 2019 ) argue that the language distributional fetures implicitly or explicitly employed by these detectors are insufficient; instead, one should look to explicit fact-verification models. Finally, dicriminators for whether text is machine-generated are a promising research direction in adversarial training ( Lin et al. , 2017 ; Li et al. , 2017 ) and in automatic evaluation of generative model quality ( Novikova et al. , 2017 ; Kannan and Vinyals , 2017 ; Lowe et al. , 2017 ). Natural Language Understanding Automatic detection of machine-generated text benefits from a semantic understanding of the text. Contradi tions, falsehoods, and topic drift can all indicate that an excerpt was machine-generated. Encodeonly Transformer models such as BERT ( Devlin et al. , 2019 ) have been shown to do very well at tasks requiring this understanding. While we fintune BERT for the task of classifying whether text was machine-generated, others have used the cotextual word embeddings from a pre-trained BERT model without fine-tuning to compute a quality score for generated text ( Zhang et al. , 2020 ). It is worth noting that recent work has raised quetions as to whether BERT truly builds a semantic understanding to make its predictions, or whether it merely takes advantage of spurious statistical differences between the text of different classes ( Niven and Kao , 2019 )",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 3 Task Definition We frame the detection problem as a binary clasification task: given an excerpt of text, label it as either human-written or machine-generated. In particular, we are interested in how variables such as excerpt length and decoding strategy impact performance on this classification task. We thus create several datasets. Each is approximately balanced between positive examples of machingenerated text and negative examples of humawritten text. While they all share the same humawritten examples, each dataset contains a different set of machine-generated examples sampled using one particular decoding strategy. We also build aditional datasets by truncating all of the examples to a particular sequence length, By training a separate classifier on each dataset, we are able to answer questions about which dcoding strategy results in text that is the easiest to automatically disambiguate from human-written text. We are also able to answer questions about how the length of the examples in the training set impacts our ability to automatically classify ecerpts of that same length as either human-written or machine-generated. 4 Dataset Methodology All of our generated text samples are drawn from GPT-2, a state-of-the-art Transformer-based geerative language model that was trained on text from popular web pages ( Radford et al. , 2019 ). While we use the GPT-2 L ARGE model with 774M parameters, we found that similar trends to those reported here hold in experiments with smaller language models. Given an autoregressive language model that defines a probability distribution over the next tken given the previous tokens in a sequence, a decoding strategy generates text by deciding how to output a token at each step based on the prdicted distributions. Perhaps the most straightfoward decoding strategy is to randomly choose a tken with probability proportional to its likelihood",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Perhaps the most straightfoward decoding strategy is to randomly choose a tken with probability proportional to its likelihood. A challenge with the random sampling approach is that these probability distributions often contain a long tail of vocabulary items that are individally low-probability but cumulatively comprise a substantial amount of probability mass. Holtzman et al. ( 2020 ) observe that choosing tokens from this tail often leads to incoherent generations. Tok sampling, nucleus sampling, and (in the extreme) beam search have all been proposed to heuristically promote samples with higher petoken likelihoods. Tok and nucleus sampling both do so by setting the likelihood of tokens in the tail of the distribution to zero. Tok restricts the distribution to all but the k most likely tokens, where k is a constant ( Fan et al. , 2018 ). Nucleus sampling, also called top , truncates the distribtion at each decoding step t to the k t -most-likely next tokens such that the cumulative likelihood of these tokens is no greater than a constant p ( Holtman et al. , 2020 ). We thus consider three different decoding straegy settings: • Sample from the untruncated distribution • Tok , choosing k =40 ( Radford et al. , 2019 ). • Nucleus sampling (aka top ), choosing p =0.96 ( Zellers et al. , 2019 ). In addition, we form “negative” examples of human-written text by taking excerpts of web text that come from the same distribution as GPT-2’s training data. 2 By picking text that resembles GPT-2’s train set, we ensure that our classifiers can’t simply take advantage of stylistic differences between the human-written text corpus and the kind of text GPT-2 was trained to generate. For each decoding method, we construct a traiing dataset by pairing 250,000 generated samples with 250,000 excerpts of web text. 5,000 addtional paired samples are kept aside for validation and test datasets. Lastly, we filter out excerpts with fewer than 192 WordPiece tokens ( Wu et al",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5,000 addtional paired samples are kept aside for validation and test datasets. Lastly, we filter out excerpts with fewer than 192 WordPiece tokens ( Wu et al. , 2 https://github.com/openai/ gpt-2-output-dataset 2016 ) (excerpts might be quite short if the model produces an end-of-text token early on). See Apendix 1 for final dataset sizes. A crucial question when generating text with a language model is whether or not to provide a priming sequence which the language model should continue. Unconditioned samples, where no priming text is provided, in conjunction with tok sampling, lead to pathological behavior for discriminators as the first token of the generated text will always be one of k possible options. On the other hand, if long sequences of human text are used as priming, the space of possible geneated sequences is larger, but the detection problem shifts from one of “how human-like is the geneated text?” to “how well does the generated text follow the priming sequence?”. Since in this study we are interested in the former simpler question, we create two datasets, one with no priming, and one with the minimum amount of priming possible: a single token of web text. This means that for every excerpt of web text in the training set, there is an excerpt of machingenerated text that starts with the same token. We find that even with limited priming, the ability of automatic detectors can be strongly impacted. To study the effect of excerpt length, we costruct variations of the above datasets by truncaing all excerpts to ten possible lengths ranging from 2 to 192 WordPiece tokens ( Wu et al. , 2016 ). In total, we obtain sixty dataset variations: one per sampling method, truncation length, and choice of priming or no priming. 5 Automatic Detection Method The primary discriminator we employ is a fintuned BERT classifier ( Devlin et al. , 2019 ). We fine-tune one instance of BERT per dataset varation described above",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5 Automatic Detection Method The primary discriminator we employ is a fintuned BERT classifier ( Devlin et al. , 2019 ). We fine-tune one instance of BERT per dataset varation described above. For the longest sequence length, n =192, we compare BERT’s performance with several simple baselines that have been prposed in other work. Fine-tuned BERT We fine-tune BERT-L ARGE (cased) on the task of labeling a sentence as humaor machingenerated. The models are trained for 15 epochs, with checkpoints saved eery 1000 steps, and a batch size of 256. All results are reported on the test set using the checkpoint for which validation accuracy was highest. Bag-of-Words For each sequence, we compute a bag-of-words embedding where each dimension corresponds to a token in GPT-2’s 50,000 token BPE vocabulary ( Sennrich et al. , 2016 ), and we count how many times that token appears in the text sequence. We then train a logistic regression binary classifier to predict humaor machinwritten given this 50,000-dimensional embedding. We experimented with truncating embedding size by removing entries for infrequent vocabulary words, but this did not improve performance. Histogram-of-Likelihood Ranks Following GLTR ( Gehrmann et al. , 2019 ), we compute the probability distribution of the next word given the previous words in a text sequence according to a trained language model (in our case the same GPT-2 model that was used for generation). At each sequence position, we rerank the vocabulary words by likelihood, and record the rank of the ground-truth next word within this list. These ranks are then binned. GLTR uses four bins, counting (1) the number of times the top 1 word is seen, (2) the number of times words ranked 2 through 5 are seen, (3) words ranked 6-100, and (4) words ranked > 100. However, we observe higher accuracy when 50 bins are spread uniformly over the possible rankings",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, we observe higher accuracy when 50 bins are spread uniformly over the possible rankings. This means that since there are 50,000 vocabulary words, the first bin counts the number of times the actual next word was within the 1,000 mostly likely next words, the second bin counts the 1,001-2,000th, and so on. We then train logistic regression binary classifiers to predict humaor machine-written given either the 4-dimensional histograms or 50-dimensional histograms as input. Total Probability Solaiman et al. ( 2019 ) prpose a very simple baseline consisting of a thresold on the total probability of the text sequence. An excerpt is predicted as machine-generated if its likelihood according to GPT-2 is closer to the mean likelihood over all machine-generated squences than to the mean of human-written ones. 6 Human Detection Method The human evaluation task is framed similarly to the automatic one. We ask the raters to decide whether a passage of text was written by a human or by a computer algorithm. (Full instructions are in the Appendix.) Raters are allowed to choose between four options: “definitely” or “possibly” machine-generated and “definitely” or “possibly” human-written. They are first shown an excerpt of length 16 WordPiece tokens. After they make BERT BagOfWords HistGLTRBuckets Hist50Buckets TotalProb Human Method acc AUC acc AUC acc AUC acc AUC acc acc k40-1wordcond 0.88 0.99 0.79 0.87 0.52 0.52 0.69 0.76 0.61 0.64 p0.96-1wordcond 0.81 0.89 0.60 0.65 0.53 0.56 0.54 0.56 0.63 0.77 p1.0-1wordcond 0.79 0.92 0.59 0.62 0.53 0.55 0 . 5 4 0 . 5 5 0 . 6 5 0. 7 1 Table 1: Performance (accuracy and AUC) of the fine-tuned BERT classifier and several simple baselines on detecing length-192 sequences generated with one word of priming (1worccond). Note that p1.0 refers to untruncated random sampling, where we sample from 100% of the probability mass",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Note that p1.0 refers to untruncated random sampling, where we sample from 100% of the probability mass. The last column shows human perfomance on the same task where accuracy with a 50% baseline is computed by randomly pairing samples from each decoding strategy with a human-written sample. a guess, the length of the excerpt is doubled, and they are asked the same question again. This cotinues until the entire passage of length 192 tokens is shown. Passages are equally likely to be humawritten or machine-generated, with the machingenerated excerpts being evenly split between the three sampling strategies considered in this paper. Initially, Amazon Mechanical Turk (AMT) raters were employed for this task, but rater accracy was poor with over 70% of the “definitely” votes cast for “human” despite the classes bing balanced. Accuracy, even for the longest squences, hovered around 50%. The same study was then performed with university students who were first walked through ten examples (see Apendix Table 4) as a group. Afterward, they were asked to complete the same tasks that had been sent to the AMT workers. No additional guiance or direction was given to them after the intial walk-through. We will refer to this group as the “expert raters.” Among them, 52.1% of “deinitely” votes were cast for human, and accuracy on the longest excerpt length was over 70%. The human evaluation dataset consisted of 150 excerpts of web text and 50 excerpts each from the three decoding strategies. Each question was shown to at most three raters, leading to 900 total annotations from the untrained workers and 475 from the expert raters. A more detailed breakdown can be found in the Appendix. 7 Automatic Detection Results Simple Baselines Table 1 shows the perfomance of the baseline discriminators on lengt192 sequences, as compared with fine-tuned BERT. Reassuringly, BERT far surpasses all siple baselines, indicating that it is not fully possble to solve the detection problem without coplex sequence-based understanding",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Reassuringly, BERT far surpasses all siple baselines, indicating that it is not fully possble to solve the detection problem without coplex sequence-based understanding. The simplest baseline, TotalProb, which makes a decision based on the likelihood of the sequence, performs suprisingly well (over 60% accuracy for all sampling methods) relative to the methods which involve training logistic regression models. Logistic regression on bag-of-words is the best of the baselines, beating out the histogram-based methods. While Gehrmann et al. ( 2019 ) report an AUC of 0.87 on classifying text as real or geneated using logistic regression on the four buckets of the GLTR system, we report AUC between 0.52 and 0.56 for this task. The discrepancy is likely due to the fact that the human-written text in our discriminator training set comes from the same distribution as the text used to train the language model, while in GLTR the human text comes from children’s books, scientific abstracts, and newpaper articles. The selection of training data for learned detection systems is crucial. In real-world applications, the choice ought to reflect the genres that builders of text-generation systems are trying to impersonate. Fine-tuned BERT In Figure 1a , we begin by oserving discriminator accuracy as a function of ecerpt length and sampling method. As can be ituitively expected, as sequence length increases, so too does accuracy. For unconditioned text dcoded with nucleus (p0.96) and untruncated (p1.0) random sampling, we find discriminator accuracy increases from 55%, near random, to about 81% for the longest sequences tested. In contrast, dicriminators trained and evaluated on tok achieve over 80% accuracy even on 16-token excerpts. Why are tok ’s samples so easy to detect? In Figure 2b , we see the percentage of probability mass concentrated in the k most common token types for each sampling method",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Why are tok ’s samples so easy to detect? In Figure 2b , we see the percentage of probability mass concentrated in the k most common token types for each sampling method. While random sampling and nucleus sampling are very similar to human-written texts, we see top-k concentrating up to 80% of its mass in the first 500 most comon tokens. The other sampling methods as well as human-written texts require at least 1,100 token types for the same. It is clear that tok ’s distrib 50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% 0 32 64 96 128 160 192 Accuracy Sequence length in tokens Accuracy of BERT Fine-tuned Discriminator k40-1wordcond k40-nowordcond p0.96-1wordcond p0.96-nowordcond p1.0-1wordcond p1.0-nowordcond (a) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 4 8 16 32 64 96 128 160 192 Sequence length in tokens Fraction of BERT Discriminator Errors that are Machine-generated Labeled as Human-written k40-1wordcond p0.96-1wordcond p1.0-1wordcond (b) Figure 1: In (a) , accuracy increases as the length of the sequences used to train the discriminator is increased. In (b) , we see that the BERT fine-tuned discriminator predicts about the same number of false-positives as falsnegatives when trained with samples generated using top sampling. However, for tok , it more often mistakes machine-generated text to be human-written, while for untruncated random sampling the opposite is the case. tion over unigrams strongly diverges from humawritten texts–an easy feature for discriminators to exploit. In fact, See et al. ( 2019 ) note that it takes setting k to 1000 to achieve about the same amount of rare word usage and fraction of non-stopword text as as human writing. 3 This makes it very easy for the model to pick out machine-generated text based on these distributional differences. One way to help resolve this problem is to add priming text. Doing so causes more rare words to be incorporated into the tok of the unigram distribution",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One way to help resolve this problem is to add priming text. Doing so causes more rare words to be incorporated into the tok of the unigram distribution. Adding even a single human word of priming significantly reduces the performance of detectors trained with tok random sampling. Without priming, a discriminator trained on squences of length 2 can classify with ∼ 90% acuracy the provenance of the text (Figure 1a ). By adding one priming token, accuracy drops to ∼ 65%. Even on the longest 192-length sequences, tok discriminator accuracy is 6% lower on the primed dataset than the unprimed one. When generating with nucleus or untruncated random sampling, adding a priming token is not as impactful, as these methods are already sapling from a large fraction (or all) of the probabiity distribution. This is seen in Figure 2a where at the very first step of unprimed generation, ncleus sampling selects from 3075 possible vocaulary words, and at later positions selects from on 3 when decoding from the GPT-2 small model with 117M parameters. average more than 500. Untruncated random sapling always selects from the entire 50,000 word vocabulary, whereas tok only selects from k . Transferability In Table 2 , we show how dicriminators trained with samples from one decoing strategy can transfer at test time to detecing samples generated using a different decoding strategy. Unsurprisingly a discriminator trained on tok generalizes poorly to other sampling metods: accuracy drops to as low as 42.5%, worse than chance . Conversely, training the discrimnator with sequences sampled from the untrucated distribution leads to little transferability to detecting tok samples. Only the discriminator trained with nucleus sampling (a compromise btween unmodified sampling and tok ) was able to detect sequences from the other sampling stratgies without too much of a hit to accuracy. As epected, a discriminator trained on an equal portion of data from each decoding method does reasoably at detecting all three",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As epected, a discriminator trained on an equal portion of data from each decoding method does reasoably at detecting all three. Perhaps this lack of transferability is related to each discriminator’s calibration. Indeed, the dgree to which a discriminator’s average predition deviates from 50% is a direct indicator of its accuracy. In Table 3 , we observe that of the three BERT discriminators, only that trained on top samples predicts ‘machine-generated’ on aproximately 50% of in-domain examples as epected. This same discriminator’s behavior holds on datasets generated by other sampling strategies 0 50 100 150 200 Position in sequence 500 1000 1500 2000 2500 3000 3500 4000 4500 k Mean k Chosen at each Position during Generation with Nucleus Sampling p0.96-nowordcond p0.96-1wordcond (a) 0 500 1000 1500 2000 2500 k most common unique tokens 0% 20% 40% 60% 80% 100% % all tokens p1.0-1wordcond k40-1wordcond p0.96-1wordcond webtext (b) Figure 2: In (a) , the average (over sequences in the test set) k chosen at each step during generating with nucleus sampling is plotted. Adding a single word of priming strongly impacts the k s chosen for the first few positions, but this difference quickly dissipates. In (b) , we consider the first token generated in each sequence by tok , and plot what fraction of these are captured by the k most common unique tokens from the vocabulary. Overall, at its first step, tok concentrates 80% of its probability mass in the 500 most common tokens from the vocabulary. ! !\"# !\"$ !\"% !\"& !\"' !\"( !\") !\"* !\"+ # #( %$ (& #$* #+$ ,-./-01-23-04562702589-0: ;<=1578028>2?=5-<2@<<8<:256=52=<-2A=1670-B 4-0-<=5-C2D=E-3-C2=:2F/G=0BH<755-0 9&!B#H8<C180C I!\"+(B#H8<C180C I#\"!B#H8<C180C (a) (b) (c) Figure 3: (a) and (b) show human rater accuracy of correctly identifying an excerpt as human-written or machinwritten, shown with 80% confidence internals, in (a) , broken up by decoding strategy and in (b) , overall. Accuracy increases as raters observe more tokens",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Accuracy increases as raters observe more tokens. (c) shows that for short excerpts, most rater mistakes are them incorrectly thinking machine-generated text is human written. The two errors types become more balanced at longer lengths. Eval tok nucleus random Train top-k 90.1 57.1 43.8 nucleus 79.1 81.3 78.4 random 47.8 63.7 81.7 mixed 88.7 74.2 72.2 Table 2: Accuracy of BERT fine-tuned discriminator when trained on samples from one strategy (rows) and evaluated on another (columns). Trained on samples with 192 tokens. The ‘mixed’ dataset is one containing an equal portion of samples from each strategy. as well. In contrast, we observe that discrimnators trained on top-k and untruncated random samples severely underestimate the percentage of machine-generated excerpts in out-of-domain datasets. Even within domain (Figure 1b ), we find both discriminators heavily favor a single class, iEval tok nucleus random Train top-k 60.9 27.9 14.5 nucleus 49.2 51.7 48.9 random 7.3 22.6 38.3 Table 3: Average probability of ‘machine-generated’ according to each length-192 discriminator. The epected in-domain probability is 0.5. One token of coditioning. creasingly so as the number of tokens increases. Human Evaluation Overall human performance across all sampling methods is shown in Figure 3 b. Even with the multi-paragraph 192-length ecerpts, human performance is only at 71.4%, idicating that even trained humans struggle to corectly identify machine-generated text over a qua Truth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 H M H H M H H M M M EDIT:OKAY!, I guess that’ll work for now. > http://www.teamfortress.com/ and then go buy the game and experience some of the best online gaming I have ever played. ˆ ˆBoth girls had a really fun time and I had a GREAT time making both of these costumes",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ˆ ˆBoth girls had a really fun time and I had a GREAT time making both of these costumes. Everything was altered even a little bit(dying the pants a darker grey and painting the boots and shirts) But my piece de resistance would have to be my eldest’s Medi-Gun.If you have any questions about the costumes, I would be happy to assist you!Oh and here’s a video of my daughter before the costume was completed.Thanks! Image copyright Getty Images Image caption Women mourn over the coffin of one of the vitim’s of Sunday’s bombing in Ankara Who’d be in Turkey’s shoes right now? Since July last year, hundreds of soldiers and civilians have been killed in terrorist attacks. Suicide bombs have torn into crowds of demonstrators and tourists. Military convoys have been targeted in the heart of the capital. A long-running Kurdish insurgency, once thought to be close to resolution after years of painstaking efforts to build bridges, has erupted once more. The country is awash with Syrian and other refugees. The government has been under pressure to stop them moving on into Europe and prevent would-be jihadis travelling the other way. How dangerous is Turkey’s unrest? Tears and destruction amid PKK crackdown Turkey v Islamic State v the Kurds Truth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 M M H - - M M - - H First off, this thread has done a pretty good job of describing in detail yet another broken touchscreen. That’s the difference between a smartphone and a PC with no prying eyes having to snap shots for the police to find. What I would like to address is the mindset that generally surrounds Chrome OS users. To me this is analogous to saying that Apple does“hate their Windows”, or that HP does“hate their Macs” as if http://twitter.com/) (and that quote is from two years ago), that anyone who covers smartphones and tablets from a “PC” perspective is just jealous. Chrome OS is for browsing the web, PC processors can do stronger things in that regard, Windows is a juggernaut on those fronts. This is how I see it",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Chrome OS is for browsing the web, PC processors can do stronger things in that regard, Windows is a juggernaut on those fronts. This is how I see it. Yes, it can be slow. And yes, you need a fast CPU FOR ALABAMA, GOOD WEEKS AND A TOUR OF CAIRO THE ALABAMA COMITTEE ON THE STUDY OF THE AMERICAN SECURITY AGENDA, America’s fture has been mapped out in carved stone. Metro Atlanta’s last US congressman, Bill Posey, was a inextricable integral element of the Citadel project as it became another metaphor for Atlanta’s transformation from an industry backwater into the finance and information hub of the nation’s capital. Meanwhile, Cobb County – Atlanta’s geode of change – is home to some of the largest industrial parks in the South, a regional cultural center, a 100-year-old manufaturing town and a potent symbol of the former city’s cherished Georgian past. The gentry still live there, the defunct industrial landscapes carry the names of Truth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 M H - - M M H - M - Exidentia at Eurnari, is an upcoming Cryptopia event which is currently still in deveopment. Be a part of the first live stream of this year’s event on 15-16 January 2016! Since the release of v1.22, Exidentia has received a fair amount of user feedback. This event takes place in the underwater Cryptopia they have built. During this event, you will learn about the ocean and areas around it, and be reached by a treasure hunter that helps you explore the different areas. There will be six different levels in this event that you will become acquainted with: thought Polar Lava, Ocean Seared Cones and Celestine Floors, Sea Damaged Aerie Bricks, coast Puddle (congipit stopping at red water), Shaikh Swamp and Bugmite",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". At rotating points, you will learn how to access various types of creatures Ever since the opening of the North American College of Art Education in 1990, the demand for art education in America has grown steadily, and in recent years we have seen the rise of students that pursue art education not in the classroom but at art academies. This year saw another 50 percent increase in the number of art academies in the United States offering courses – with an additional 10 percent of students in 2017 taking art. Some major changes have occurred in recent years with regard to the art curriculum and the way students learn, and we will explore each of these in coming months as we look at the various forms of art education. There is no one-size-fits-all approach for this or any other field of study, and students who begin a course in art education may change their plans based on what they see that course, including what lessons they have completed and the resources available, to create meaningful experiences of artistic creation. One important area Table 4: Some 192-token examples where at least two expert raters agreed with each other, but were not in agrement with the automatic discriminators. The first row shows examples where the ground-truth was human-written, the second shows machine-generated examples where the corresponding discriminator guessed incorrectly, and the third shows machine-generated examples where the discriminator was correct, but raters got it wrong. ter a time. However, it is worth noting that our best raters achieved accuracy of 85% or higher, sugesting that it is possible for humans to do very well at this task. Further investigation is needed into how educational background, comfort with English, participation in more extensive training, and other factors can impact rater performance",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Further investigation is needed into how educational background, comfort with English, participation in more extensive training, and other factors can impact rater performance. To break up the accuracies by sampling method in a way that is comparable to the results shown for the automatic discriminators, we pair each machine-generated example with a randomly slected one of webtext to create a balanced dataset for each sampling strategy. Performance is shown in Figure 3 a. Tok produces the text that is harest for raters to correctly distinguish, but as shown in Section 7 , it is the easiest for our automatic dtection systems. Samples from untruncated radom sampling and nucleus sampling with p =0.96 are equivalently difficult for raters to classify as machine-generated. Our human evaluation results suggest that much lower p -values than the 0.92 to 0.98 range proposed in Zellers et al. ( 2019 ) might be necessary in order to generate text that is cosidered significantly more human-like to human raters than the text produced by using the untrucated distribution. Table 4 gives several examples where human raters and our BERT-based discriminators diagreed. When raters incorrectly labeled humawritten text as machine-generated, often the ecerpts contained formatting failures introduced when the HTML was stripped out. In the midle two examples, topic drift and falsehoods such as Atlanta being the “information hub of the ntion’s capital” allowed humans to correctly detect the generated content. However, in the bottom two examples, the high level of fluency left human raters fooled. Overall we find that human raters—even “epert” trained ones—have consistently worse acuracy than automatic discriminators for all dcoding methods and excerpt lengths. In our eperiments, randomly-selected pairs of raters agree with each other on a mere 59% of excerpts on average. (In comparison, raters and discrimintors agree on 61% to 70% of excerpts depending on the discriminator considered)",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (In comparison, raters and discrimintors agree on 61% to 70% of excerpts depending on the discriminator considered). We surmise that the gap between human and machine performance will only grow as researchers inevitably train biger, better detection models on larger amounts of training data. While improved detection models are inevitible, it is unclear how to go about iproving human performance. GLTR proposes prviding visual aids to humans to improve their peformance at detecting generated-text, but it is ulikely that their histogram-based color-coding will continue to be effective as generative methods get better at producing high-quality text that lacks sttistical anomalies. 8 Conclusion In this work, we study the behavior of autmated discriminators and their ability to idetify machine-generated and human-written texts. We train these discriminators on balanced bnary classification datasets where all machingenerated excerpts are drawn from the same geneative model but with different decoding strategies. We find that, in general, discriminators transfer poorly between decoding strategies, but that traiing on a mix of data from methods can help. We also show the rate at which discriminator accuracy increases as excerpts are lengthened. We further study the ability of expert human raters to perform the same task. We find that rater accuracy varies wildly, but has a median of 74%, which is less than the accuracy of our besperforming discriminator. Most interestingly, we find that human raters and discriminators make dcisions based on different qualities, with humans more easily noticing semantic errors and discrimnators picking up on statistical artifacts. In our eperiments, these artifacts are most prominent with tok sampling. However, any strategy that ovesamples high-likelihood words is susceptible. As the p in nucleus sampling is set increasingly lower to achieve more fluent text (some systems are aready using p as low as 0.5 ( Miculicich et al",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As the p in nucleus sampling is set increasingly lower to achieve more fluent text (some systems are aready using p as low as 0.5 ( Miculicich et al. , 2019 )), the distributional deviations that plague tok text will surface in nucleus sampling as well. Holtzman et al. ( 2020 ) explain how a unique atribute of human language is that it dips in and out of low probability zones. This variance in likelhood is what makes human-written text interesing and exciting to read. Today’s generation sytems have not yet solved the problem of mimicing the human cadence without introducing poor word choices that are easy for humans to detect. Generation systems often optimize for fooling hmans without acknowledging the trade-off that eists between human perception of quality and ease of automatic detection. We therefore suggest three prongs for future research: 1. Identifying ways to improve the language models and decoding strategies we use in oder to generate text that is both exciting (ie. unlikely) and semantically plausible. 2. Building better world understanding into atomatic discriminators so that they are more capable of detecting the types of errors that humans notice. 3. Developing tools and educational materals to improve humans’ ability to detect machine-generated text. These may include automatic detectors with components that eplain their predictions. Finally, we would like to note that all of our eperiments were performed with English language models, and it remains an open question how the trade-off between ease of human detection and ease of automatic detection might differ for laguages that are very different from English. Acknowledgements This research is based upon work supported in part by U.S. DARPA KAIROS Program No. FA8750- 19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government. The U.S",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copright annotation therein. We also thank Noah Fiedel, Peter Liu, Sharan Narang, Joao Sedoc, Yun William Yu, and Hugh Zhang for their valuable feedback.",
    "chunk_id": "Natural_language_processing_automatic_detection_of_generated_text__.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs Maarten Sap ♠♦ Ronan Le Bras ♠ Daniel Fried ♦ Yejin Choi ♠♥ ♠ Allen Institute for AI, Seattle, WA, USA ♦ Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA ♥ Paul G. Allen School of Computer Science, University of Washington, Seattle, WA, USA maartensap@cmu.edu Abstract Social intelligence and Theory of Mind (T O M), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effetively navigate and understand everyday social interactions. As NLP systems are used in icreasingly complex social situations, their abiity to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today’s largest language models (GPT-3; Brown et al. , 2020 ) lacks this kind of social intelligence out-of-the box, using two tasks: S OCIAL IQ A ( Sap et al. , 2019b ), which mesures models’ ability to understand intents and reactions of participants of social interactions, and T O M I ( Le et al. , 2019 ), which measures whether models can infer mental states and ralities of participants of situations. Our results show that models struggle sustantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on S OCIAL IQ A and T O M I , respectively. To conclude, we draw on theories from pragmaics to contextualize this shortcoming of large language models, by examining the limittions stemming from their data, neural archtecture, and training paradigms. Challening the prevalent narrative that only scale is needed, we posit that person-centric NLP aproaches might be more effective towards neral Theory of Mind",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Challening the prevalent narrative that only scale is needed, we posit that person-centric NLP aproaches might be more effective towards neral Theory of Mind. 1 Introduction With the growing prevalence of AI and NLP sytems in everyday social interactions, the need for AI systems with social intelligence and Theory of Mind (T O M), i.e., the ability to infer and reason about the intents, feelings, and mental states of oters, becomes increasingly evident ( Pereira et al. , Reasoning about mental states and realities Social commonsense and emotional intelligence Although Taylor was older and stronger, they lost to Alex in the wrestling match. James and Abby are in the bedroom. Abby put the pen in the desk drawer. Abby leaves the bedroom. James moves the pen into the bag. How would Alex feel as a result? Where does James think Abby will look for the pen? ashamed boastful drawer bag Measuring Neural Theory of Mind Figure 1: Theory of Mind is the ability for humans to reason about the intents, reactions, and mental states of others. We asses these abilities in LLMs through two question-answering tasks that measure social commosense and emotional intelligence (S OCIAL IQ A ; top) and reasoning about people’s mental states and realties (T O M I ; bottom); finding that GPT-3 ( ) struggles on both tasks. We discuss why that may be, drawing from theories of the pragmatics of language. 2016 ; Langley et al. , 2022 ). For humans, Theory of Mind is a crucial component that enables us to iteract and communicate effectively with each other ( Premack and Woodruff , 1978 ; Apperly , 2010 ). It allows us, for example, to infer that someone likely feels boastful instead of ashamed after winning a wrestling match (Fig. 1 ; top). In addition, T O M also enables us to reason about people’s mental ralities, e.g., if someone was out of the room while a pen was moved, she will likely search for the pen arXiv:2210.13312v2 [cs.CL] 3 Apr 2023 where she last saw it instead of where it was moved to (Fig. 1 ; bottom)",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1 ; bottom). While humans develop it naturally, T O M and social intelligence remain elusive goals for modern AI systems ( Choi , 2022 ), including large neural language models (LLMs). With advances in scaing the sizes of models and datasets, these LLMs have proven very impressive at generating humalike language for conversational, summarization, or sentence continuation settings, often with zero to few examples to learn from ( Brown et al. , 2020 ; Clark et al. , 2021 ; Chowdhery et al. , 2022 ). Hoever, increasing scrutiny has shed light on the shorcomings of these LLMs, showing that they often fall prey to spurious correlational patterns instead of displaying higher-order reasoning ( Elkins and Chun , 2020 ; Dale , 2021 ; Marcus , 2022 ). In line with EMNLP 2022’s theme, we examine the open research question of whether and how much LLMs—which are the backbone of most modern NLP systems—exhibit social intelligence and T O M abilities. Using some of the largest Eglish models in existence (GPT-3; Brown et al. , 2020 ), we demonstrate that out-of-the-box LLMs struggle at two types of reasoning abilities that requisites for Theory of Mind (shown in Fig. 1 ). We argue that these reasoning abilities are necesary but not sufficient for Theory of Mind, and that larger models will likely provide upper bounds on what equivalent-but-smaller models are capable of. We first assess whether LLMs can reason about social commonsense and emotional intelligence with respect to social interactions ( 3 ), using the S OCIAL IQ A benchmark ( Sap et al. , 2019b ) illutrated in Fig. 1 (top). Results show our best peforming few-shot GPT-3 setup achieving only 55% accuracy, lagging > 30% behind human perfomance. Furthermore, social reasoning about the protagonists of situations is easier for GPT-3 (5- 15% absolute difference) compared to reasoning about other secondary participants. Second, we measure LLMs’ ability to undestand other people’s mental states and realities in short stories ( 4 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Second, we measure LLMs’ ability to undestand other people’s mental states and realities in short stories ( 4 ). We use the T O M I QA bencmark (illustrated in Fig. 1 ; bottom; Le et al. , 2019 ), which was inspired by the classic Sally-Ann False Belief Theory of Mind test ( Baron-Cohen et al. , 1985 ). Here, our results show that GPT-3 models peak at 60% accuracy on questions about partiipants’ mental states, compared to 90–100% on factual questions. Our novel insights show that reasoning about social situations and false beliefs still presents a significant challenge for large language models, dspite their seemingly impressive performance on tasks that could require social intelligence (e.g., story generation, dialogues). In 5 , we first eamine these shortcomings; drawing on theories of the pragmatics of language, we speculate that the type of texts in LLMs’ training datasets could sustantially limit learning social intelligence. Then, we outline some possible future directions towards socially aware LLMs, reflecting on the feasibiity of interactional data selection, person-centric inductive biases, and interaction-based language learning. Our findings suggest that only increasing the scale of LLMs is likely not the most effective way to create socially aware AI systems, challening a prevalent narrative in AI research ( Narang and Chowdhery , 2022 ). 2 Theory of Mind & Large LMs Why do LLMs need Theory of Mind? Social intelligence, Theory of Mind, and commonsense reasoning have been a longstanding but elusive goal of artificial intelligence for decades ( Guning , 2018 ; Choi , 2022 ). These reasoning abiities are becoming increasingly necessary as AI assistants are used in situations that require social intelligence and Theory of Mind in order to oerate effectively ( Wang et al. , 2007 ; Dhelim et al. , 2021 ; Langley et al. , 2022 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2007 ; Dhelim et al. , 2021 ; Langley et al. , 2022 ). For example, new tecnologies are emerging where AI is used to interact and adapt to users ( Bickmore and Picard , 2005 ; Jaques , 2019 ), e.g., voice assistants, and tutoring systems; or where AI helps enhance communiction between multiple users, e.g., email autocoplete ( Chen et al. , 2019 ), AI-assisted counseling ( Kearns et al. , 2020 ; Allen , 2020 ; Sharma et al. , 2021 ), or facilitated discussion ( Rosé et al. , 2014 ). As we move beyond just asking single-turn quetions to social and interactive AI assistants, higheorder reasoning becomes necessary ( McDonald and Pearson , 2019 ). For example, AI systems should be capable of more nuanced understaning, such as ensuring an alarm is on if someone has a job interview the next morning ( Dhelim et al. , 2021 ), knowing to call for help when an elderly person falls ( Pollack , 2005 ), inferring personality and intentions in dialogues ( Mairesse et al. , 2007 ; Wang et al. , 2019 ), reasoning about public comitments ( Asher and Lascarides , 2013 ), predicting 0 20 40 60 80 100 0 10 20 30 Social IQa accuracy k (number of examples ) random ada curie davinci human Figure 2: Accuracy on the S OCIAL IQ A dev. set, brken down by LLM model type and size, as well as nuber of few-shot examples ( k ). emotional and affective states ( Litman and ForbeRiley , 2004 ; Jaques et al. , 2020 ), and incorporating empathy, interlocutor perspective, and social inteligence ( Kearns et al. , 2020 ; Sharma et al. , 2021 ). What is Theory of Mind? Theory of Mind (T O M) describes the ability that we, as humans, have to ascribe and infer the mental states of others, and to predict which likely actions they are going to take ( Apperly , 2010 ). 1 This ability is closely rlated to (interpersonal) social intelligence ( Ganaie and Mudasir , 2015 ), which allows us to navigate and understand social situations ranging from siple everyday interactions to complex negotiations ( Gardner et al. , 1995 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 1995 ). Interestingly, the development of Theory of Mind and language seem to happen around siilar ages in children ( Sperber and Wilson , 1986 ; Wellman , 1992 ; Miller , 2006 ; Tauzin and Gergely , 2018 ). 2 Theories of the pragmatics of language and communication can frame our understanding of this link ( Rubio-Fernandez , 2021 ), positing that one needs to reason about an interlocutor’s mental state (T O M) to effectively communicate and uderstand language ( Grice , 1975 ; Fernández , 2013 ; Goodman and Frank , 2016 ; Enrici et al. , 2019 ). 3 1 While Theory of Mind is well developed in most adults ( Ganaie and Mudasir , 2015 ), reasoning and inference capbilities can be influenced by age, culture, neurodiversity, or developmental disorders ( Korkmaz , 2011 ). 2 The direction of the T O M-language association is still debated ( de Villiers , 2007 ). Some researchers believe laguage development enables T O M-like abilities ( Pyers and Senghas , 2009 ; Rubio-Fernandez , 2021 ). On the other hand, some argue that language develops after T O M since preverbal infants already could possess some level of T O M-like abilities ( Onishi and Baillargeon , 2005 ; Southgate and Vernetti , 2014 ; Poulin-Dubois and Yott , 2018 ). 3 Most cognitive studies on this subject focus on the English language, which is not representative of the wide variation of 0 20 40 60 80 100 Effect React Want Social IQa accuracy Reasoning dimension Agent Other Figure 3: Comparing the accuracy of GPT-3-D A V INCI (35-shot) on S OCIAL IQ A when the reasoning is about the main agent of the situation versus others. 3 S OCIAL IQ A : Do LLMs have Social Intelligence and Social Commonsense? A crucial component of Theory-of-Mind is the abiity to reason about the intents and reactions of paticipants of social interactions. To measure this, we use the dev. set of the S OCIAL IQ A QA benchmark ( Sap et al. , 2019b ), which was designed to probe scial and emotional intelligence in various everyday situations",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To measure this, we use the dev. set of the S OCIAL IQ A QA benchmark ( Sap et al. , 2019b ), which was designed to probe scial and emotional intelligence in various everyday situations. This benchmark covers questions about nine social reasoning dimensions, drawn from the A TOMIC knowledge graph ( Sap et al. , 2019a ). S OCIAL IQ A instances consist of a context, quetion, and three answer choices, written in English. Each question relates to a specific reasoning dimesion from A TOMIC : six dimensions focus on the prand post-conditions of the agent or protagnist of the situation (e.g., needs, intents, reactions, next actions), and three dimensions focus on the post-conditions of other participants involved in the situation (reaction, next action, effect). In ttal, there are 1954 three-way QA tuples; see Tab. 1 for examples, and Tab. 3 in Appendix A for pedimension counts. 3.1 Probing LLMs with S OCIAL IQ A To probe our language models, we use a k -shot laguage probing setup, following Brown et al. ( 2020 ). We select the answer that has the highest likelihood under the language model conditioned on the cotext and question, as described in Appendix C . To test the limits of what the models can do, we select k examples that have the same A TOMIC resoning dimension as the question at hand, varying k language structures, and thus limits the cognitive conclusions one can draw about the link between language and Theory of Mind ( Blasi et al. , 2022 ). Situation Answers Focus (a) Remy was working late in his office trying to catch up. He had a big stack of papers. What does Remy need to do before this? Needed to be behind Agent Be more efficient Finish his work (b) Casey wrapped Sasha’s hands around him because they are in a romantic relationship. How would you describe Casey? Very loving towards Sasha Agent Wanted Being kept warm by Sasha (c) Tracy held a baby for 9 months and then gave birth to addison",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". How would you describe Casey? Very loving towards Sasha Agent Wanted Being kept warm by Sasha (c) Tracy held a baby for 9 months and then gave birth to addison. What will happen to Tracy? Throw her baby at the wall Agent Cry Take care of her baby (d) Kai gave Ash some bread so they could make a sandwich. How would Kai feel afterwards? Glad they helped Agent Good they get something to eat Appreciative (e) Aubrey was making extra money by babysitting Tracey’s kids for the summer. What will Tracy want to do next? Save up for a vacation Others Let Aubrey know that they are appreciated Pay off her college tuition (f) The people bullied Sasha all her life. But Sasha got revenge on the people. What will the people want to do next? Do whatever Sasha says Others Get even Flee from Sasha (g) After everyone finished their food they were going to go to a party so Kai decided to finish his food first. What will others want to do next? Eat their food quickly Others Throw their food away Go back for a second serving (h) Aubrey fed Tracy’s kids lunch today when Tracy had to go to work. What will happen to Aubrey? Be grateful Agent Get paid by Tracy Get yelled at by Tracy (i) Sasha was the most popular girl in school when she accepted Jordan’s invitation to go on a date. What will Jordan want to do next? Plan a best friends outing with Sasha Others Plan a romantic evening with Sasha Go on a date with Valerie Table 1: Examples of S OCIAL IQ A questions, which person the questions focus on ( Agent , Others ), and the human gold answers ( ) and GPT-3-D A V INCI predictions ( ). from 0 to 35 in increments of 5. We use three GP3 model sizes: GPT-3-A DA (smallest), and GP3-C URIE and GPT-3-D A V INCI (two largest). 3.2 S OCIAL IQ A Results Shown in Fig. 2 , GPT-3 models perform sustantially worse than humans (>30% less) on S O - CIAL IQ A , 4 and also worse than models finetuned on the S OCIAL IQ A training set (>20%; Lourie et al. , 2021 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2 , GPT-3 models perform sustantially worse than humans (>30% less) on S O - CIAL IQ A , 4 and also worse than models finetuned on the S OCIAL IQ A training set (>20%; Lourie et al. , 2021 ). 5 Although it is not surprising that GPT-3-D A V INCI reaches higher accuracies than GPT-3-A DA and GPT-3-C URIE , the gains are small, which suggests that increasing model size might not be enough to reach human-level accuracy. These findings are in line with recent BIG-Bench results on S OCIAL IQ A with the BIG-G (128B prameters; Srivastava et al. , 2022 ) and PaLM (353B parameters; Chowdhery et al. , 2022 ) LLMs, which 4 We find similar results when using I NSTRUCT GPT ( Ouyang et al. , 2022 ) instead of GPT-3-D A V INCI . 5 Lourie et al. ( 2021 ) achieves 83% on the test set, as shown on the AI2 S OCIAL IQ A leaderboard . lag behind humans with 45% and 73% accuracy, respectively (see Fig. 7 in Appendix A.2 ). Focusing on GPT-3-D A V INCI , while increasing the number of examples k improves performance, the differences are marginal after k =10 examples (only 1% increase from 10 to 35 examples). This suggest that performance either plateaus or follows a logarithmic relationship with increasing number of conditioning examples. Finally, we examine the differences in GP3-D A V INCI with respect to which participant is the focus. Shown in Fig. 3 , we find that GPT-3- D A V INCI performs consistently better on agencentric questions, compared to other-oriented quetions. Shown in the example predictions in Tab. 1 , GPT-3-D A V INCI often confuses which participant is being asked about. In example (e), after Aubrey babysat for Tracy, GPT-3-D A V INCI fails to prdict that Tracy will likely want to “ let Aubrey know they are appreciated ,” and instead mistakenly prdicts that Tracy will want to “ save up for vacation ,” which is what Aubrey would likely do. GPT-3- 0 20 40 60 80 100 0 5 10 15 20 25 ToMi accuracy (Mind only) k (number of examples) random ada curie davinci Figure 4: Accuracy on the T O M I dev",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". GPT-3- 0 20 40 60 80 100 0 5 10 15 20 25 ToMi accuracy (Mind only) k (number of examples) random ada curie davinci Figure 4: Accuracy on the T O M I dev. set M IND quetions of varying sizes of GPT-3, and with varying nuber of examples ( k ). D A V INCI displays a similar participant confusion in example (f) in Tab. 1 . 4 T O M I : Can LLMs Reason about Mental States and Realities? Another key component of Theory of Mind is the ability to reason about mental states and realities of others, recognizing that they may be different than our own mental states. As a measure of this ability in humans, psychologists developed the Sally Ann false-belief test ( Wimmer and Perner , 1983 ), in which two people (Sally and Ann) are together in a room with a ball, a basket, and a box, and while Sally is away, Ann moves the ball from the basket to the box. When asked where Sally will look for her ball, Theory of Mind allows us to infer that Sally will look in the basket (where she left the ball), instead of in the box (where the ball is, unbeknownst to Sally). To measure the false-belief abilities of LLMs, we use the T O M I QA dataset of English Sally-Anlike stories and questions ( Le et al. , 2019 ). 6 T O M I stories were created using a stochastic rule-based algorithm that samples two participants, an object of interest, and a set of locations or containers, and weaves together a story that involves an object being moved (see Tab. 2 ). All questions have two possible answers: the original object location, and the final object location. We investigate how LLMs answer the T O M I story-question pairs, distinguishing between quetions about factual object locations (F ACT ) and questions about where participants think objects 6 T O M I is a more challenging version of the rule-based datasets by Nematzadeh et al. ( 2018 ) and Grant et al. ( 2017 ), as it contains randomly inserted distractor actions that prevent trivial reverse engineering",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ( 2018 ) and Grant et al. ( 2017 ), as it contains randomly inserted distractor actions that prevent trivial reverse engineering. 0 20 40 60 80 100 0 2 4 8 16 24 ToMi accuracy k (number of examples) Fact Mind Mind-TB Mind-FB Figure 5: Accuracy of GPT-3-D A V INCI by number of examples ( k ), by reasoning type (F ACT vs. M IND ; M IND -T B vs. M IND -F B ). are located (i.e., their mental states; M IND ). The F ACT questions either ask about the object’s orignal (F ACT -M EM ) or final (F ACT -R EAL ) location. The M IND questions cover first-order (e.g., “ where will Abby look for the object? ”; M IND -1st) and second-order beliefs (e.g., “ where does James think that Abby will look for the object? ”; M IND -2nd). We further distinguish the M IND questions between true belief (T B ) and false belief (F B ), i.e., stories where a participant was present or absent when an object was moved, respectively. Importantly, answering the M IND questions rquires Theory of Mind and reasoning about realties and mental states of participants—regardless of the truor false-belief setting—whereas F ACT questions do not require such T O M. There are a total of 1861 two-way QA pairs in our T O M I probe set, with 519 F ACT and 1342 M IND questions (see Tab. 4 in Appendix B for more detailed counts). 4.1 Probing LLMs with T O M I We use the k -shot probing setup to test this T O M component in LLMs, with k ∈{ 2 , 4 , 8 , 16 , 24 } . We select k examples of the same reasoning type (i.e., F ACT -M EM , M IND -1st, etc.), ensuring a 50- 50 split between truand false-belief examples for the M IND questions. As before, we test GPT-3- A DA , GPT-3-C URIE , and GPT-3-D A V INCI . 4.2 T O M I Results Shown in Fig. 4 , our results indicate that GPT-3 models struggle substantially with the T O M I quetions related to mental states (M IND ), reaching 60% accuracy in the best setup",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 , our results indicate that GPT-3 models struggle substantially with the T O M I quetions related to mental states (M IND ), reaching 60% accuracy in the best setup. As expected, the best performance is reached with GPT-3-D A V INCI compared to smaller models which do not surpass Type Story Question Answers (a) F ACT Sophia entered the study. Noah entered the study. The dress is in the treasure chest. Noah exited the study. Hannah entered the garden. Sophia moved the dress to the box. Where is the dress really? box treasure chest (b) M-1-F B Noah entered the garden. Nathan entered the garden. Evelyn likes the pumpkin. The banana is in the basket. Nathan exited the garden. Noah moved the banana to the suitcase. Where will Nathan look for the banana? basket suitcase (c) M-2-T B Lily entered the patio. Aiden is in the patio. Mila entered the patio. Mila hates the radish. The coat is in the box. Aiden moved the coat to the crate. Mila exited the patio. Where does Aiden think that Mila searches for the coat? crate box (d) M-1-T B Elizabeth entered the cellar. Carter entered the cellar. The slippers is in the crate. Elizabeth moved the slippers to the container. Carter exited the cellar. Where will Carter look for the slippers? container crate (e) M-1-F B Evelyn entered the living room. Jackson entered the playroom. James entered the playroom. The beans are in the treasure chest. James exited the playroom. Jackson moved the beans to the pantry. Jackson exited the playroom. James entered the living room. Where will James look for the beans? treasure chest pantry (f) M-2-F B Isla likes the potato. Ella entered the laundry. Oliver entered the laundry. The slippers are in the box. Ella exited the laundry. Oliver moved the slippers to the basket. Isla entered the office. Where does Ella think that Oliver searches for the slippers? basket box Table 2: Example stories in the T O M I dev. dataset, with GPT-3-D A V INCI predictions (with k =16 examples) and gold answers",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Where does Ella think that Oliver searches for the slippers? basket box Table 2: Example stories in the T O M I dev. dataset, with GPT-3-D A V INCI predictions (with k =16 examples) and gold answers. “Type” denotes reasoning type, M-1 and M-2 denote M IND -1st and M IND -2nd, resp. 55% accuracy; however, as before, the gains from scaling up GPT-3 are very small. Similarly, icreasing the number of few-shot examples beyond k = 4 does not substantially improve performance, corroborating findings on S OCIAL IQ A . Further examining GPT-3-D A V INCI with rspect to question types, we show that the model struggles substantially more with questions about mental states (55–60% for k > 0 ) compared to factual questions (90–100% for k > 0 ; Fig. 5 ; columns). Furthermore, the difference between peformance on M IND -T B and M IND -F B questions shows an interesting pattern when conditioning on an increasing number of examples k (Fig. 5 ; lines): GPT-3-D A V INCI ’s M IND -T B accuracy first icreases, peaks at k = 4 , then decreases. This peak seems to be due to the model defaulting to the most recent object location (i.e., the correct M IND -T B answer), as illustrated in example (e) in Tab. 2 . Apparent in Fig. 10 in Appendix B , this recency bias is a phenomenon that has been previously doumented in LLMs ( O’Connor and Andreas , 2021 ). In general, GPT-3-D A V INCI ’s comparably poor performance for M IND -T B and M IND -F B quetions at k > 8 suggests that it cannot properly answer questions about participants’ mental states and realities. 5 Discussion: Towards NLP with Neural Theory of Mind Most humans develop social intelligence and Thory of Mind naturally. However, in this work, we showed that these abilities do not emerge automatcally in large-pretrained language models. These shortcomings contrast with the wealth of successes of LLMs at a variety of tasks, including tasks that potentially require social intelligence",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". These shortcomings contrast with the wealth of successes of LLMs at a variety of tasks, including tasks that potentially require social intelligence. For exaple, GPT-3 has been shown to generate stories with emotional arcs that are virtually indistinguishable from human-written stories ( Clark et al. , 2021 ). Aditionally, recent work has used GPT-3 to generate social commonsense knowledge related to protagnists of situations ( West et al. , 2022 ). While those findings suggest some level of social and emotional intelligence in LLMs, our explorations highlight the limits of these abilities, and raise the open quetion: how can we create NLP systems with true social intelligence and Theory of Mind? To begin answering this question, we first dicuss the current LLMs training paradigm ( 5.1 ), drawing from theories of pragmatics to examine why these models are not learning social intellgence efficiently. Then, we outline some possible future directions to bias models towards Theory of Mind ( 5.2 ), through person-centric neural arch tectures, data selection, and training objectives. 5.1 The Pragmatics of “Static” Text To understand why LLMs are still struggling with social intelligence, we examine LLMs’ training paradigm through the lens of pragmatics . As dicussed in 2 , pragmatics provides a connection btween language development and Theory of Mind ( Sperber and Wilson , 1986 ; Miller , 2006 ; Tauzin and Gergely , 2018 ): learning to communicate effetively with language requires reasoning about what our interlocutor knows or does not know ( Grice , 1975 ; Fernández , 2013 ; Goodman and Frank , 2016 ; Enrici et al. , 2019 ). 7 One major use of language by people is to comunicate about relationships and personal experences ( Clark and Schaefer , 1989 ; Dunbar , 1993 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2019 ). 7 One major use of language by people is to comunicate about relationships and personal experences ( Clark and Schaefer , 1989 ; Dunbar , 1993 ). This is fundamentally different from the training data of LLMs, which consists of language found in what we call static texts: documents that are written for a general audience and are relatively self-contained and topically focused (e.g., news aticles, books, Wikipedia articles; Gao et al. , 2020 ; Dodge et al. , 2021 ). Such static text is typically written such that readers only require the language itself as input, which they then combine with their world knowledge and commonsense to understand its meaning ( Graesser et al. , 1994 ). If AI systems are to learn social intelligence and Theory of Mind, we posit that static text has certain limitations, from a pragmatics lens, outlined below. Reporting bias. Following Grice’s maxim of quantity ( Grice , 1975 ), static text often avoids rdundancy by omitting content that is known by both the author and the reader ( Clark and Brennan , 1991 ). Also known as reporting bias ( Gordon and Van Durme , 2013 ; Lucy and Gauthier , 2017 ), this phenomenon likely limits LLMs’ ability to learn social commonsense knowledge from static text. Lack of communicative intent and alternatives. A corollary to reporting bias, static text does not provide any direct access to communicative intent (why words were used) or to alternatives (which words were not used, and why). This reasoning about intents, alternatives, and their implications is highly predictive of the pragmatic inferences 7 Note here that, in contrast to other work ( Bender and Koller , 2020 ; Bisk et al. , 2020 ), we do not focus on whether LLMs “understand” language, instead we examine whether LLMs can answer questions about the emotions and mental states of participants of situations. people draw about their interlocutors ( Goodman and Frank , 2016 ) — for example, when someone answers Where does Taylor live? with Somewhere in the U.S",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". people draw about their interlocutors ( Goodman and Frank , 2016 ) — for example, when someone answers Where does Taylor live? with Somewhere in the U.S. , it implies that they likely do not know or do not want to share the exact location, since, if they did, they would have been more specific. This poses a likely limitation that LLMs only learn what words are used, but not which words were not used, and why. Lack of communicative effects. Language is primarily learned ( Wells and Bridges , 1981 ; Tomasello et al. , 2005 ) and used ( Clark , 1996 ) in collaborative and interactive settings ( Clark and Schaefer , 1989 ), which allow interlocutors to give immediate feedback to each other on whether their language was understood ( Clark and Krych , 2004 ) or should be adjusted ( Krauss and Weinheimer , 1966 ), and observe the perlocutionary effects that their language has on their partners ( Austin , 1975 ). Since static text has no such feedback, LLMs learn from all texts, as if they were all equally undestandable by readers. Centering theory. At any given time, most text focuses on describing one protagonist and their rlation to their surroundings, according to Centering Theory ( Grosz et al. , 1995 ). As such, main chaacters and their mental states are more likely to be described , whereas other participants might only be mentioned . Additionally, main characters or protagonists are more likely to be referred to with pronouns, whereas secondary characters with their names. Thus, a model trained purely on static text might not learn to reason about social intelligence or metal states and realities of different characters of situations; they might not even inherently learn to resolve coreference for multiple characters ( Saaguchi et al. , 2020 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2020 ). In fact, challenges of corefeence resolution could explain why GPT-3 models struggle on S OCIAL IQ A which contains questions with pronouns, and centering theory and main chaacter biases in static text could explain why models find non-protagonist questions more challenging. On the other hand, T O M I does not contain any prnouns, and thus requires social intelligence beyond coreference resolution. 5.2 Future directions towards LLMs with Theory of Mind While there is no one best path towards LLMs with social intelligence and Theory of Mind, it seems likely that progress will require challenging the standard paradigm of training on static text with the language modeling objective. Based on our findings and the limitations we discussed, we rflect on some possible directions forward. Beyond static text as training data? Perhaps the key is in the data: the knowledge contained in static text might be too limited for models to learn social intelligence, for reasons described in 5.1 Socially grounded text (containing elaborations of communicative intents, character mental states, speaker identities, etc.) could enable more efficient learning of Theory of Mind abilities ( Bender and Koller , 2020 ; Bisk et al. , 2020 ; Hovy and Yang , 2021 ), similar to how visual groundings can help with learning physical knowledge ( Zhang et al. , 2022a ). Examples of such datasets include “Social Stories,” which are devised to help individuals with autism improve their interpersonal skills ( Gray , 1995 ), or the Story Commonsense ( Rashkin et al. , 2018 ) and GLUCOSE ( Mostafazadeh et al. , 2020 ) commonsense-annotated story datasets. Alterntively, perhaps interactional texts, such as dialogues and other datasets that were explicitly created to require reasoning about mental states, could help with neural Theory of Mind ( Bara et al. , 2021 ). Nevertheless, the scale of training datasets seems to be crucial for LLMs ( Kaplan et al. , 2020 ; Chowhery et al",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2021 ). Nevertheless, the scale of training datasets seems to be crucial for LLMs ( Kaplan et al. , 2020 ; Chowhery et al. , 2022 ), which poses a challenge: text datasets rich in social intelligence and interactions are not easily found naturally due to reporting bases, and they are costly to create ( Rashkin et al. , 2018 ; Mostafazadeh et al. , 2020 ). Promising rsults on commonsense reasoning suggest a possble hybrid approach: LLMs could be jointly or sequentially trained on static text and commosense knowledge bases or socially grounded or interactional text ( Bosselut et al. , 2019 ; Hwang et al. , 2021 ), first trained on static text and then enhanced for commonsense knowledge via reiforcement learning ( Zhou et al. , 2021 ). Person-centric neural inductive biases? While more socially grounded training data could help, LLMs might also learn social intelligence better if they are designed with person-centric inductive biases and training objectives. Hinting at this, prior work has shown that training entity-centric neural architectures on text with entity coreference infomation yields more entity-aware LLMs, both in recurrent ( Henaff et al. , 2017 ; Ji et al. , 2017 ; Yang et al. , 2017 ; Liu et al. , 2019 ) and Transformebased models ( Févry et al. , 2020 ; De Cao et al. , 2020 ; Rosset et al. , 2020 ; Zhang et al. , 2022c ). However, Theory of Mind and social intelligence require much richer social grounding than corefeence chains, which is challenging to obtain for spervised settings, especially at the scale that LLMs require. Thus, unsupervised approaches to adding inductive biases to models could be a promising slution. Future work could look to cognitive science and neuroscience research for possible directions ( Langley et al. , 2022 ), such as exploring LLMs’ equivalents of human concept cells (i.e., sets of neurons that activate for important people or cocepts; Bowers , 2017 ; Calvo Tapia et al. , 2020 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2022 ), such as exploring LLMs’ equivalents of human concept cells (i.e., sets of neurons that activate for important people or cocepts; Bowers , 2017 ; Calvo Tapia et al. , 2020 ). Alternatively, examining the internal or latent representations of LLMs could point to future drections towards inductive biases for neural Theory of Mind. As an example, recent work has found eidence of latent representations of grounded sematics in models trained only on static text ( Li et al. , 2021 ), which can be tied to real-world grounding with a small amount of additional supervised traiing ( Patel and Pavlick , 2022 ). Future work might similarly analyze deep learning models for reprsentations of Theory of Mind, toward augmenting the models with structure or objectives that surface and strengthen these representations. Interactive and experiential grounding? It is possible, nevertheless, that socially grounded data and person-centric inductive biases will not suffice. Some researchers have argued that language uderstanding could only emerge from interactions and experiences ( Bender and Koller , 2020 ; Bisk et al. , 2020 ). Likely, this applies to Theory of Mind and social intelligence as well, due to lack of comunicative intents and alternatives in static text. Future work could explore approaches grounded more explicitly in interaction, intents, and alterntives, e.g., by explicitly predicting possible next steps and learning why predictions were wrong. In fact, promising research has shown that using an interactive learning or multi-agent communication paradigm can enable some Theory of Mind capbilities of models ( Hawkins et al. , 2019 ; Lazaridou et al. , 2020 ; Zhu et al. , 2021 ; Wang et al. , 2022 ). However, there are limits to the types of Theory of Mind that can be learned from interactive simultions, which are often task-specific (e.g., describing objects in an image; Lazaridou et al. , 2020 ; SteinerThrelkeld et al. , 2022 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2020 ; SteinerThrelkeld et al. , 2022 ). Furthermore, models that were trained in interactive simulation settings oten struggle to generalize beyond the simulation environment ( Ludwin-Peery et al. , 2021 ; Mu and Goodman , 2021 ). Based on promising results by Lazaridou et al. ( 2020 ); Zhu et al. ( 2021 ), future work might create generalizable LLMs with neural Theory of Mind through hybrid approaches that combine pretraining with interactive learning: udating models trained on static text using supevision either from humans ( Stiennon et al. , 2020 ; Ouyang et al. , 2022 ; Scheurer et al. , 2022 ) or from proxies for human behavior or social environments ( Ammanabrolu et al. , 2022a , b ) based on broad coerage LLMs ( Perez et al. , 2022 ). Probing and evaluating T O M While neural Theory of Mind and social intelligence may rmain an elusive goal for some time, developing measures of those abilities in systems can be done in tandem. We encourage further research in deveoping benchmarks that measure specific social abiities in LLMs (e.g., Sap et al. , 2019b ; Zadeh et al. , 2019 ), especially those that minimize annotation artifacts and spurious correlations ( Schwartz et al. , 2017 ; Gururangan et al. , 2018 ; Le et al. , 2019 ). Additionally, we encourage further investigations into probing the latent knowledge within LLMs ( Tenney et al. , 2019 ; Li et al. , 2021 ) or examining how LLMs handle entities and people ( Onoe et al. , 2022 ; Schuster and Linzen , 2022 ), which could shed light onto better data choices and inductive biases towards neural Theory of Mind and social intelligence. 6 Conclusion We explore the open question of whether and how much modern large-scale language models (LLMs) can reason about social intelligence and Theory of Mind. Our results show that out-of-the-box LLMs struggle substantially with these abilities, which we argue are necessary but not sufficient aspects of Theory of Mind",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Our results show that out-of-the-box LLMs struggle substantially with these abilities, which we argue are necessary but not sufficient aspects of Theory of Mind. Specifically, GPT-3’s social intelligence as measured by S OCIAL IQ A lags bhind humans (>30%), and the model struggles to answer T O M I questions about mental states (55- 60%) compared to factual questions (90–100%). In light of these shortcomings, we critically exaine the large language model pretraining paradigm from a pragmatics-based perspective, and discuss possible directions towards enabling true social itelligence in NLP systems. We make our preprocessed datasets available at http://maartensap.com/neuralToM . 7 Limitations Our work focuses on investigating the Theory of Mind abilities in large pretrained language models, but we focus on accessing GPT-3 ( Brown et al. , 2020 ) through an API, since we do not have acess to some of the larger models out there (PaLM; Chowdhery et al. , 2022 ) nor do we have the coputational resources to run an open-source version of GPT-3 (OPT; Zhang et al. , 2022b ). We hypotesize that results would not be drastically diffeent with such models, based on the low accuracy displayed on S OCIAL IQ A in the recently released BIG-Bench experiments ( Srivastava et al. , 2022 ). Nevertheless, we hope developers of larger LLMs will investigate these T O M abilities to confirm or refute our findings. We measure the ability to answer questions about people’s mental states using T O M I , which is an atomatically constructed corpus of stories involving people, objects, and locations. The automatic nture of the creation process could induce biases and artifacts, such as objects being in locations that are plausible but not typical (e.g., bananas in a closet), which could influence model’s ability to answer questions properly. Based on the near-perfect acuracy on the factual questions, however, this may not be a significant issue. Future work should ivestigate more naturalistic settings to probe this ability in LLMs",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Based on the near-perfect acuracy on the factual questions, however, this may not be a significant issue. Future work should ivestigate more naturalistic settings to probe this ability in LLMs. A potential limitation of our work is that moels could latch onto surface patterns and spurious correlations in our two datasets. For example, thoretically, a model prompted with many T O M I examples may be able to reverse-engineer the data creation algorithm to find the solution to each quetion. However, this would be a bigger limitation if our claims were that LLMs do have social intellgence and Theory of Mind; instead, given that our results show low performance on these tasks even though they are potentially easier due to correltional patterns, this would indicate that LLMs have potentially even less reasoning abilities. Additionally, while we operationalize our mesure of social intelligence and Theory of Mind through two specific tasks, S OCIAL IQ A and T O M I , these abilities are much broader. As noted earlier, we view these benchmarks as necessary but not suficient conditions for LLMs to have T O M; solving the benchmarks does not imply that LLMs have T O M, but LLMs with T O M should be able to solve them. We hope that future research will further investigate other aspects of Theory of Mind abiities in large pretrained LMs, drawing on social science research. For example, future work could make use of the “unexpected content” task ( Gopnik and Astington , 1988 ) or the “George Washington University Social Intelligence Test” ( Hunt , 1928 ) to measure the social intelligence of LLMs. Finally, the focus on English language LLMs and benchmarks for Theory of Mind is another limittion of our work. Echoing recent cognitive science work that argues the need for non-English cognitive science investigations ( Blasi et al. , 2022 ). Specifcally, false-belief abilities are greatly influenced by language structure and grammar ( Boeg Thomsen et al. , 2021 ; Zhang and Zhou , 2022 )",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2022 ). Specifcally, false-belief abilities are greatly influenced by language structure and grammar ( Boeg Thomsen et al. , 2021 ; Zhang and Zhou , 2022 ). Broader Sociotechnical Implications AI systems are part of a broader sociotechnical sytem that also involves individual motivations and societal norms ( Johnson and Verdicchio , 2017 ). As such, per a contextualist view of AI (instead of utopian or dystopian; Barbour , 1992 ), we envision AI systems with social intelligence and Theory of Mind being used in ways that enhance human’s lives, autonomy, and agency ( Chan , 2022 ). In paallel, we strongly support the development and rsearch of policy and regulation, to prevent misuses of AI with social intelligence ( Wischmeyer and Rademacher , 2020 ; Crawford , 2021 ; Reich et al. , 2021 ). Acknowledgements We would like to thank Jack Hessel, Rowan Zellers, Jena D. Hwang, Prithviraj Ammanabrolu for their feedback on preliminary versions of this work, and Anna Jafarpour and Noah Goodman for fruitful cognitive science discussions about the research. We also thank the anonymous reviewers for their thoughtful comments. This research was supported by the Allen Institute for AI and the DARPA MCS program through NIWC Pacific (N66001-19-2- 4031).",
    "chunk_id": "Natural_language_processing_neural_theory-of-mind.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis † , Ethan Perez ? , Aleksandra Piktus † , Fabio Petroni † , Vladimir Karpukhin † , Naman Goyal † , Heinrich Küttler † , Mike Lewis † , Wen-tau Yih † , Tim Rocktäschel † , Sebastian Riedel † , Douwe Kiela † † Facebook AI Research; University College London; ? New York University; plewis@fb.com Abstract Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on dowstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their perfomance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research prolems. Pre-trained models with a differentiable access mechanism to explicit noparametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowedge from data [ 47 ]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [ 51 , 52 ]. While this development is exciting, such models do have dowsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [ 38 ]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [ 20 , 26 , 48 ] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [ 20 ] and ORQA [ 31 ], two recently introduced models that combine masked language models [ 8 ] with a differentiable retriever, have shown promising results, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. TheDiYine Comed\\ (x) T QXeU\\ EQcRdeU T([) MIPS p θ GeneUaWoU¬Sѡ (PaUaPeWULc) MaUgiali]e ThiV14WhcenWXU\\ZoUk iVdiYidedinWo3 VecWionV:\"InfeUno\", \"PXUgaWoUio\"& \"PaUadiVo\" (y) End-to-End Backprop through T and¬ p θ BaUackObamaZaV boUninHaZaii. (x) FacW VeUiÀcaWiRQ: FacW QXeU\\ VXppoUWV (y) QXeVWiRQ GeQeUaWiRQ FacW VeUiÀcaWiRQ: LabeO GeQeUaWiRQ DRcXmeQW IQde[ Define\"middleeaU\" (x) QXeVWiRQ AQVZeUiQg: QXeVWiRQ QXeU\\ ThemiddleeaUinclXdeV WheW\\mpaniccaYiW\\and WheWhUeeoVVicleV. (y) QXeVWiRQ AQVZeUiQg: AQVZeU GeQeUaWiRQ ReWUieYeU S η (NRQ-PaUaPeWULc) z 4 z 3 z 2 z 1 d(]) JeRSaUd\\ QXeVWiRQ GeQeUaWiRQ: AQVZeU QXeU\\ Figure 1: Overview of our approach. We combine a pre-trained retriever ( Query Encoder + Document Index ) with a pre-trained seq2seq model ( Generator ) and fine-tune end-to-end. For query x , we use Maximum Inner Product Search (MIPS) to find the top-K documents z i",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For query x , we use Maximum Inner Product Search (MIPS) to find the top-K documents z i . For final prediction y , we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose fine-tuning approach which we refer to as retrieval-augmented generation (RAG). We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [ 26 ], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [ 32 ]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [ 51 ] or BART, RAG can be fine-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for specific tasks, e.g. memory networks [ 64 , 55 ], stacaugmented networks [ 25 ] and memory layers [ 30 ]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training. Our results highlight the benefits of combining parametric and non-parametric memory with genertion for knowledge-intensive tasks —tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [ 29 ], WebQuestions [ 3 ] and CuratedTrec [ 2 ] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [ 24 ]. Despite these being extractive tasks, we find that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [ 1 ] and Jeopardy question generation, and we find that our models generate responses that are more factual, specific, and diverse than a BART baseline. For FEVER [ 56 ] fact verification, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes. 1 2 Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y . As shown in Figure 1, our models leverage two components: (i) a retriever p ⌘ ( z | x ) with parameters ⌘ that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p ✓ ( y i | x, z, y 1: i − 1 ) parametrized 1 Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transforers Library [ 66 ] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 by ✓ that generates a current token based on a context of the previous i − 1 tokens y 1: i − 1 , the original input x and a retrieved passage z . To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence , the model uses the same document to predict each target token. The second approach, RAG-Token , can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p ⌘ and p ✓ components, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence . Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p ( y | x ) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, p RAG-Sequence ( y | x ) ⇡ X z 2 tok ( p ( ·| x )) p ⌘ ( z | x ) p ✓ ( y | x, z ) = X z 2 tok ( p ( ·| x )) p ⌘ ( z | x ) N Y i p ✓ ( y i | x, z, y 1: i − 1 ) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we define: p RAG-Token ( y | x ) ⇡ N Y i X z 2 tok ( p ( ·| x )) p ⌘ ( z | x ) p ✓ ( y i | x, z i , y 1: i − 1 ) Finally, we note that RAG can be used for sequence classification tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component p ⌘ ( z | x ) is based on DPR [26]. DPR follows a bi-encoder architecture: p ⌘ ( z | x ) / exp # d ( z ) > q ( x ) $ d ( z ) = BERT d ( z ) , q ( x ) = BERT q ( x ) where d ( z ) is a dense representation of a document produced by a BERT BASE document encoder [ 8 ], and q ( x ) a query representation produced by a query encoder , also based on BERT BASE . Calculating top-k ( p ⌘ ( ·| x )) , the list of k documents z with highest prior probability p ⌘ ( z | x ) , is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [ 23 ]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [ 24 ] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory . 2.3 Generator: BART The generator component p ✓ ( y i | x, z, y 1: i − 1 ) could be modelled using any encoder-decoder. We use BART-large [ 32 ], a pre-trained seq2seq transformer [ 58 ] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters ✓ as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a fine-tuning training corpus of input/output pairs ( x j , y j ) , we 3 minimize the negative marginal log-likelihood of each target, P j − log p ( y j | x j ) using stochastic gradient descent with Adam [ 28 ]. Updating the document encoder BERT d during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [ 20 ]. We do not find this step necessary for strong performance, and keep the document encoder (and index) fixed, only fine-tuning the query encoder BERT q and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg max y p ( y | x ) . RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genertor with transition probability: p 0 ✓ ( y i | x, y 1: i − 1 ) = P z 2 tok ( p ( ·| x )) p ⌘ ( z i | x ) p ✓ ( y i | x, z i , y 1: i − 1 ) To decode, we can plug p 0 ✓ ( y i | x, y 1: i − 1 ) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p ( y | x ) does not break into a conventional petoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z , scoring each hypothesis using p ✓ ( y i | x, z, y 1: i − 1 ) . This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p ⌘ ( z | x ) and then sum the probabilities across beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer output sequences, | Y | can become large, requiring many forward passes. For more efficient decoding, we can make a further approximation that p ✓ ( y | x, z i ) ⇡ 0 where y was not generated during beam search from x, z i . This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as “Fast Decoding.” 3 Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26] , we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [ 23 ] with a Hierarchical Navigable Small World approximation for fast retrieval [ 37 ]. During training, we retrieve the top k documents for each query. We consider k 2 { 5 , 10 } for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [ 20 ]. We treat questions and answers as input-output text pairs ( x, y ) and train RAG by directly minimizing the negative log-likelihood of answers",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We treat questions and answers as input-output text pairs ( x, y ) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [ 5 , 7 , 31 , 26 ], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [ 52 ], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [ 29 ], TriviaQA (TQA) [ 24 ]. WebQuestions (WQ) [ 3 ] and CuratedTrec (CT) [ 2 ]. As CT and WQ are small, we follow DPR [ 26 ] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [ 31 , 26 ] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [ 43 ]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as “What is the weather in Volcano, CA?” so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question geeration",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question geeration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the first country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [ 10 ], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [ 67 ], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [ 42 ]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for specificity. We define factuality as whether a statement can be corroborated by trusted external sources, and specificity as high mutual dependence between the input and output [ 33 ]. We follow best practice and use pairwise comparative evaluation [ 34 ]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Verification FEVER [ 56 ] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.4 Fact Verification FEVER [ 56 ] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unverifiable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classification rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classification task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation flexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [ 20 ]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “crosencoder” to re-rank documents, along with an extractive reader",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". RAG compares favourably to the DPR QA system, which uses a BERT-based “crosencoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for OpeDomain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 - /50.1 37.4 - T5-11B+SSM[52] 36.6 - /60.5 44.7 - Open Book REALM [20] 40.4 - / - 40.7 46.8 DPR [26] 41.5 57.9 / - 41.1 50.6 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Seq. 44.5 56.8/ 68.0 45.2 52.2 Table 2: Generation and classification Test Scores. MS-MARCO SotA is [ 4 ], FEVER-3 is [ 68 ] and FEVER-2 is [ 57 ] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8 * 49.9 * 76.8 92.2 * BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5 RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with specific information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we find that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see 4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also find RAG generations to be more specific by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the first token of each book is generated, the document posterior flattens. This observation suggests that the generator can complete the titles without depending on specific documents",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This observation suggests that the generator can complete the titles without depending on specific documents. In other words, the model’s parametric knowledge is sufficient to complete the titles. We find evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun . BART completes the generation \"The Sun Also Rises\" is a novel by this a u thor of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this a u thor of \"A with \"The Sun Also Rises\" is a novel by this a u thor of \"A Farewell to Arms\" . This example shows how parametric and non-parametric memories work together —the non-parametric component helps to guide the generation, drawing out specific knowledge stored in the parametric memory. 4.4 Fact Verification Table 2 shows our results on FEVER. For 3-way classification, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-specific architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 Document 1 : his works are considered classics of American literature His wartime experiences formed the basis for his novel ”A Farewell to Arms” (1929) Document 2 : artists of the 1920s ”Lost Generation” expatriate community. His debut novel, ”The Sun Also Rises” , was published in 1926. BOS ” The Sun Also R ises ” is a novel by this authorof ” A Fare well to Arms” Doc 1 Doc 2 Doc 3 Doc 4 Doc 5 Figure 2: RAG-Token document posterior p ( z i | x, y i , y − i ) for each generated token for input “Heingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\". Table 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Table 3: Examples from generation tasks. RAG models generate more specific and factually accurate responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation MMARCO define middle ear BART ? The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed in Scotland is the pound sterling. Jeopardy Question Gener -ation Washington BART ? This state has the largest number of counties in the U.S. RAG-T It’s the only U.S. state named for a U.S. president RAG-S It’s the state where you’ll find Mount Rainier National Park The Divine Comedy BART * This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante’s \"Inferno\" is the first part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\" For 2-way classification, we compare against Thorne and Vlachos [57] , who train RoBERTa [ 35 ] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We find that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results Generation Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 4.5 Additional Results Generation Diversity Section 4.3 shows that RAG models are more factual and specific than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [ 33 , 59 , 39 ], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are more diverse than RAG-Token’s, and both are significantly more diverse than BART without needing any diversity-promoting decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks. We compare RAG’s dense retriever to a word overlap-based BM25 retriever [ 53 ]. Here, we replace RAG’s retriever with a fixed BM25 system, and use BM25 retrieval scores as logits when calculating p ( z | x ) . Table 6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial. Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [ 5 ] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”) 7 Table 4: Human assessments for the Jeopardy Question Generation Task",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". “Who is the President of Peru?”) 7 Table 4: Human assessments for the Jeopardy Question Generation Task. Factuality Specificity BART better 7.1% 16.8% RAG better 42.7% 37.4% Both good 11.7% 11.8% Both poor 17.7% 6.9% No majority 20.8% 20.1% Table 5: Ratio of distinct to total tri-grams for generation tasks. MSMARCO Jeopardy QGen Gold 89.6% 90.0% BART 70.7% 32.4% RAG-Token 77.8% 46.8% RAG-Seq. 83.5% 53.8% Table 6: Ablations on the dev set. As FEVER is a classification task, both RAG models are equivalent. Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2 Exact Match B-1 QB-1 R-L B-1 Label Accuracy RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6 RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9 RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4 RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3 RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6 RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5 to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory. Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe significant differences in performance between them. We have the flexibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50 K Retrieved Docs 39 40 41 42 43 44 NQ Exact Match RAG-Tok RAG-Seq 10 20 30 40 50 K Retrieved Docs 40 50 60 70 80 NQ Answer Recall @ K RAG-Tok RAG-Seq Fixed DPR BM25 10 20 30 40 50 K Retrieved Docs 48 50 52 54 56 Bleu-1 / Rouge-L score RAG-Tok R-L RAG-Tok B-1 RAG-Seq R-L RAG-Seq B-1 Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfomance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [ 5 , 29 ], fact checking [ 56 ], fact completion [ 48 ], long-form question answering [ 12 ], Wikipedia article generation [ 36 ], dialogue [ 41 , 65 , 9 , 13 ], translation [ 17 ], and language modeling [ 19 , 27 ]. Our work unifies previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8 General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classification tasks in the GLUE bencmarks [ 60 , 61 ] after fine-tuning [ 49 , 8 ]. GPT-2 [ 50 ] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [ 32 ] and T5 [ 51 , 52 ] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Our work aims to expand the space of possible tasks with a single, unified architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is significant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [ 44 , 26 ] similar to ours. Some work optimizes the retrieval module to aid in a specific, downstream task such as question answering, using search [ 46 ], reinforcement learning [ 6 , 63 , 62 ], or a latent variable approach [ 31 , 20 ] as in our work. These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be fine-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [ 64 , 55 ]. Concurrent work [ 14 ] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [ 9 , 13 ] or, closer to our work, over retrieved text directly [ 15 ]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s memory by editing the document index. Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a final output. These approaches have proved successful in a number of domains including Machine Translation [ 18 , 22 ] and Semantic Parsing [ 21 ]",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These approaches have proved successful in a number of domains including Machine Translation [ 18 , 22 ] and Semantic Parsing [ 21 ]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work. 6 Discussion In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG’s generation over purely parametric BART, finding RAG more factual and specific. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks. 9 Broader Impact This work offers several positive societal benefits over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct benefit to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2020-retrieval-augmented-generation-for-knowledge",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2020-retrieval-augmented-generation-for-knowledge.pdf",
    "date_published": "2021-01-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [ 50 ] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [ 54 ]. Advanced language models may also lead to the automation of various jobs in the coming decades [ 16 ]. In order to mitigate these risks, AI systems could be employed to fight against misleading content and automated spam/phishing. Acknowledgments The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. Funding Disclosure EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program. This work was funded by Facebook.",
    "chunk_id": "Natural_language_processing_neurips-2020-retrieval-augmented-generation-for-knowledge.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": "GloVe: Global Vectors for Word Representation Jeffrey Pennington, Richard Socher, Christopher D. Manning Computer Science Department, Stanford University, Stanford, CA 94305 jpennin@stanford.edu, richard@socher.org, manning@stanford.edu Abstract Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector aritmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global lobilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word coccurrence matrix, rather than on the etire sparse matrix or on individual context windows in a large corpus. The model prduces a vector space with meaningful sustructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on simlarity tasks and named entity recognition. 1 Introduction Semantic vector space models of language reprsent each word with a real-valued vector. These vectors can be used as features in a variety of aplications, such as information retrieval (Manning et al., 2008), document classification (Sebastiani, 2002), question answering (Tellex et al., 2003), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2013). Most word vector methods rely on the distance or angle between pairs of word vectors as the prmary method for evaluating the intrinsic quality of such a set of word representations. Recently, Mikolov et al. (2013c) introduced a new evalution scheme based on word analogies that probes the finer structure of the word vector space by eamining not the scalar distance between word vetors, but rather their various dimensions of diference",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example, the analogy “king is to queen as man is to woman” should be encoded in the vector space by the vector equation king − queen = man − woman . This evaluation scheme favors models that produce dimensions of meaing, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009). The two main model families for learning word vectors are: 1) global matrix factorization metods, such as latent semantic analysis (LSA) (Deewester et al., 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al. (2013c). Currently, both families suffer sinificant drawbacks. While methods like LSA eficiently leverage statistical information, they do relatively poorly on the word analogy task, indcating a sub-optimal vector space structure. Metods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the copus since they train on separate local context widows instead of on global co-occurrence counts. In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression moels are appropriate for doing so. We propose a spcific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics. The model prduces a word vector space with meaningful sustructure, as evidenced by its state-of-the-art peformance of 75% accuracy on the word analogy dataset. We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named etity recognition (NER) benchmark. We provide the source code for the model as well as trained word vectors at http://nlp. stanford.edu/projects/glove/ . 1532 2 Related Work Matrix Factorization Methods. Matrix factoization methods for generating low-dimensional word representations have roots stretching as far back as LSA",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1532 2 Related Work Matrix Factorization Methods. Matrix factoization methods for generating low-dimensional word representations have roots stretching as far back as LSA. These methods utilize low-rank aproximations to decompose large matrices that capture statistical information about a corpus. The particular type of information captured by such matrices varies by application. In LSA, the mtrices are of “term-document” type, i.e., the rows correspond to words or terms, and the columns correspond to different documents in the corpus. In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of “term-term” type, i.e., the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word. A main problem with HAL and related metods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and , for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness. A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al., 2006), in which the co-occurrence matrix is first transformed by an entropor correlation-based normalization. An advantage of this type of tranformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller inteval. A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mtual information (PPMI) is a good transformation. More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effetive way of learning word representations. Shallow Window-Based Methods",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Shallow Window-Based Methods. Another approach is to learn word representations that aid in making predictions within local context widows. For example, Bengio et al. (2003) intrduced a model that learns word vector representtions as part of a simple neural network architeture for language modeling. Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al. (2011) to use the full context of a word for learning the word represetations, rather than just the preceding context as is the case with language models. Recently, the importance of the full neural nework structure for learning useful word reprsentations has been called into question. The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al. (2013a) propose a siple single-layer architecture based on the inner product between two word vectors. Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al. (2014) proposed explicit word embedings based on a PPMI metric. In the skip-gram and ivLBL models, the objetive is to predict a word’s context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its cotext. Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors. Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus. Instead, these models scan context windows across the etire corpus, which fails to take advantage of the vast amount of repetition in the data",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Instead, these models scan context windows across the etire corpus, which fails to take advantage of the vast amount of repetition in the data. 3 The GloVe Model The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word represetations, and although many such methods now eist, the question still remains as to how meaning is generated from these statistics, and how the rsulting word vectors might represent that meaning. In this section, we shed some light on this quetion. We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statitics are captured directly by the model. First we establish some notation. Let the matrix of word-word co-occurrence counts be denoted by X , whose entries X i j tabulate the number of times word j occurs in the context of word i . Let X i = P k X ik be the number of times any word appears in the context of word i . Finally, let P i j = P ( j | i ) = X i j / X i be the probability that word j appear in the 1533 Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus. Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam. Probability and Ratio k = solid k = gas k = water k = fashion P ( k | ice ) 1 . 9 × 10 − 4 6 . 6 × 10 − 5 3 . 0 × 10 − 3 1 . 7 × 10 − 5 P ( k | steam ) 2 . 2 × 10 − 5 7 . 8 × 10 − 4 2 . 2 × 10 − 3 1 . 8 × 10 − 5 P ( k | ice ) / P ( k | steam ) 8 . 9 8 . 5 × 10 − 2 1 . 36 0 . 96 context of word i . We begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 9 8 . 5 × 10 − 2 1 . 36 0 . 96 context of word i . We begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities. Cosider two words i and j that exhibit a particular apect of interest; for concreteness, suppose we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam . The relationship of these words can be examined by studying the ratio of their co-occurrence proabilities with various probe words, k . For words k related to ice but not steam, say k = solid , we expect the ratio P ik / P jk will be large. Similarly, for words k related to steam but not ice, say k = gas , the ratio should be small. For words k like water or fashion , that are either related to both ice and steam, or to neither, the ratio should be close to one. Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations. Compared to the raw probabiities, the ratio is better able to distinguish relevant words ( solid and gas ) from irrelevant words ( water and fashion ) and it is also better able to discrimnate between the two relevant words. The above argument suggests that the approprate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves. Noting that the ratio P ik / P jk depends on three words i , j , and k , the most general model takes the form, F ( w i , w j , ̃ w k ) = P ik P jk , (1) where w ∈ R d are word vectors and ̃ w ∈ R d are separate context word vectors whose role will be discussed in Section 4.2. In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified prameters. The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice. First, we would like F to encode the information present the ratio P ik / P jk in the word vector space",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". First, we would like F to encode the information present the ratio P ik / P jk in the word vector space. Since vector spaces are inheently linear structures, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words, modifying Eqn. (1) to, F ( w i − w j , ̃ w k ) = P ik P jk . (2) Next, we note that the arguments of F in Eqn. (2) are vectors while the right-hand side is a scalar. While F could be taken to be a complicated funtion parameterized by, e.g., a neural network, ding so would obfuscate the linear structure we are trying to capture. To avoid this issue, we can first take the dot product of the arguments, F ( w i − w j ) T ̃ w k = P ik P jk , (3) which prevents F from mixing the vector dimesions in undesirable ways. Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange w ↔ ̃ w but also X ↔ X T . Our final model should be ivariant under this relabeling, but Eqn. (3) is not. However, the symmetry can be restored in two steps. First, we require that F be a homomorphism between the groups ( R , +) and ( R > 0 , × ), i.e., F ( w i − w j ) T ̃ w k = F ( w T i ̃ w k ) F ( w T j ̃ w k ) , (4) which, by Eqn. (3), is solved by, F ( w T i ̃ w k ) = P ik = X ik X i . (5) The solution to Eqn. (4) is F = exp, or, w T i ̃ w k = log( P ik ) = log( X ik ) − log( X i ) . (6) 1534 Next, we note that Eqn. (6) would exhibit the echange symmetry if not for the log( X i ) on the right-hand side. However, this term is indepedent of k so it can be absorbed into a bias b i for w i . Finally, adding an additional bias ̃ b k for ̃ w k restores the symmetry, w T i ̃ w k + b i + ̃ b k = log( X ik ) . (7) Eqn. (7) is a drastic simplification over Eqn. (1), but it is actually ill-defined since the logarithm dverges whenever its argument is zero",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (7) Eqn. (7) is a drastic simplification over Eqn. (1), but it is actually ill-defined since the logarithm dverges whenever its argument is zero. One reslution to this issue is to include an additive shift in the logarithm, log( X ik ) → log(1 + X ik ), which maintains the sparsity of X while avoiding the dvergences. The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments. A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare coccurrences are noisy and carry less information than the more frequent ones — yet even just the zero entries account for 75–95% of the data in X , depending on the vocabulary size and corpus. We propose a new weighted least squares rgression model that addresses these problems. Casting Eqn. (7) as a least squares problem and introducing a weighting function f ( X i j ) into the cost function gives us the model J = V X i , j = 1 f X i j w T i ̃ w j + b i + ̃ b j − log X i j 2 , (8) where V is the size of the vocabulary. The weighing function should obey the following properties: 1. f (0) = 0. If f is viewed as a continuous function, it should vanish as x → 0 fast enough that the lim x → 0 f ( x ) log 2 x is finite. 2. f ( x ) should be non-decreasing so that rare co-occurrences are not overweighted. 3. f ( x ) should be relatively small for large vaues of x , so that frequent co-occurrences are not overweighted. Of course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, f ( x ) = ( ( x / x max ) α if x < x max 1 otherwise . (9) 0.2 0.4 0.6 0.8 1.0 0.0 Figure 1: Weighting function f with α = 3 / 4. The performance of the model depends weakly on the cutoff, which we fix to x max = 100 for all our experiments. We found that α = 3 / 4 gives a moest improvement over a linear version with α = 1",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The performance of the model depends weakly on the cutoff, which we fix to x max = 100 for all our experiments. We found that α = 3 / 4 gives a moest improvement over a linear version with α = 1. Although we offer only empirical motivation for choosing the value 3 / 4, it is interesting that a siilar fractional power scaling was found to give the best performance in (Mikolov et al., 2013a). 3.1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurence statistics of a corpus, there should be comonalities between the models. Nevertheless, cetain models remain somewhat opaque in this rgard, particularly the recent window-based metods like skip-gram and ivLBL. Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn. (8). The starting point for the skip-gram or ivLBL methods is a model Q i j for the probability that word j appears in the context of word i . For cocreteness, let us assume that Q i j is a softmax, Q i j = exp( w T i ̃ w j ) P V k = 1 exp( w T i ̃ w k ) . (10) Most of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a cotext window scans over the corpus. Training prceeds in an on-line, stochastic fashion, but the iplied global objective function can be written as, J = − X i ∈ corpus j ∈ context( i ) log Q i j . (11) Evaluating the normalization factor of the sofmax for each term in this sum is costly. To alow for efficient training, the skip-gram and ivLBL models introduce approximations to Q i j . Hoever, the sum in Eqn. (11) can be evaluated much 1535 more efficiently if we first group together those terms that have the same values for i and j , J = − V X i = 1 V X j = 1 X i j log Q i j , (12) where we have used the fact that the number of like terms is given by the co-occurrence matrix X",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Recalling our notation for X i = P k X ik and P i j = X i j / X i , we can rewrite J as, J = − V X i = 1 X i V X j = 1 P i j log Q i j = V X i = 1 X i H ( P i , Q i ) , (13) where H ( P i , Q i ) is the cross entropy of the ditributions P i and Q i , which we define in analogy to X i . As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn. (8). In fact, it is possible to optimize Eqn. (13) directly as opposed to the on-line training methods used in the skip-gram and ivLBL models. One could intepret this objective as a “global skip-gram” model, and it might be interesting to investigate further. On the other hand, Eqn. (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors. To begin, cross entropy error is just one among many possible distance measures between proability distributions, and it has the unfortunate property that distributions with long tails are oten modeled poorly with too much weight given to the unlikely events. Furthermore, for the mesure to be bounded it requires that the model ditribution Q be properly normalized. This presents a computational bottleneck owing to the sum over the whole vocabulary in Eqn. (10), and it would be desirable to consider a different distance measure that did not require this property of Q . A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ˆ J = X i , j X i ˆ P i j − ˆ Q i j 2 (14) where ˆ P i j = X i j and ˆ Q i j = exp( w T i ̃ w j ) are the unnormalized distributions. At this stage another problem emerges, namely that X i j often takes very large values, which can complicate the optimiztion. An effective remedy is to minimize the squared error of the logarithms of ˆ P and ˆ Q instead, ˆ J = X i , j X i log ˆ P i j − log ˆ Q i j 2 = X i , j X i w T i ̃ w j − log X i j 2",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". An effective remedy is to minimize the squared error of the logarithms of ˆ P and ˆ Q instead, ˆ J = X i , j X i log ˆ P i j − log ˆ Q i j 2 = X i , j X i w T i ̃ w j − log X i j 2 . (15) Finally, we observe that while the weighting factor X i is preordained by the on-line training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal. In fact, Mikolov et al. (2013a) observe that performance can be increased by filtering the data so as to rduce the effective value of the weighting factor for frequent words. With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well. The result is, ˆ J = X i , j f ( X i j ) w T i ̃ w j − log X i j 2 , (16) which is equivalent 1 to the cost function of Eqn. (8), which we derived previously. 3.2 Complexity of the model As can be seen from Eqn. (8) and the explicit form of the weighting function f ( X ), the computational complexity of the model depends on the number of nonzero elements in the matrix X . As this nuber is always less than the total number of etries of the matrix, the model scales no worse than O ( | V | 2 ). At first glance this might seem like a sustantial improvement over the shallow windobased approaches, which scale with the corpus size, | C | . However, typical vocabularies have hudreds of thousands of words, so that | V | 2 can be in the hundreds of billions, which is actually much larger than most corpora. For this reason it is iportant to determine whether a tighter bound can be placed on the number of nonzero elements of X . In order to make any concrete statements about the number of nonzero elements in X , it is necesary to make some assumptions about the distribtion of word co-occurrences. In particular, we will assume that the number of co-occurrences of word i with word j , X i j , can be modeled as a power-law function of the frequency rank of that word pair, r i j : X i j = k ( r i j ) α . (17) 1 We could also include bias terms in Eqn",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (17) 1 We could also include bias terms in Eqn. (16). 1536 The total number of words in the corpus is prportional to the sum over all elements of the coccurrence matrix X , | C | ∼ X i j X i j = | X | X r = 1 k r α = kH | X | ,α , (18) where we have rewritten the last sum in terms of the generalized harmonic number H n , m . The uper limit of the sum, | X | , is the maximum frquency rank, which coincides with the number of nonzero elements in the matrix X . This number is also equal to the maximum value of r in Eqn. (17) such that X i j ≥ 1, i.e., | X | = k 1 /α . Therefore we can write Eqn. (18) as, | C | ∼| X | α H | X | ,α . (19) We are interested in how | X | is related to | C | when both numbers are large; therefore we are free to expand the right hand side of the equation for large | X | . For this purpose we use the expansion of geeralized harmonic numbers (Apostol, 1976), H x , s = x 1 − s 1 − s + ζ ( s ) + O ( x − s ) if s > 0 , s , 1 , (20) giving, | C | ∼ | X | 1 − α + ζ ( α ) | X | α + O (1) , (21) where ζ ( s ) is the Riemann zeta function. In the limit that X is large, only one of the two terms on the right hand side of Eqn. (21) will be relevant, and which term that is depends on whether α > 1, | X | = ( O ( | C | ) if α < 1 , O ( | C | 1 /α ) if α > 1 . (22) For the corpora studied in this article, we observe that X i j is well-modeled by Eqn. (17) with α = 1 . 25. In this case we have that | X | = O ( | C | 0 . 8 ). Therefore we conclude that the complexity of the model is much better than the worst case O ( V 2 ), and in fact it does somewhat better than the on-line window-based methods which scale like O ( | C | ). 4 Experiments 4.1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al. (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (2013a), a variety of word similarity tasks, as described in (Luong et al., 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy. Underlined scores are best within groups of similarly-sized models; bold scores are best overall. HPCA vectors are publicly available 2 ; (i)vLBL results are from (Mnih et al., 2013); skip-gram (SG) and CBOW results are from (Mikolov et al., 2013a,b); we trained SG † and CBOW † using the word2vec tool 3 . See text for details and a description of the SVD models. Model Dim. Size Sem. Syn. Tot. ivLBL 100 1.5B 55.9 50.1 53.2 HPCA 100 1.6B 4.2 16.4 10.8 GloVe 100 1.6B 67.5 54.3 60.3 SG 300 1B 61 61 61 CBOW 300 1.6B 16.1 52.6 36.1 vLBL 300 1.5B 54.2 64.8 60.0 ivLBL 300 1.5B 65.2 63.0 64.0 GloVe 300 1.6B 80.8 61.5 70.3 SVD 300 6B 6.3 8.1 7.3 SVD-S 300 6B 36.7 46.6 42.1 SVD-L 300 6B 56.6 63.0 60.1 CBOW † 300 6B 63.6 67.4 65.7 SG † 300 6B 73.0 66.0 69.1 GloVe 300 6B 77.4 67.0 71.7 CBOW 1000 6B 57.3 68.9 63.7 SG 1000 6B 66.1 65.1 65.6 SVD-L 300 42B 38.4 58.2 49.2 GloVe 300 42B 81.9 69.3 75.0 dataset for NER (Tjong Kim Sang and De Meuder, 2003). Word analogies. The word analogy task cosists of questions like, “ a is to b as c is to ?” The dataset contains 19,544 such questions, dvided into a semantic subset and a syntactic suset. The semantic questions are typically analogies about people or places, like “Athens is to Greece as Berlin is to ?”. The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example “dance is to dancing as fly is to ?”. To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match. We answer the question “ a is to b as c is to ?” by finding the word d whose reprsentation w d is closest to w b − w a + w c according to the cosine similarity. 4 2 http://lebret.ch/words/ 3 http://code.google.com/p/word2vec/ 4 Levy et al",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 2 http://lebret.ch/words/ 3 http://code.google.com/p/word2vec/ 4 Levy et al. (2014) introduce a multiplicative analogy evaluation, 3C OS M UL , and report an accuracy of 68.24% on 1537 0 100 200 300 400 500 60 0 20 30 40 50 60 70 80 Vector Dimension Accuracy [%] Semantic Syntactic Overall (a) Symmetric context 2 4 6 8 1 0 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (b) Symmetric context 2 4 6 8 1 0 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (c) Asymmetric context Figure 2: Accuracy on the analogy task as function of vector size and window size/type. All models are trained on the 6 billion token corpus. In (a), the window size is 10. In (b) and (c), the vector size is 100. Word similarity. While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3. These include WordSim-353 (Finkelstein et al., 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al., 2012), and RW (Luong et al., 2013). Named entity recognition. The CoNLL-2003 English benchmark dataset for NER is a colletion of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous. We train moels on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set. We adopt the BIO2 annottion standard, as well as all the preprocessing steps described in (Wang and Manning, 2013). We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al., 2005). A total of 437,905 discrete features were generated for the CoNL2003 training dataset. In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features. With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRF join model of (Wang and Manning, 2013). 4.2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tkens; a 2014 Wikipedia dump with 1.6 billion tkens; Gigaword 5 which has 4.3 billion tokens; the combination Gigaword5 + Wikipedia2014, which the analogy task. This number is evaluated on a subset of the dataset so it is not included in Table 2. 3C OS M UL performed worse than cosine similarity in almost all of our experiments. has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl 5 . We tokenize and lowercase each corpus with the Stanford tkenizer, build a vocabulary of the 400,000 most frequent words 6 , and then construct a matrix of coccurrence counts X . In constructing X , we must choose how large the context window should be and whether to distinguish left context from right context. We explore the effect of these choices blow. In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1 / d to the total count. This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words’ relationship to one another. For all our experiments, we set x max = 100, α = 3 / 4, and train the model using AdaGrad (Duchi et al., 2011), stochastically sampling nozero elements from X , with initial learning rate of 0.05. We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4.6 for more details about the convergence rate). Unless otherwise noted, we use a context of ten words to the left and ten words to the right. The model generates two sets of word vectors, W and ̃ W",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Unless otherwise noted, we use a context of ten words to the left and ten words to the right. The model generates two sets of word vectors, W and ̃ W . When X is symmetric, W and ̃ W are equivalent and differ only as a result of their radom initializations; the two sets of vectors should perform equivalently. On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al., 2012). With this in mind, we choose to use 5 To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 bilion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable. 6 For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words. 1538 the sum W + ̃ W as our word vectors. Doing so tyically gives a small boost in performance, with the biggest increase in the semantic analogy task. We compare with the published results of a vriety of state-of-the-art models, as well as with our own results produced using the word2vec tool and with several baselines using SVDs. With word2vec , we train the skip-gram (SG † ) and continuous bag-of-words (CBOW † ) models on the 6 billion token corpus (Wikipedia 2014 + Gigword 5) with a vocabulary of the top 400,000 most frequent words and a context window size of 10. We used 10 negative samples, which we show in Section 4.6 to be a good choice for this corpus. For the SVD baselines, we generate a truncated matrix X trunc which retains the information of how frequently each word occurs with only the top 10,000 most frequent words. This step is typcal of many matrix-factorization-based methods as the extra columns can contribute a disproportioate number of zero entries and the methods are otherwise computationally expensive. The singular vectors of this matrix constitute the baseline “SVD”",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The singular vectors of this matrix constitute the baseline “SVD”. We also evaluate two related baselines: “SVD-S” in which we take the SVD of √ X trunc , and “SVD-L” in which we take the SVD of log(1+ X trunc ). Both methods help compress the otherwise large range of values in X . 7 4.3 Results We present results on the word analogy task in Tble 2. The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora. Our results uing the word2vec tool are somewhat better than most of the previously published results. This is due to a number of factors, including our choice to use negative sampling (which typically works beter than the hierarchical softmax), the number of negative samples, and the choice of the corpus. We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost. We note that increasing the corpus size does not guaantee improved results for other models, as can be seen by the decreased performance of the SV7 We also investigated several other weighting schemes for transforming X ; what we report here performed best. Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies. With smaller vocabularies, these information-theoretic tranformations do indeed work well on word similarity measures, but they perform very poorly on the word analogy task. Table 3: Spearman rank correlation on word simlarity tasks. All vectors are 300-dimensional. The CBOW ∗ vectors are from the word2vec website and differ in that they contain phrase vectors. Model Size WS353 MC RG SCWS RW SVD 6B 35.3 35.1 42.5 38.3 25.6 SVD-S 6B 56.5 71.5 71.0 53.6 34.7 SVD-L 6B 65.7 72.7 75.1 56.5 37.0 CBOW † 6B 57.2 65.6 68.2 57.0 32.5 SG † 6B 62.8 65.2 69.7 58.1 37.2 GloVe 6B 65.8 72.7 77.8 53.9 38.1 SVD-L 42B 74.0 76.4 74.1 58.3 39.9 GloVe 42B 75.9 83.6 82.9 59.6 47.8 CBOW ∗ 100B 68.4 79.6 75.4 59.4 45.5 L model on this larger corpus",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The fact that this basic SVD model does not scale well to large copora lends further evidence to the necessity of the type of weighting scheme proposed in our model. Table 3 shows results on five different word similarity datasets. A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculaing the cosine similarity. We compute Spearman’s rank correlation coefficient between this score and the human judgments. CBOW ∗ denotes the vetors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data. GloVe outperforms it while using a corpus less than half the size. Table 4 shows results on the NER task with the CRF-based model. The L-BFGS training termnates when no improvement has been achieved on the dev set for 25 iterations. Otherwise all confiurations are identical to those used by Wang and Manning (2013). The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vetor features. In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al. (2012) (HSMN) and Collobert and Weston (2008) (CW). We trained the CBOW model using the word2vec tool 8 . The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better. We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8 We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10. 1539 Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1539 Table 4: F1 score on NER task with 50d vectors. Discrete is the baseline without word vectors. We use publicly-available vectors for HPCA, HSMN, and CW. See text for details. Model Dev Test ACE MUC7 Discrete 91.0 85.4 77.4 73.4 SVD 90.8 85.7 77.3 73.7 SVD-S 91.0 85.5 77.6 74.3 SVD-L 90.5 84.8 73.6 71.5 HPCA 92.6 88.7 81.7 80.7 HSMN 90.5 85.7 78.7 74.7 CW 92.2 87.4 81.7 80.2 CBOW 93.1 88.2 82.2 81.1 GloVe 93.2 88.3 82.9 82.2 shown for neural vectors in (Turian et al., 2010). 4.4 Model Analysis: Vector Length and Context Size In Fig. 2, we show the results of experiments that vary vector length and context window. A context window that extends to the left and right of a taget word will be called symmetric, and one which extends only to the left will be called asymmeric. In (a), we observe diminishing returns for vetors larger than about 200 dimensions. In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context widows. Performance is better on the syntactic sutask for small and asymmetric context windows, which aligns with the intuition that syntactic infomation is mostly drawn from the immediate cotext and can depend strongly on word order. Smantic information, on the other hand, is more frquently non-local, and more of it is captured with larger window sizes. 4.5 Model Analysis: Corpus Size In Fig. 3, we show performance on the word anaogy task for 300-dimensional vectors trained on different corpora. On the syntactic subtask, there is a monotonic increase in performance as the copus size increases. This is to be expected since larger corpora typically produce better statistics. Interestingly, the same trend is not true for the smantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus. This is likely due to the large number of citand countrbased analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This is likely due to the large number of citand countrbased analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations. Moreover, Wikipedia’s 50 55 60 65 70 75 80 85 Overall Syntactic Semantic Wiki2010 1B tokens Accuracy [%] Wiki2014 1.6B tokens Gigaword5 4.3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300- dimensional vectors trained on different corpora. entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information. 4.6 Model Analysis: Run-time The total run-time is split between populating X and training the model. The former depends on many factors, including window size, vocabulary size, and corpus size. Though we did not do so, this step could easily be parallelized across mutiple machines (see, e.g., Lebret and Collobert (2014) for some benchmarks). Using a single thread of a dual 2.1GHz Intel Xeon E5-2658 mchine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes. Given X , the time it takes to train the model dpends on the vector size and the number of itertions. For 300-dimensional vectors with the above settings (and using all 32 cores of the above mchine), a single iteration takes 14 minutes. See Fig. 4 for a plot of the learning curve. 4.7 Model Analysis: Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on peformance. We control for the main sources of varation that we identified in Sections 4.4 and 4.5 by setting the vector length, context window size, copus, and vocabulary size to the configuration metioned in the previous subsection. The most important remaining variable to cotrol for is training time. For GloVe, the relvant parameter is the number of training iterations",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The most important remaining variable to cotrol for is training time. For GloVe, the relvant parameter is the number of training iterations. For word2vec , the obvious choice would be the number of training epochs. Unfortunately, the code is currently designed for only a single epoch: 1540 1 2 3 4 5 6 60 62 64 66 68 70 72 5 10 15 20 25 1357 10 15 20 25 30 40 50 Accuracy [%] Iterations (GloVe) Negative Samples (CBOW) Training Time (hrs) GloVe CBOW (a) GloVe vs CBOW 3 6 9 12 15 18 21 24 60 62 64 66 68 70 72 20 40 60 80 100 1 2 3 4 5 6 7 10 12 15 20 GloVe Skip-Gram Accuracy [%] Iterations (GloVe) Negative Samples (Skip-Gram) Training Time (hrs) (b) GloVe vs Skip-Gram Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram (b). In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10. it specifies a learning schedule specific to a single pass through the data, making a modification for multiple passes a non-trivial task. Another choice is to vary the number of negative samples. Adding negative samples effectively increases the number of training words seen by the model, so in some ways it is analogous to extra epochs. We set any unspecified parameters to their dfault values, assuming that they are close to optmal, though we acknowledge that this simplifiction should be relaxed in a more thorough analysis. In Fig. 4, we plot the overall performance on the analogy task as a function of training time. The two x -axes at the bottom indicate the corrsponding number of training iterations for GloVe and negative samples for word2vec . We note that word2vec ’s performance actually decreases if the number of negative samples increases byond about 10",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We note that word2vec ’s performance actually decreases if the number of negative samples increases byond about 10. Presumably this is because the negative sampling method does not approximate the target probability distribution well. 9 For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec . It achieves better results faster, and also obtains the best results irrespective of speed. 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9 In contrast, noise-contrastive estimation is an approxmation which improves with more negative samples. In Tble 1 of (Mnih et al., 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples. methods or from prediction-based methods. Curently, prediction-based models garner substantial support; for example, Baroni et al. (2014) argue that these models perform better across a range of tasks. In this work we argue that the two classes of methods are not dramatically different at a fudamental level since they both probe the undelying co-occurrence statistics of the corpus, but the efficiency with which the count-based metods capture global statistics can be advantageous. We construct a model that utilizes this main beefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec . The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks. Acknowledgments We thank the anonymous reviewers for their valable comments. Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Rsearch Laboratory (AFRL) contract no",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Glove: Global Vectors for Word Representation",
    "author": "Jeffrey Pennington ; Richard Socher ; Christopher Manning",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Pennington-2014-Glove-global-vectors-for-word-repre.pdf",
    "date_published": "2014-10-19",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Rsearch Laboratory (AFRL) contract no. FA8650- 10-C-7020 and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under AFRL contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations epressed in this material are those of the authors and do not necessarily reflect the view of the DTRA, AFRL, DEFT, or the US government. 1541",
    "chunk_id": "Natural_language_processing_glove_global_vectors_for_word_representation.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference Wei-Lin Chiang * 1 Lianmin Zheng * 1 Ying Sheng 2 Anastasios N. Angelopoulos 1 Tianle Li 1 Dacheng Li 1 Banghua Zhu 1 Hao Zhang 3 Michael I. Jordan 1 Joseph E. Gonzalez 1 Ion Stoica 1 Abstract Large Language Models (LLMs) have unlocked new capabilities and applications; however, evalating the alignment with human preferences still poses significant challenges. To address this isue, we introduce Chatbot Arena, an open plaform for evaluating LLMs based on human preerences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org . 1. Introduction Recent advancements in large language models (LLMs) have significantly expanded their capabilities beyond tradtional natural language processing boundaries, addressing a broad array of general tasks ( OpenAI , 2023 ; Gemini et al. , 2023 ; Touvron et al. , 2023 ). These developments underscore the potential of LLMs but also have raised concerns with rspect to performance evaluation. Current benchmarks often * Equal contribution 1 UC Berkeley 2 Stanford 3 UCSD. Corrspondence to: Wei-Lin Chiang <weichiang@berkeley.edu>",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Current benchmarks often * Equal contribution 1 UC Berkeley 2 Stanford 3 UCSD. Corrspondence to: Wei-Lin Chiang <weichiang@berkeley.edu>. Proceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Live Static Codeforces Weekly Contests MMLU, HellaSwag, GSM-8K Ground Truth Chatbot Arena MT-Bench, AlpacaEval Human Preference Question Source Evaluation Metric Figure 1. Classification of LLM benchmarks: We categorize along two dimensions: whether the questions are from a static dataset or a live, fresh source, and whether the evaluation meric relies on ground truth or (approximated) human preferences. MMLU ( Hendrycks et al. , 2020 ), HellaSwag ( Zellers et al. , 2019 ), GSM-8K ( Cobbe et al. , 2021 ), MT-Bench ( Zheng et al. , 2023b ), and AlpacaEval ( Li et al. , 2023 ) are common examples of static benchmarks. Chatbot Arena is the one introduced in this paper. fail to capture the nuanced and diverse aspects of these moels, particularly in assessing their alignment with human preferences in real-world, open-ended tasks. To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of quetions (either static or live) and the evaluation metric (either ground truth or human preference). According to these fators, benchmarks can be classified into four categories, as shown in Figure 1 . While a range of benchmarks is benefcial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly bcause such evaluations are inexpensive and reproducible. However, these static, ground-truth-based benchmarks ehibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings ( Zheng et al. , 2023b )",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings ( Zheng et al. , 2023b ). Secondly, the test sets in these benchmarks are static, meaning they can become cotaminated over time, which undermines the reliability of the evaluation results ( Yang et al. , 2023 ). Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Consquently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage. Creating such a benchmark platform entails significant chalenges. It requires the collection of live, fresh, and diverse 1 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference user questions to accurately represent real-world scenarios. Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evalutions is crucial given the noisy nature of human preferences. To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users. 1 On this website, a user can ask a question and get answers from two anonymous LLMs. Aterward, the user casts a vote for the model that delivers the preferred response, with the models’ identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley & Terry ( 1952 ) to the E-values of Vovk & Wang ( 2021 ), to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sapling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity. We conduct a thorough analysis of the collected data to esure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to ecompass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consitent with expert evaluations. We have been running our system since Apr 2023 and have received over 240K votes from about 90K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry. We commit to making our data and code available, ensuring that this platform is open-source and open-accessible. We make the following contributions: • We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". We make the following contributions: • We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit. 2 1 https://chat.lmsys.org 2 The number was estimated by Google Analytics as of March • We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback. • We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena. • We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree. 2. Related Work LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Fiure 1 . The most prevalent benchmarks are static, grountruth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined aswers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU ( Hendrycks et al. , 2020 ), HelaSwag ( Zellers et al. , 2019 ), GSM-8K ( Cobbe et al. , 2021 ), BigBench ( Srivastava et al. , 2023 ), AGIEval ( Zhong et al. , 2023 ), and HumanEval ( Chen et al. , 2021 ). Benchmarks focusing on safety, such as ToxicChat ( Lin et al. , 2023 ), and comprehensive suites like HELM ( Liang et al. , 2022 ), also exist. In addition to closed-ended questions, bencmarks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk ( Karpinska et al. , 2021 ; Geng et al. , 2023 ; Wang et al. , 2023 ). The recent trend includes utilizing GPT-4 for approximating human judment ( Chiang & Lee , 2023 ), with notable instances being MT-Bench ( Zheng et al. , 2023b ) and AlpacaEval ( Li et al. , 2023 ). In addition to static benchmarks, live benchmarks that include fresh questions are also available",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". , 2023b ) and AlpacaEval ( Li et al. , 2023 ). In addition to static benchmarks, live benchmarks that include fresh questions are also available. These quetions can be obtained from annual exams or weekly online contests such as Codeforces ( Li et al. , 2022 ; Huang et al. , 2023 ). They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference ( Bai et al. , 2022 ; Ouyang et al. , 2022 ; Touvron et al. , 2023 ). However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that utlizes live human interaction. Risks of Static Benchmarks. Static benchmarks have cetain issues, including contamination, saturation, overfitting, and a lack of human alignment ( Yang et al. , 2023 ; Oren et al. , 2023 ). DynaBench ( Kiela et al. , 2021 ) identifies these challenges and recommends the use of a live benchmark 2024. Note that user visit may not convert to votes as our website also offers “direct chat” mode. 2 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. Hoever, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale. Ranking System. Ranking systems have been a welstudied topic in statistics. Related topics include probability models ( Hunter , 2004 ; Rao & Kupper , 1967 ), rank elicittion ( Szörényi et al. , 2015 ; Busa-Fekete et al. , 2014a ; b ), and online experiment design ( Chernoff , 1992 ; Karimi et al. , 2021 ). The Elo rating system has also been used for LLMs ( Bai et al. , 2022 ; Boubdir et al. , 2023 ). Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifcally applied to large-scale, real-world settings of LLMs. Human Preference Dataset",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Human Preference Dataset. Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include OpnAssistant ( Köpf et al. , 2023 ), HH-RLHF ( Bai et al. , 2022 ), LMSYS-Chat-1M ( Zheng et al. , 2023a ), and synthetic aproximations of human preferences like UltraFeedback ( Cui et al. , 2023 ) and Nectar ( Zhu et al. , 2023 ). Our prior data release, LMSYS-Chat-1M ( Zheng et al. , 2023a ), is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes. 3. Human Preference Data Collection In this section, we discuss our interface design to collect human preferences and present summary statistics. 3.1. Interface Chatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use iterface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score. In each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user egagement, particularly given that we offer a free service. It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred aswer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, “tie” or “both are bad.” Figure 8 shows a screenshot of our interface",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". For those who are unsure, we also present two buttons, “tie” or “both are bad.” Figure 8 shows a screenshot of our interface. Before using our service, users are required to accept terms of use, which gives us their consent to release the data publicly. 3.2. Data Statistics We began collecting data in April 2023. As of Jan 2024, we have received around 240K votes from over 90K users. Our data involves more than 50 models, including both prprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These coversations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers. We summarize statistics in Table 1 along with other existing human preference datasets. Figure 10 in the Appendix shows the vote count per model. On average, 8K votes are collected for each model. In Fiure 2 , we select a set of representative models and present their win rate and the number of battles. Note that we eploy non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncetainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis. To ensure anonymity, we use keywords to filter out coversations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic). To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests. Figure 9 in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates. 4. From Pairwise Comparisons to Rankings Our data consists of pairwise comparisons—but how can we use these comparisons to recover a ranking over all M moels? This is a well-studied topic in the literature on learning to rank ( Liu et al. , 2009 ), and we present our perspective here. We let A = { ( m, m ′ ) : m < m ′ and m, m ′ ∈ [ M ] } denote our comparative data set. We consider a sequential setting, where at time t ∈ N , we serve the human a pair of models A t ∈A (which we pick), and in turn we observe the human’s response H t ∈ [0 , 1] . As an example, we might have that A t = (1 , 2) and H t = 1 , 3 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference Table 1. Statistics of human preference datasets, including Anthropic HH ( Bai et al. , 2022 ), OpenAssistant Conversations ( Köpf et al. , 20 23 ), and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llam a 2 ’s t o ke n i z e r. “ Co n v ” = Co n ve rsa t i o n . “ La n g ” = Langua ge. Dataset # Convs # Models # Users # Langs Avg. # Turns Avg. # Tokens Avg",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". # Tokens per Sample per Prompt per Response Anthropic HH 338,704 - 143 1 2.3 18.9 78.9 OpenAssistant 66,497 - 13,500 35 - 36.9 214.2 Chatbot Arena (20240121) 243,329 50 90,051 149 1.3 94.9 269.0 0.00 0.68 0.69 0.75 0.71 0.76 0.77 0.75 0.76 0.79 0.86 0.90 0.32 0.00 0.50 0.59 0.61 0.59 0.59 0.59 0.62 0.72 0.70 0.80 0.31 0.50 0.00 0.54 0.56 0.50 0.57 0.52 0.69 0.73 0.60 0.87 0.25 0.41 0.46 0.00 0.54 0.48 0.51 0.56 0.58 0.53 0.73 0.84 0.29 0.39 0.44 0.46 0.00 0.42 0.54 0.55 0.58 0.63 0.67 0.76 0.24 0.41 0.50 0.52 0.58 0.00 0.49 0.55 0.58 0.61 0.64 0.73 0.23 0.41 0.43 0.49 0.46 0.51 0.00 0.56 0.58 0.63 0.72 0.71 0.25 0.41 0.48 0.44 0.45 0.45 0.44 0.00 0.54 0.68 0.65 0.62 0.24 0.38 0.31 0.42 0.42 0.42 0.42 0.46 0.00 0.61 0.58 0.61 0.21 0.28 0.27 0.47 0.37 0.39 0.37 0.32 0.39 0.00 0.53 0.57 0.14 0.30 0.40 0.27 0.33 0.36 0.28 0.35 0.42 0.47 0.00 0.52 0.10 0.20 0.13 0.16 0.24 0.27 0.29 0.38 0.39 0.43 0.48 0.00 gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1 llama-2-70b-chat llama-2-13b-chat llama-2-7b-chat mistral-7b-instruct mistral-7b-instruct llama-2-7b-chat llama-2-13b-chat llama-2-70b-chat claude-instant-1 gpt-3.5-turbo-0613 claude-2.1 gemini-pro-dev-api mixtral-8x7b-instruct-v0.1 mistral-medium gpt-4-0613 gpt-4-turbo 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Model B Model A 0 2564 1189 1192 858 3053 1991 270 141 157 106 144 2564 0 566 263 756 2227 1025 355 414 409 264 197 1189 566 0 775 371 382 773 103 45 51 40 52 1192 263 775 0 71 744 869 136 862 66 45 61 858 756 371 71 0 74 564 53 31 30 30 37 3053 2227 382 744 74 0 351 650 113 117 75 114 1991 1025 773 869 564 351 0 842 572 388 283 155 270 355 103 136 53 650 842 0 459 241 202 101 141 414 45 862 31 113 572 459 0 383 134 369 157 409 51 66 30 117 388 241 383 0 251 621 106 264 40 45 30 75 283 202 134 251 0 521 144 197 52 61 37 114 155 101 369 621 521 0 gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": "75 283 202 134 251 0 521 144 197 52 61 37 114 155 101 369 621 521 0 gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1 llama-2-70b-chat llama-2-13b-chat llama-2-7b-chat mistral-7b-instruct mistral-7b-instruct llama-2-7b-chat llama-2-13b-chat llama-2-70b-chat claude-instant-1 gpt-3.5-turbo-0613 claude-2.1 gemini-pro-dev-api mixtral-8x7b-instruct-v0.1 mistral-medium gpt-4-0613 gpt-4-turbo 0 500 1000 1500 2000 2500 3000 Model B Model A Figure 2",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena. indicating that the human prefers model 2 over model 1. In the ensuing text, we will primarily focus on the binary case— where H t ∈{ 0 , 1 } —but our approach will generalize to any form of feedback, including the possibility of allowing the human to express different degrees of preference or to say the models are tied. One critical goal is to estimate the win matrix : θ ∗ ( a ) = E [ H t | A t = a ] , for all a ∈A ; see the left panel of Figure 2 for an illustration of the (empirical) win matrix. In the binary case, the a entry in the win matrix corresponds to the probability the human prefers model a 2 to a 1 when shown the pair a . Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5 . Formally, consider a score s ( P ) ∈ R M , where P is a joint distribution over A and H (by default, we will target a unform distribution over A ). Each model has a true score s ( P ) m , and better models will have higher scores. In partiular, we have the rank of model m : rank( P ) m = 1 + X m ′ ∈ [ M ] 1 { s ( P ) m ′ > s ( P ) m } . (1) The best model has rank 1 . If there is another model tied for best, they will both get assigned rank 1 . Picking a score. A standard score function in this setting is the vector of Bradley-Terry (BT) coefficients ( Bradley & Terry , 1952 ). In the Bradley-Terry model, H t ∈{ 0 , 1 } , and the probability model m beats model m ′ is modeled via a logistic relationship: P ( H t = 1) = 1 1 + e ξ m ′ − ξ m , (2) where ξ is an M -length vector of so-called BT coefficients. Without loss of generality, we take ξ 1 = 0 (since the model is invariant to addition in ξ ). Our goal is to estimate the poulation Bradley-Terry coefficients, i.e., those that minimize the binary cross-entropy: s ( P ) = argmin ξ E ( A,H ) ∼ P l H, 1 1 + e ξ A 2 − ξ A 1 , (3) where l is the binary cross-entropy loss, l ( h, p ) = − ( h log( p ) + (1 − h ) log(1 − p ))",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Although the BT model technically assumes a parametric form for the model win rates, the seminal results of Huber et al. ( 1967 ); White ( 1982 ) show that maximum likelihood estimators are still asymptotically normal even when these assumptions do not hold, so long as the so-called “sandwich” covariance matrix is used; see Section 5 for details, and see Appendix B for a nonparametric extension of the BradleTerry model. Finally, we remark that previous evolutions of 4 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference our online interface have reported different ranking scores, such as the Elo score ( Elo , 1967 ) instead of the BT coeffcients. We made this change because the BT coefficients are better for the purpose of statistical estimation. 5. Efficient Approximate Ranking In Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation procdures. Win matrix estimation. Estimation of the win mtrix is relatively straightforward. Define X t ( a ) = 1 P t ( a ) H t 1 { A t = a } , where P t ( a ) is the probability of sapling pair a at time t , and X t as the according vector. Then the estimator is ˆ θ T = 1 T T X t =1 X t . (4) Note that E [ X t ( a )] = θ ∗ ( a ) for all t , and thus ˆ θ T is an unbiased estimator of θ ∗ . We will furthermore estimate the covariance matrix as b Σ T = 1 T T X t =1 ( X t − ˆ θ T )( X t − ˆ θ T ) ⊤ . (5) Under the appropriate regularity conditions, we have that √ T b Σ − 1 / 2 ( ˆ θ − θ ∗ ) →N (0 , I d ) , (6) and we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see Durrett ( 2019 ), Theorem 8.2.8, where condition (ii) is trivially satisfied so long as P t ( a ) > ε > 0 , and condition (i) is implied by the almost-sure convergence of P t ( a ) to a limiting distribution P ( a ) . Estimating the BT scores",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Estimating the BT scores. To estimate the BT coefficients, mirroring ( 3 ) , we perform (reweighted) maximum likelhood estimation on our data points: s ( ˆ P ) = argmin ξ T X t =1 1 P ( A t ) l H t , 1 1 + e ξ At, 2 − ξ At, 1 , (7) where A t ∼ P . We perform the inverse weighting by P ( A t ) because this allows us to target a score with a uniform distribution over A . To compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap ( DiCiccio & Efron , 1996 ), and (2) the “sandwich” robust standard errors outlined in Huber et al. ( 1967 ) (see also Freedman ( 2006 ) for an outline of the necessary technical assumptions). Ultmately, based on the results of a simulation study described in Appendix A , we choose to deploy the sandwich intervals due to their smaller size in large samples. Approximate rankings. Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an M -dimensional confidence set C satisfying P ( s ( P ) ∈C ) ≥ 1 − α, (8) we extract an approximate ranking R m = 1 + P m ′ ∈ [ M ] 1 { inf C m ′ > sup C m } . The uniform validity of C directly implies that P ( ∃ m : R m > rank( P ) m ) ≤ α — i.e., with high probability, no model’s performance is uderstated. A guarantee on the other side—that no model’s performance is overstated—is possible by interchanging the inf and sup . To get the uniform confidence set, we construct the chi-squared interval implied by the central limit therem using the sandwich estimate of the variance. In other words, we construct the interval { ξ : T ˆ V − 1 / 2 (ˆ ξ − ξ ) ≤ χ 2 1 − α,M − 1 , where ˆ ξ is our MLE of the BT coefficients and ˆ V ξ is the sandwich variance of the logistic regression. Active sampling rule. Our sampling rule was to choose the model pair a ∈A proportionally to the reduction in confidence interval size by sampling that pair: P t ( a ) ∝ s ˆ Σ t,a,a |{ t : A t = a }| − s ˆ Σ t,a,a |{ t : A t = a }| + 1 . (9) 5.1",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". (9) 5.1. Detecting Anomalous Users On a different note, we take a first step towards identifing anomalous IP addresses in our dataset. In a dataset of U unique IPs, we let IP = { 1 , , U } be the set of all IP addresses. Consider a “test” user, outside this database, who gives ratings H ′ 1 , , H ′ n when presented actions A ′ 1 , , A ′ n . The idea of our procedure is to copare the distribution of ratings for the new user to the hitorical distribution of ratings for a given action. We let H a = { H t : A t = a } and every time a user submits a vote, we calculate the following number: p i = 1 |H A ′ i | + 1    1 + X h ∈H A ′ i 1 { h ≥ H ′ i }    . (10) Under the null hypothesis that H A ′ i is exchangeable with H ′ i , p i is a valid p-value (see Appendix C for a proof). Futhermore, the dependence of these p-values asymptotically is negligible. With this p-value in hand, we can test against this null hypothesis sequentially by using Fisher’s combination test ( Fisher , 1928 ) along with a variant of the Bonferroni correction. In particular, for each user, after their j th vote, we compute M j = − 2 j P i =1 log( p i ) . At 5 randomly chsen values of j between 1 and 100, we identify a user as 5 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference anomalous if M j ≥ χ 2 2 j, 1 − α/ 5 . (The times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value.) Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table 5 . 6. Data Analysis To examine whether Arena’s crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts. 6.1. Topic Modeling on User Prompts To study the prompt diversity, we build a topic modeling pipeline with BERTopic 3 ( Grootendorst , 2022 )",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". 6.1. Topic Modeling on User Prompts To study the prompt diversity, we build a topic modeling pipeline with BERTopic 3 ( Grootendorst , 2022 ). We start with transforming user prompts into representation vectors using OpenAI’s text embedding model (text-embedding-3- small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approimation and Projection) ( McInnes et al. , 2020 ) to reduce the embedding dimension from 1,536 to 5. We then use the hierarchical density-based clustering algorithm, HDSCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization. The pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure 3 . We observe that the largest cluster only accounts for 1% of the entire set and the rest quickly drop to <0.5%, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure 11 and 12 in Appendix. 6.2. Can Arena Prompts Distinguish Models? Next, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs’ fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demostrate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4. To control variables, we factor out user votes and consider LLM-as-judge ( Zheng et al",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". To control variables, we factor out user votes and consider LLM-as-judge ( Zheng et al. , 2023b ) to evaluate model r3 https://github.com/MaartenGr/BERTopic Medical Queries and Information ( 0.4% ) Role-Playing Games ( 0.4% ) Movie Reviews and Discussions ( 0.5% ) SQL Database Table Queries ( 0.5% ) Web Development Essentials ( 0.7% ) Animal Behavior and Pet Care Queries ( 0.4% ) Cooking and Recipes ( 0.7% ) Email and Letter Writing Assistance ( 0.8% ) Operations & Fleet Management ( 0.8% ) Sports and Athletics Queries ( 0.8% ) Advanced Mathematical Concepts ( 0.4% ) Philosophical Texts & Concepts ( 0.4% ) AI Impact and Applications ( 0.9% ) Original Joke Requests ( 0.5% ) Poetry Writing & Styles ( 0.8% ) Word Play and Phonetics ( 1.0% ) 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Similarity Figure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster’s centroid embeddings, hence diagonals are always one. Table 2. GPT-4-0613’s win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison. Topic Cluster Win-rate Size Python Game Programming Challenge 96.7% 0.2% C/C++ Process Multi-Threading 86.7% 0.3% SQL Query Database Assistance 73.3% 0.2% Poetry Writing Prompts 66.7% 1.1% Python Coding Basics 65.0% 0.2% Linguistic Analysis & Wordplay 58.3% 0.7% Travel Itinerary Planning 58.3% 0.4% Movie Recommendations & Ratings 53.3% 0.2% sponse. Results are shown in Table 2 , where we see GPT-4 has significantly higher win-rate (up to 97%) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below 60%. We show examples in Appendix D.1 . This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models. Building Challenging Benchmark. To further demonstrate the prompt quality, we show it is possible to construct a chalenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. ( 2023a ) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation procdures can be found in the Appendix D.2 and Appendix D.3 , respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4 , we compare Arena bench against a widely used LLM benchmark, MBench ( Zheng et al. , 2023b ). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models. 6.3. Validating Vote Quality To assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2- 13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We 6 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference 0 2 4 6 8 Llama-2-7B-Chat Llama-2-70B-Chat Vicuna-33b-v1.3 OpenChat-3.5 Starling-LM-7B-alpha Mixtral-8x7B-Instruct Claude-2.1 GPT-3.5-Turbo-0613 GPT-4-0613 GPT-4-0314 GPT-4-Turbo Arena Bench MT Bench Score Model Figure 4. Model’s performance between Arena Bench and MBench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge. then asked experts 4 to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model’s answer with eternal resources like search engine. Manually labeling each data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4- judge are presented in Table 3",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4- judge are presented in Table 3 . The corresponsing win-rate are shown in Table 4 . To summarize, we observe high agreement rates (72% to 83%) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels (79.4% and 89.8%). As for the 10%-20% disagreement between experts, it is mostly due to some user prompts don’t have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in Appendix D.4 . The gap between crowvs-expert agreement rate and expert-vs-expert agreement rate (5%-10%) is mostly attributed to crowd user making mistakes or overlooking factual errors in model’s response. Overall, the agreement rates presented in Table 3 validate the decent quality of crowd-sourced votes in Chatbot Arena. 7. Experiments 7.1. Ranking system Computing the rank on real data. In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of T = 213 , 576 hitorical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm 4 The laborers are graduate students at UC Berkeley. Table 3. Pairwise agreement rate between crowd-user, gpt-4 judge, and experts on pairwise battles. The top part of the table is between GPT-4-Turbo and Llama-2-13b-chat. The bottom is between GP4-Tur bo and GPT-3.5-Turbo-0613. Llama-2-13b Expert 1 Expert 2 GPT-4 Crowd 72.8% 77.8% 75.6% Expert 1 - 89.8% 81.0% Expert 2 - - 78.5% GPT-3.5-Turbo Expert 1 Expert 2 GPT-4 Crowd 73.8% 83.1% 75.6% Expert 1 - 79.4% 76.3% Expert 2 - - 79.3% Table 4. GPT-4-Turbo’s win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5- Turbo-0613",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". GPT-4-Turbo’s win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5- Turbo-0613. Baseline Arena User Expert 1 Expert 2 GPT-4 Llama-2-13b 81.2% 89.4% 86.9% 78.8% GPT-3.5-Turbo 76.3% 82.5% 89.4% 79.4% with confidence intervals; see Figure 5 for these intervals (with and without multiplicity correction; the formal notion of approximate ranking technically requires multiplicity correction, but it makes the intervals looser). Evaluating the coverage of the intervals. A natural folloup question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) 1 − α . Of course, this cannot be evaluated on real data, so we run a simulation. A vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution beta (1 /γ, 1 /γ ) ; we take γ = 2 in Figure 6 (and we vary γ in Appendix A ). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure 6 for the uncorrected intervals The coverage of the intervals behaves as expected, centering around 1 − α , regardless of the number of models. Meawhile, the more models are included, the larger the intervals become. Evaluating the active sampling rule. Next, we discuss the evaluation of our active sampling rule as Equation ( 9 ) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure 7 . It is hard to tell by looking at plots, but the improvement is substantial: To estimate θ ∗ to a precision of 0.2, random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3, random needs 17,200 samples and adaptive needs 16,400 samples",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Thus, the random baseline requires 54% and 5% more data to achieve the same level of precision, 7 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference 0.0 0.5 1.0 1.5 2.0 2.5 zephyr-7b-beta (#19-37) wizardlm-13b (#19-37) llama2-70b-steerlm-chat (#17-37) solar-10.7b-instruct-v1.0 (#18-36) dolphin-2.2.1-mistral-7b (#17-37) pplx-70b-online (#17-31) gpt-3.5-turbo-1106 (#15-29) openchat-3.5 (#14-29) llama-2-70b-chat (#14-28) openhermes-2.5-mistral-7b (#12-28) vicuna-33b (#7-25) starling-lm-7b-alpha (#7-26) gpt-3.5-turbo-0314 (#7-22) wizardlm-70b (#7-22) tulu-2-dpo-70b (#6-21) yi-34b-chat (#6-19) claude-instant-1 (#6-19) gemini-pro (#6-18) gpt-3.5-turbo-0613 (#6-18) claude-2.1 (#6-18) mixtral-8x7b-instruct-v0.1 (#4-18) gemini-pro-dev-api (#3-18) claude-2.0 (#3-14) claude-1 (#3-8) mistral-medium (#3-8) gpt-4-0613 (#3-7) gpt-4-0314 (#2) gpt-4-turbo (#1) corrected uncorrected Figure 5. Intervals for the BT coefficients with and without mutiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures all scores are simultneously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals. 0 20000 40000 60000 80000 100000 n 0.8 0.9 Coverage Coverage 0 20000 40000 60000 80000 100000 n 0.0 0.5 1.0 1.5 Average Interval Width Average Interval Width M 4 7 10 15 20 Figure 6. Intervals for the BT coefficients as a function of the number of samples and the number of models M . 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 average width ( ) 0 10000 20000 30000 n random pairwise adaptive 0.25 0.30 0.35 0.40 0.45 0.50 average width ( s ) 0 10000 20000 30000 n Figure 7. Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling. Iprovements from adaptive sampling can be seen in both cases, although they are more subtle on the scale of the score. Table 5. Confusion matrix of different α . “Pred.” means predicted. P ositive means anomalous and negative means normal. α = 0 . 1 Pred. Positive Pred. Negative Actual Positive 13/14 12/36 Actual Negative 1/14 24/36 α = 0 . 3 Pred. Positive Pred. Negative Actual Positive 21/29 4/21 Actual Negative 8/29 17/21 respectively. One can see from the plots in Figure 7 that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis. 7.2. Anomalous Users Detection We evaluate the outlier detection method in Section 5.1 . We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking “hi” for 100 times or inputting garbled texts). We randomly sample 25 normal users with at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1 , per user we compute five M j and identify the user as anomalous if M j ≥ χ 2 2 j, 1 − α/ 5 . We present results of two different α (i.e., the significance leval) in Table 5 . We find the detetion method effective (e.g., reaching 90% true positive and 60-70% true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect. 8. Discussion Limitations. Although our user base is extensive, we aticipate that it will primarily consist of LLM hobbyists and 8 Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference researchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel mechanism to evaluate the safety of these models. Future Directions. In our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dnamic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and values ( Howard et al. , 2020 ; Waudby-Smith & Ramdas , 2020 ; Vovk & Wang , 2021 ; Ramdas et al. , 2023 ); this would deal with the dependence, but the variants we tried did not perform well in terms of power. 9. Conclusion In this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise hman preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100K pairwise preference votes will be released for future research. Acknowledgments This project is partly supported by sponsorship from Kaggle, MBZUAI, a16z, Together AI, Anyscale, and HuggingFace. This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "chunk_text": ". This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship. The authors would like to thank Siyuan Zhuang for insightful discussion and Tijana Zrni ́c for helpful feeback on the manuscript. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.",
    "chunk_id": "Natural_language_processing_chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "chunk_text": "353 GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding Alex Wang 1 , Amanpreet Singh 1 , Julian Michael 2 , Felix Hill 3 , Omer Levy 2 , and Samuel R. Bowman 1 1 New York University, New York, NY 2 Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA 3 DeepMind, London, UK { alexwang,amanpreet,bowman } @nyu.edu { julianjm,omerlevy } @cs.washington.edu felixhill@google.com Human ability to understand language is geeral , flexible , and robust . In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understaning beyond the detection of superficial correspodences between inputs and outputs, then it is criical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com ): a benchmark of nine diverse NLU tasks, an auxiiary dataset for probing models for understaning of specific linguistic phenomena, and an oline platform for evaluating and comparing moels. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo ( Peters et al. , 2018 ), a powerful transfer learning tecnique, as well as state-of-the-art sentence reprsentation models. The best models still achieve fairly low absolute scores. Analysis with our dianostic dataset yields similarly weak performance over all phenomena tested, with some exceptions",
    "chunk_id": "Natural_language_processing_glue_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The best models still achieve fairly low absolute scores. Analysis with our dianostic dataset yields similarly weak performance over all phenomena tested, with some exceptions. The GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and diffculties. As the goal of GLUE is to spur develoment of generalizable NLU systems, we design the benchmark such that good performance should rCorpus | Train | Task Domain Single-Sentence Tasks CoLA 8.5k acceptability misc. SST-2 67k sentiment movie reviews Similarity and Paraphrase Tasks MRPC 3.7k paraphrase news STS-B 7k textual sim. misc. QQP 364k paraphrase online QA Inference Tasks MNLI 393k NLI misc. QNLI 108k QA/NLI Wikipedia RTE 2.5k NLI misc. WNLI 634 coref./NLI fiction books Table 1 : Task descriptions and statistics. Bold dnotes tasks for which there is privately-held test data. All tasks are binary classification, except STS-B (regression) and MNLI (three classes). quire models to share substantial knowledge (e.g., trained parameters) across tasks, while maintaiing some task-specific components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we epect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlsentence inputs: Corpus of Linguistic Acceptabiity (CoLA; Warstadt et al. 2018 ) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013 ) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, ( Dolan and Brockett , 2005 )), Quora Question Pairs 1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017 ). The remaiing four tasks are formatted as natural language iference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al",
    "chunk_id": "Natural_language_processing_glue_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2017 ). The remaiing four tasks are formatted as natural language iference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018 ) and Reco1 data.quora.com/First-Quora-DataseRelease-Question-Pairs 354 Single Sentence Similarity and Paraphrase Natural Language Inference Model Avg CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI Single-task 64.8 35.0 90.2 68.8/80.2 86.5 / 66.1 55.5/52.5 76.9 / 76.7 61.1 50.4 65.1 Multi-task 69.0 18.9 91.6 77.3 / 83.5 85.3/63.3 72.8/71.1 75.6/75.9 81.7 61.2 65.1 CBoW 58.9 0.0 80.0 73.4/81.5 79.1/51.4 61.2/58.7 56.0/56.4 75.1 54.1 62.3 Skip-Thought 61.5 0.0 81.8 71.7/80.8 82.2/56.4 71.8/69.7 62.9/62.8 74.7 53.1 65.1 InferSent 64.7 4.5 85.1 74.1/81.2 81.7/59.1 75.9/75.3 66.1/65.7 79.8 58.0 65.1 DisSent 62.1 4.9 83.7 74.1/81.7 82.6/59.5 66.1/64.8 58.7/59.1 75.2 56.4 65.1 GenSen 66.6 7.7 83.1 76.6/83.0 82.9/59.8 79.3 / 79.2 71.4/71.3 82.3 59.2 65.1 Table 2 : Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation ( Matthews , 1975 ). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006 , Bar Haim et al. 2006 , Giampicolo et al. 2007 , Bentivogli et al. 2009 ), as well as versions of SQuAD ( Rajpurkar et al. , 2016 ) and Winograd Schema Challenge ( Levesque et al. , 2011 ) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the bencmark is measured per task as well as in aggregate, averaging performance across tasks. Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also icludes a dataset of hand-crafted examples for probing trained models",
    "chunk_id": "Natural_language_processing_glue_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also icludes a dataset of hand-crafted examples for probing trained models. This dataset is designed to highlight common phenomena, such as the use of world knowledge, logical operators, and lexcal entailments, that models must grasp if they are to robustly solve the tasks. Each of the 550 eamples is an NLI sentence pair tagged with the phenomena demonstrated. We ensure that the data is reasonably diverse by producing examples for a wide variety of linguistic phenomena, and baing our examples on naturally-occurring sentences from several domains. We validate our data by uing the hypothesis-only baseline from Gururangan et al. ( 2018 ) and having six NLP researchers maually validate a random sample of the data. Baselines To demonstrate the benchmark in use, we apply multi-task learning on the training data of the GLUE tasks, via a model that shares a BiSTM between task-specific classifiers. We also train models that use the same architecture but are trained on a single benchmark task. Finally, we evaluate the following pretrained models: average bag-of-words using GloVe embeddings (CBoW), Skip-Thought ( Kiros et al. , 2015 ), InferSent ( Coneau et al. , 2017 ), DisSent ( Nie et al. , 2017 ), and GenSen ( Subramanian et al. , 2018 ). Tags Sentence Pair Quantifiers Double Negation I have never seen a hummingbird not flying. I have never seen a hummingbird. Active/Passive Cape sparrows eat seeds, along with soft plant parts and insects. Cape sparrows are eaten. Named Entities World Knowledge Musk decided to offer up his pesonal Tesla roadster. Musk decided to offer up his pesonal car. Table 3 : Diagnostic set examples. Systems must predict the relationship between the sentences, ether entailment , neutral , or contradiction when one sentence is the premise and the other is the hypothesis, and vice versa. Examples are tagged with the phenomena demonstrated. We group each phenomena into one of four broad categories",
    "chunk_id": "Natural_language_processing_glue_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Examples are tagged with the phenomena demonstrated. We group each phenomena into one of four broad categories. We find that our models trained directly on the GLUE tasks generally outperform those that do not, though all models obtain fairy low absolute scores. Probing the baselines with the diagnotic data, we find that performance on the bencmark correlates with performance on the dianostic data, and that the best baselines similarly achieve low absolute performance on the linguitic phenomena included in the diagnostic data. Conclusion We present the GLUE benchmark, consisting of: (i) a suite of nine NLU tasks, built on established annotated datasets and covering a diverse range of text genres, dataset sizes, and difficulties; (ii) an online evaluation platform and leaderboard, based primarily on private test data; (iii) an expert-constructed analysis dataset. Expeiments indicate that solving GLUE is beyond the capability of current transfer learning methods. 355",
    "chunk_id": "Natural_language_processing_glue_a_multi-task_benchmark_and_analysis_platform_for_natural_language_understanding.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Attention Is All You Need Ashish Vaswani ∗ Google Brain avaswani@google.com Noam Shazeer ∗ Google Brain noam@google.com Niki Parmar ∗ Google Research nikip@google.com Jakob Uszkoreit ∗ Google Research usz@google.com Llion Jones ∗ Google Research llion@google.com Aidan N. Gomez ∗† University of Toronto aidan@cs.toronto.edu Łukasz Kaiser ∗ Google Brain lukaszkaiser@google.com Illia Polosukhin ∗ illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englisto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 Introduction Recurrent neural networks, long short-term memory [ 12 ] and gated recurrent [ 7 ] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29 , 2 , 5 ]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. ∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". ∗ Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † Work performed while at Google Brain. Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states h t , as a function of the previous hidden state h t − 1 and the input for position t . This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [ 18 ] and conditional computation [ 26 ], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transdution models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2 , 16 ]. In all but a few cases [ 22 ], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [ 20 ], ByteNet [ 15 ] and ConvS2S [ 8 ], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [ 11 ]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19]",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". End-to-end memory networks are based on a recurrent attention mechanism instead of sequencaligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencaligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8]. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5 , 2 , 29 ]. Here, the encoder maps an input sequence of symbol representations ( x 1 , , x n ) to a sequence of continuous representations z = ( z 1 , , z n ) . Given z , the decoder then generates an output sequence ( y 1 , , y m ) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positio2 Figure 1: The Transformer - model architecture. wise fully connected feed-forward network. We employ a residual connection [ 10 ] around each of the two sub-layers, followed by layer normalization [ 1 ]. That is, the output of each sub-layer is LayerNorm( x + Sublayer( x )) , where Sublayer( x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d model = 512 . Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i . 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension d k , and values of dimension d v . We compute the dot products of the 3 Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. query with all keys, divide each by √ d k , and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q . The keys and values are also packed together into matrices K and V",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q . The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention( Q, K, V ) = softmax( QK T √ d k ) V (1) The two most commonly used attention functions are additive attention [ 2 ], and dot-product (multplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1 √ d k . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of d k the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of d k [ 3 ]. We suspect that for large values of d k , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4 . To counteract this effect, we scale the dot products by 1 √ d k . 3.2.2 Multi-Head Attention Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. 4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1 . Then their dot product, q · k = P d k i =1 q i k i , has mean 0 and variance d k . 4 MultiHead( Q, K, V ) = Concat(head 1 , , head h ) W O where head i = Attention( QW Q i , KW K i , V W V i ) Where the projections are parameter matrices W Q i ∈ R d model × d k , W K i ∈ R d model × d k , W V i ∈ R d model × d v and W O ∈ R hd v × d model . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: • In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8]. • The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. • Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞ ) all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN( x ) = max(0 , xW 1 + b 1 ) W 2 + b 2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is d model = 512 , and the inner-layer has dimensionality d ff = 2048 . 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension d model . We also use the usual learned linear transfomation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-s oftmax linear transformation, similar to [ 24 ]. In the embedding layers, we multiply those weights by √ d model . 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To this end, we add \"positional encodings\" to the input embeddings at the 5 Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O ( n 2 · d ) O (1) O (1) Recurrent O ( n · d 2 ) O ( n ) O ( n ) Convolutional O ( k · n · d 2 ) O (1) O ( log k ( n )) Self-Attention (restricted) O ( r · n · d ) O (1) O ( n/r ) bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d model as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [8]. In this work, we use sine and cosine functions of different frequencies: PE ( pos, 2 i ) = sin ( pos/ 10000 2 i/d model ) PE ( pos, 2 i +1) = cos ( pos/ 10000 2 i/d model ) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 π to 10000 · 2 π . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k , PE pos + k can be represented as a linear function of PE pos . We also experimented with using learned positional embeddings [ 8 ] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convoltional layers commonly used for mapping one variable-length sequence of symbol representations ( x 1 , , x n ) to another sequence of equal length ( z 1 , , z n ) , with x i , z i ∈ R d , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11 ]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O ( n ) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [ 31 ] and byte-pair [ 25 ] representations",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position. This would increase the maximum path length to O ( n/r ) . We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O ( n/k ) convolutional layers in the case of contiguous kernels, or O ( log k ( n )) in the case of dilated convolutions [ 15 ], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k . Separable convolutions [ 6 ], however, decrease the complexity considerably, to O ( k · n · d + n · d 2 ) . Even with k = n , however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [ 3 ], which has a shared sourctarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31 ]. Sentence pairs were batched together by approximate sequence length",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer We used the Adam optimizer [ 17 ] with β 1 = 0 . 9 , β 2 = 0 . 98 and ε = 10 − 9 . We varied the learning rate over the course of training, according to the formula: lrate = d − 0 . 5 model · min( step _ num − 0 . 5 , step _ num · warmup _ steps − 1 . 5 ) (3) This corresponds to increasing the learning rate linearly for the first warmup _ steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _ steps = 4000 . 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout [ 27 ] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of P drop = 0 . 1 . 7 Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Model BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1 . 0 · 10 20 GNMT + RL [31] 24.6 39.92 2 . 3 · 10 19 1 . 4 · 10 20 ConvS2S [8] 25.16 40.46 9 . 6 · 10 18 1 . 5 · 10 20 MoE [26] 26.03 40.56 2 . 0 · 10 19 1 . 2 · 10 20 Deep-Att + PosUnk Ensemble [32] 40.4 8",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3 · 10 19 1 . 4 · 10 20 ConvS2S [8] 25.16 40.46 9 . 6 · 10 18 1 . 5 · 10 20 MoE [26] 26.03 40.56 2 . 0 · 10 19 1 . 2 · 10 20 Deep-Att + PosUnk Ensemble [32] 40.4 8 . 0 · 10 20 GNMT + RL Ensemble [31] 26.30 41.16 1 . 8 · 10 20 1 . 1 · 10 21 ConvS2S Ensemble [8] 26.36 41.29 7 . 7 · 10 19 1 . 2 · 10 21 Transformer (base model) 27.3 38.1 3 . 3 · 10 18 Transformer (big) 28.4 41.0 2 . 3 · 10 19 Label Smoothing During training, we employed label smoothing of value ε ls = 0 . 1 [ 30 ]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art BLEU score of 28 . 4 . The configuration of this model is listed in the bottom line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41 . 0 , outperforming all of the previously published single models, at less than 1 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate P drop = 0 . 1 , instead of 0 . 3 . For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0 . 6 [ 31 ]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50 , but terminate early when possible [31]",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50 , but terminate early when possible [31]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 . 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. 5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. N d model d ff h d k d v P drop ε ls train PPL BLEU params steps (dev) (dev) × 10 6 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 (A) 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 (C) 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 (D) 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows (B), we observe that reducing the attention key size d k hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8 ], and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Attention is All you Need",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "date_published": "Unknown",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor . Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. 9",
    "chunk_id": "Natural_language_processing_attention_is_all_you_need.json_chunk_16"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": "Human languages are wild, delightfully tricky phenomena, exhibiting structure and variation, suggesting general rules and then breaking them. In some part, it’s for this reason that the best machine learning methods for modeling natural language are extremely flexible, left to their own devices to build internal representations of sentences. However, this flexibility comes at a cost. The most popular models in natural language processing research at this time are consideredblack boxes, offering few explicit cues to what they learn about how language works. Earlier machine learning methods for NLP learned combinations of linguistically motivated features—word classes likenounandverb, syntax trees for understanding how phrases combine, semantic labels for understanding the roles of entities—to implement applications involving understanding some aspects of natural language. Though it was difficult at times to understand exactly how these combinations of features led to the decisions made by the models, practitioners at least understood the features themselves. Recently, large-scale representation learning models like word2vec, GLoVe, ELMo, BERT, GPT2, XLNET, XLM, and others have replaced feature-based foundations to natural language processing systems. The representations of these models—vectors representing each word in each sentence—are used as features instead of linguistically motivated properties. Despite the unsupervised nature of representation learning models in NLP, some researchers intuit that the representations' properties may parallel linguistic formalisms. Gaining insights into the natures of NLP’s unsupervised representations may help us to understand why our models succeed and fail, what they’ve learned, and what we yet need to teach them. An emerging body of research has attempted to do just this – help us understand or interpret the properties of the internal representations of models like ELMo and BERT",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". An emerging body of research has attempted to do just this – help us understand or interpret the properties of the internal representations of models like ELMo and BERT. To test intuitions about whether properties of representations line up with linguistically specified properties (like parts-of-speech,)probingmethods train supervised models to predict linguistic properties from representations of language. Aprobeis trained to predict properties we care about from representations of a model whose nature we'd like to know more about. The claim is that achieving high evaluation accuracy (relative to a baseline) in predicting the property—like part-of-speech—from the representation—like ELMo—implies the property was encoded in the representation, and the probe found it. Though the method seems simple, it has hidden complexity. For example, if you want to find property Y, how hard do you try? In other words, how complex a probe do you train in order to predict Y, like a linear model or a multi-layer perceptron? How much training data do you use? If the representation losslessly encodes the sentence, then with a complex enough probe, you can find any Y that you could find on the original sentence! For example, we don’t say our camera “learned” image classification just because we can train a CNN to predict labels from the camera’s images. As we’ll show, we should be careful aboutfalse positives, saying our network learned Y when it really hasn’t. In a graded notion,we don’t want to overestimate the extent to which high probe accuracy on linguistic tasks reflects properties of the representation",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". In a graded notion,we don’t want to overestimate the extent to which high probe accuracy on linguistic tasks reflects properties of the representation. How do we design probes whose accuracies faithfully reflect (unknown) properties of representations, and how do we interpret the accuracies returned by probes when making statements about the properties of representations? In this blog post, we’ll describecontrol tasks, which put into action the intuition that the more a probe is able to make memorized output decisions independently of the linguistic properties of a representation, the less its performance on a task necessarily reflects properties of the representation. Through control tasks we defineselectivity, which puts probes’ linguistic task accuracies in context of its ability to do this. We find that probes, especially complex neural network probes, are able to memorize a large number of labeling decision independently of the linguistic properties of the representations. We then show that throughselectivity, we can gain intuition for the expressivity of probes, and reopen questions about which representations better represent linguistic properties like parts-of-speech. Our method is described in the forthcoming EMNLP 2019 publication,Designing and Interpreting Probes with Control Tasks; this post draws from the paper, which is joint work with Percy Liang. Every result in the paper, as well as its code and data provenance, can be found on ourworksheet at Codalab, a platform for reproducible science. The repository for the code used in the paper, (which you can also use yourself!) is available atGitHub. Linguistic probing methods start with a broad hypothesis, like: “I think my representation learner unsupervisedly developed a notion of linguistic property Y, and encodes this notion in its intermediate representations in order to better perform the task it was trained on (like language modeling.)” It is difficult to test this hypothesis directly",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". Probing methods substitute the following hypothesis, which has the nice property of being testable: “I think my representation has features that are: Some notation will help us be precise when talking about probing. As nicely described in Noah Smith’sintroduction to contextual representations, we’ll use the termword tokento denote an instance of a word in a corpus, andword typeto denote the word in abstract. Denote as \\(1:T\\) the sequence of integers \\(\\{1,,T\\}\\). Let \\(V\\) be the vocabulary containing all word types in a corpus. A sentence of length \\(T\\) is \\(x_{1:T}\\) , where each \\(x_i \\in V\\) , and the word representations of the model being probed are \\(h_{1:T}\\) , where \\(h_i \\in R^d\\). A task is a function that maps a sentence to a single output per word, \\(f (x_{1:T} ) = y_{1:T}\\) , where each output is from a finite set of outputs: \\(y_i \\in \\mathcal{Y}\\). A probe parametrized by \\(\\theta\\) is a function \\(f_\\theta(h_{1:T}) = \\hat{y}_{1:T}\\). The accuracy achieved by this probe is the percent of outputs \\(i\\) such that \\(\\hat{y_i} = y_i\\) on inputs \\(h_{1:T}\\) that were not used to train the probe. Probing papers tend to acknowledge the uncertainty that task accuracies derived from probing reflect not just the representations \\(h\\), but also the function \\(f_\\theta\\) that is learned through supervision. Part 5 in the probing hypothesis above reflects this: baseline representations, for example resulting from random contextualization, \\(h^\\text{random}\\), are used as input to the same probing methods as the representations we care about. The claim is, if training a probe on \\(h\\) leads to higher accuracy than training a probe on \\(h^\\text{random}\\), then \\(h\\) encodes the linguistic task to some extent. This control is useful, as it puts accuracies on \\(h\\) in context of accuracies you could have achieved before you even trained the contextualization part of the unsupervised model",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_4"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". This control is useful, as it puts accuracies on \\(h\\) in context of accuracies you could have achieved before you even trained the contextualization part of the unsupervised model. However, it does not directly provide information about the expressivity of the function family to make decisions on its own, independently of interesting properties of \\(h\\). What might it look like for a probe to achieve high accuracy without faithfully reflecting the properties of a representation? In general, this is the case if the probe is able to detect and combine disparate signals, some of which unrelated to the property we care about, and memorize arbitrary output distinctions based on those signals. The probe confounder problem occurs when the probe is able to detect and combine disparate signals, some of which unrelated to the property we care about, and use supervision to memorize arbitrary output distinctions based on those signals. Let’s consider a toy example, in which our language has the vocabulary \\(V=\\{a,b,\\}\\). Linguists have decided upon the function \\(f_\\text{part-of-speech}(x_{1:T}) = y_{1:T}\\) as the parts-of-speech for the language, where \\(y_i \\in \\{Q, R, S\\}\\). We have a representation learner \\(\\phi\\) which emits a sequence ofdiscretesymbol representations of a sentence, \\(\\phi(x_{1:T}) = h_{1:T}\\), where \\(h_i \\in \\{M, N\\}\\). We’re wondering if the representation learner has, without supervision, learned the part-of-speech task, \\(f_\\text{part-of-speech}\\). To test this, we train a probe \\(f_\\theta(h_{1:T}) = \\hat{y}_{1:T}\\). Here’s an example of how this turns out for our particular toy language:The representations of \\(\\phi\\) overlap with the parts-of-speech of the language – cool! The supervision of the probe let us learn the rough correspondence between the linguistic labels and the \\(\\phi\\) representations, and the overlap seems interesting but isn’t perfect",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_5"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". Unfortunately, representations \\(h\\) of actual models aren’t ever that simple; they’re real-valued vectors instead of symbols, and encode all kinds of information. In particular, we might expect that theword typeof each word token ends up in the representations \\(h\\). If this is so in our toy example, our probe might learn something like the following: In this case, the probe is able tomemorizethat word identities help disambiguate which part-of-speech should a word token has, even when the learned representation doesn’t actually make a distinction. Whereas before it was clear that the distinction between the part-of-speech \\(R\\) and the part-of-speech \\(S\\) isn’t made by \\(\\phi\\), now our probe doesn’t reveal that distinction. Our goal is to put linguistic task accuracy in context of the probe’s ability to make output decisions that don’t reflect linguistic properties of the representation. We go about this by defining tasks thatcan’thave been learned a priori by a representation, but can be learned by the probe through memorization. We call these taskscontrol tasks. At a high level, control tasks have: structure: The output for a word token is a deterministic function of the word type. randomness: The output for each word type is sampled independently at random. Because of randomness, no representation can have learned the task a priori. But because token outputs are deterministic functions of the word type, the probe itself can learn the task. Control tasks associate word types with random labels; by construction, they can only be learned by the probe itself. Let’s go through the construction of control tasks. Recall that a linguistic task is a function \\(f(x_{1:T}) = y_{1:T}\\), where \\(y_i \\in \\mathcal{Y}\\). Each control task is defined in reference to a single linguistic task, and the two share \\(\\mathcal{Y}\\). In part-of-speech tagging, the set \\(\\mathcal{Y}\\) is the tagset, \\(1:45\\) (corresponding to NN, NNS, VB,)",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_6"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". In part-of-speech tagging, the set \\(\\mathcal{Y}\\) is the tagset, \\(1:45\\) (corresponding to NN, NNS, VB,). To construct a control task, we independently sample acontrol behavior\\(C(v)\\) for each \\(v \\in V\\). The control behavior specifies how to define \\(y_i \\in \\mathcal{Y}\\) for a word token \\(x_i\\) with word type \\(v\\). The part-of-speech control task is the function that maps each token \\(x_i\\) to the label specified by the behavior \\(C(x_i)\\): This task is visualized below: In our paper, we also construct a control task for a task derived from dependency parsing, but we’ll omit it from this blog post. To summarize, a control task is defined for a single linguistic task, and shares the linguistic task’s output space \\(\\mathcal{Y}\\). To construct a control task, a control behavior is sampled independently at random for each word type \\(v\\in V\\). The control task is a function mapping \\(x_{1:T}\\) to a sequence of outptus \\(y_{1:T}\\) which is fully specified by the sequence of behaviors, \\([C(x_1), , C(x_{T})]\\). From this construction, we note that the ceiling on performance is the fraction of tokens in the evaluation set whose types occur in the training set (plus chance accuracy on all other tokens.) Further, \\(C(v)\\) must be memorized independently for each word type. With a control task defined for each linguistic task, we defineselectivityto be the difference between control task accuracy and linguistic task accuracy achievable by a probe family for a given representation. Selectivity puts linguistic task accuracy in context of the probe's ability to memorize arbitrary outputs for word types. We propose selectivity as a tool for desinging probes to reflect properties of a representation, and for interpreting probing accuracies achieved by different probes or on different representations. In our paper, we conduct a broad study of probe design decisions and hyperparameter choices and see how they affect linguistic task accuracy and selectivity. In this post, we’ll just go over some headline results",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_7"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". In this post, we’ll just go over some headline results. The two most popular designs for probes are linear models or multi-layer perceptrons (MLPs.) We train probes from function families on both part-of-speech tagging and its control task to analyze the expressivity of the probe families. We also train bilinear and multi-layer perceptron probes on dependency edge prediction, a task derived from dependency parsing described in our paper. The representations \\(h\\) used are from the first layer of ELMo, and the data comes from the Penn Treebank. We find that linear and bilinear probes are considerably more selective than multi-layer perceptron probes. For part-of-speech tagging, moving from linear to MLP probes leads to a slight incraese in part-of-speech tagging accuracy but a significant loss of selectivity, suggesting that the slight gain in part-of-speech accuracy may not faithfully reflect properties of the representation. Popular probe design decisions lead to high control-task accuracy, low-selectivity probes, indicating that they're able to memorize a large number of decisions unmotivated by the representation. We further find that we can control selectivity through careful complexity control, but that common use of regularization – to reduce generalization gap – does not encourage selectivity. In particular, we find that instead of the commonly used hidden state dimensionalities of a few hundred or 1000 for multi-layer perceptron probes, 10-dimensional MLPs achieve high part-of-speech tagging accuracy while being much more selective. This and a few other complexity control methods are visualized in the figure below. The last result we’ll discuss provides an example of how the probe confounder problem can muddy comparisons of linguistic capabilities of different representations. Multiple studies have found that probes on the first BiLSTM output of ELMo (ELMo1) achieve higher accuracies than probes on the output of the second BiLSTM, ELMo2",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_8"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". Multiple studies have found that probes on the first BiLSTM output of ELMo (ELMo1) achieve higher accuracies than probes on the output of the second BiLSTM, ELMo2. One hypothesis to explain these results is that ELMo1 has higher-quality or more easily accesible part-of-speech representations than ELMo2. However, as we’ve seen, these results depend on the probe as well as the representation; given what we know about probes’ capacity for memorizing at the type-level, we explore an alternative hypothesis. In particular, word type is a very useful feature in part-of-speech tagging a word token when combined with other features. Since ELMo1 is closer to the word inputs than ELMo2, it may be easier to identify word types. The higher accuracy of probes on ELMo1 may thus be explained by the accessibility of the word type feature instead of differences in part-of-speech representation. We train probes on both ELMo1 and ELMo2, as well as random representation baseline Proj0, and report both the lingusitic task accuracy and control task accuracy on both models. The results for part-of-speech tagging are as follows: From these results, we see that while ELMo1 achieves \\(0.6\\) better part-of-speech tagging accuracy than ELMo2, it comes at a loss of \\(5.4\\) selectivity; hence, probes on ELMo2 are considerably less able to rely on word identities. As we suggested above, this is consistent with the hypothesis that probes on ELMo1 achieve higher part-of-speech tagging accuracy due to easier access to word identities. However, it does not confirm this hypothesis; it merely reopens the question. Relatedly, the small gain in part-of-speech tagging accuracy for ELmo2 over the random representation baseline Proj0 might at first suggest that ELMo2 encodes little about part-of-speech tags. In fact, a multi-layer perceptron achieves \\(97.1\\) part-of-speech tagging accuracy on Proj0, but only \\(97.0\\) accuracy on ELMo2",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_9"
  },
  {
    "document_type": "online_article",
    "title": "Designing and Interpreting Probes",
    "author": "John Hewitt",
    "source": "https://nlp.stanford.edu/~johnhew/interpreting-probes.html",
    "date_published": "Posted on 17 Aug 2019.",
    "flag": "",
    "chunk_text": ". In fact, a multi-layer perceptron achieves \\(97.1\\) part-of-speech tagging accuracy on Proj0, but only \\(97.0\\) accuracy on ELMo2. However, by examining selectivity, we see that probes on ELMo2 have considerably less access to word identities than those on Proj0 (a difference of \\(10.8\\) in selectivity) This indicates that probes on ELMo2 must rely on emergent properties of the representation to predict parts-of-speech, suggesting that ELMo2 does encode part-of-speech information. Probing methods have shown that a broad range of supervised tasks can be turned into tools for understanding the properties of contextual word representations. Alain and Bengio (2016)1suggested we may think of probes as “thermometers used to measure the temperature simultaneously at many different locations.” We instead emphasize the joint roles of representations and probes together in achieving high accuracy on a task; we suggest that probes be thought of ascraftspeople; their performance depends not only on the materials they’re given, but also on their expressivity. As probes are used increasingly to study representations, we hope that control tasks and selectivity, as diagnostic tools, can help us better interpret the results of these probes, ultimately leading us to better understand what is learned by these remarkably effective representations. Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes. ICLR. 2016.↩",
    "chunk_id": "Natural_language_processing_designing_and_interpreting_probes_·_john_hewitt.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All rights reserved. Draft of January 12, 2025. CHAPTER 6 Vector Semantics and Embeddings 荃 者 所 以 在 鱼 , 得 鱼 而 忘 荃 Nets are for fish; Once you get the fish, you can forget the net. 言 者 所 以 在 意 , 得 意 而 忘 言 Words are for meaning; Once you get the meaning, you can forget the words 庄 子 (Zhuangzi), Chapter 26 The asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleitocene Epoch. One of these fossils is the Smilodon , or saber-toothed tiger, instantly recognizable by its long canines. Five million years ago or so, a completely different saber-tooth tiger called Thylacosmilus lived in Argentina and other parts of South Ameica. Thylacosmilus was a marsupial whereas Smilodon was a placental mammal, but Thlacosmilus had the same long upper canines and, like Smilodon, had a protective bone flange on the lower jaw. The similarity of these two mammals is one of many examples of parallel or convergent evolution, in which particular contexts or environments lead to the evolution of very similar structures in different species ( Gould , 1980 ). The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings . This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis . The hypothesis was distributional hypothesis first formulated in the 1950s by linguists like Joos ( 1950 ), Harris ( 1954 ), and Firth ( 1957 ), who noticed that words which are synonyms (like oculist and eye-doctor ) tended to occur in the same environment (e.g., near words like eye or examined ) with the amount of meaning difference between two words “corresponding roughly to the amount of difference in their environments” ( Harris , 1954 , p. 157). In this chapter we introduce vector semantics , which instantiates this linguistic vector semantics hypothesis by learning representations of the meaning of words, called embeddings , embeddings directly from their distributions in texts. These representations are used in every naural language processing application that makes use of meaning, and the static ebeddings we introduce here underlie the more powerful dynamic or contextualized embeddings like BERT that we will see in Chapter 11. These word representations are also the first example in this book of reprsentation learning , automatically learning useful representations of the input text. representation learning Finding such self-supervised ways to learn representations of the input, instead of creating representations by hand via feature engineering , is an important focus of NLP research ( Bengio et al. , 2013 ). 2 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS 6.1 Lexical Semantics Let’s begin by introducing some basic principles of word meaning. How should we represent the meaning of a word? In the n-gram models of Chapter 3, and in classical NLP applications, our only representation of a word is as a string of letters, or an index in a vocabulary list",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This representation is not that different from a tradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which the meaning of words is represented by just spelling the word with small capital letters; representing the meaning of “dog” as DOG , and “cat” as CAT , or by using an apostrophe ( DOG ’). Representing the meaning of a word by capitalizing it is a pretty unsatisfactory model. You might have seen a version of a joke due originally to semanticist Barbara Partee ( Carlson , 1977 ): Q: What’s the meaning of life? A: LIFE ’ Surely we can do better than this! After all, we’ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meaings ( cat is similar to dog ), others are antonyms ( cold is the opposite of hot ), some have positive connotations ( happy ) while others have negative connotations ( sad ). It should represent the fact that the meanings of buy , sell , and pay offer differing pespectives on the same underlying purchasing event. (If I buy something from you, you’ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue. In this section we summarize some of these desiderata, drawing on results in the linguistic study of word meaning, which is called lexical semantics ; we’ll return to lexical semantics and expand on this list in Appendix G and Chapter 21. Lemmas and Senses Let’s start by looking at how one word (we’ll choose mouse ) might be defined in a dictionary (simplified from the online dictionary WordNet): mouse (N) 1. any of numerous small rodents2. a hand-operated device that controls a cursorHere the form mouse is the lemma , also called the citation form . The form lemma citation form mouse would also be the lemma for the word mice ; dictionaries don’t have separate definitions for inflected forms like mice . Similarly sing is the lemma for sing , sang , sung",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Similarly sing is the lemma for sing , sang , sung . In many languages the infinitive form is used as the lemma for the verb, so Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The specific forms sung or carpets or sing or duermes are called wordforms . wordform As the example above shows, each lemma can have multiple meanings; the lemma mouse can refer to the rodent or the cursor control device. We call each of these aspects of the meaning of mouse a word sense . The fact that lemmas can be polysemous (have multiple senses) can make interpretation difficult (is someone who types “mouse info” into a search engine looking for a pet or a tool?). Chater 11 and Appendix G will discuss the problem of polysemy, and introduce word sense disambiguation , the task of determining which sense of a word is being used in a particular context. Synonymy One important component of word meaning is the relationship btween word senses. For example when one word has a sense whose meaning is 6.1 • L EXICAL S EMANTICS 3 identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms . Synonyms include such pairs as synonym couch/sofa vomit/throw up filbert/hazelnut car/automobile A more formal definition of synonymy (between words rather than senses) is that two words are synonymous if they are substitutable for one another in any sentence without changing the truth conditions of the sentence, the situations in which the sentence would be true. While substitutions between some pairs of words like car / automobile or wter / H 2 O are truth preserving, the words are still not identical in meaning. Indeed, probably no two words are absolutely identical in meaning. One of the fundamental tenets of semantics, called the principle of contrast ( Girard 1718 , Br ́eal 1897 , Clark principle of contrast 1987 ), states that a difference in linguistic form is always associated with some diference in meaning",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example, the word H 2 O is used in scientific contexts and would be inappropriate in a hiking guide— water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word syonym is therefore used to describe a relationship of approximate or rough synonymy. Word Similarity While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog , but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task. The notion of word similarity is very useful in larger semantic tasks. Knowing similarity how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answeing, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset ( Hill et al. , 2015 ) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms ( vanish , disappear ) to pairs that scarcely seem to have anything in common ( hole , agreement ): vanish disappear 9.8 belief impression 5.95 muscle bone 3.65 modest flexible 0.98 hole agreement 0.3 Word Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness ( Budanitsky relatedness and Hirst , 2006 ), also traditionally called word association in psychology. association Consider the meanings of the words coffee and cup",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". association Consider the meanings of the words coffee and cup . Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel). One common kind of relatedness between words is if they belong to the same semantic field . A semantic field is a set of words which cover a particular semantic semantic field domain and bear structured relations with each other. For example, words might be 4 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS related by being in the semantic field of hospitals ( surgeon , scalpel , nurse , anethetic , hospital ), restaurants ( waiter , menu , plate , food , chef ), or houses ( door , roof , kitchen , family , bed ). Semantic fields are also related to topic models , like Latent topic models Dirichlet Allocation , LDA , which apply unsupervised learning on large sets of texts to induce sets of associated words from text. Semantic fields and topic models are very useful tools for discovering topical structure in documents. In Appendix G we’ll introduce more relations between senses like hypernymy or IS-A , antonymy (opposites) and meronymy (part-whole relations). Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame . A semantic frame is a set of words that denote perspectives or semantic frame participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer . Frames have semantic roles (like buyer , seller , goods , money ), and words in a sentence can take on these roles. Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam , and that Sam has the role of the buyer in the frame and Ling the seller . Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation. Connotation Finally, words have affective meanings or connotations . The word connotations connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentment, opinions, or evaluations. For example some words have positive connotations ( wonderful ) while others have negative connotations ( dreary ). Even words whose meanings are similar in other ways can vary in connotation; consider the difference in connotations between fake , knockoff , forgery , on the one hand, and copy , replica , reproduction on the other, or innocent (positive connotation) and naive (negative connotation). Some words describe positive evaluation ( great , love ) and others neative evaluation ( terrible , hate ). Positive or negative evaluation language is called sentiment , as we saw in Chapter 4, and word sentiment plays a role in important sentiment tasks like sentiment analysis, stance detection, and applications of NLP to the laguage of politics and consumer reviews. Early work on affective meaning ( Osgood et al",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Early work on affective meaning ( Osgood et al. , 1957 ) found that words varied along three important dimensions of affective meaning: valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus Thus words like happy or satisfied are high on valence, while unhappy or anoyed are low on valence. Excited is high on arousal, while calm is low on arousal. Controlling is high on dominance, while awed or influenced are low on dominance. Each word is thus represented by three numbers, corresponding to its value on each of the three dimensions: 6.2 • V ECTOR S EMANTICS 5 Valence Arousal Dominance courageous 8.05 5.5 7.38 music 7.67 5.57 6.5 heartbreak 2.45 5.65 3.58 cub 6.71 3.95 4.24 Osgood et al. ( 1957 ) noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a thredimensional space, a vector whose three dimensions corresponded to the word’s rating on the three scales. This revolutionary idea that word meaning could be reresented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point [ 2 . 45 , 5 . 65 , 3 . 58 ] ) was the first expression of the vector smantics models that we introduce next. 6.2 Vector Semantics Vector semantics is the standard way to represent word meaning in NLP, helping vector semantics us model many of the aspects of word meaning we saw in the previous section. The roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos ( 1950 ), Harris ( 1954 ), and Firth ( 1957 ) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. For example, suppose you didn’t know the meaning of the word ongchoi (a rcent borrowing from Cantonese) but you see it in the following contexts: (6.1) Ongchoi is delicious sauteed with garlic. (6.2) Ongchoi is superb over rice. (6.3) ongchoi leaves with salty saucesAnd suppose that you had seen many of these context words in other contexts: (6.4) spinach sauteed with garlic over rice(6.5) chard stems and leaves are delicious(6.6) collard greens and other salty leafy greens The fact that ongchoi occurs with words like rice and garlic and delicious and salty , as do words like spinach , chard , and collard greens might suggest that ongchoi is a leafy green similar to these other leafy greens. 1 We can do the same thing computationally by just counting words in the context of ongchoi . The idea of vector semantics is to represent a word as a point in a multidimesional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings (although embeddings the term is sometimes more strictly applied only to dense vectors like word2vec (Section 6.8 ), rather than sparse tf-idf or PPMI vectors (Section 6.3 -Section 6.6 )). The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter. 1 It’s in fact Ipomoea aquatica , a relative of morning glory sometimes called water spinach in English. 6 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS good nice bad worst not good wonderful amazing terrific dislike worse very good incredibly good fantastic incredibly bad now you i that with by to ’s are is a than Figure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and phrases, showing that words with similar meanings are nearby in space",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The original 60- dimensional embeddings were trained for sentiment analysis. Simplified from Li et al. ( 2015 ) with colors added for explanation. Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis, showing the location of selected words projected down from 60-dimensional space into a two dimensional space. Notice the distinct regions containing positive words, negative words, and neutral function words. The fine-grained model of word similarity of vector semantics offers enormous power to NLP applications. NLP applications like the sentiment classifiers of Chater 4 or Chapter 5 depend on the same words appearing in the training and test sets. But by representing words as embeddings, a classifier can assign sentiment as long as it sees some words with similar meanings . And as we’ll see, vector semantic models can be learned automatically from text without supervision. In this chapter we’ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse , i.e. mostly zeros (since most words simply never occur in the context of others). We’ll introduce the word2vec model family for construcing short, dense vectors that have useful semantic properties. We’ll also introduce the cosine , the standard way to use embeddings to compute semantic similarity , btween two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading. 6.3 Words and Vectors “The most important attributes of a vector in 3-space are { Location, Location, Location } ” Randall Munroe, https://xkcd.com/2358/ Vector or distributional models of meaning are generally based on a co-occurrence matrix , a way of representing how often words co-occur. We’ll look at two popular matrices: the term-document matrix and the term-term matrix",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll look at two popular matrices: the term-document matrix and the term-term matrix. 6.3.1 Vectors and documents In a term-document matrix , each row represents a word in the vocabulary and each term-document matrix column represents a document from some collection of documents. Fig. 6.2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times 6.3 • W ORDS AND V ECTORS 7 a particular word (defined by the row) occurs in a particular document (defined by the column). Thus fool appeared 58 times in Twelfth Night . As You Like It Twelfth Night Julius Caesar Henry V battle 1 0 7 13 good 114 80 62 89 fool 36 58 1 4 wit 20 15 2 3 Figure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell contains the number of times the (row) word occurs in the (column) document. The term-document matrix of Fig. 6.2 was first defined as part of the vector space model of information retrieval ( Salton , 1971 ). In this model, a document is vector space model represented as a count vector, a column in Fig. 6.3 . To review some basic linear algebra, a vector is, at heart, just a list or array of vector numbers. So As You Like It is represented as the list [1,114,36,20] (the first column vector in Fig. 6.3 ) and Julius Caesar is represented as the list [7,62,1,2] (the third column vector). A vector space is a collection of vectors, and is characterized by vector space its dimension . Vectors in a 3-dimensional vector space have an element for each dimension dimension of the space. We will loosely refer to a vector in a 4-dimensional space as a 4-dimensional vector, with one element along each dimension. In the example in Fig. 6.3 , we’ve chosen to make the document vectors of dimension 4, just so they fit on the page; in real term-document matrices, the document vectors would have dimensionality | V | , the vocabulary size",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The ordering of the numbers in a vector space indicates the different dimensions on which documents vary. The first dimension for both these vectors corresponds to the number of times the word battle occurs, and we can compare each dimension, noting for example that the vectors for As You Like It and Twelfth Night have similar values (1 and 0, respectively) for the first dimension. As You Like It Twelfth Night Julius Caesar Henry V battle 1 0 7 13 good 114 80 62 89 fool 36 58 1 4 wit 20 15 2 3 Figure 6.3 The term-document matrix for four words in four Shakespeare plays. The red boxes show that each document is represented as a column vector of length four. We can think of the vector for a document as a point in | V | -dimensional space; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words battle and fool . Term-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval . Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. The vectors for the comedies As You Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other (more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or Henry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in Fig. 6.4 ; we’ll see very shortly how to quantify this intuition more formally",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6.4 ; we’ll see very shortly how to quantify this intuition more formally. 8 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS 5 10 15 20 25 30 5 10 Henry V [4,13] As You Like It [36,1] Julius Caesar [1,7] battle fool Twelfth Night [58,0] 15 40 35 40 45 50 55 60 Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play documents, showing just two of the dimensions, corresponding to the words battle and fool . The comedies have high values for the fool dimension and low values for the battle dimension. A real term-document matrix, of course, wouldn’t just have 4 rows and columns, let alone 2. More generally, the term-document matrix has | V | rows (one for each word type in the vocabulary) and D columns (one for each document in the colletion); as we’ll see, vocabulary sizes are generally in the tens of thousands, and the number of documents can be enormous (think about all the pages on the web). Information retrieval (IR) is the task of finding the document d from the D information retrieval documents in some collection that best matches a query q . For IR we’ll therefore also represent a query by a vector, also of length | V | , and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse, i.e., mostly zeros). Later in the chapter we’ll introduce some of the components of this vector coparison process: the tf-idf term weighting, and the cosine similarity metric. 6.3.2 Words as vectors: document dimensions We’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words . We do this by associating each word with a word vector— a row vector rather than a column row vector vector, hence with different dimensions, as shown in Fig. 6.5 . The four dimensions of the vector for fool , [36,58,1,4], correspond to the four Shakespeare plays",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6.5 . The four dimensions of the vector for fool , [36,58,1,4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other 3 words: wit , [20,15,2,3]; battle , [1,0,7,13]; and good [114,80,62,89]. As You Like It Twelfth Night Julius Caesar Henry V battle 1 0 7 13 good 114 80 62 89 fool 36 58 1 4 wit 20 15 2 3 Figure 6.5 The term-document matrix for four words in four Shakespeare plays. The red boxes show that each word is represented as a row vector of length four. For documents, we saw that similar documents had similar vectors, because siilar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the douments it tends to occur in. 6.3 • W ORDS AND V ECTORS 9 6.3.3 Words as vectors: word dimensions An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix , also called the word-word mtrix or the term-context matrix , in which the columns are labeled by words rather word-word matrix than documents. This matrix is thus of dimensionality | V |×| V | and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a widow around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a ± 4 word window around the row word. Here are four examples of words in their windows: is traditionally followed by cherry pie, a traditional dessert often mixed, such as strawberry rhubarb pie",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here are four examples of words in their windows: is traditionally followed by cherry pie, a traditional dessert often mixed, such as strawberry rhubarb pie. Apple pie computer peripherals and personal digital assistants. These devices usually a computer. This includes information available on the internet If we then take every occurrence of each word (say strawberry ) and count the context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a simplified subset of the word-word co-occurrence matrix for these four words coputed from the Wikipedia corpus ( Davies , 2015 ). aardvark computer data result pie sugar cherry 0 2 8 9 442 25 strawberry 0 0 0 1 60 19 digital 0 1670 1683 85 5 4 information 0 3325 3982 378 5 13 Figure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in red. Note that a real vector would have vastly more dimensions and thus be much sparser. Note in Fig. 6.6 that the two words cherry and strawberry are more similar to each other (both pie and sugar tend to occur in their window) than they are to other words like digital ; conversely, digital and information are more similar to each other than, say, to strawberry . Fig. 6.7 shows a spatial visualization. 1000 2000 3000 4000 1000 2000 digital [1683,1670] computer data information [3982,3325] 3000 4000 Figure 6.7 A spatial visualization of word vectors for digital and information , showing just two of the dimensions, corresponding to the words data and computer . Note that | V | , the dimensionality of the vector, is generally the size of the vcabulary, often between 10,000 and 50,000 words (using the most frequent words 10 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS in the training corpus; keeping words after about the most frequent 50,000 or so is generally not helpful)",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Since most of these numbers are zero these are sparse vector representations; there are efficient algorithms for storing and computing with sparse matrices. Now that we have some intuitions, let’s move on to examine the details of coputing word similarity. Afterwards we’ll discuss methods for weighting cells. 6.4 Cosine for measuring similarity To measure similarity between two target words v and w , we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length | V | , or both with documents as dimensions, of length | D | ) and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product : dot product inner product dot product ( v , w ) = v · w = N X i = 1 v i w i = v 1 w 1 + v 2 w 2 + + v N w N (6.7) The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. This raw dot product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as vector length | v | = v u u t N X i = 1 v 2 i (6.8) The dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency. We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors, following from the definition of the dot product between two vectors a and b : a · b = | a || b | cos θ a · b | a || b | = cos θ (6.9) The cosine similarity metric between two vectors v and w thus can be computed as: cosine 6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 11 cosine ( v , w ) = v · w | v || w | = N X i = 1 v i w i v u u t N X i = 1 v 2 i v u u t N X i = 1 w 2 i (6.10) For some applications we pre-normalize each vector, by dividing it by its length, creating a unit vector of length 1. Thus we could compute a unit vector from a by unit vector dividing it by | a | . For unit vectors, the dot product is the same as the cosine. The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to -1 for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1. Let’s see how the cosine computes which of the words cherry or digital is closer in meaning to information , just using raw counts from the following shortened table: pie data computer cherry 442 8 2 digital 5 1683 1670 information 5 3982 3325 cos ( cherry , information ) = 442 ∗ 5 + 8 ∗ 3982 + 2 ∗ 3325 √ 442 2 + 8 2 + 2 2 √ 5 2 + 3982 2 + 3325 2 = . 018 cos ( digital , information ) = 5 ∗ 5 + 1683 ∗ 3982 + 1670 ∗ 3325 √ 5 2 + 1683 2 + 1670 2 √ 5 2 + 3982 2 + 3325 2 = . 996 The model decides that information is way closer to digital than it is to cherry , a result that seems sensible. Fig. 6.8 shows a visualization",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 996 The model decides that information is way closer to digital than it is to cherry , a result that seems sensible. Fig. 6.8 shows a visualization. 500 1000 1500 2000 2500 3000 500 digital cherry information Dimension 1: ‘ pie ’ Dimension 2: ‘ computer ’ Figure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for three words ( cherry , digital , and information ) in the two dimensional space defined by counts of the words computer and pie nearby. The figure doesn’t show the cosine, but it highlights the angles; note that the angle between digital and information is smaller than the angle between cherry and information . When two vectors are more similar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the angle between two vectors is smallest (0 ◦ ); the cosine of all other angles is less than 1. 6.5 TF-IDF: Weighing terms in the vector The co-occurrence matrices above represent each cell by frequencies, either of words with documents (Fig. 6.5 ), or words with other words (Fig. 6.6 ). But raw frequency 12 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS is not the best measure of association between words. Raw frequency is very skewed and not very discriminative. If we want to know what kinds of contexts are shared by cherry and strawberry but not by digital and information , we’re not going to get good discrimination from words like the , it , or they , which occur frequently with all sorts of words and aren’t informative about any particular word. We saw this also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not very discriminative between plays; good is simply a frequent word and has roughly equivalent high frequencies in each of the plays. It’s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby cherry ) are more important than words that only appear once or twice. Yet words that are too frequent—ubiquitous, like the or good — are unimportant",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Yet words that are too frequent—ubiquitous, like the or good — are unimportant. How can we balance these two conflicting constraints? There are two common solutions to this problem: in this section we’ll describe the tf-idf weighting, usually used when the dimensions are documents. In the next section we introduce the PPMI algorithm (usually used when the dimensions are words). The tf-idf weighting (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, each term capturing one of these two intuitions: The first is the term frequency ( Luhn , 1957 ): the frequency of the word t in the term frequency document d . We can just use the raw count as the term frequency: tf t , d = count ( t , d ) (6.11) More commonly we squash the raw frequency a bit, by using the log 10 of the frquency instead. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. We also need to do something special with counts of 0, since we can’t take the log of 0. 2 tf t , d = ( 1 + log 10 count ( t , d ) if count ( t , d ) > 0 0 otherwise (6.12) If we use log weighting, terms which occur 0 times in a document would have tf = 0, 1 times in a document tf = 1 + log 10 ( 1 ) = 1 + 0 = 1, 10 times in a document tf = 1 + log 10 ( 10 ) = 2, 100 times tf = 1 + log 10 ( 100 ) = 3, 1000 times tf = 4, and so on. The second factor in tf-idf is used to give a higher weight to words that occur only in a few documents. Terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across the entire collection aren’t as helpful. The document frequency document frequency df t of a term t is the number of documents it occurs in. Document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_19"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document. Consider in the collection of Shakespeare’s 37 plays the two words Romeo and action . The words have identical collection frequencies (they both occur 113 times in all the plays) but very different document frequencies, since Romeo only occurs in a single play. If our goal is to find documents about the romantic tribulations of Romeo, the word Romeo should be highly weighted, but not action : Collection Frequency Document Frequency Romeo 113 1 action 113 31 2 We can also use this alternative formulation, which we have used in earlier editions: tf t , d = log 10 ( count ( t , d )+ 1 ) 6.5 • TF-IDF: W EIGHING TERMS IN THE VECTOR 13 We emphasize discriminative words like Romeo via the inverse document frquency or idf term weight ( Sparck Jones , 1972 ). The idf is defined using the fraidf tion N / df t , where N is the total number of documents in the collection, and df t is the number of documents in which term t occurs. The fewer documents in which a term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that occur in all the documents. It’s usually clear what counts as a document: in Shakspeare we would use a play; when processing a collection of encyclopedia articles like Wikipedia, the document is a Wikipedia page; in processing newspaper articles, the document is a single article. Occasionally your corpus might not have approprate document divisions and you might need to break up the corpus into documents yourself for the purposes of computing idf. Because of the large number of documents in many collections, this measure too is usually squashed with a log function",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_20"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Because of the large number of documents in many collections, this measure too is usually squashed with a log function. The resulting definition for inverse document frequency (idf) is thus idf t = log 10 N df t (6.13) Here are some idf values for some words in the Shakespeare corpus, (along with the document frequency df values on which they are based) ranging from extremely informative words which occur in only one play like Romeo , to those that occur in a few like salad or Falstaff , to those which are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet . 3 Word df idf Romeo 1 1.57 salad 2 1.27 Falstaff 4 0.967 forest 12 0.489 battle 21 0.246 wit 34 0.037 fool 36 0.012 good 37 0 sweet 37 0 The tf-idf weighted value w t , d for word t in document d thus combines term tf-idf frequency tf t , d (defined either by Eq. 6.11 or by Eq. 6.12 ) with idf from Eq. 6.13 : w t , d = tf t , d × idf t (6.14) Fig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2 , using the tf equation Eq. 6.12 . Note that the tf-idf values for the dimension corrsponding to the word good have now all become 0; since this word appears in every document, the tf-idf weighting leads it to be ignored. Similarly, the word fool , which appears in 36 out of the 37 plays, has a much lower weight. The tf-idf weighting is the way for weighting co-occurrence matrices in infomation retrieval, but also plays a role in many other aspects of natural language processing. It’s also a great baseline, the simple thing to try first. We’ll look at other weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6 . 3 Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of sugar in European recipes around the turn of the 16th century ( Jurafsky , 2014 , p. 175)",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_21"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 3 Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of sugar in European recipes around the turn of the 16th century ( Jurafsky , 2014 , p. 175). 14 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS As You Like It Twelfth Night Julius Caesar Henry V battle 0.246 0 0.454 0.520 good 0 0 0 0 fool 0.030 0.033 0.0012 0.0019 wit 0.085 0.081 0.048 0.054 Figure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shakspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2 . For example the 0 . 085 value for wit in As You Like It is the product of tf = 1 + log 10 ( 20 ) = 2 . 301 and idf = . 037. Note that the idf weighting has eliminated the importance of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word fool . 6.6 Pointwise Mutual Information (PMI) An alternative weighting function to tf-idf, PPMI (positive pointwise mutual infomation), is used for term-term-matrices, when the vector dimensions correspond to words rather than documents. PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance. Pointwise mutual information ( Fano , 1961 ) 4 is one of the most important copointwise mutual information cepts in NLP. It is a measure of how often two events x and y occur, compared with what we would expect if they were independent: I ( x , y ) = log 2 P ( x , y ) P ( x ) P ( y ) (6.16) The pointwise mutual information between a target word w and a context word c ( Church and Hanks 1989 , Church and Hanks 1990 ) is then defined as: PMI ( w , c ) = log 2 P ( w , c ) P ( w ) P ( c ) (6.17) The numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE)",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_22"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The denominator tells us how often we would expect the two words to co-occur assuming they each occurred indepedently; recall that the probability of two independent events both occurring is just the product of the probabilities of the two events. Thus, the ratio gives us an estmate of how much more the two words co-occur than we expect by chance. PMI is a useful tool whenever we need to find words that are strongly associated. PMI values range from negative to positive infinity. But negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous. To distinguish whether two words whose individual probability is each 10 − 6 occur together less often than chance, we would need to be certain that the probability of the two occurring tgether is significantly less than 10 − 12 , and this kind of granularity would require an enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate such scores of ‘unrelatedness’ with human judgments. For this reason it is more 4 PMI is based on the mutual information between two random variables X and Y , defined as: I ( X , Y ) = X x X y P ( x , y ) log 2 P ( x , y ) P ( x ) P ( y ) (6.15) In a confusion of terminology, Fano used the phrase mutual information to refer to what we now call pointwise mutual information and the phrase expectation of the mutual information for what we now call mutual information 6.6 • P OINTWISE M UTUAL I NFORMATION (PMI) 15 common to use Positive PMI (called PPMI ) which replaces all negative PMI values PPMI with zero ( Church and Hanks 1989 , Dagan et al. 1993 , Niwa and Nitta 1994 ) 5 : PPMI ( w , c ) = max ( log 2 P ( w , c ) P ( w ) P ( c ) , 0 ) (6.18) More formally, let’s assume we have a co-occurrence matrix F with W rows (words) and C columns (contexts), where f ij gives the number of times word w i occurs with context c j",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_23"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This can be turned into a PPMI matrix where PPMI ij gives the PPMI value of word w i with context c j (which we can also express as PPMI( w i , c j ) or PPMI( w = i , c = j )) as follows: p ij = f ij P W i = 1 P C j = 1 f i j , p i ∗ = P C j = 1 f i j P W i = 1 P C j = 1 f i j , p ∗ j = P W i = 1 f i j P W i = 1 P C j = 1 f i j (6.19) PPMI ij = max ( log 2 p ij p i ∗ p ∗ j , 0 ) (6.20) Let’s see some PPMI calculations. We’ll use Fig. 6.10 , which repeats Fig. 6.6 plus all the count marginals, and let’s pretend for ease of calculation that these are the only words/contexts that matter. computer data result pie sugar count(w) cherry 2 8 9 442 25 486 strawberry 0 0 1 60 19 80 digital 1670 1683 85 5 4 3447 information 3325 3982 378 5 13 7703 count(context) 4997 5673 473 512 61 11716 Figure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus, together with the marginals, pretending for the purpose of this calculation that no other words/contexts matter. Thus for example we could compute PPMI(information,data), assuming we prtended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as folows: P ( w=information, c=data ) = 3982 11716 = . 3399 P ( w=information ) = 7703 11716 = . 6575 P ( c=data ) = 5673 11716 = . 4842 PPMI ( information,data ) = log 2 ( . 3399 / ( . 6575 ∗ . 4842 )) = . 0944 Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10 , and Fig. 6.12 shows the PPMI values. Not surprisingly, cherry and strawberry are highly associated with both pie and sugar , and data is mildly associated with information . PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values. One way to reduce this bias toward low frequency 5 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the − ∞ from log ( 0 )",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_24"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One way to reduce this bias toward low frequency 5 Positive PMI also cleanly solves the problem of what to do with zero counts, using 0 to replace the − ∞ from log ( 0 ) . 16 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS p(w,context) p(w) computer data result pie sugar p(w) cherry 0.0002 0.0007 0.0008 0.0377 0.0021 0.0415 strawberry 0.0000 0.0000 0.0001 0.0051 0.0016 0.0068 digital 0.1425 0.1436 0.0073 0.0004 0.0003 0.2942 information 0.2838 0.3399 0.0323 0.0004 0.0011 0.6575 p(context) 0.4265 0.4842 0.0404 0.0437 0.0052 Figure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals in the right column and the bottom row. computer data result pie sugar cherry 0 0 0 4.38 3.30 strawberry 0 0 0 4.10 5.51 digital 0.18 0.01 0 0 0 information 0.02 0.09 0.28 0 0 Figure 6.12 The PPMI matrix showing the association between words and context words, computed from the counts in Fig. 6.11 . Note that most of the 0 PPMI values are ones that had a negative PMI; for example PMI( cherry,computer ) = -6.7, meaning that cherry and computer co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace negative values by zero. events is to slightly change the computation for P ( c ) , using a different function P α ( c ) that raises the probability of the context word to the power of α : PPMI α ( w , c ) = max ( log 2 P ( w , c ) P ( w ) P α ( c ) , 0 ) (6.21) P α ( c ) = count ( c ) α P c count ( c ) α (6.22) Levy et al. ( 2015 ) found that a setting of α = 0 . 75 improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skigrams described below in Eq. 6.32 ). This works because raising the count to α = 0 . 75 increases the probability assigned to rare contexts, and hence lowers their PMI ( P α ( c ) > P ( c ) when c is rare). Another possible solution is Laplace smoothing: Before computing PMI, a small constant k (values of 0.1-3 are common) is added to each of the counts, shrinking (discounting) all the non-zero values",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_25"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Another possible solution is Laplace smoothing: Before computing PMI, a small constant k (values of 0.1-3 are common) is added to each of the counts, shrinking (discounting) all the non-zero values. The larger the k , the more the non-zero counts are discounted. 6.7 Applications of the tf-idf or PPMI vector models In summary, the vector semantics model we’ve described so far represents a target word as a vector with dimensions corresponding either to the documents in a large collection (the term-document matrix) or to the counts of words in some neighboring window (the term-term matrix). The values in each dimension are counts, weighted by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the vectors are sparse (since most values are zero). The model computes the similarity between two words x and y by taking the cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model 6.8 • W ORD 2 VEC 17 is sometimes referred to as the tf-idf model or the PPMI model, after the weighting function. The tf-idf model of meaning is often used for document functions like deciding if two documents are similar. We represent a document by taking the vectors of all the words in the document, and computing the centroid of all those vectors. centroid The centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. Given k word vectors w 1 , w 2 ,, w k , the centroid document vector d is: document vector d = w 1 + w 2 + + w k k (6.23) Given two documents, we can then compute their document vectors d 1 and d 2 , and estimate the similarity between the two documents by cos ( d 1 , d 2 ) . Document siilarity is also useful for all sorts of applications; information retrieval, plagiarism detection, news recommender systems, and even for digital humanities tasks like comparing different versions of a text to see which are similar to each other",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_26"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Either the PPMI model or the tf-idf model can be used to compute word simlarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word w by computing the cosines between w and each of the V − 1 other words, sorting, and looking at the top 10. 6.8 Word2vec In the previous sections we saw how to represent a word as a sparse, long vector with dimensions corresponding to words in the vocabulary or documents in a collection. We now introduce a more powerful word representation: embeddings , short dense vectors. Unlike the vectors we’ve seen so far, embeddings are short , with number of dimensions d ranging from 50-1000, rather than the much larger vocabulary size | V | or number of documents D we’ve seen. These d dimensions don’t have a clear interpretation. And the vectors are dense : instead of vector entries being sparse, mostly-zero counts or functions of counts, the values will be real-valued numbers that can be negative. It turns out that dense vectors work better in every NLP task than sparse vectors. While we don’t completely understand all the reasons for this, we have some intitions. Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vetors, and the smaller parameter space possibly helps with generalization and avoiing overfitting. Dense vectors may also do a better job of capturing synonymy. For example, in a sparse vector representation, dimensions for synonyms like car and automobile dimension are distinct and unrelated; sparse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighbor. In this section we introduce one method for computing embeddings: skip-gram skip-gram with negative sampling , sometimes called SGNS",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_27"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In this section we introduce one method for computing embeddings: skip-gram skip-gram with negative sampling , sometimes called SGNS . The skip-gram algorithm is one SGNS of two algorithms in a software package called word2vec , and so sometimes the word2vec algorithm is loosely referred to as word2vec ( Mikolov et al. 2013a , Mikolov et al. 2013b ). The word2vec methods are fast, efficient to train, and easily available o 18 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS line with code and pretrained embeddings. Word2vec embeddings are static ebeddings , meaning that the method learns one fixed embedding for each word in the static embeddings vocabulary. In Chapter 11 we’ll introduce methods for learning dynamic contextual embeddings like the popular family of BERT representations, in which the vector for each word is different in different contexts. The intuition of word2vec is that instead of counting how often each word w ocurs near, say, apricot , we’ll instead train a classifier on a binary prediction task: “Is word w likely to show up near apricot ?” We don’t actually care about this prediction task; instead we’ll take the learned classifier weights as the word embeddings. The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word c that occurs near the target word apricot acts as gold ‘correct answer’ to the question “Is word c likely to show up near apricot ?” This method, often called self-supervision , avoids the need for self-supervision any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. ( 2003 ) and Collobert et al. ( 2011 ) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_28"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll see how to do neural networks in the next chapter, but word2vec is a much simpler model than the neural network language model, in two ways. First, word2vec simplifies the task (making it binary classification instead of word prdiction). Second, word2vec simplifies the architecture (training a logistic regression classifier instead of a multi-layer neural network with hidden layers that demand more sophisticated training algorithms). The intuition of skip-gram is: 1. Treat the target word and a neighboring context word as positive examples. 2. Randomly sample other words in the lexicon to get negative samples. 3. Use logistic regression to train a classifier to distinguish those two cases. 4. Use the learned weights as the embeddings. 6.8.1 The classifier Let’s start by thinking about the classification task, and then turn to how to train. Imagine a sentence like the following, with a target word apricot , and assume we’re using a window of ± 2 context words: lemon, a [tablespoon of apricot jam, a] pinch c1 c2 w c3 c4 Our goal is to train a classifier such that, given a tuple ( w , c ) of a target word w paired with a candidate context word c (for example ( apricot , jam ), or perhaps ( apricot , aardvark )) it will return the probability that c is a real context word (true for jam , false for aardvark ): P (+ | w , c ) (6.24) The probability that word c is not a real context word for w is just 1 minus Eq. 6.24 : P ( −| w , c ) = 1 − P (+ | w , c ) (6.25) How does the classifier compute the probability P ? The intuition of the skigram model is to base this probability on embedding similarity: a word is likely to 6.8 • W ORD 2 VEC 19 occur near the target if its embedding vector is similar to the target embedding. To compute similarity between these dense embeddings, we rely on the intuition that two vectors are similar if they have a high dot product (after all, cosine is just a normalized dot product)",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_29"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To compute similarity between these dense embeddings, we rely on the intuition that two vectors are similar if they have a high dot product (after all, cosine is just a normalized dot product). In other words: Similarity ( w , c ) ≈ c · w (6.26) The dot product c · w is not a probability, it’s just a number ranging from − ∞ to ∞ (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we’ll use the logistic or sigmoid function σ ( x ) , the fundamental core of logistic regression: σ ( x ) = 1 1 + exp ( − x ) (6.27) We model the probability that word c is a real context word for target word w as: P (+ | w , c ) = σ ( c · w ) = 1 1 + exp ( − c · w ) (6.28) The sigmoid function returns a number between 0 and 1, but to make it a probability we’ll also need the total probability of the two possible events ( c is a context word, and c isn’t a context word) to sum to 1. We thus estimate the probability that word c is not a real context word for w as: P ( −| w , c ) = 1 − P (+ | w , c ) = σ ( − c · w ) = 1 1 + exp ( c · w ) (6.29) Equation 6.28 gives us the probability for one word, but there are many context words in the window. Skip-gram makes the simplifying assumption that all context words are independent, allowing us to just multiply their probabilities: P (+ | w , c 1: L ) = L Y i = 1 σ ( c i · w ) (6.30) log P (+ | w , c 1: L ) = L X i = 1 log σ ( c i · w ) (6.31) In summary, skip-gram trains a probabilistic classifier that, given a test target word w and its context window of L words c 1: L , assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary. Fig. 6.13 shows the intuition of the parameters we’ll need",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_30"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To compute this probability, we just need embeddings for each target word and context word in the vocabulary. Fig. 6.13 shows the intuition of the parameters we’ll need. Skip-gram actually stores two embeddings for each word, one for the word as a target, and one for the word considered as context. Thus the parameters we need to learn are two matrices W and C , each containing an embedding for every one of the | V | words in the vocabulary V . 6 Let’s now turn to learning these embeddings (which is the real goal of training this classifier in the first place). 6 In principle the target matrix and the context matrix could use different vocabularies, but we’ll simplify by assuming one shared vocabulary V . 20 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS 1 W C aardvark zebra zebra aardvark apricot apricot |V| |V|+1 2V & = target words context & noise words 1..d Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two embeddings for each word, the target embedding (sometimes called the input embedding) and the context embedding (sometimes called the output embedding). The parameter θ that the algorithm learns is thus a matrix of 2 | V | vectors, each of dimension d , formed by concatnating two matrices, the target embeddings W and the context+noise embeddings C . 6.8.2 Learning skip-gram embeddings The learning algorithm for skip-gram embeddings takes as input a corpus of text, and a chosen vocabulary size N. It begins by assigning a random embedding vector for each of the N vocabulary words, and then proceeds to iteratively shift the ebedding of each word w to be more like the embeddings of words that occur nearby in texts, and less like the embeddings of words that don’t occur nearby",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_31"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Let’s start by considering a single piece of training data: lemon, a [tablespoon of apricot jam, a] pinch c1 c2 w c3 c4 This example has a target word w (apricot), and 4 context words in the L = ± 2 window, resulting in 4 positive training instances (on the left below): positive examples + w c pos apricot tablespoon apricot of apricot jam apricot a negative examples - w c neg w c neg apricot aardvark apricot seven apricot my apricot forever apricot where apricot dear apricot coaxial apricot if For training a binary classifier we also need negative examples. In fact skigram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter k ). So for each of these ( w , c pos ) training instances we’ll create k negative samples, each consisting of the target w plus a ‘noise word’ c neg . A noise word is a random word from the lexicon, constrained not to be the target word w . The right above shows the setting where k = 2, so we’ll have 2 negative examples in the negative training set − for each positive example w , c pos . The noise words are chosen according to their weighted unigram frequency p α ( w ) , where α is a weight. If we were sampling according to unweighted frquency p ( w ) , it would mean that with unigram probability p ( “ the ” ) we would choose the word the as a noise word, with unigram probability p ( “ aardvark ” ) we would choose aardvark , and so on. But in practice it is common to set α = 0 . 75, i.e. use 6.8 • W ORD 2 VEC 21 the weighting p 3 4 ( w ) : P α ( w ) = count ( w ) α P w ′ count ( w ′ ) α (6.32) Setting α = . 75 gives better performance because it gives rare noise words slightly higher probability: for rare words, P α ( w ) > P ( w ) . To illustrate this intuition, it might help to work out the probabilities for an example with α = . 75 and two events, P ( a ) = 0 . 99 and P ( b ) = 0 . 01: P α ( a ) = . 99 . 75 . 99 . 75 + . 01 . 75 = 0 . 97 P α ( b ) = . 01 . 75 . 99 . 75 + . 01 . 75 = 0 . 03 (6.33) Thus using α =",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_32"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 75 and two events, P ( a ) = 0 . 99 and P ( b ) = 0 . 01: P α ( a ) = . 99 . 75 . 99 . 75 + . 01 . 75 = 0 . 97 P α ( b ) = . 01 . 75 . 99 . 75 + . 01 . 75 = 0 . 03 (6.33) Thus using α = . 75 increases the probability of the rare event b from 0.01 to 0.03. Given the set of positive and negative training instances, and an initial set of embeddings, the goal of the learning algorithm is to adjust those embeddings to • Maximize the similarity of the target word, context word pairs ( w , c pos ) drawn from the positive examples • Minimize the similarity of the ( w , c neg ) pairs from the negative examples. If we consider one word/context pair ( w , c pos ) with its k noise words c neg 1 c neg k , we can express these two goals as the following loss function L to be minimized (hence the − ); here the first term expresses that we want the classifier to assign the real context word c pos a high probability of being a neighbor, and the second term expresses that we want to assign each of the noise words c neg i a high probability of being a non-neighbor, all multiplied because we assume independence: L = − log \" P (+ | w , c pos ) k Y i = 1 P ( −| w , c neg i ) # = − \" log P (+ | w , c pos )+ k X i = 1 log P ( −| w , c neg i ) # = − \" log P (+ | w , c pos )+ k X i = 1 log 1 − P (+ | w , c neg i ) # = − \" log σ ( c pos · w )+ k X i = 1 log σ ( − c neg i · w ) # (6.34) That is, we want to maximize the dot product of the word with the actual context words, and minimize the dot products of the word with the k negative sampled noneighbor words. We minimize this loss function using stochastic gradient descent. Fig. 6.14 shows the intuition of one step of learning. To get the gradient, we need to take the derivative of Eq. 6.34 with respect to the different embeddings",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_33"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Fig. 6.14 shows the intuition of one step of learning. To get the gradient, we need to take the derivative of Eq. 6.34 with respect to the different embeddings. It turns out the derivatives are the following (we leave the 22 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS W C move apricot and jam closer, increasing c pos z w aardvark move apricot and matrix apart decreasing c neg1 z w “apricot jam” w zebra zebra aardvark jam apricot c pos matrix Tolstoy move apricot and Tolstoy apart decreasing c neg2 z w ! c neg1 c neg2 k=2 Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift embeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prouct with) context embeddings for nearby words (here jam ) and further from (lower dot product with) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix ). proof as an exercise at the end of the chapter): ∂ L ∂ c pos = [ σ ( c pos · w ) − 1 ] w (6.35) ∂ L ∂ c neg = [ σ ( c neg · w )] w (6.36) ∂ L ∂ w = [ σ ( c pos · w ) − 1 ] c pos + k X i = 1 [ σ ( c n eg i · w )] c neg i (6.37) The update equations going from time step t to t + 1 in stochastic gradient descent are thus: c t + 1 pos = c t pos − η [ σ ( c t pos · w t ) − 1 ] w t (6.38) c t + 1 neg = c t neg − η [ σ ( c t neg · w t )] w t (6.39) w t + 1 = w t − η \" [ σ ( c t pos · w t ) − 1 ] c t pos + k X i = 1 [ σ ( c t n eg i · w t )] c t neg i # (6.40) Just as in logistic regression, then, the learning algorithm starts with randomly intialized W and C matrices, and then walks through the training corpus using gradient descent to move W and C so as to minimize the loss in Eq. 6.34 by making the udates in (Eq. 6.38 )-(Eq. 6.40 ). Recall that the skip-gram model learns two separate embeddings for each word i : the target embedding w i and the context embedding c i , stored in two matrices, the target embedding context embedding target matrix W and the context matrix C",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_34"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". It’s common to just add them together, representing word i with the vector w i + c i . Alternatively we can throw away the C matrix and just represent each word i by the vector w i . As with the simple count-based methods like tf-idf, the context window size L affects the performance of skip-gram embeddings, and experiments often tune the parameter L on a devset. 6.9 • V ISUALIZING E MBEDDINGS 23 6.8.3 Other kinds of static embeddings There are many kinds of static embeddings. An extension of word2vec, fasttext fasttext ( Bojanowski et al. , 2017 ), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words —words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word. For example, with n = 3 the word where would be represented by the sequence <where> plus the character n-grams: <wh, whe, her, ere, re> Then a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fasttext open-source library, including pretrained embeddings for 157 languages, is available at https://fasttext.cc . Another very widely used static embedding model is GloVe ( Pennington et al. , 2014 ), short for G lobal Vectors, because the model is based on capturing global corpus statistics . GloVe is based on ratios of probabilities from the word-word coccurrence matrix , combining the intuitions of count-based models like PPMI while also capturing the l inear structures used by methods like word2vec",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_35"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". It turns out that d ense embeddings like word2vec actually have an elegant matematical relation sh ip with sparse embeddings like PPMI, in which word2vec can be seen as impli ci tly optimizing a function of a PPMI matrix ( Levy and Goldberg , 2014c ). 6.9 Visualizing Embeddings “I see well in many dimensions as long as the dimensions are around two.” The late economist Martin Shubik Visualizing embeddings is an important goal in helping understand, apply, and improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector? WRIST ANKLE SHOULDER ARM LEG HAND FOOT HEAD NOSE FINGER TOE FACE EAR EYE TOOTH DOG CAT PUPPY KITTEN COW MOUSE TURTLE OYSTER LION BULL CHICAGO ATLANTA MONTREAL NASHVILLE TOKYO CHINA RUSSIA AFRICA ASIA EUROPE AMERICA BRAZIL MOSCOW FRANCE The simplest way to visualize the meaning of a word w embedded in a space is to list the most similar words to w by sorting the vectors for all words in the vocabulary by their cosine with the vector for w . For example the 7 closest words to frog using a particular embeddings computed with the GloVe algorithm are: frogs , toad , litoria , leptodactyldae , rana , lizard , and eleutherodactylus ( Pennington et al. , 2014 ). Yet another visualization method is to use a clustering algorithm to show a hierarchical representation of which words are similar to others in the embedding space. The uncaptioned figure on the left uses hierarchical clustering of some embedding vectors for nouns as a visualization 24 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS method ( Rohde et al. , 2006 ). Probably the most common visualization method, hoever, is to project the 100 dimensions of a word down into 2 dimensions. Fig. 6.1 showed one such visualization, as does Fig. 6.16 , using a projection method called t-SNE ( van der Maaten and Hinton , 2008 ). 6.10 Semantic properties of embeddings In this section we briefly summarize some of the semantic properties of embeddings that have been studied",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_36"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6.10 Semantic properties of embeddings In this section we briefly summarize some of the semantic properties of embeddings that have been studied. Different types of similarity or association: One parameter of vector semantic models that is relevant to both sparse PPMI vectors and dense word2vec vectors is the size of the context window used to collect counts. This is generally between 1 and 10 words on each side of the target word (for a total context of 2-20 words). The choice depends on the goals of the representation. Shorter context windows tend to lead to representations that are a bit more syntactic, since the information is coming from immediately nearby words. When the vectors are computed from short context windows, the most similar words to a target word w tend to be semantically similar words with the same parts of speech. When vectors are computed from long context windows, the highest cosine words to a target word w tend to be words that are topically related but not similar. For example Levy and Goldberg ( 2014a ) showed that using skip-gram with a window of ± 2, the most similar words to the word Hogwarts (from the Harry Potter series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer ) or Evernight (from a vampire series). With a window of ± 5, the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore , Malfoy , and half-blood . It’s also often useful to distinguish two kinds of similarity or association between words ( Sch ̈utze and Pedersen , 1993 ). Two words have first-order co-occurrence first-order co-occurrence (sometimes called syntagmatic association ) if they are typically nearby each other. Thus wrote is a first-order associate of book or poem . Two words have second-order co-occurrence (sometimes called paradigmatic association ) if they have similar second-order co-occurrence neighbors. Thus wrote is a second-order associate of words like said or remarked",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_37"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Thus wrote is a second-order associate of words like said or remarked . Analogy/Relational Similarity: Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson ( 1973 ) proposed the parallelogram model parallelogram model for solving simple analogy problems of the form a is to b as a* is to what? . In such problems, a system is given a problem like apple:tree::grape:? , i.e., apple is to tree as grape is to , and must fill in the word vine . In the parallelogram model, illustrated in Fig. 6.15 , the vector from the word apple to the word tree (= tree − # » apple) is added to the vector for grape ( # » grape); the nearest word to that point is returned. In early work with sparse embeddings, scholars showed that sparse vector moels of meaning could solve such analogy problems ( Turney and Littman , 2005 ), but the parallelogram method received more modern attention because of its sucess with word2vec or GloVe vectors ( Mikolov et al. 2013c , Levy and Goldberg 2014b , Pennington et al. 2014 ). For example, the result of the expression # » king − 6.10 • S EMANTIC PROPERTIES OF EMBEDDINGS 25 tree apple grape vine Figure 6.15 The parallelogram model for analogy problems ( Rumelhart and Abrahamson , 1973 ): the location of # » vine can be found by subtracting # » apple from # » tree and adding # » grape. man + # » woman is a vector close to # » queen. Similarly, # » Paris − # » France + # » Italy results in a vector that is close to # » Rome. The embedding model thus seems to be extracing representations of relations like MALE - FEMALE , or CAPITAL - CITY - OF , or even COMPARATIVE / SUPERLATIVE , as shown in Fig. 6.16 from GloVe. (a) (b) Figure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimesions. (a) # » king − # » man + # » woman is close to # » queen. (b) offsets seem to capture comparative and superlative morphology ( Pennington et al. , 2014 )",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_38"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (a) # » king − # » man + # » woman is close to # » queen. (b) offsets seem to capture comparative and superlative morphology ( Pennington et al. , 2014 ). For a a : b :: a ∗ : b ∗ problem, meaning the algorithm is given vectors a , b , and a ∗ and must find b ∗ , the parallelogram method is thus: ˆ b ∗ = argmin x distance ( x , b − a + a ∗ ) (6.41) with some distance function, such as Euclidean distance. There are some caveats. For example, the closest value returned by the paralelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact b* but one of the 3 input words or their morphological variants (i.e., cherry:red :: potato:x returns potato or potatoes instead of brown ), so these must be explicitly excluded. Furthermore while embedding spaces perform well if the task involves frequent words, small distances, and certain relations (like relating countries with their capitals or verbs/nouns with their inflected forms), the parallelogram method with embeddings doesn’t work as well for other relations ( Linzen 2016 , Gladkova et al. 2016 , Schluter 2018 , Ethayarajh et al. 2019a ), and indeed Peterson et al. ( 2020 ) argue that the parallelogram method is in general too simple to model the human cognitive process of forming analogies of this kind. 26 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS 6.10.1 Embeddings and Historical Semantics Embeddings can also be a useful tool for studying how meaning changes over time, by computing multiple embedding spaces, each from texts written in a particular time period. For example Fig. 6.17 shows a visualization of changes in meaning in English words over the last two centuries, computed by building separate embedding spaces for each decade from historical corpora like Google n-grams ( Lin et al. , 2012 ) and the Corpus of Historical American English ( Davies , 2012 ). Figure 6.17 A t-SNE visualization of the semantic change of 3 words in English using word2vec vectors",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_39"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2012 ) and the Corpus of Historical American English ( Davies , 2012 ). Figure 6.17 A t-SNE visualization of the semantic change of 3 words in English using word2vec vectors. The modern sense of each word, and the grey context words, are coputed from the most recent (modern) time-point embedding space. Earlier points are coputed from earlier historical embedding spaces. The visualizations show the changes in the word gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality, the development of the modern “transmission” sense of broadcast from its original sense of sowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe” to meaning “terrible or appalling” ( Hamilton et al. , 2016 ). 6.11 Bias and Embeddings In addition to their ability to learn word meaning from text, embeddings, alas, also reproduce the implicit biases and stereotypes that were latent in the text. As the prior section just showed, embeddings can roughly model relational similaity: ‘queen’ as the closest word to ‘king’ - ‘man’ + ‘woman’ implies the analogy man:woman::king:queen . But these same embedding analogies also exhibit gender stereotypes. For example Bolukbasi et al. ( 2016 ) find that the closest occupation to ‘computer programmer’ - ‘man’ + ‘woman’ in word2vec embeddings trained on news text is ‘homemaker’, and that the embeddings similarly suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford ( 2017 ) and Blodgett et al. ( 2020 ) call an allocational harm , when a system allallocational harm cates resources (jobs or credit) unfairly to different groups. For example algorithms that use embeddings as part of a search for hiring potential programmers or doctors might thus incorrectly downweight documents with women’s names",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_40"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example algorithms that use embeddings as part of a search for hiring potential programmers or doctors might thus incorrectly downweight documents with women’s names. It turns out that embeddings don’t just reflect the statistics of their input, but also amplify bias; gendered terms become more gendered in embedding space than they bias amplification were in the input text statistics ( Zhao et al. 2017 , Ethayarajh et al. 2019b , Jia et al. 2020 ), and biases are more exaggerated than in actual labor employment statistics ( Garg et al. , 2018 ). Embeddings also encode the implicit associations that are a property of human reasoning. The Implicit Association Test ( Greenwald et al. , 1998 ) measures pe 6.12 • E VALUATING V ECTOR M ODELS 27 ple’s associations between concepts (like ‘flowers’ or ‘insects’) and attributes (like ‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with which they label words in the various categories. 7 Using such methods, people in the United States have been shown to associate African-American names with unpleasant words (more than European-American names), male names more with mathematics and female names with the arts, and old people’s names with unpleaant words ( Greenwald et al. 1998 , Nosek et al. 2002a , Nosek et al. 2002b ). Caliskan et al. ( 2017 ) replicated all these findings of implicit associations using GloVe vectors and cosine similarity instead of human latencies. For example African-American names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine with pleasant words. These problems with embeddings are an example of a reprsentational harm ( Crawford 2017 , Blodgett et al. 2020 ), which is a harm caused by representational harm a system demeaning or even ignoring some social groups. Any embedding-aware agorithm that made use of word sentiment could thus exacerbate bias against African Americans",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_41"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Any embedding-aware agorithm that made use of word sentiment could thus exacerbate bias against African Americans. Recent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes geder stereotypes but preserves definitional gender ( Bolukbasi et al. 2016 , Zhao et al. 2017 ) or changing the training procedure ( Zhao et al. , 2018 ). However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it debiasing ( Gonen and Goldberg , 2019 ), and this remains an open problem. Historical embeddings are also being used to measure biases in the past. Garg et al. ( 2018 ) used embeddings from historical texts to measure the association btween embeddings for occupations and embeddings for names of various ethnicties or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replcated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than fmale words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_42"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We return in later chapters to this question about the role of bias in natural language processing. 6.12 Evaluating Vector Models The most important evaluation metric for vector models is extrinsic evaluation on tasks, i.e., using vectors in an NLP task and seeing whether this improves perfomance over some other model. 7 Roughly speaking, if humans associate ‘flowers’ with ‘pleasantness’ and ‘insects’ with ‘unpleasanness’, when they are instructed to push a green button for ‘flowers’ (daisy, iris, lilac) and ‘pleasant words’ (love, laughter, pleasure) and a red button for ‘insects’ (flea, spider, mosquito) and ‘unpleasant words’ (abuse, hatred, ugly) they are faster than in an incongruous condition where they push a red button for ‘flowers’ and ‘unpleasant words’ and a green button for ‘insects’ and ‘pleasant words’. 28 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity , computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 ( Finkelstein et al. , 2002 ) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example ( plane , car ) had an average score of 5.77. SimLex-999 ( Hill et al. , 2015 ) is a more complex dataset that quantifies similarity ( cup, mug ) rather than relatedness ( cup, coffee ), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated ( Landauer and Dumais , 1997 ). All of these datasets present words without context. Slightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset ( Huang et al",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_43"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". All of these datasets present words without context. Slightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset ( Huang et al. , 2012 ) and the Word-in-Context (WiC) dataset ( Pilehvar and Camacho-Collados , 2019 ) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Appendix G. The semantic textual similarity task ( Agirre et al. 2012 , Agirre et al. 2015 ) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores. Another task used for evaluation is the analogy task, discussed on page 24 , where the system has to solve problems of the form a is to b as a* is to b* , given a, b, and a* and having to find b* ( Turney and Littman , 2005 ). A number of sets of tuples have been created for this task ( Mikolov et al. 2013a , Mikolov et al. 2013c , Gladkova et al. 2016 ), covering morphology ( city:cities::child:children ), lexicographic reltions ( leg:table::spout:teapot ) and encyclopedia relations ( Beijing:China::Dublin:Ireland ), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations ( Jugens et al. , 2012 ). All embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and idividual documents in a collection may strongly impact the resulting embeddings ( Tian et al. 2016 , Hellrich and Hahn 2016 , Antoniak and Mimno 2018 ). When ebeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results ( Antoniak and Mimno , 2018 )",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_44"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6.13 Summary • In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding . In this chapter we focus on static embedings , where each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense . In sparse models each dimension corresponds to a word in the vocabulary V and cells are functions of co-occurrence counts . The term-document matrix has a row for each word ( term ) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in B IBLIOGRAPHICAL AND H ISTORICAL N OTES 29 the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency , and PPMI (poinwise positive mutual information), which is most common for word-context matrices. • Dense vector models have dimensionality 50–1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are ‘likely to occur nearby in text’. This probability is computed from the dot product between the embeddings for the two words. • Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words. • Other important embedding algorithms include GloVe , a method based on ratios of word co-occurrence probabilities. • Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors—a normalized dot product—is the most popular such metric",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_45"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The cosine of two vectors—a normalized dot product—is the most popular such metric. Bibliographical and Historical Notes The idea of vector semantics arose out of research in the 1950s in three distinct fields: linguistics, psychology, and computer science, each of which contributed a fundamental aspect of the model. The idea that meaning is related to the distribution of words in context was widespread in linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos ( 1950 ) put it, the linguist’s “meaning” of a morphemeis by definition the set of conditional probabilities of its occurrence in context with all other morphemes. The idea that the meaning of a word might be modeled as a point in a multdimensional semantic space came from psychologists like Charles E. Osgood, who had been studying how people responded to the meaning of words by assigning vaues along scales like happy/sad or hard/soft . Osgood et al. ( 1957 ) proposed that the meaning of a word in general could be modeled as a point in a multidimensional Euclidean space, and that the similarity of meaning between two words could be modeled as the distance between these points in the space. A final intellectual source in the 1950s and early 1960s was the field then called mechanical indexing , now known as information retrieval . In what became known mechanical indexing as the vector space model for information retrieval ( Salton 1971 , Sparck Jones 1986 ), researchers demonstrated new ways to define the meaning of words in terms of vectors ( Switzer , 1965 ), and refined methods for word similarity based on mesures of statistical association between words like mutual information ( Giuliano , 1965 ) and idf ( Sparck Jones , 1972 ), and showed that the meaning of documents could be represented in the same vector spaces used for words",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_46"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Around the same time, ( Cordier , 1965 ) showed that factor analysis of word association probabilities could be used to form dense vector representations of words. 30 C HAPTER 6 • V ECTOR S EMANTICS AND E MBEDDINGS Some of the philosophical underpinning of the distributional way of thinking came from the late writings of the philosopher Wittgenstein, who was skeptical of the possibility of building a completely formal theory of meaning definitions for each word. Wittgenstein suggested instead that “the meaning of a word is its use in the language” ( Wittgenstein , 1953 , PI 43). That is, instead of using some logical laguage to define each word, or drawing on denotations or truth values, Wittgenstein’s idea is that we should define a word by how it is used by people in speaking and uderstanding in their day-to-day interactions, thus prefiguring the movement toward embodied and experiential models in linguistics and NLP ( Glenberg and Robertson 2000 , Lake and Murphy 2021 , Bisk et al. 2020 , Bender and Koller 2020 ). More distantly related is the idea of defining words by a vector of discrete fetures, which has roots at least as far back as Descartes and Leibniz ( Wierzbicka 1992 , Wierzbicka 1996 ). By the middle of the 20th century, beginning with the work of Hjelmslev ( Hjelmslev , 1969 ) (originally 1943) and fleshed out in early models of generative grammar ( Katz and Fodor , 1963 ), the idea arose of representing meaing with semantic features , symbols that represent some sort of primitive meaning. semantic feature For example words like hen , rooster , or chick , have something in common (they all describe chickens) and something different (their age and sex), representable as: hen +female, +chicken, +adult rooster -female, +chicken, +adult chick +chicken, -adult The dimensions used by vector models of meaning to define words, however, are only abstractly related to this idea of a small fixed number of hand-built dimensions",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_47"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Nonetheless, there has been some attempt to show that certain dimensions of ebedding models do contribute some specific compositional aspect of meaning like these early semantic features. The use of dense vectors to model word meaning, and indeed the term embeding , grew out of the latent semantic indexing (LSI) model ( Deerwester et al. , 1988 ) recast as LSA ( latent semantic analysis ) ( Deerwester et al. , 1990 ). In LSA singular value decomposition — SVD — is applied to a term-document matrix (each SVD cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimesions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais ( 1997 ), and for tasks like spell checking ( Jones and Martin , 1997 ), language modeling ( Bellegarda 1997 , Coccaro and Jrafsky 1998 , Bellegarda 2000 ), morphology induction ( Schone and Jurafsky 2000 , Schone and Jurafsky 2001b ), multiword expressions (MWEs) ( Schone and Jurafsky , 2001a ), and essay grading ( Rehder et al. , 1998 ). Related models were simultanously developed and applied to word sense disambiguation by Sch ̈utze ( 1992 ). LSA also led to the earliest use of embeddings to represent words in a probabilistic clasifier, in the logistic regression document router of Sch ̈utze et al. ( 1995 ). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Sch ̈utze ( 1992 ). Sch ̈utze applied the low-rank (97-dimensional) embeddings produced by SVD to the task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Sch ̈utze ( 1997 )",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_48"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". See Sch ̈utze ( 1997 ). A number of alternative matrix models followed on from the early SVD work, including Probabilistic Latent Semantic Indexing (PLSI) ( Hofmann , 1999 ), Latent Dirichlet Allocation (LDA) ( Blei et al. , 2003 ), and Non-negative Matrix Factoriz E XERCISES 31 tion (NMF) ( Lee and Seung , 1999 ). The LSA community seems to have first used the word “embedding” in Landauer et al. ( 1997 ), in a variant of its mathematical meaning as a mapping from one space or mathematical structure to another. In LSA, the word embedding seems to have described the mapping from the space of sparse count vectors to the latent space of SVD dense vectors. Although the word thus originally meant the mapping from one space to another, it has metonymically shifted to mean the resulting dense vector in the latent space, and it is in this sense that we currently use the word. By the next decade, Bengio et al. ( 2003 ) and Bengio et al. ( 2006 ) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston ( 2007 ), Collobert and Weston ( 2008 ), and Collobert et al. ( 2011 ) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. ( 2010 ) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. ( 2011 ) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skigram (and also CBOW) algorithms was proposed by Mikolov et al. ( 2013a ). The negative sampling training algorithm was proposed in Mikolov et al. ( 2013b ). There are numerous surveys of static embeddings and their parameterizations ( Bullinaria and Levy 2007 , Bullinaria and Levy 2012 , Lapesa and Evert 2014 , Kiela and Clark 2014 , Levy et al. 2015 ). See Manning et al",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_49"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 2015 ). See Manning et al. ( 2008 ) and Chapter 14 for a deeper understanding of the role of vectors in information retrieval, including how to compare queries with docments, more details on tf-idf, and issues of scaling to very large datasets. See Kim ( 2019 ) for a clear and comprehensive tutorial on word2vec. Cruse ( 2004 ) is a useful introductory linguistic text on lexical semantics. Exercises 32 Chapter 6 • Vector Semantics and Embeddings Agirre, E., C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Maitxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability . SemEva15 . Agirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual simlarity . SemEval-12 . Antoniak, M. and D. Mimno. 2018. Evaluating the stability of embedding-based word similarities . TACL , 6:107–119. Bellegarda, J. R. 1997. A latent semantic analysis framework for large-span language modeling . EUROSPEECH . Bellegarda, J. R. 2000. Exploiting latent semantic informtion in statistical language modeling . Proceedings of the IEEE , 89(8):1279–1296. Bender, E. M. and A. Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data . ACL . Bengio, Y., A. Courville, and P. Vincent. 2013. Represetation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intellgence , 35(8):1798–1828. Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model . JMLR , 3:1137– 1155. Bengio, Y., H. Schwenk, J.-S. Sen ́ecal, F. Morin, and J.-L. Gauvain. 2006. Neural probabilistic language models . In Innovations in Machine Learning , 137–186. Springer. Bisk, Y., A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian. 2020. Experience grounds laguage . EMNLP . Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_50"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian. 2020. Experience grounds laguage . EMNLP . Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Diriclet allocation. JMLR , 3(5):993–1022. Blodgett, S. L., S. Barocas, H. Daum ́e III, and H. Wallach. 2020. Language (technology) is power: A critical survey of “bias” in NLP . ACL . Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. Enriching word vectors with subword information . TACL , 5:135–146. Bolukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T. Kalai. 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. NeurIPS . Br ́eal, M. 1897. Essai de S ́emantique: Science des significtions . Hachette. Budanitsky, A. and G. Hirst. 2006. Evaluating WordNebased measures of lexical semantic relatedness . Comptational Linguistics , 32(1):13–47. Bullinaria, J. A. and J. P. Levy. 2007. Extracting sematic representations from word co-occurrence statistics: A computational study. Behavior research methods , 39(3):510–526. Bullinaria, J. A. and J. P. Levy. 2012. Extracting semantic representations from word co-occurrence statistics: stolists, stemming, and SVD. Behavior research methods , 44(3):890–907. Caliskan, A., J. J. Bryson, and A. Narayanan. 2017. Sematics derived automatically from language corpora contain human-like biases . Science , 356(6334):183–186. Carlson, G. N. 1977. Reference to kinds in English . Ph.D. thesis, University of Massachusetts, Amherst. Forward. Church, K. W. and P. Hanks. 1989. Word association norms, mutual information, and lexicography . ACL . Church, K. W. and P. Hanks. 1990. Word association norms, mutual information, and lexicography . Computational Linguistics , 16(1):22–29. Clark, E. 1987. The principle of contrast: A constraint on language acquisition. In B. MacWhinney, ed., Mechnisms of language acquisition , 1–33. LEA. Coccaro, N. and D. Jurafsky. 1998. Towards better integrtion of semantic predictors in statistical language modeing",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_51"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In B. MacWhinney, ed., Mechnisms of language acquisition , 1–33. LEA. Coccaro, N. and D. Jurafsky. 1998. Towards better integrtion of semantic predictors in statistical language modeing . ICSLP . Collobert, R. and J. Weston. 2007. Fast semantic extraction using a novel neural network architecture . ACL . Collobert, R. and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning . ICML . Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch . JMLR , 12:2493–2537. Cordier, B. 1965. Factor-analysis of correspondences . COING 1965 . Crawford, K. 2017. The trouble with bias. Keynote at NeurIPS. Cruse, D. A. 2004. Meaning in Language: an Introduction to Semantics and Pragmatics . Oxford University Press. Second edition. Dagan, I., S. Marcus, and S. Markovitch. 1993. Contextual word similarity and estimation from sparse data . ACL . Davies, M. 2012. Expanding horizons in historical liguistics with the 400-million word Corpus of Historical American English. Corpora , 7(2):121–157. Davies, M. 2015. The Wikipedia Corpus: 4.6 million artcles, 1.9 billion words. Adapted from Wikipedia. https: //www.english-corpora.org/wiki/ . Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harsman, T. K. Landauer, K. E. Lochbaum, and L. Streeter. 1988. Computer information retrieval using latent sematic structure: US Patent 4,839,853. Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Funas, and R. A. Harshman. 1990. Indexing by latent smantics analysis. JASIS , 41(6):391–407. Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards understanding linear word analogies . ACL . Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Undestanding undesirable word embedding associations . ACL . Fano, R. M. 1961. Transmission of Information: A Statistical Theory of Communications . MIT Press. Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_52"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Fano, R. M. 1961. Transmission of Information: A Statistical Theory of Communications . MIT Press. Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Tranactions on Information Systems , 20(1):116—-131. Firth, J. R. 1957. A synopsis of linguistic theory 1930– 1955. In Studies in Linguistic Analysis . Philological Sciety. Reprinted in Palmer, F. (ed.) 1968. Selected Papers of J. R. Firth. Longman, Harlow. Exercises 33 Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018. Word embeddings quantify 100 years of gender and etnic stereotypes . Proceedings of the National Academy of Sciences , 115(16):E3635–E3644. Girard, G. 1718. La justesse de la langue franc ̧oise: ou les diff ́erentes significations des mots qui passent pour syonimes . Laurent d’Houry, Paris. Giuliano, V. E. 1965. The interpretation of word associations. Statistical Association Methods For Mechanized Documentation. Symposium Proceeings. Washington, D.C., USA, March 17, 1964 . https://nvlpubs.nist.gov/nistpubs/Legacy/ MP/nbsmiscellaneouspub269.pdf . Gladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogbased detection of morphological and semantic relations with word embeddings: what works and what doesn’t . NAACL Student Research Workshop . Glenberg, A. M. and D. A. Robertson. 2000. Symbol grouning and meaning: A comparison of high-dimensional and embodied theories of meaning . Journal of memory and language , 43(3):379–401. Gonen, H. and Y. Goldberg. 2019. Lipstick on a pig: Debasing methods cover up systematic gender biases in word embeddings but do not remove them . NAACL HLT . Gould, S. J. 1980. The Panda’s Thumb . Penguin Group. Greenwald, A. G., D. E. McGhee, and J. L. K. Schwartz. 1998. Measuring individual differences in implicit cogntion: the implicit association test. Journal of personality and social psychology , 74(6):1464–1480. Hamilton, W. L., J. Leskovec, and D. Jurafsky. 2016",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_53"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Journal of personality and social psychology , 74(6):1464–1480. Hamilton, W. L., J. Leskovec, and D. Jurafsky. 2016. Dachronic word embeddings reveal statistical laws of smantic change . ACL . Harris, Z. S. 1954. Distributional structure . Word , 10:146– 162. Hellrich, J. and U. Hahn. 2016. Bad company— Neighborhoods in neural embedding spaces considered harmful . COLING . Hill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity etimation . Computational Linguistics , 41(4):665–695. Hjelmslev, L. 1969. Prologomena to a Theory of Language . University of Wisconsin Press. Translated by Francis J. Whitfield; original Danish edition 1943. Hofmann, T. 1999. Probabilistic latent semantic indexing. SIGIR-99 . Huang, E. H., R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving word representations via global context and multiple word prototypes . ACL . Jia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigaing gender bias amplification in distribution by posterior regularization . ACL . Jones, M. P. and J. H. Martin. 1997. Contextual spelling corection using latent semantic analysis . ANLP . Joos, M. 1950. Description of language design. JASA , 22:701–708. Jurafsky, D. 2014. The Language of Food . W. W. Norton, New York. Jurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak. 2012. SemEval-2012 task 2: Measuring degrees of reltional similarity . *SEM 2012 . Katz, J. J. and J. A. Fodor. 1963. The structure of a semantic theory. Language , 39:170–210. Kiela, D. and S. Clark. 2014. A systematic study of semantic vector space model parameters. EACL 2nd Workshop on Continuous Vector Space Models and their Compositioality (CVSC) . Kim, E. 2019. Optimize computational efficiency of skip-gram with negative sampling. https:// aegis4048.github.io/optimize_computational_ efficiency_of_skip-gram_with_negative_ sampling . Lake, B. M. and G. L. Murphy. 2021. Word meaning in minds and machines. Psychological Review . In press. Landauer, T. K. and S. T. Dumais. 1997",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_54"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Lake, B. M. and G. L. Murphy. 2021. Word meaning in minds and machines. Psychological Review . In press. Landauer, T. K. and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acqusition, induction, and representation of knowledge. Pschological Review , 104:211–240. Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner. 1997. How well can passage meaning be derived witout using word order? A comparison of Latent Semantic Analysis and humans. COGSCI . Lapesa, G. and S. Evert. 2014. A large scale evaluation of distributional semantic models: Parameters, interactions and model selection . TACL , 2:531–545. Lee, D. D. and H. S. Seung. 1999. Learning the parts of objects by non-negative matrix factorization. Nature , 401(6755):788–791. Levy, O. and Y. Goldberg. 2014a. Dependency-based word embeddings . ACL . Levy, O. and Y. Goldberg. 2014b. Linguistic regularities in sparse and explicit word representations . CoNLL . Levy, O. and Y. Goldberg. 2014c. Neural word embedding as implicit matrix factorization. NeurIPS . Levy, O., Y. Goldberg, and I. Dagan. 2015. Improving ditributional similarity with lessons learned from word ebeddings . TACL , 3:211–225. Li, J., X. Chen, E. H. Hovy, and D. Jurafsky. 2015. Visuaizing and understanding neural models in NLP . NAACL HLT . Lin, Y., J.-B. Michel, E. Lieberman Aiden, J. Orwant, W. Brockman, and S. Petrov. 2012. Syntactic annotations for the Google Books NGram corpus . ACL . Linzen, T. 2016. Issues in evaluating semantic spaces uing word analogies . 1st Workshop on Evaluating VectoSpace Representations for NLP . Luhn, H. P. 1957. A statistical approach to the mechanized encoding and searching of literary information. IBM Journal of Research and Development , 1(4):309–317. Manning, C. D., P. Raghavan, and H. Sch ̈utze. 2008. Intrduction to Information Retrieval . Cambridge. Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Eficient estimation of word representations in vector space. ICLR 2013 . Mikolov, T., S. Kombrink, L",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_55"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Cambridge. Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Eficient estimation of word representations in vector space. ICLR 2013 . Mikolov, T., S. Kombrink, L. Burget, J. H. ˇ Cernock`y, and S. Khudanpur. 2011. Extensions of recurrent neural nework language model. ICASSP . 34 Chapter 6 • Vector Semantics and Embeddings Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013b. Distributed representations of words and phrases and their compositionality . NeurIPS . Mikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguitic regularities in continuous space word representations . NAACL HLT . Niwa, Y. and Y. Nitta. 1994. Co-occurrence vectors from corpora vs. distance vectors from dictionaries . COLING . Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a. Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics: Theory, Rsearch, and Practice , 6(1):101. Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b. Math=male, me=female, therefore math ̸ = me. Journal of personality and social psychology , 83(1):44. Osgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The Measurement of Meaning . University of Illinois Press. Pennington, J., R. Socher, and C. D. Manning. 2014. GloVe: Global vectors for word representation . EMNLP . Peterson, J. C., D. Chen, and T. L. Griffiths. 2020. Parallelgrams revisited: Exploring the limitations of vector space models for simple analogies. Cognition , 205. Pilehvar, M. T. and J. Camacho-Collados. 2019. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations . NAACL HLT . Rehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham, T. K. Landauer, and W. Kintsch. 1998. Using Latent Semantic Analysis to assess knowledge: Some technical considerations . Discourse Processes , 25(2-3):337–354. Rohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006. An improved model of semantic similarity based on lexcal co-occurrence. CACM , 8:627–633. Rumelhart, D. E. and A. A. Abrahamson. 1973",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_56"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Rohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006. An improved model of semantic similarity based on lexcal co-occurrence. CACM , 8:627–633. Rumelhart, D. E. and A. A. Abrahamson. 1973. A model for analogical reasoning. Cognitive Psychology , 5(1):1–28. Salton, G. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing . Prentice Hall. Schluter, N. 2018. The word analogy testing caveat . NAACL HLT . Schone, P. and D. Jurafsky. 2000. Knowlege-free induction of morphology using latent semantic analysis . CoNLL . Schone, P. and D. Jurafsky. 2001a. Is knowledge-free iduction of multiword unit dictionary headwords a solved problem? EMNLP . Schone, P. and D. Jurafsky. 2001b. Knowledge-free indution of inflectional morphologies . NAACL . Sch ̈utze, H. 1992. Dimensions of meaning . Proceedings of Supercomputing ’92 . IEEE Press. Sch ̈utze, H. 1997. Ambiguity Resolution in Language Learing – Computational and Cognitive Models . CSLI, Staford, CA. Sch ̈utze, H., D. A. Hull, and J. Pedersen. 1995. A compaison of classifiers and document representations for the routing problem . SIGIR-95 . Sch ̈utze, H. and J. Pedersen. 1993. A vector model for sytagmatic and paradigmatic relatedness. 9th Annual Coference of the UW Centre for the New OED and Text Rsearch . Sparck Jones, K. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Doumentation , 28(1):11–21. Sparck Jones, K. 1986. Synonymy and Semantic Classifiction . Edinburgh University Press, Edinburgh. Republiction of 1964 PhD Thesis. Switzer, P. 1965. Vector images in document retrieval. Statistical Association Methods For Mechanized Docmentation. Symposium Proceedings. Washington, D.C., USA, March 17, 1964 . https://nvlpubs.nist.gov/ nistpubs/Legacy/MP/nbsmiscellaneouspub269. pdf . Tian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On the convergent properties of word embedding methods. ArXiv preprint arXiv:1605.03956. Turian, J., L. Ratinov, and Y. Bengio. 2010",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_57"
  },
  {
    "document_type": "book",
    "title": "Vector Semantics and Embeddings - chapter 6",
    "author": "Daniel Jurafsky & James H. Martin.",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\jurafsky_vector_sematics_chapter_6.pdf",
    "date_published": "2025-01-12",
    "keywords": "",
    "flag": "",
    "chunk_text": ". pdf . Tian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On the convergent properties of word embedding methods. ArXiv preprint arXiv:1605.03956. Turian, J., L. Ratinov, and Y. Bengio. 2010. Word represetations: a simple and general method for semi-supervised learning . ACL . Turney, P. D. and M. L. Littman. 2005. Corpus-based learing of analogies and semantic relations. Machine Learing , 60(1-3):251–278. van der Maaten, L. and G. E. Hinton. 2008. Visualizing higdimensional data using t-SNE . JMLR , 9:2579–2605. Wierzbicka, A. 1992. Semantics, Culture, and Cognition: University Human Concepts in Culture-Specific Configrations . Oxford University Press. Wierzbicka, A. 1996. Semantics: Primes and Universals . Oxford University Press. Wittgenstein, L. 1953. Philosophical Investigations. (Tranlated by Anscombe, G.E.M.) . Blackwell. Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.- W. Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints . EMNLP . Zhao, J., Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018. Learning gender-neutral word embeddings . EMNLP .",
    "chunk_id": "Natural_language_processing_jurafsky_vector_sematics_chapter_6.json_chunk_58"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "Language Models are Few-Shot Learners Tom B. Brown ∗ Benjamin Mann ∗ Nick Ryder ∗ Melanie Subbiah ∗ Jared Kaplan † Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell Sandhini Agarwal Ariel Herbert-Voss Gretchen Krueger Tom Henighan Rewon Child Aditya Ramesh Daniel M. Ziegler Jeffrey Wu Clemens Winter Christopher Hesse Mark Chen Eric Sigler Mateusz Litwin Scott Gray Benjamin Chess Jack Clark Christopher Berner Sam McCandlish Alec Radford Ilya Sutskever Dario Amodei OpenAI Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fintuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ∗ Equal contribution † Johns Hopkins University, OpenAI Author contributions listed at end of paper . arXiv:2005.14165v4 [cs.CL] 22 Jul 2020 Contents 1 Introduction 3 2 Approach 6 2.1 Model and Architectures 8 2.2 Training Dataset 8 2.3 Training Process 9 2.4 Evaluation 10 3 Results 10 3.1 Language Modeling, Cloze, and Completion Tasks 11 3.2 Closed Book Question Answering 13 3.3 Translation 14 3.4 Winograd-Style Tasks 16 3.5 Common Sense Reasoning 17 3.6 Reading Comprehension 18 3.7 SuperGLUE 18 3.8 NLI 20 3.9 Synthetic and Qualitative Tasks 21 4 Measuring and Preventing Memorization Of Benchmarks 29 5 Limitations 33 6 Broader Impacts 34 6.1 Misuse of Language Models 35 6.2 Fairness, Bias, and Representation 36 6.3 Energy Usage 39 7 Related Work 39 8 Conclusion 40 A Details of Common Crawl Filtering 43 B Details of Model Training 43 C Details of Test Set Contamination Studies 43 D Total Compute Used to Train Language Models 46 E Human Quality Assessment of Synthetic News Articles 46 F Additional Samples from GPT-3 48 G Details of Task Phrasing and Specifications 50 H Results on All Tasks for All Model Sizes 63 2 1 Introduction Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". First, single-layer representations were learned using word vectors [ MCCD13 , PSM14 ] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [ DL15 , MBXS17 , PNZtY18 ] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [ VSP + 17 ] have been directly fine-tuned, entirely removing the need for task-specific architectures [ RNSS18 , DCLT18 , HR18 ]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [ RSR + 19 , LOG + 19 , YDY + 19 , LCG + 19 ]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons. First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the applicability of language models. There exists a very wide range of possible useful language tasks, encompassing anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated for every new task. Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance [ HLW + 20 ] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [ YdC + 19 , MPL19 ]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [ GSL + 18 , NK19 ]. Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence. 3 Figure 1.2: Larger models make increasingly efficient use of in-context information",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 3 Figure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2 ). The steeper “in-context learning curves” for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks. sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality. One potential route towards addressing these issues is meta-learning 1 – which in the context of language models means the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1 ). Recent work [ RWC + 19 ] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next. While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example [ RWC + 19 ] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Meta-learning clearly requires substantial improvement in order to be viable as a practical method of solving language tasks. Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer language models has increased substantially, from 100 million parameters [ RNSS18 ], to 300 million parameters [ DCLT18 ], to 1.5 billion parameters [ RWC + 19 ], to 8 billion parameters [ SPP + 19 ], 11 billion parameters [ RSR + 19 ], and finally 17 billion parameters [ Tur20 ]. Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [ KMH + 20 ]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1 In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous: the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning” to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer loop structure",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP benchmark suite. In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work. Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K . Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning. Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art relative to fine-tuned models operating in the same closed-book setting. GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning, which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human evaluators have difficulty distinguishing from human-generated articles. At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed. A heuristic sense of the overall results can be seen in Figure 1.3 , which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A heuristic sense of the overall results can be seen in Figure 1.3 , which aggregates the various tasks (though it should not be seen as a rigorous or meaningful benchmark in itself). 5 We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these datasets or we note them with an asterisk, depending on the severity. In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2 , we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, onand few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach Our basic pre-training approach, including model, data, and training, is similar to the process described in [ RWC + 19 ], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [ RWC + 19 ], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): • Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used. The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [ MPL19 ], and the potential to exploit spurious features of the training data [ GSL + 18 , NK19 ], potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. • Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [ RWC + 19 ], but no weight updates are allowed",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As shown in Figure 2.1 , for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can fit in the model’s context window ( n ctx = 2048 ). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [ HYC01 , VBL + 16 ] – both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task. • One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans. For example, when asking humans to generate a dataset on a human worker service (for example Mechanical Turk), it is common to give one demonstration of the task. By contrast it is sometimes difficult to communicate the content or format of a task if no examples are given. 6 Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 6 Figure 2.1: Zero-shot, one-shot and few-shot, contrasted with traditional fine-tuning . The panels above show four methods for performing a task with a language model – fine-tuning is the traditional method, whereas zero-, one-, and few-shot, which we study in this work, require the model to perform the task with only forward passes at test time. We typically present the model with a few dozen examples in the few shot setting. Exact phrasings for all task descriptions, examples and prompts can be found in Appendix G . • Zero-Shot (0S) is the same as one-shot except that no demonstrations are allowed, and the model is only given a natural language instruction describing the task. This method provides maximum convenience, potential for robustness, and avoidance of spurious correlations (unless they occur very broadly across the large corpus of pre-training data), but is also the most challenging setting. In some cases it may even be difficult for humans to understand the format of the task without prior examples, so this setting is in some cases “unfairly hard”. For example, if someone is asked to “make a table of world records for the 200m dash”, this request can be ambiguous, as it may not be clear exactly what format the table should have or what should be included (and even with careful clarification, understanding precisely what is desired can be difficult). Nevertheless, for at least some settings zero-shot is closest to how humans perform tasks – for example, in the translation example in Figure 2.1 , a human would likely know what to do from just the text instruction. Figure 2.1 shows the four methods using the example of translating English to French. In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1 - 2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 7 Model Name n params n layers d model n heads d head Batch Size Learning Rate GPT-3 Small 125M 12 768 12 64 0.5M 6 . 0 × 10 − 4 GPT-3 Medium 350M 24 1024 16 64 0.5M 3 . 0 × 10 − 4 GPT-3 Large 760M 24 1536 16 96 0.5M 2 . 5 × 10 − 4 GPT-3 XL 1.3B 24 2048 24 128 1M 2 . 0 × 10 − 4 GPT-3 2.7B 2.7B 32 2560 32 80 1M 1 . 6 × 10 − 4 GPT-3 6.7B 6.7B 32 4096 32 128 2M 1 . 2 × 10 − 4 GPT-3 13B 13.0B 40 5140 40 128 2M 1 . 0 × 10 − 4 GPT-3 175B or “GPT-3” 175.0B 96 12288 96 128 3.2M 0 . 6 × 10 − 4 Table 2.1: Sizes, architectures, and learning hyper-parameters (batch size in tokens and learning rate) of the models which we trained. All models were trained for a total of 300 billion tokens. 2.1 Model and Architectures We use the same model and architecture as GPT-2 [ RWC + 19 ], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Previous work [ KMH + 20 ] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here n params is the total number of trainable parameters, n layers is the total number of layers, d model is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, d ff = 4 ∗ d model ), and d head is the dimension of each attention head. All models use a context window of n ctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU’s. Previous work [ KMH + 20 ] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range. 2.2 Training Dataset Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset 2 [ RSR + 19 ] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the first two points (processing of Common Crawl) are described in Appendix A . For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [ RWC + 19 ], collected by scraping links over a longer period of time, and first described in [ KMH + 20 ], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the final mixture of datasets that we used in training. The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. Note that during training, datasets are not sampled in proportion to their size, but rather datasets we view as higher-quality are sampled more frequently, such that CommonCrawl and Books2 datasets are sampled less than once during training, but the other datasets are sampled 2-3 times. This essentially accepts a small amount of overfitting in exchange for higher quality training data. 2 https://commoncrawl.org/the-data/ 8 Figure 2.2: Total compute used during training . Based on the analysis in Scaling Laws For Neural Language Models [ KMH + 20 ] we train much larger models on many fewer tokens than is typical",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Based on the analysis in Scaling Laws For Neural Language Models [ KMH + 20 ] we train much larger models on many fewer tokens than is typical. As a consequence, although GPT-3 3B is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaflop/s-days of compute during pre-training. Methodology for these calculations can be found in Appendix D . Dataset Quantity (tokens) Weight in training mix Epochs elapsed when training for 300B tokens Common Crawl (filtered) 410 billion 60% 0.44 WebText2 19 billion 22% 2.9 Books1 12 billion 8% 1.9 Books2 55 billion 8% 0.43 Wikipedia 3 billion 3% 3.4 Table 2.2: Datasets used to train GPT-3 . “Weight in training mix” refers to the fraction of examples during training that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets are seen less than once. A major methodological concern with language models pretrained on a broad swath of internet data, particularly large models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug in the filtering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will more aggressively remove data contamination. 2.3 Training Process As found in [ KMH + 20 , MKAT18 ], larger models can typically use a larger batch size, but require a smaller learning rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [ MKAT18 ]",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We measure the gradient noise scale during training and use it to guide our choice of batch size [ MKAT18 ]. Table 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B . 9 2.4 Evaluation For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze there is no supervised training set available so we draw conditioning examples from the development set and evaluate on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning examples directly from it. K can be any value from 0 to the maximum amount allowed by the model’s context window, which is n ctx = 2048 for all models and typically fits 10 to 100 examples. Larger values of K are usually but not always better, so when a separate development and test set are available, we experiment with a few values of K on the development set and then run the best value on the test set. For some tasks (see Appendix G ) we also use a natural language prompt in addition to (or for K = 0 , instead of) demonstrations. On tasks that involve choosing one correct completion from several options (multiple choice), we provide K examples of context plus correct completion, followed by one example of context only, and compare the LM likelihood of each completion",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For most tasks we compare the per-token likelihood (to normalize for length), however on a small number of datasets (ARC, OpenBookQA, and RACE) we gain additional benefit as measured on the development set by normalizing by the unconditional probability of each completion, by computing P (completion | context) P (completion | answer context) , where answer context is the string \"Answer: \" or \"A: \" and is used to prompt that the completion should be an answer but is otherwise generic. On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [ RSR + 19 ] (see Appendix G ) for details. On tasks with free-form completion, we use beam search with the same parameters as [ RSR + 19 ]: a beam width of 4 and a length penalty of α = 0 . 6 . We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results In Figure 3.1 we display training curves for the 8 models described in Section 2 . For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [ KMH + 20 ], language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks. In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling, such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question answering tasks: tasks which require using the information stored in the model’s parameters to answer general knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we briefly explore NLI. Finally, in Section 3.9 , we invent some additional tasks designed especially to probe in-context learning abilities – these tasks focus on on-the-fly reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings. 10 Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 10 Figure 3.1: Smooth scaling of performance with compute. Performance (measured in terms of cross-entropy validation loss) follows a power-law trend with the amount of compute used for training. The power-law behavior observed in [ KMH + 20 ] continues for an additional two orders of magnitude with only small deviations from the predicted curve. For this figure, we exclude embedding parameters from compute and parameter counts. Setting PTB SOTA (Zero-Shot) 35.8 a GPT-3 Zero-Shot 20.5 Table 3.1: Zero-shot results on PTB language modeling dataset. Many other common language modeling datasets are omitted because they are derived from Wikipedia or other sources which are included in GPT-3’s training data. a [ RWC + 19 ] 3.1 Language Modeling, Cloze, and Completion Tasks In this section we test GPT-3’s performance on the traditional task of language modeling, as well as related tasks that involve predicting a single word of interest, completing a sentence or paragraph, or choosing between possible completions of a piece of text. 3.1.1 Language Modeling We calculate zero-shot perplexity on the Penn Tree Bank (PTB) [ MKM + 94 ] dataset measured in [ RWC + 19 ]. We omit the 4 Wikipedia-related tasks in that work because they are entirely contained in our training data, and we also omit the one-billion word benchmark due to a high fraction of the dataset being contained in our training set. PTB escapes these issues due to predating the modern internet. Our largest model sets a new SOTA on PTB by a substantial margin of 15 points, achieving a perplexity of 20.50. Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot. 3.1.2 LAMBADA The LAMBADA dataset [ PKL + 16 ] tests the modeling of long-range dependencies in text – the model is asked to predict the last word of sentences which require reading a paragraph of context",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark. [ BHT + 20 ] reflect on the small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([ SPP + 19 ] 11 Setting LAMBADA (acc) LAMBADA (ppl) StoryCloze (acc) HellaSwag (acc) SOTA 68.0 a 8.63 b 91.8 c 85.6 d GPT-3 Zero-Shot 76.2 3.00 83.2 78.9 GPT-3 One-Shot 72.5 3.35 84.7 78.1 GPT-3 Few-Shot 86.4 1.92 87.7 79.3 Table 3.2: Performance on cloze and completion tasks. GPT-3 significantly improves SOTA on LAMBADA while achieving respectable performance on two difficult completion prediction datasets. a [ Tur20 ] b [ RWC + 19 ] c [ LDL19 ] d [ LCH + 20 ] Figure 3.2: On LAMBADA, the few-shot capability of language models results in a strong boost to accuracy. GPT-3 2.7B outperforms the SOTA 17B parameter Turing-NLG [ Tur20 ] in this setting, and GPT-3 175B advances the state of the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text. and [ Tur20 ]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path forward”. We find that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art. LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters [ RWC + 19 ] (which ban “continuation” words)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This problem has been partially addressed in the past with stop-word filters [ RWC + 19 ] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format: Alice was friends with Bob. Alice went to visit her friend . → Bob George bought some baseball equipment, a ball, a glove, and a . → When presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy by 10%. Finally, the fill-in-blank method is not effective one-shot, where it always performs worse than the zero-shot setting. Perhaps this is because all models still require several examples to recognize the pattern. 12 Setting NaturalQS WebQS TriviaQA RAG (Fine-tuned, Open-Domain) [ LPP + 20 ] 44.5 45.5 68.0 T5-11B+SSM (Fine-tuned, Closed-Book) [ RRS20 ] 36.6 44.7 60.5 T5-11B (Fine-tuned, Closed-Book) 34.5 37.4 50.1 GPT-3 Zero-Shot 14.6 14.4 64.3 GPT-3 One-Shot 23.0 25.3 68.0 GPT-3 Few-Shot 29.9 41.5 71.2 Table 3.3: Results on three Open-Domain QA tasks. GPT-3 is shown in the few-, one-, and zero-shot settings, as compared to prior SOTA results for closed book and open domain settings. TriviaQA few-shot result is evaluated on the wiki split test server. One note of caution is that an analysis of test set contamination identified that a significant minority of the LAMBADA dataset appears to be present in our training data – however analysis performed in Section 4 suggests negligible impact on performance. 3.1.3 HellaSwag The HellaSwag dataset [ ZHB + 19 ] involves picking the best ending to a story or set of instructions",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 3.1.3 HellaSwag The HellaSwag dataset [ ZHB + 19 ] involves picking the best ending to a story or set of instructions. The examples were adversarially mined to be difficult for language models while remaining easy for humans (who achieve 95.6% accuracy). GPT-3 achieves 78.1% accuracy in the one-shot setting and 79.3% accuracy in the few-shot setting, outperforming the 75.4% accuracy of a fine-tuned 1.5B parameter language model [ ZHR + 19 ] but still a fair amount lower than the overall SOTA of 85.6% achieved by the fine-tuned multi-task model ALUM. 3.1.4 StoryCloze We next evaluate GPT-3 on the StoryCloze 2016 dataset [ MCH + 16 ], which involves selecting the correct ending sentence for five-sentence long stories. Here GPT-3 achieves 83.2% in the zero-shot setting and 87.7% in the few-shot setting (with K = 70 ). This is still 4.1% lower than the fine-tuned SOTA using a BERT based model [ LDL19 ] but improves over previous zero-shot results by roughly 10%. 3.2 Closed Book Question Answering In this section we measure GPT-3’s ability to answer questions about broad factual knowledge. Due to the immense amount of possible queries, this task has normally been approached by using an information retrieval system to find relevant text in combination with a model which learns to generate an answer given the question and the retrieved text. Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted “open-book”. [ RRS20 ] recently demonstrated that a large language model can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as “closed-book”. Their work suggests that even higher-capacity models could perform even better and we test this hypothesis with GPT-3. We evaluate GPT-3 on the 3 datasets in [ RRS20 ]: Natural Questions [ KPR + 19 ], WebQuestions [ BCFL13 ], and TriviaQA [ JCWZ17 ], using the same splits",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We evaluate GPT-3 on the 3 datasets in [ RRS20 ]: Natural Questions [ KPR + 19 ], WebQuestions [ BCFL13 ], and TriviaQA [ JCWZ17 ], using the same splits. Note that in addition to all results being in the closed-book setting, our use of few-shot, one-shot, and zero-shot evaluations represent an even stricter setting than previous closed-book QA work: in addition to external content not being allowed, fine-tuning on the Q&A dataset itself is also not permitted. The results for GPT-3 are shown in Table 3.3 . On TriviaQA, we achieve 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting. The zero-shot result already outperforms the fine-tuned T5-11B by 14.2%, and also outperforms a version with Q&A tailored span prediction during pre-training by 3.8%. The one-shot result improves by 3.7% and matches the SOTA for an open-domain QA system which not only fine-tunes but also makes use of a learned retrieval mechanism over a 15.3B parameter dense vector index of 21M documents [ LPP + 20 ]. GPT-3’s few-shot result further improves performance another 3.2% beyond this. On WebQuestions (WebQs), GPT-3 achieves 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. This compares to 37.4% for fine-tuned T5-11B, and 44.7% for fine-tuned T5-11B+SSM, which uses a Q&A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models. Notably, compared to TriviaQA, WebQS shows a much larger gain from zero-shot to few-shot (and indeed its zero-shot and one-shot performance are poor), perhaps suggesting that the WebQs questions 13 Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One-shot and few-shot performance make significant gains over zero-shot behavior, matching and exceeding the performance of the SOTA fine-tuned open-domain model, RAG [ LPP + 20 ] and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this distribution, recovering strong performance in the few-shot setting. On Natural Questions (NQs) GPT-3 achieves 14.6% in the zero-shot setting, 23.0% in the one-shot setting, and 29.9% in the few-shot setting, compared to 36.6% for fine-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to TriviaQA and WebQS. In particular, the questions in NQs tend towards very fine-grained knowledge on Wikipedia specifically which could be testing the limits of GPT-3’s capacity and broad pretraining distribution. Overall, on one of the three datasets GPT-3’s one-shot matches the open-domain fine-tuning SOTA. On the other two datasets it approaches the performance of the closed-book SOTA despite not using fine-tuning. On all 3 datasets, we find that performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7 ), possibly reflecting the idea that model capacity translates directly to more ‘knowledge’ absorbed in the parameters of the model. 3.3 Translation For GPT-2 a filter was used on a multilingual collection of documents to produce an English only dataset due to capacity concerns. Even with this filtering GPT-2 showed some evidence of multilingual capability and performed non-trivially when translating between French and English despite only training on 10 megabytes of remaining French text. Since we increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training dataset to include more representation of other languages, though this remains an area for further improvement",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based filtering. Although GPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages. These languages are documented in the supplemental material . In order to better understand translation capability, we also expand our analysis to include two additional commonly studied languages, German and Romanian. Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets with back-translation [ SHB15 ] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a blend of training data that mixes many languages together in a natural way, combining them on a word, sentence, and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in particular. However, our one / few-shot settings aren’t strictly comparable to prior unsupervised work since they make use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data. Results are shown in Table 3.4 . Zero-shot GPT-3, which only receives on a natural language description of the task, still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for 14 Setting En → Fr Fr → En En → De De → En En → Ro Ro → En SOTA (Supervised) 45.6 a 35.0 b 41.2 c 40.2 d 38.5 e 39.9 e XLM [ LC19 ] 33.4 33.3 26.4 34.3 33.3 31.8 MASS [ STQ + 19 ] 37.5 34.9 28.3 35.2 35.2 33.1 mBART [ LGG + 20 ] - - 29.8 34.0 35.0 30.5 GPT-3 Zero-Shot 25.2 21.2 24.6 27.2 14.1 19.9 GPT-3 One-Shot 28.3 33.7 26.2 30.4 20.6 38.6 GPT-3 Few-Shot 32.6 39.2 29.7 40.6 21.0 39.5 Table 3.4: Few-shot GPT-3 outperforms previous unsupervised NMT work by 5 BLEU when translating into English reflecting its strength as an English LM",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We report BLEU scores on the WMT’14 Fr ↔ En, WMT’16 De ↔ En, and WMT’16 Ro ↔ En datasets as measured by multi-bleu.perl with XLM’s tokeniztion in order to compare most closely with prior unsupervised NMT work. SacreBLEU f [ Pos18 ] results rported in Appendix H . Underline indicates an unsupervised or few-shot SOTA, bold indicates supervised SOTA with relative confidence. a [ EOAG18 ] b [ DHKH14 ] c [ WXH + 18 ] d [ oR16 ] e [ LGG + 20 ] f [SacreBLEU signature: BLEU+case.mixed+numrefs.1+smooth.exp+tok.intl+version.1.2.20] Figure 3.4: Few-shot translation performance on 6 language pairs as model capacity increases. There is a consistent trend of improvement across all datasets as the model scales, and as well as tendency for translation into English to be stronger than translation from English. 15 Setting Winograd Winogrande (XL) Fine-tuned SOTA 90.1 a 84.6 b GPT-3 Zero-Shot 88.3* 70.2 GPT-3 One-Shot 89.7* 73.2 GPT-3 Few-Shot 88.6* 77.7 Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section 4 for details on potential contamination of the Winograd test set. a [ SBBC19 ] b [ LYN + 20 ] Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales. Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B is competitive with a fine-tuned RoBERTA-large. each translation task improves performance by over 7 BLEU and nears competitive performance with prior work. GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For the three input languages studied, GPT-3 significantly outperforms prior unsupervised NMT work when translating into English but underperforms when translating in the other direction. Performance on En-Ro is a noticeable outlier at over 10 BLEU worse than prior unsupervised NMT work. This could be a weakness due to reusing the byte-level BPE tokenizer of GPT-2 which was developed for an almost entirely English training dataset. For both Fr-En and De-En, few shot GPT-3 outperforms the best supervised result we could find but due to our unfamiliarity with the literature and the appearance that these are un-competitive benchmarks we do not suspect those results represent true state of the art. For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, supervised finetuning on 608K labeled examples, and backtranslation [ LHCG19b ]. Finally, across all language pairs and across all three settings (zero-, one-, and few-shot), there is a smooth trend of improvement with model capacity. This is shown in Figure 3.4 in the case of few-shot results, and scaling for all three settings is shown in Appendix H . 3.4 Winograd-Style Tasks The Winograd Schemas Challenge [ LDM12 ] is a classical task in NLP that involves determining which word a pronoun refers to, when the pronoun is grammatically ambiguous but semantically unambiguous to a human. Recently fine-tuned language models have achieved near-human performance on the original Winograd dataset, but more difficult versions 16 Setting PIQA ARC (Easy) ARC (Challenge) OpenBookQA Fine-tuned SOTA 79.4 92.0 [ KKS + 20 ] 78.5 [ KKS + 20 ] 87.2 [ KKS + 20 ] GPT-3 Zero-Shot 80.5 * 68.8 51.4 57.6 GPT-3 One-Shot 80.5 * 71.2 53.2 58.8 GPT-3 Few-Shot 82.8 * 70.1 51.5 65.4 Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". GPT-3 Few-Shot PIQA result is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test set. Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a score on the development set in all three conditions that exceeds the best recorded score on the task. such as the adversarially-mined Winogrande dataset [ SBBC19 ] still significantly lag human performance. We test GPT-3’s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting. On Winograd we test GPT-3 on the original set of 273 Winograd schemas, using the same “partial evaluation” method described in [ RWC + 19 ]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires entity extraction to convert to the form described in this section. On Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human performance. We note that contamination analysis found some Winograd schemas in the training data but this appears to have only a small effect on results (see Section 4 ). On the more difficult Winogrande dataset, we do find gains to in-context learning: GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a fine-tuned RoBERTA model achieves 79%, state-of-the-art is 84.6% achieved with a fine-tuned high capacity model (T5), and human performance on the task as reported by [ SBBC19 ] is 94.0%. 3.5 Common Sense Reasoning Next we consider three datasets which attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The first, PhysicalQA (PIQA) [ BZB + 19 ], asks common sense questions about how the physical world works and is intended as a probe of grounded understanding of the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot (the last measured on PIQA’s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a 17 Setting CoQA DROP QuAC SQuADv2 RACE-h RACE-m Fine-tuned SOTA 90.7 a 89.1 b 74.4 c 93.0 d 90.0 e 93.1 e GPT-3 Zero-Shot 81.5 23.6 41.5 59.5 45.5 58.4 GPT-3 One-Shot 84.0 34.3 43.3 65.4 45.9 57.4 GPT-3 Few-Shot 85.0 36.5 44.3 69.8 46.8 58.1 Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy. a [ JZC + 19 ] b [ JN20 ] c [ AI19 ] d [ QIA20 ] e [ SPP + 19 ] fine-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human performance, but GPT-3’s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis flagged PIQA for a potential data contamination issue (despite hidden test labels), and we therefore conservatively mark the result with an asterisk. See Section 4 for details. ARC [ CCE + 18 ] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the “Challenge” version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline (55.9%) from UnifiedQA [ KKS + 20 ]. On the “Easy” version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a fine-tuned RoBERTa baseline from [ KKS + 20 ]",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3’s few-shot results by 27% on the challenge set and 22% on the easy set. On OpenBookQA [ MCKS18 ], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3’s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard. Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT-3 performs best (within 3 points of the human baseline) on CoQA [ RCM19 ] a free-form conversational dataset and performs worst (13 F1 below an ELMo baseline) on QuAC [ CHI + 18 ] a dataset which requires modeling structured dialog acts and answer span selections of teacher-student interactions. On DROP [ DWD + 19 ], a dataset testing discrete reasoning and numeracy in the context of reading comprehension, GPT-3 in a few-shot setting outperforms the fine-tuned BERT baseline from the original paper but is still well below both human performance and state-of-the-art approaches which augment neural networks with symbolic systems [ RLL + 19 ]",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". On SQuAD 2.0 [ RJL18 ], GPT-3 demonstrates its few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to slightly outperform the best fine-tuned result in the original paper. On RACE [ LXL + 17 ], a multiple choice dataset of middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with the earliest work utilizing contextual representations and is still 45% behind SOTA. 3.7 SuperGLUE In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark [ WPN + 19 ] [ WPN + 19 ] [ CLC + 19 ] [ DMST19 ] [ RBG11 ] [ KCR + 18 ] [ ZLL + 18 ] [ DGM06 ] [ BHDD + 06 ] [ GMDD07 ] [ BDD + 09 ] [ PCC18 ] [ PHR + 18 ]. GPT-3’s test-set performance on the SuperGLUE dataset is shown in Table 3.8 . In the few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC 18 Figure 3.7: GPT-3 results on CoQA reading comprehension task. GPT-3 175B achieves 85 F1 in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. Zero-shot and one-shot performance is a few points behind, with the gains to few-shot being largest for bigger models. SuperGLUE BoolQ CB CB COPA RTE Average Accuracy Accuracy F1 Accuracy Accuracy Fine-tuned SOTA 89.0 91.0 96.9 93.9 94.8 92.5 Fine-tuned BERT-Large 69.0 77.4 83.6 75.7 70.6 71.7 GPT-3 Few-Shot 71.8 76.4 75.6 52.0 92.0 69.0 WiC WSC MultiRC MultiRC ReCoRD ReCoRD Accuracy Accuracy Accuracy F1a Accuracy F1 Fine-tuned SOTA 76.1 93.8 62.3 88.2 92.5 93.3 Fine-tuned BERT-Large 69.6 64.6 24.1 70.0 71.3 72.0 GPT-3 Few-Shot 49.4 80.1 30.5 75.4 90.2 91.1 Table 3.8: Performance of GPT-3 on SuperGLUE compared to fine-tuned baselines and SOTA. All results are reported on the test set",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". All results are reported on the test set. GPT-3 few-shot is given a total of 32 examples within the context of each task and performs no gradient updates. 19 Figure 3.8: Performance on SuperGLUE increases with model size and number of examples in context. A value of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in SuperGLUE. We report GPT-3 values on the dev set, so our numbers are not directly comparable to the dotted reference lines (our test set results are in Table 3.8 ). The BERT-Large reference model was fine-tuned on the SuperGLUE training set (125K examples), whereas BERT++ was first fine-tuned on MultiNLI (392K examples) and SWAG (113K examples) before further fine-tuning on the SuperGLUE training set (for a total of 630K fine-tuning examples). We find the difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between GPT-3 with one example per context versus eight examples per context. and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated. We observe a wide range in GPT-3’s performance across tasks. On COPA and ReCoRD GPT-3 achieves near-SOTA performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving second place on the leaderboard, where first place is held by a fine-tuned 11 billion parameter model (T5). On WSC, performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the original Winograd dataset as described in Section 3.4 ). On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance at 49.4% (at random chance)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". On CB, we see signs of life at 75.6% in the few-shot setting. WiC is a notable weak spot with few-shot performance at 49.4% (at random chance). We tried a number of different phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same way in two sentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another. This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these weaknesses, GPT-3 still outperforms a fine-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to the state-of-the-art held by a fine-tuned 11 billion parameter model. Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of examples in the context showing increasing benefits from in-context learning (Figure 3.8 ). We scale K up to 32 examples per task, after which point additional examples will not reliably fit into our context. When sweeping over values of K , we find that GPT-3 requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall SuperGLUE score. 3.8 NLI Natural Language Inference (NLI) [ Fyo00 ] concerns the ability to understand the relationship between two sentences. In practice, this task is usually structured as a two or three class classification problem where the model classifies 20 Figure 3.9: Performance of GPT-3 on ANLI Round 3. Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2%)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Results are on the dev-set, which has only 1500 examples and therefore has high variance (we estimate a standard deviation of 1.2%). We find that smaller models hover around random chance, while few-shot GPT-3 175B closes almost half the gap from random chance to SOTA. Results for ANLI rounds 1 and 2 are shown in the appendix. whether the second sentence logically follows from the first, contradicts the first sentence, or is possibly true (neutral). SuperGLUE includes an NLI dataset, RTE, which evaluates the binary version of the task. On RTE, only the largest version of GPT-3 performs convincingly better than random (56%) in any evaluation setting, but in a few-shot setting GPT-3 performs similarly to a single-task fine-tuned BERT Large. We also evaluate on the recently introduced Adversarial Natural Language Inference (ANLI) dataset [ NWD + 19 ]. ANLI is a difficult dataset employing a series of adversarially mined natural language inference questions in three rounds (R1, R2, and R3). Similar to RTE, all of our models smaller than GPT-3 perform at almost exactly random chance on ANLI, even in the few-shot setting ( ∼ 33% ), whereas GPT-3 itself shows signs of life on Round 3. Results for ANLI R3 are highlighted in Figure 3.9 and full results for all rounds can be found in Appendix H . These results on both RTE and ANLI suggest that NLI is still a very difficult task for language models and they are only just beginning to show signs of progress. 3.9 Synthetic and Qualitative Tasks One way to probe GPT-3’s range of abilities in the few-shot (or zerand one-shot) setting is to give it tasks which require it to perform simple on-the-fly computational reasoning, recognize a novel pattern that is unlikely to have occurred in training, or adapt quickly to an unusual task. We devise several tasks to test this class of abilities. First, we test GPT-3’s ability to perform arithmetic",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We devise several tasks to test this class of abilities. First, we test GPT-3’s ability to perform arithmetic. Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3’s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models. 3.9.1 Arithmetic To test GPT-3’s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language: • 2 digit addition (2D+) – The model is asked to add two integers sampled uniformly from [0 , 100) , phrased in the form of a question, e.g. “Q: What is 48 plus 76? A: 124.” • 2 digit subtraction (2D-) – The model is asked to subtract two integers sampled uniformly from [0 , 100) ; the answer may be negative. Example: “Q: What is 34 minus 53? A: -19”. • 3 digit addition (3D+) – Same as 2 digit addition, except numbers are uniformly sampled from [0 , 1000) . 21 Figure 3.10: Results on all 10 arithmetic tasks in the few-shot settings for models of different sizes. There is a significant jump from the second largest model (GPT-3 13B) to the largest model (GPT-3 175), with the latter being able to reliably accurate 2 digit arithmetic, usually accurate 3 digit arithmetic, and correct answers a significant fraction of the time on 4-5 digit arithmetic, 2 digit multiplication, and compound operations. Results for one-shot and zero-shot are shown in the appendix. • 3 digit subtraction (3D-) – Same as 2 digit subtraction, except numbers are uniformly sampled from [0 , 1000) . • 4 digit addition (4D+) – Same as 3 digit addition, except uniformly sampled from [0 , 10000)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". • 4 digit addition (4D+) – Same as 3 digit addition, except uniformly sampled from [0 , 10000) . • 4 digit subtraction (4D-) – Same as 3 digit subtraction, except uniformly sampled from [0 , 10000) . • 5 digit addition (5D+) – Same as 3 digit addition, except uniformly sampled from [0 , 100000) . • 5 digit subtraction (5D-) – Same as 3 digit subtraction, except uniformly sampled from [0 , 100000) . • 2 digit multiplication (2Dx) – The model is asked to multiply two integers sampled uniformly from [0 , 100) , e.g. “Q: What is 24 times 42? A: 1008”. • One-digit composite (1DC) – The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two. For example, “Q: What is 6+(4*8)? A: 38”. The three 1 digit numbers are selected uniformly on [0 , 10) and the operations are selected uniformly from { +,-,* } . In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances. First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10 . On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations. As Figure 3.10 makes clear, small models do poorly on all of these tasks – even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time. One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly 22 Setting 2D+ 23D+ 34D+ 45D+ 52Dx 1DC GPT-3 Zero-shot 76.9 58.0 34.2 48.3 4.0 7.5 0.7 0.8 19.8 9.8 GPT-3 One-shot 99.6 86.4 65.5 78.7 14.0 14.0 3.5 3.8 27.4 14.3 GPT-3 Few-shot 100.0 98.9 80.4 94.2 25.5 26.8 9.3 9.9 29.2 21.3 Table 3.9: Results on basic arithmetic tasks for GPT-3 175B. { 2,3,4,5 } D { +,- } is 2, 3, 4, and 5 digit addition or subtraction, 2Dx is 2 digit multiplication. 1DC is 1 digit composite operations. Results become progressively stronger moving from the zero-shot to one-shot to few-shot setting, but even the zero-shot shows significant arithmetic abilities. Setting CL A1 A2 RI RW GPT-3 Zero-shot 3.66 2.28 8.91 8.26 0.09 GPT-3 One-shot 21.7 8.62 25.9 45.4 0.48 GPT-3 Few-shot 37.9 15.1 39.7 67.2 0.44 Table 3.10: GPT-3 175B performance on various word unscrambling and word manipulation tasks, in zero-, one-, and few-shot settings. CL is “cycle letters in word”, A1 is anagrams of but the first and last letters, A2 is anagrams of all but the first and last two letters, RI is “Random insertion in word”, RW is “reversed words”. outperforms few-shot learning for all smaller models",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". outperforms few-shot learning for all smaller models. All three settings for the full GPT-3 are shown in Table 3.9 , and model capacity scaling for all three settings is shown in Appendix H . To spot-check whether the model is simply memorizing specific arithmetic problems, we took the 3-digit arithmetic problems in our test set and searched for them in our training data in both the forms \"<NUM1> + <NUM2> =\" and \"<NUM1> plus <NUM2>\" . Out of 2,000 addition problems we found only 17 matches (0.8%) and out of 2,000 subtraction problems we found only 2 matches (0.1%), suggesting that only a trivial fraction of the correct answers could have been memorized. In addition, inspection of incorrect answers reveals that the model often makes mistakes such as not carrying a “1”, suggesting it is actually attempting to perform the relevant computation rather than memorizing a table. Overall, GPT-3 displays reasonable proficiency at moderately complex arithmetic in few-shot, one-shot, and even zero-shot settings. 3.9.2 Word Scrambling and Manipulation Tasks To test GPT-3’s ability to learn novel symbolic manipulations from a few examples, we designed a small battery of 5 “character manipulation” tasks. Each task involves giving the model a word distorted by some combination of scrambling, addition, or deletion of characters, and asking it to recover the original word. The 5 tasks are: • Cycle letters in word (CL) – The model is given a word with its letters cycled, then the “=” symbol, and is expected to generate the original word. For example, it might be given “lyinevitab” and should output “inevitably”. • Anagrams of all but first and last characters (A1) – The model is given a word where every letter except the first and last have been scrambled randomly, and must output the original word. Example: criroptuon = corruption. • Anagrams of all but first and last 2 characters (A2) – The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". • Anagrams of all but first and last 2 characters (A2) – The model is given a word where every letter except the first 2 and last 2 have been scrambled randomly, and must recover the original word. Example: opoepnnt → opponent. • Random insertion in word (RI) – A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession. • Reversed words (RW) – The model is given a word spelled backwards, and must output the original word. Example: stcejbo → objects. For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [ Nor09 ] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11 . Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing 23 Figure 3.11: Few-shot performance on the five word scrambling tasks for different sizes of model. There is generally smooth improvement with model size although the random insertion task shows an upward slope of improvement with the 175B model solving the task the majority of the time. Scaling of one-shot and zero-shot performance is shown in the appendix. All tasks are done with K = 100 . random insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word. In the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10 ). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We can further quantify performance by plotting “in-context learning curves”, which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2 . We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions. Finally, it is worth adding that solving these tasks requires character-level manipulations, whereas our BPE encoding operates on significant fractions of a word (on average ∼ 0 . 7 words per token), so from the LM’s perspective succeeding at these tasks involves not just manipulating BPE tokens but understanding and pulling apart their substructure. Also, CL, A1, and A2 are not bijective (that is, the unscrambled word is not a deterministic function of the scrambled word), requiring the model to perform some search to find the correct unscrambling. Thus, the skills involved appear to require non-trivial pattern-matching and computation. 3.9.3 SAT Analogies To test GPT-3 on another task that is somewhat unusual relative to the typical distribution of text, we collected a set of 374 “SAT analogy” problems [ TLBS03 ]. Analogies are a style of multiple choice question that constituted a section of the SAT college entrance exam before 2005. A typical example is “audacious is to boldness as (a) sanctimonious is to hypocrisy, (b) anonymous is to identity, (c) remorseful is to misdeed, (d) deleterious is to result, (e) impressionable is to temptation”. The student is expected to choose which of the five word pairs has the same relationship as the original word pair; in this example the answer is “sanctimonious is to hypocrisy”. On this task GPT-3 achieves 65.2% in the few-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting, whereas the average score among college applicants was 57% [ TL05 ] (random guessing yields 20%)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As shown in Figure 3.12 , the results improve with scale, with the the full 175 billion model improving by over 10% compared to the 13 billion parameter model. 24 Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model. The largest model achieves 65% accuracy in the few-shot setting, and also demonstrates significant gains to in-context learning which are not present in smaller models. 3.9.4 News Article Generation Previous work on generative language models qualitatively tested their ability to generate synthetic “news articles” by conditional sampling from the model given a human-written prompt consisting of a plausible first sentence for a news story [ RWC + 19 ]. Relative to [ RWC + 19 ], the dataset used to train GPT-3 is much less weighted towards news articles, so trying to generate news articles via raw unconditional samples is less effective – for example GPT-3 often interprets the proposed first sentence of a “news article” as a tweet and then posts synthetic responses or follow-up tweets. To solve this problem we employed GPT-3’s few-shot learning abilities by providing three previous news articles in the model’s context to condition it. With the title and subtitle of a proposed next article, the model is able to reliably generate short articles in the “news” genre. To gauge the quality of news article generation from GPT-3 (which we believe is likely to be correlated with conditional sample generation quality in general), we decided to measure human ability to distinguish GPT-3-generated articles from real ones. Similar work has been carried out by Kreps et al. [ KMB20 ] and Zellers et al. [ ZHR + 19 ]. Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Generative language models are trained to match the distribution of content generated by humans, so the (in)ability of humans to distinguish the two is a potentially important measure of quality. 3 In order to see how well humans can detect model generated text, we arbitrarily selected 25 article titles and subtitles from the website newser.com (mean length: 215 words). We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words). For each model, we presented around 80 US-based participants with a quiz consisting of these real titles and subtitles followed by either the human written article or the article generated by the model 4 . Participants were asked to select whether the article was “very likely written by a human”, “more likely written by a human”, “I don’t know”, “more likely written by a machine”, or “very likely written by a machine”. The articles we selected were not in the models’ training data and the model outputs were formatted and selected programmatically to prevent human cherry-picking. All models used the same context to condition outputs on and were pre-trained with the same context size and the same article titles and subtitles were used as prompts for each model. However, we also ran an experiment to control for participant effort and attention that followed the same format but involved intentionally bad model generated articles. This was done by generating articles from a “control model”: a 160M parameter model with no context and increased output randomness. 3 This task is also relevant to the potential misuse of language models discussed in Section 6.1 . 4 We wanted to identify how good an average person on the internet is at detecting language model outputs, so we focused on participants drawn from the general US population. See Appendix E for details",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". See Appendix E for details. 25 Mean accuracy 95% Confidence Interval (low, hi) t compared to control ( p -value) “I don’t know” assignments Control (deliberately bad model) 86% 83%–90% - 3.6 % GPT-3 Small 76% 72%–80% 3.9 (2 e -4) 4.9% GPT-3 Medium 61% 58%–65% 10.3 (7 e -21) 6.0% GPT-3 Large 68% 64%–72% 7.3 (3 e -11) 8.7% GPT-3 XL 62% 59%–65% 10.7 (1 e -19) 7.5% GPT-3 2.7B 62% 58%–65% 10.4 (5 e -19) 7.1% GPT-3 6.7B 60% 56%–63% 11.2 (3 e -21) 6.2% GPT-3 13B 55% 52%–58% 15.3 (1 e -32) 7.1% GPT-3 175B 52% 49%–54% 16.9 (1 e -34) 7.8% Table 3.11: Human accuracy in identifying whether short ( ∼ 200 word) news articles are model generated . We find that human accuracy (measured by the ratio of correct assignments to non-neutral assignments) ranges from 86% on the control model to 52% on GPT-3 175B. This table compares mean accuracy between five different models, and shows the results of a two-sample T-Test for the difference in mean accuracy between each model and the control model (an unconditional GPT-3 Small model with increased output randomness). Mean human accuracy (the ratio of correct assignments to non-neutral assignments per participant) at detecting that the intentionally bad articles were model generated was ∼ 86% where 50% is chance level performance. By contrast, mean human accuracy at detecting articles that were produced by the 175B parameter model was barely above chance at ∼ 52% (see Table 3.11 ). 5 Human abilities to detect model generated text appear to decrease as model size increases: there appears to be a trend towards chance accuracy with model size, and human detection of GPT-3 is close to chance. 6 This is true despite the fact that participants spend more time on each output as model size increases (see Appendix E ). Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15 . 7 Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Examples of synthetic articles from GPT-3 are given in Figures 3.14 and 3.15 . 7 Much of the text is—as indicated by the evaluations—difficult for humans to distinguish from authentic human content. Factual inaccuracies can be an indicator that an article is model generated since, unlike human authors, the models have no access to the specific facts that the article titles refer to or when the article was written. Other indicators include repetition, non sequiturs, and unusual phrasings, though these are often subtle enough that they are not noticed. Related work on language model detection by Ippolito et al. [ IDCBE19 ] indicates that automatic discriminators like G ROV E R [ ZHR + 19 ] and GLTR [ GSR19 ] may have greater success at detecting model generated text than human evaluators. Automatic detection of these models may be a promising area of future research. Ippolito et al. [ IDCBE19 ] also note that human accuracy at detecting model generated text increases as humans observe more tokens. To do a preliminary investigation of how good humans are at detecting longer news articles generated by GPT-3 175B, we selected 12 world news articles from Reuters with an average length of 569 words and generated completions of these articles from GPT-3 with an average length of 498 words (298 words longer than our initial experiments). Following the methodology above, we ran two experiments, each on around 80 US-based participants, to compare human abilities to detect the articles generated by GPT-3 and a control model. We found that mean human accuracy at detecting the intentionally bad longer articles from the control model was ∼ 88% , while mean human accuracy at detecting the longer articles that were produced by GPT-3 175B was still barely above chance at ∼ 52% (see Table 3.12 ). This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This indicates that, for news articles that are around 500 words long, GPT-3 continues to produce articles that humans find difficult to distinguish from human written news articles. 3.9.5 Learning and Using Novel Words A task studied in developmental linguistics [ CB78 ] is the ability to learn and utilize new words, for example using a word in a sentence after seeing it defined only once, or conversely inferring a word’s meaning from only one usage. Here we qualitatively test GPT-3’s ability to do the former. Specifically, we give GPT-3 the definition of a nonexistent word, such as “Gigamuru”, and then ask it to use it in a sentence. We provide one to five previous examples of a (separate) 5 We use a two-sample Student’s T-Test to test for significant difference between the means of the participant accuracies of each model and the control model and report the normalized difference in the means (as the t-statistic) and the p-value. 6 If a model consistently produces texts that are more impressive than human articles, it is possible that human performance on this task would drop below 50%. Indeed, many individual participants scored below 50% on this task. 7 Additional non-news samples can be found in Appendix F . 26 Figure 3.13: People’s ability to identify whether news articles are model-generated (measured by the ratio of correct assignments to non-neutral assignments) decreases as model size increases. Accuracy on the outputs on the deliberatelbad control model (an unconditioned GPT-3 Small model with higher output randomness) is indicated with the dashed line at the top, and the random chance (50%) is indicated with the dashed line at the bottom. Line of best fit is a power law with 95% confidence intervals",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Line of best fit is a power law with 95% confidence intervals. Mean accuracy 95% Confidence Interval (low, hi) t compared to control ( p -value) “I don’t know” assignments Control 88% 84%–91% - 2.7% GPT-3 175B 52% 48%–57% 12.7 (3.2 e -23) 10.6% Table 3.12: People’s ability to identify whether ∼ 500 word articles are model generated (as measured by the ratio of correct assignments to non-neutral assignments) was 88% on the control model and 52% on GPT-3 175B. This table shows the results of a two-sample T-Test for the difference in mean accuracy between GPT-3 175B and the control model (an unconditional GPT-3 Small model with increased output randomness). 27 Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination. The Post notes that the denomination, which claims 12.5 million members, was in the early 20th century the \"largest Protestant denomination in the U.S.,\" but that it has been shrinking in recent decades. The new split will be the second in the church's history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The new split will be the second in the church's history. The first occurred in 1968, when roughly 10 percent of the denomination left to form the Evangelical United Brethren Church. The Post notes that the proposed split \"comes at a critical time for the church, which has been losing members for years,\" which has been \"pushed toward the brink of a schism over the role of LGBTQ people in the church.\" Gay marriage is not the only issue that has divided the church. In 2016, the denomination was split over ordination of transgender clergy, with the North Pacific regional conference voting to ban them from serving as clergy, and the South Pacific regional conference voting to allow them. Figure 3.14: The GPT-3 generated news article that humans had the greatest difficulty distinguishing from a human written article (accuracy: 12%). Title: Star’s Tux Promise Draws Megyn Kelly’s Sarcasm Subtitle: Joaquin Phoenix pledged to not change for each awards event Article: A year ago, Joaquin Phoenix made headlines when he appeared on the red carpet at the Golden Globes wearing a tuxedo with a paper bag over his head that read, \"I am a shape-shifter. I can't change the world. I can only change myself.\" It was a promise to not change to fit into the Hollywood mold: \"I think that's a really special thing, to not change yourself. I think it's a really special thing to say, `This is what's inside of me, I'm proud of it, and I'm not going to be ashamed because of the way that someone else thinks I should be.'\" Now, it's the Oscars, and Phoenix is at it again. But this time, his publicist is saying he'll be wearing a tux no matter what. Megyn Kelly was not impressed, and she let him have it on The Tonight Show. \"You know, I feel like, I feel like you could have worn the tux,\" she says. \"But you're saying you're a shape-shifter. I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn't stick",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". I don't know if you can change your tux, but you can change your mind. You can change your mind. You can change your mind.\" Phoenix says he did, but it didn't stick. \"I was like, `Okay, I'm going to wear a tuxedo to this thing.' And then I thought, `I don't want to wear a tuxedo to this thing.'\" Kelly goes on to encourage him to change his mind again, but Phoenix says it's too late: \"I'm committed to wearing this.\" Figure 3.15: The GPT-3 generated news article that humans found the easiest to distinguish from a human written article (accuracy: 61%). 28 A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is: We were traveling in Africa and we saw these very cute whatpus. To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is: One day when I was playing tag with my little sister, she got really excited and she started doing these crazy farduddles. A \"yalubalu\" is a type of vegetable that looks like a big pumpkin. An example of a sentence that uses the word yalubalu is: I was on a trip to Africa and I tried this yalubalu vegetable that was grown in a garden there. It was delicious. A \"Burringo\" is a car with very fast acceleration. An example of a sentence that uses the word Burringo is: In our garage we have a Burringo that my father drives to work every day. A \"Gigamuru\" is a type of Japanese musical instrument. An example of a sentence that uses the word Gigamuru is: I have a Gigamuru that my uncle gave me as a gift. I love to play it at home. To \"screeg\" something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeghed at each other for several minutes and then we went outside and ate ice cream. Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3’s completions, plain text is human prompts",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Figure 3.16: Representative GPT-3 completions for the few-shot task of using a new word in a sentence. Boldface is GPT-3’s completions, plain text is human prompts. In the first example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 other than the conditioning shown here. nonexistent word being defined and used in a sentence, so the task is few-shot in terms of previous examples of the broad task and one-shot in terms of the specific word. Table 3.16 shows the 6 examples we generated; all definitions were human-generated, and the first answer was human-generated as conditioning while the subsequent answers were generated by GPT-3. These examples were generated continuously in one sitting and we did not omit or repeatedly try any prompts. In all cases the generated sentence appears to be a correct or at least plausible use of the word. In the final sentence the model generates a plausible conjugation for the word “screeg” (namely “screeghed”), although the use of the word is slightly awkward (“screeghed at each other”) despite being plausible in the sense that it could describe a toy sword fight. Overall, GPT-3 appears to be at least proficient at the task of using novel words in a sentence. 3.9.6 Correcting English Grammar Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the feshot setting by giving prompts of the form \"Poor English Input: <sentence> \\ n Good English Output: <sentence>\" . We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17 . 4 Measuring and Preventing Memorization Of Benchmarks Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 Measuring and Preventing Memorization Of Benchmarks Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to. This concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [ TL18 ] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [ RWC + 19 ] also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that 29 Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I'd be more than happy to work with you on another project. Poor English input: Please provide me with a short brief of the design you’re looking for and that’d be nice if you could share some examples or project you did before. Good English output: Please provide me with a brief description of the design you're looking for and that would be nice if you could share some examples or projects you have done before. Poor English input: The patient was died",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Poor English input: The patient was died. Good English output: The patient died. Poor English input: We think that Leslie likes ourselves. Good English output: We think that Leslie likes us. Poor English input: Janet broke Bill on the finger. Good English output: Janet broke Bill's finger. Poor English input: Mary arranged for, in St. Louis, John to rent a house cheap. Good English output: Mary arranged for John to rent a house in St. Louis. Poor English input: Today I have went to the store to to buys some many bottle of water. Good English output: Today I went to the store to buy some bottles of water. Poor English input: I have tried to hit ball with bat, but my swing is has miss. Good English output: I tried to hit the ball with the bat, but my swing missed. Figure 3.17: Representative GPT-3 completions for the few-shot task of correcting English grammar. Boldface is GPT-3’s completions, plain text is human prompts. In the first few examples example both the prompt and the completion are provided by a human; this then serves as conditioning for subsequent examples where GPT-3 receives successive additional prompts and provides the completions. Nothing task-specific is provided to GPT-3 aside from the first few examples as conditioning and the “Poor English input/Good English output” framing. We note that the distinction between ”poor” and ”good” English (and the terms themselves) is complex, contextual, and contested. As the example mentioning the rental of a house shows, assumptions that the model makes about what “good” is can even lead it to make errors (here, the model not only adjusts grammar, but also removes the word ”cheap” in a way that alters meaning). 30 Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 30 Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation split of our training distribution. Though there is some gap between training and validation performance, the gap grows only minimally with model size and training time, suggesting that most of the gap comes from a difference in difficulty rather than overfitting. although models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent). GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1 ). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared. We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_53"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C . We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2 . Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance. Below, we review in more detail the few specific cases where either (1) the model performs significantly worse on the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference difficult. Our analysis flagged six groups of benchmarks for further investigation: Word Scrambling, Reading Comprehension (QuAC, SQuAD2, DROP), PIQA, Winograd, language modeling tasks (Wikitext tasks, 1BW), and German to English 31 Figure 4.2: Benchmark contamination analysis We constructed cleaned versions of each of our benchmarks to check for potential contamination in our training set. The x-axis is a conservative lower bound for how much of the dataset is known with high confidence to be clean, and the y-axis shows the difference in performance when evaluating only on the verified clean subset. Performance on most benchmarks changed negligibly, but some were flagged for further review",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_54"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Performance on most benchmarks changed negligibly, but some were flagged for further review. On inspection we find some evidence for contamination of the PIQA and Winograd results, and we mark the corresponding results in Section 3 with an asterisk. We find no evidence that other benchmarks are affected. translation. Since our overlap analysis is designed to be extremely conservative, we expect it to produce some false positives. We summarize the results for each group of tasks below: • Reading Comprehension: Our initial analysis flagged > 90% of task examples from QuAC, SQuAD2, and DROP as potentially contaminated, so large that even measuring the differential on a clean subset was difficult. Upon manual inspection, however, we found that for every overlap we inspected, in all 3 datasets, the source text was present in our training data but the question/answer pairs were not, meaning the model gains only background information and cannot memorize the answer to a specific question. • German translation: We found 25% of the examples in the WMT16 German-English test set were marked as potentially contaminated, with an associated total effect size of 1-2 BLEU. Upon inspection, none of the flagged examples contain paired sentences resembling NMT training data and collisions were monolingual matches mostly of snippets of events discussed in the news. • Reversed Words and Anagrams: Recall that these tasks are of the form “ alaok = koala ”. Due to the short length of these tasks, we used 2-grams for filtering (ignoring punctuation). After inspecting the flagged overlaps, we found that they were not typically instances of real reversals or unscramblings in the training set, but rather palindromes or trivial unscramblings, e.g “ kayak = kayak ”. The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_55"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The amount of overlap was small, but removing the trivial tasks lead to an increase in difficulty and thus a spurious signal. Related to this, the symbol insertion task shows high overlap but no effect on performance – this is because that task involves removing non-letter characters from a word, and the overlap analysis itself ignores such characters, leading to many spurious matches. • PIQA: The overlap analysis flagged 29% of examples as contaminated, and observed a 3 percentage point absolute decrease (4% relative decrease) in performance on the clean subset. Though the test dataset was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set. We found a similar decrease in a 25x smaller model with much less capacity to memorize, leading us to suspect that the shift is likely statistical bias rather than memorization; examples which workers copied may simply be easier. Unfortunately, we cannot rigorously prove this hypothesis. We therefore mark our PIQA results with an asterisk to denote this potential contamination. • Winograd: The overlap analysis flagged 45% of examples, and found a 2.6% decrease in performance on the clean subset. Manual inspection of the overlapping data point showed that 132 Winograd schemas were in fact present in our training set, though presented in a different format than we present the task to the model. Although the decrease in performance is small, we mark our Winograd results in the main paper with an asterisk. 32 • Language modeling: We found the 4 Wikipedia language modeling benchmarks measured in GPT-2, plus the Children’s Book Test dataset, to be almost entirely contained in our training data. Since we cannot reliably extract a clean subset here, we do not report results on these datasets, even though we intended to when starting this work. We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_56"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We note that Penn Tree Bank due to its age was unaffected and therefore became our chief language modeling benchmark. We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our fill-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section. An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inflates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing. Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C . 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_57"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work. First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3’s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with “common sense physics”, despite doing well on some datasets (such as PIQA [ BZB + 19 ]) that test this domain. Specifically GPT-3 has difficulty with questions of the type “If I put cheese into the fridge, will it melt?”. Quantitatively, GPT-3’s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3 , and in particular it does little better than chance when evaluated one-shot or even few-shot on some “comparison” tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3’s strong few-shot performance on many other tasks. GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_58"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising. This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [ RSR + 19 ]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3’s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with feor zero-shot learning, is a promising direction for future research, and could help achieve the “best of both worlds”. A more fundamental limitation of the general approach described in this paper – scaling up any LM-like model, whether autoregressive or bidirectional – is that it may eventually run into (or could already be running into) the limits of the 33 pretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [ RRS20 ] demonstrate benefits of customizing prediction to entities of interest",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_59"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". [ RRS20 ] demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions. Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [ BHT + 20 ]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ ZSW + 19a ], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [ CLY + 19 ]. Another limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [ Lin20 ]. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements. A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks “from scratch” at inference time, or if it simply recognizes and identifies tasks that it has learned during training",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_60"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task. Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research. A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation [ HVD15 ] of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general [ LHCG19a ] but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_61"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Finally, GPT-3 shares some limitations common to most deep learning systems – its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on. This last issue – biases in the data that may lead the model to generate stereotyped or prejudiced content – is of special concern from a societal perspective, and will be discussed along with other issues in the next section on Broader Impacts (Section 6 ). 6 Broader Impacts Language models have a wide range of beneficial applications for society, including code and writing auto-completion, grammar assistance, game narrative generation, improving search engine responses, and answering questions. But they also have potentially harmful applications. GPT-3 improves the quality of text generation and adaptability over smaller models and increases the difficulty of distinguishing synthetic text from human-written text. It therefore has the potential to advance both the beneficial and harmful applications of language models. Here we focus on the potential harms of improved language models, not because we believe the harms are necessarily greater, but in order to stimulate efforts to study and mitigate them. The broader impacts of language models like this are numerous. We focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 in Section 6.1 , and issues of bias, fairness, and representation within models like GPT-3 in Section 6.2 . We also briefly discuss issues of energy efficiency (Section 6.3 ). 34 6.1 Misuse of Language Models Malicious uses of language models can be somewhat difficult to anticipate because they often involve repurposing language models in a very different environment or for a different purpose than researchers intended",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_62"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To help with this, we can think in terms of traditional security risk assessment frameworks, which outline key steps such as identifying threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact [ Ros12 ]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures. 6.1.1 Potential Misuse Applications Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy. The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard. 6.1.2 Threat Actor Analysis Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to ‘advanced persistent threats’ (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas [ SBC + 19 ]. To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_63"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this. Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible difference in operations that may see potential gains by using language models. The assessment was that language models may not be worth investing significant resources in because there has been no convincing demonstration that current language models are significantly better than current methods for generating text, and because methods for “targeting” or “controlling” the content of language models are still at a very early stage. 6.1.3 External Incentive Structures Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their agenda. TTPs are influenced by economic factors like scalability and ease of deployment; phishing is extremely popular among all groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment. Ease of use is another significant incentive. Having stable infrastructure has a large impact on the adoption of TTPs. The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_64"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The outputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the amount of human labor required in operating this bot. But a human is still needed to filter the outputs, which restricts how scalable the operation can be. Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will eventually develop language models that are sufficiently consistent and steerable that they will be of greater interest to malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers. 35 6.2 Fairness, Bias, and Representation Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms [ Cra17 ]. We have conducted an analysis of biases in the model in order to better understand GPT-3’s limitations when it comes to fairness, bias, and representation. 8 Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model’s biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_65"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension. 6.2.1 Gender In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The { occupation } was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3. We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc. We also tested how these probabilities changed when we shifted the context to be the \"The competent { occupation } was a\" (Competent Variant), and when we shifted the context to be \"The incompetent { occupation } was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent { occupation } was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The { occupation } was a\"",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_66"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". With the prompt \"The incompetent { occupation } was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as 1 n jobs P jobs log( P (female | Context) P (male | Context)) ) - was − 1 . 11 for the Neutral Variant, − 2 . 14 for the Competent Variant and − 1 . 15 for the Incompetent Variant. We also carried out pronoun resolution on the Winogender dataset [ RNLVD18 ] using two methods which further corroborated the model’s tendency to associate most occupations with males. One method measured the moels ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications. ‘She’ refers to the\" and found the option with the lowest probability between the two possble options (Choices between Occupation Option: advisor; Participant Option: advisee). Occupation and participant words often have societal biases associated with them such as the assumption that most occupants are by default male. We found that the language models learnt some of these biases such as a tendency to associate female pronouns with participant positions more than male pronouns. GPT-3 175B had the highest accuracy of all the models (64.17%) on this task. It was also the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher than for males (81.7% vs 76.7%). All other models had a higher accuracy for male pronouns with Occupation sentences as compared to female pronouns with the exception of our second largest modeGPT-3 13B - which had the same accuracy (60%) for both. This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_67"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This offers some preliminary evidence that in places where issues of bias can make language models susceptible to error, the larger models are more robust than smaller models. We also performed co-occurrence tests, where we analyzed which words are likely to occur in the vicinity of other prselected words. We created a model output sample set by generating 800 outputs of length 50 each with a temperature 8 Evaluating fairness, bias, and representation in language models is a rapidly-developing area with a large body of prior work. See, for example, [ HZJ + 19 , NBR20 , SCNP19 ]. 36 Table 6.1: Most Biased Descriptive Words in 175B Model Top 10 Most Biased Male Descriptive Words with Raw Co-Occurrence Counts Top 10 Most Biased Female Descriptive Words with Raw Co-Occurrence Counts Average Number of Co-Occurrences Across All Words: 17.5 Average Number of Co-Occurrences Across All Words: 23.9 Large (16) Optimistic (12) Mostly (15) Bubbly (12) Lazy (14) Naughty (12) Fantastic (13) Easy-going (12) Eccentric (13) Petite (10) Protect (10) Tight (10) Jolly (10) Pregnant (10) Stable (9) Gorgeous (28) Personable (22) Sucked (8) Survive (7) Beautiful (158) of 1 and top p of 0.9 for every prompt in our dataset. For gender, we had prompts such as \"He was very\" , \"She was very\" , \"He would be described as\" , \"She would be described as\" 9 . We looked at the adjectives and adverbs in the top 100 most favored words using an off-the-shelf POS tagger [ LB02 ]. We found females were more often described using appearance oriented words such as ”beautiful” and ”gorgeous” as compared to men who were more often described using adjectives that span a greater spectrum. Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. “Most Favored” here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_68"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". “Most Favored” here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender. 6.2.2 Race To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The { race } man was very\" , \"The { race } woman was very\" and \"People would describe the { race } person as\" and generated 800 samples for each of the above prompts, with { race } replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation [ HZJ + 19 ], we explored how race impacted sentiment. We measured sentiment using Senti WordNet [ BES10 ] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet). It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so. Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_69"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Across the models we analyzed, ‘Asian’ had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, ’Black’ had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. 9 We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which ‘they’ refers to a singular noun from those where it didn’t, but other forms of gender bias are likely present and could be studied using different approaches. 37 Figure 6.1: Racial Sentiment Across Models Religion Most Favored Descriptive Words Atheism ‘Theists’, ‘Cool’, ‘Agnostics’, ‘Mad’, ‘Theism’, ‘Defensive’, ‘Complaining’, ‘Correct’, ‘Arrogant’, ‘Characterized’ Buddhism ‘Myanmar’, ‘Vegetarians’, ‘Burma’, ‘Fellowship’, ‘Monk’, ‘Japanese’, ‘Reluctant’, ‘Wisdom’, ‘Elightenment’, ‘Non-Violent’ Christianity ‘Attend’, ‘Ignorant’, ‘Response’, ‘Judgmental’, ‘Grace’, ‘Execution’, ‘Egypt’, ‘Continue’, ‘Coments’, ‘Officially’ Hinduism ‘Caste’, ‘Cows’, ‘BJP’, ‘Kashmir’, ‘Modi’, ‘Celebrated’, ‘Dharma’, ‘Pakistani’, ‘Originated’, ‘Africa’ Islam ‘Pillars’, ‘Terrorism’, ‘Fasting’, ‘Sheikh’, ‘Non-Muslim’, ‘Source’, ‘Charities’, ‘Levant’, ‘Allah’, ‘Prophet’ Judaism ‘Gentiles’, ‘Race’, ‘Semites’, ‘Whites’, ‘Blacks’, ‘Smartest’, ‘Racists’, ‘Arabs’, ‘Game’, ‘Russian’ Table 6.2: Shows the ten most favored words about each religion in the GPT-3 175B model. 6.2.3 Religion We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length ≈ 50 with a temperature of 1 and a top p of 0 . 9 for every prompt. Our prompts were of the nature \" { Religion practitioners } are\" (Eg",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_70"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 9 for every prompt. Our prompts were of the nature \" { Religion practitioners } are\" (Eg. \"Christians are\" ) for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: \"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic.\" Similar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion Islam , we found that words such as ramadan , prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent , terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3. 38 6.2.4 Future Bias and Fairness Challenges We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [ MWZ + 18 ]. Ultimately, it is important not just to characterize biases in language systems but to intervene",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_71"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [ QMZH19 , HZJ + 19 ], so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems [ BBDIW20 ]. Thus, mitigation work should not be approached purely with a metric driven objective to ‘remove’ bias as this has been shown to have blind spots [ GG19 , NvNvdG19 ] but in a holistic manner. 6.3 Energy Usage Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2 ). This means we should be cognizant of the cost and efficiency of such models, as advocated by [ SDSE19 ]. The use of large-scale pre-training also gives another lens through which to view the efficiency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_72"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Additionally, techniques like model distillation [ LHCG19a ] can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation [ HB20 ]. 7 Related Work Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters [ JVS + 16 ]. One line of work straightforwardly increases the size of transformer models, scaling up parameters and FLOPS-per-token roughly in proportion. Work in this vein has successively increased model size: 213 million parameters [ VSP + 17 ] in the original paper, 300 million parameters [ DCLT18 ], 1.5 billion parameters [ RWC + 19 ], 8 billion parameters [ SPP + 19 ], 11 billion parameters [ RSR + 19 ], and most recently 17 billion parameters [ Tur20 ]. A second line of work has focused on increasing parameter count but not computation, as a means of increasing models’ capacity to store information without increased computational cost. These approaches rely on the conditional computation framework [ BLC13 ] and specifically, the mixture-of-experts method [ SMM + 17 ] has been used to produce 100 billion parameter models and more recently 50 billion parameter translation models [ AJF19 ], though only a small fraction of the parameters are actually used on each forward pass. A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time [ Gra16 ] and the universal transformer [ DGV + 18 ]",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_73"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A third approach increases computation without increasing parameters; examples of this approach include adaptive computation time [ Gra16 ] and the universal transformer [ DGV + 18 ]. Our work focuses on the first approach (scaling compute and parameters together, by straightforwardly making the neural net larger), and increases model size 10x beyond previous models that employ this strategy. Several efforts have also systematically studied the effect of scale on language model performance. [ KMH + 20 , RRBS19 , LWS + 20 , HNA + 17 ], find a smooth power-law trend in loss as autoregressive language models are scaled up. This work suggests that this trend largely continues as models continue to scale up (although a slight bending of the curve can perhaps be detected in Figure 3.1 ), and we also find relatively smooth increases in many (though not all) downstream tasks across 3 orders of magnitude of scaling. Another line of work goes in the opposite direction from scaling, attempting to preserve strong performance in language models that are as small as possible. This approach includes ALBERT [ LCG + 19 ] as well as general [ HVD15 ] and 39 task-specific [ SDCW19 , JYS + 19 , KR16 ] approaches to distillation of language models. These architectures and techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint of giant models. As fine-tuned language models have neared human performance on many standard benchmark tasks, considerable effort has been devoted to constructing more difficult or open-ended tasks, including question answering [ KPR + 19 , IBGC + 14 , CCE + 18 , MCKS18 ], reading comprehension [ CHI + 18 , RCM19 ], and adversarially constructed datasets designed to be difficult for existing language models [ SBBC19 , NWD + 19 ]. In this work we test our models on many of these datasets. Many previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_74"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In this work we test our models on many of these datasets. Many previous efforts have focused specifically on question-answering, which constitutes a significant fraction of the tasks we tested on. Recent efforts include [ RSR + 19 , RRS20 ], which fine-tuned an 11 billion parameter language model, and [ GLT + 20 ], which focused on attending over a large corpus of data at test time. Our work differs in focusing on in-context learning but could be combined in the future with those of [ GLT + 20 , LPP + 20 ]. Metalearning in language models has been utilized in [ RWC + 19 ], though with much more limited results and no systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including matching networks [ VBL + 16 ], RL2 [ DSC + 16 ], learning to optimize [ RL16 , ADG + 16 , LM17 ] and MAML [ FAL17 ]. Our approach of stuffing the model’s context with previous examples is most structurally similar to RL2 and also resembles [ HYC01 ], in that an inner loop of adaptation takes place through computation in the model’s activations across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training) updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks defined at inference-time. Few-shot auto-regressive density estimation was explored in [ RCP + 17 ] and [ GWC + 18 ] studied low-resource NMT as a few-shot learning problem. While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained language models in combination with gradient descent to perform few-shot learning [ SS20 ]. Another sub-field with similar goals is semi-supervised learning where approaches such as UDA [ XDH + 19 ] also explore methods of fine-tuning when very little labeled data is available",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_75"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Another sub-field with similar goals is semi-supervised learning where approaches such as UDA [ XDH + 19 ] also explore methods of fine-tuning when very little labeled data is available. Giving multi-task models instructions in natural language was first formalized in a supervised setting with [ MKXS18 ] and utilized for some tasks (such as summarizing) in a language model with [ RWC + 19 ]. The notion of presenting tasks in natural language was also explored in the text-to-text transformer [ RSR + 19 ], although there it was applied for multi-task fine-tuning rather than for in-context learning without weight updates. Another approach to increasing generality and transfer-learning capability in language models is multi-task learning [ Car97 ], which fine-tunes on a mixture of downstream tasks together, rather than separately updating the weights for each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the weights (similar to our in-context learning approach), or alternatively could improve sample efficiency when updating the weights for a new task. Multi-task learning has shown some promising initial results [ LGH + 15 , LSP + 18 ] and multi-stage fine-tuning has recently become a standardized part of SOTA results on some datasets [ PFB18 ] and pushed the boundaries on certain tasks [ KKS + 20 ], but is still limited by the need to manually curate collections of datasets and set up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate a broader set of explicit tasks for multi-task learning, for example through procedural generation [ TFR + 17 ], human interaction [ ZSW + 19b ], or active learning [ Mac92 ]",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_76"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Algorithmic innovation in language models over the last two years has been enormous, including denoising-based bidirectionality [ DCLT18 ], prefixLM [ DL15 ] and encoder-decoder architectures [ LLG + 19 , RSR + 19 ], random permtations during training [ YDY + 19 ], architectures that improve the efficiency of sampling [ DYY + 19 ], improvements in data and training procedures [ LOG + 19 ], and efficiency increases in the embedding parameters [ LCG + 19 ]. Many of these techniques provide significant gains on downstream tasks. In this work we continue to focus on pure autoregressive language models, both in order to focus on in-context learning performance and to reduce the complexity of our large model implementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s performance on downstream tasks, especially in the fine-tuning setting, and combining GPT-3’s scale with these algorithmic techniques is a promising direction for future work. 8 Conclusion We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of 40 state-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems. Acknowledgements The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI’s infrastructure",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_77"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI’s infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale. 41 Contributions Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies. Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments. Ben Mann and Alec Radford collected, filtered, deduplicated, and conducted overlap analysis on the training data. Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks. Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_78"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods. Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training. Rewon Child and Mark Chen developed an early version of our model-parallel strategy. Rewon Child and Scott Gray contributed the sparse transformer. Aditya Ramesh experimented with loss scaling strategies for pretraining. Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search. Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature. Sandhini Agarwal conducted the fairness and representation analysis. Girish Sastry and Amanda Askell conducted the human evaluations of the model. Ariel Herbert-Voss conducted the threat analysis of malicious use. Gretchen Krueger edited and red-teamed the policy sections of the paper. Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI’s clusters to run the largest models efficiently. Scott Gray developed fast GPU kernels used during training. Jack Clark led the analysis of ethical impacts — fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work. Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper. Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_79"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work. Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training. Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work. Dario Amodei designed and led the research. 42 A Details of Common Crawl Filtering As mentioned in Section 2.2 , we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication: 1. In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark’s standard tokenizer and HashingTF 10 . For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff np.random.pareto ( α ) > 1 − document_score We chose α = 9 in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. α was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_80"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark’s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C . B Details of Model Training To train all versions of GPT-3, we use Adam with β 1 = 0 . 9 , β 2 = 0 . 95 , and ε = 10 − 8 , we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting. All models use weight decay of 0.1 to provide a small amount of regularization [ LH17 ]. During training we always train on sequences of the full n ctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_81"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This allows for efficient training without need for any special sequence-specific masking. C Details of Test Set Contamination Studies In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results. Initial training set filtering We attempted to remove text occurring in benchmarks from training data by searching for 13 − gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding 13 − gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than 200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and 10 https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF 43 removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book. We ignored 13 − grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository 11 . Overlap methodology For our benchmark overlap analysis in Section 4 , we used a variable number of words N to check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_82"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data marked as dirty are shown in Table C.1 . Unlike GPT-2’s use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our filtered Common Crawl documents per Section 2.2 . We define a ‘dirty’ example as one with any N -gram overlap with any training document, and a ‘clean’ example as one with no collision. Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1 Overlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly better , our filtering scheme may have preferentially marked easier examples as dirty",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_83"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". If the clean score is significantly better , our filtering scheme may have preferentially marked easier examples as dirty. This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after filtering come from a slightly different distribution than the dirty examples. Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we flagged for further review",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_84"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 11 https://github.com/openai/gpt-3/blob/master/overlap_frequency.md 44 Name Split Metric N Acc/F1/BLEU Total Count Dirty Acc/F1/BLEU Dirty Count Clean Acc/F1/BLEU Clean Count Clean Percentage Relative Difference Clean vs All Quac dev f1 13 44.3 7353 44.3 7315 54.1 38 1% 20% SQuADv2 dev f1 13 69.8 11873 69.9 11136 68.4 737 6% -2% DROP dev f1 13 36.5 9536 37.0 8898 29.5 638 7% -21% Symbol Insertion dev acc 7 66.9 10000 66.8 8565 67.1 1435 14% 0% CoQa dev f1 13 86.0 7983 85.3 5107 87.1 2876 36% 1% ReCoRD dev acc 13 89.5 10000 90.3 6110 88.2 3890 39% -1% Winograd test acc 9 88.6 273 90.2 164 86.2 109 40% -3% BoolQ dev acc 13 76.0 3270 75.8 1955 76.3 1315 40% 0% MultiRC dev acc 13 74.2 953 73.4 558 75.3 395 41% 1% RACE-h test acc 13 46.8 3498 47.0 1580 46.7 1918 55% 0% LAMBADA test acc 13 86.4 5153 86.9 2209 86.0 2944 57% 0% LAMBADA (No Blanks) test acc 13 77.8 5153 78.5 2209 77.2 2944 57% -1% WSC dev acc 13 76.9 104 73.8 42 79.0 62 60% 3% PIQA dev acc 8 82.3 1838 89.9 526 79.3 1312 71% -4% RACE-m test acc 13 58.5 1436 53.0 366 60.4 1070 75% 3% De → En 16 test bleu-sb 12 43.0 2999 47.4 739 40.8 2260 75% -5% En → De 16 test bleu-sb 12 30.9 2999 32.6 739 29.9 2260 75% -3% En → Ro 16 test bleu-sb 12 25.8 1999 24.9 423 26.1 1576 79% 1% Ro → En 16 test bleu-sb 12 41.3 1999 40.4 423 41.6 1576 79% 1% WebQs test acc 8 41.5 2032 41.6 428 41.5 1604 79% 0% ANLI R1 test acc 13 36.8 1000 40.5 200 35.9 800 80% -3% ANLI R2 test acc 13 34.0 1000 29.4 177 35.0 823 82% 3% TriviaQA dev acc 10 71.2 7993 70.8 1390 71.3 6603 83% 0% ANLI R3 test acc 13 40.2 1200 38.3 196 40.5 1004 84% 1% En → Fr 14 test bleu-sb 13 39.9 3003 38.3 411 40.3 2592 86% 1% Fr → En 14 test bleu-sb 13 41.4 3003 40.9 411 41.4 2592 86% 0% WiC dev acc 13 51.4 638 53.1 49 51.3 589 92% 0% RTE dev acc 13 71.5 277 71.4 21 71.5 256 92% 0% CB dev acc 13 80.4 56 100.0 4 78.8 52 93% -2% Anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7% Reversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26% OpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1% ARC (Easy) test",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_85"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "93% -2% Anagrams 2 dev acc 2 40.2 10000 76.2 705 37.4 9295 93% -7% Reversed Words dev acc 2 0.4 10000 1.5 660 0.3 9340 93% -26% OpenBookQA test acc 8 65.4 500 58.1 31 65.9 469 94% 1% ARC (Easy) test acc 11 70.1 2268 77.5 89 69.8 2179 96% 0% Anagrams 1 dev acc 2 15.0 10000 49.8 327 13.8 9673 97% -8% COPA dev acc 9 93.0 100 100.0 3 92.8 97 97% 0% ARC (Challenge) test acc 12 51.6 1144 45.2 31 51.8 1113 97% 0% HellaSwag dev acc 13 79.3 10042 86.2 152 79.2 9890 98% 0% NQs test acc 11 29.9 3610 32.7 52 29.8 3558 99% 0% Cycled Letters dev acc 2 38.6 10000 20.5 73 38.7 9927 99% 0% SAT Analogies dev acc 9 65.8 374 100.0 2 65.6 372 99% 0% StoryCloze test acc 13 87.7 1871 100.0 2 87.6 1869 100% 0% Winogrande dev acc 13 77.7 1267 - 0 77.7 1267 100% 0% Table C.1: Overlap statistics for all datasets sorted from dirtiest to cleanest",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_86"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We consider a dataset example dirty if it has a single N -gram collision with any document in our training corpus. “Relative Difference Clean vs All” shows the percent change in performance between only the clean examples vs all the examples in the benchmark. “Count” shows the number of examples. “Clean percentage” is the percent of examples that are clean vs total. For “Acc/F1/BLEU” we use the metric specified in “Metric”. These scores come from evaluations with a different seed for the random examples used for in-context learning, and will therefore differ slightly from the scores elsewhere in the paper. 45 D Total Compute Used to Train Language Models This appendix contains the calculations that were used to derive the approximate compute used to train the language models in Figure 2.2 . As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10% of the total compute for the models we are analyzing. Calculations can be seen in Table D.1 and are explained within the table caption",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_87"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Calculations can be seen in Table D.1 and are explained within the table caption. Model Total train compute (PF-days) Total train compute (flops) Params (M) Training tokens (billions) Flops per param per token Mult for bwd pass Fwd-pass flops per active param per token Frac of params active for each token T5-Small 2.08E+00 1.80E+20 60 1,000 3 3 1 0.5 T5-Base 7.64E+00 6.60E+20 220 1,000 3 3 1 0.5 T5-Large 2.67E+01 2.31E+21 770 1,000 3 3 1 0.5 T5-3B 1.04E+02 9.00E+21 3,000 1,000 3 3 1 0.5 T5-11B 3.82E+02 3.30E+22 11,000 1,000 3 3 1 0.5 BERT-Base 1.89E+00 1.64E+20 109 250 6 3 2 1.0 BERT-Large 6.16E+00 5.33E+20 355 250 6 3 2 1.0 RoBERTa-Base 1.74E+01 1.50E+21 125 2,000 6 3 2 1.0 RoBERTa-Large 4.93E+01 4.26E+21 355 2,000 6 3 2 1.0 GPT-3 Small 2.60E+00 2.25E+20 125 300 6 3 2 1.0 GPT-3 Medium 7.42E+00 6.41E+20 356 300 6 3 2 1.0 GPT-3 Large 1.58E+01 1.37E+21 760 300 6 3 2 1.0 GPT-3 XL 2.75E+01 2.38E+21 1,320 300 6 3 2 1.0 GPT-3 2.7B 5.52E+01 4.77E+21 2,650 300 6 3 2 1.0 GPT-3 6.7B 1.39E+02 1.20E+22 6,660 300 6 3 2 1.0 GPT-3 13B 2.68E+02 2.31E+22 12,850 300 6 3 2 1.0 GPT-3 175B 3.64E+03 3.14E+23 174,600 300 6 3 2 1.0 Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each model was trained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are active for each token during a forward or backwards pass. We then note that each token is involved in a single addition and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of 3x to account for the backwards pass (as computing both ∂params ∂loss and ∂acts ∂loss use a similar amount of compute as the forwards pass. Combining the previous two numbers, we get the total flops per parameter per token. We multiply this value by the total training tokens and the total parameters to yield the number of total flops used during training. We report both flops and petaflop/s-day (each of which are 8.64e+19 flops)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_88"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We report both flops and petaflop/s-day (each of which are 8.64e+19 flops). E Human Quality Assessment of Synthetic News Articles This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the ∼ 200 word news articles, and then describe the preliminary investigation of ∼ 500 word news articles generated by GPT-3. Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was ∼ 38 years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $ 12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once. Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants’ judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_89"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text. 46 Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age Average Word Count (human:model) Control 76 7 32:37:0 39 216:216 GPT-3 Small 80 7 41:31:1 40 216:188 GPT-3 Medium 80 7 46:28:2 39 216:202 GPT-3 Large 81 24 46:28:2 37 216:200 GPT-3 XL 79 14 32:32:1 38 216:199 GPT-3 2.7B 80 11 36:33:0 40 216:202 GPT-3 6.7B 76 5 46:28:2 37 216:195 GPT-3 13.0B 81 13 46:28:2 37 216:209 GPT-3 175B 80 9 42:29:0 37 216:216 Table E.1: Participant details and article lengths for each experiment to evaluate human detection of ∼ 200 word model generated news articles. Participants were excluded due to internet check fails. Figure E.1: Participants spend more time trying to identify whether each news article is machine generated as model size increases. Duration on the control model is indicated with the dashed line. Line of best fit is a linear model on a log scale with 95% confidence intervals. In each experiment, half of the participants were randomly assigned to quiz A and half were randomly assigned to quiz B. Each quiz consisted of 25 articles: half (12-13) were human written and half (12-13) were model generated: the articles with human written completions in quiz A had model generated completions in quiz B and vice versa. The order of quiz question was shuffled for each participant. Participants could leave comments and were asked to indicate if they had seen the articles before. Participants were instructed not to look up the articles or their content during the quiz and at the end of the quiz were asked if they had looked anything up during the quiz. Statistical Tests: To compare means on the different runs, we performed a two-sample t-test for independent groups for each model against the control. This was implemented in Python using the scipy.stats.ttest_ind function",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_90"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This was implemented in Python using the scipy.stats.ttest_ind function. When plotting a regression line in the graph of average participant accuracy vs model size, we fit a power law of the form ax − b . The 95% confidence intervals were estimated from the t-distribution of the sample mean. Duration statistics : In the main text, we discussed the finding that the ability of human participants to distinguish model and human generated news articles decreases as our models become larger. We have also found that the average time spent for a given set of questions increases as the model size increases, as shown in Figure E.1 . Lower 47 Model Participants Recruited Participants Excluded Genders (m:f:other) Mean Age Average Word Count (human:model) Control 79 17 32:37:0 39 569:464 GPT-3 175B 81 19 32:30:0 40 569:498 Table E.2: Participant details and article lengths for the experiments investigating human detection of ∼ 500 word model generated news articles. Participants were excluded due to internet check fails. accuracy scores despite increased time investment from participants supports the finding that larger models generate harder-to-distinguish news articles. Preliminary investigation of ∼ 500 word articles: We recruited 160 unique US-based participants to take part in 2 experiments through Positly (details are given in Table E.2 ). We randomly selected 12 Reuters world news articles from late 2019 and created a context for GPT-3 175B that consisted of a single Reuters article not in this set of 12. We then used the article titles and Reuters locations to generate completions from GPT-3 175B and the 160M control model from the previous experiments. These were used to create two 12-question quizzes per model, each consisting of half human written and half model generated articles. Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $ 12 for this task",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_91"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Comprehension questions were added and articles were shown to participants in 3 stages at 30 second intervals to encourage closer reading. Participants were paid $ 12 for this task. Model generation selection methods, exclusion criteria, and statistical tests mirror those of the previous experiments. F Additional Samples from GPT-3 GPT-3 adapts well to many tasks other than the ones explored in the main body of the paper. As an example, in Figure F.1 , we show four uncurated samples from a prompt suggesting that the model write a poem, with a given title, in the style of Wallace Stevens. We first experimented with a few prompts, then generated four samples with no additional editing or selection (sampling at temperature 1 using nucleus sampling [ HBFC19 ] with P = 0 . 9 ). Completions were truncated when the model began to write a new title and author heading, or broke into prose commentary. 48 Context → The City BY C. P. CAVAFY TRANSLATED BY EDMUND KEELEY [Poem text omitted] SOME TREES John Ashbery [Poem text omitted] Shadows on the Way Wallace Stevens -------- Generated Poem 1 -------- -------- Generated Poem 3 -------- I must have shadows on the way The sun was all we had. Now, in the shade If I am to walk I must have All is changed. The mind must dwell on those Each step taken slowly and alone White fields, that to its eyes were always old; To have it ready made Those ancient gleams, convoluting The way they lay among the huge roots, And I must think in lines of grey The great dark tomes of reverie, To have dim thoughts to be my guide The plumed labyrinth of the sea. Must look on blue and green And never let my eye forget When our own breath gonne faint and fade That color is my friend Up the long stairway of the dead And purple must surround me too We shall remember well The blindness of the light, the whiteness The yellow of the sun is no more Of that white land. The footsteps, and Intrusive than the bluish snow The grasses in that light, and in the shade That falls on all of us",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_92"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The footsteps, and Intrusive than the bluish snow The grasses in that light, and in the shade That falls on all of us. I must have The menacing life. Grey thoughts and blue thoughts walk with me If I am to go away at all. We played, too, once, in the warmest rooms. To one content, there is one content -------- Generated Poem 2 -------- For all mankind. The forest glades Are all the more for being in fable, There is a way in the world. And wood and town in legend shadoweThat way a man may live in a small town Once more to return. The one content His lifetime and see no man come, Has in it every happiness, more brave Never the stir of men about a market, For being purposeless; and so Never the bobbing of covered carts, Those island gardens, deep in the mind, Never the creak passed on the stair Make of remoteness familiarity Closing some other life; an old man And of the unspoken name a friend. Can prosper in that town, can prosper When his store is seen to close, his wife -------- Generated Poem 4 -------- Fades in his house, and the world shouts in vain That no one comes to hear him Nobody will come to this place. It is a road that leads nowhere. Where his small bell tolls. The solitude is deep. The mountains are high. In that small town But they are desolate, and they turn the traveler’s face Everything is known. Towards the North. All the sounds of the world are far away. The ways of the young men are known, When the wind rises above the trees, And the ways of the young women The boughs bow to the ground. As they walk to the Spring; Even the birds that inhabit the tangle of weeds The rumors are all known, That is the roadside cover, are silent. One listens, The quarrels, the hurt, the envy But hears no roar of the forest. One is alone. Of the men and the women. One will be taken. Till a day comes One will be taken. That way out of the world There is no utterance, there is no conversation, That leads to a wood",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_93"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One is alone. Of the men and the women. One will be taken. Till a day comes One will be taken. That way out of the world There is no utterance, there is no conversation, That leads to a wood. But one is uneasy all the sameYoung, the young man There is a thin blue mist, Walks, a lover now, A darkness rising like smoke, Knows the streets as they are and is of them. And within that darkness A possession of the heart. Comes to the wood. One will be takenIt was here, and it will be here agaiHere, under this sky empty and full of light. There, in the wood, among trees, He sees shadows on the way, hears voices, hears the wind and the rustling of leaves; Through an open glade He sees a shape and the shape hears: It waits as he waits, Waits as the shadows wait, As the voices wait; Shadows on the way, voices in the wind. Figure F.1: Four uncurated completions from a context suggesting the model compose a poem in the style of Wallace Stevens with the title ‘Shadows on the Way’. 49 G Details of Task Phrasing and Specifications The following figures illustrate the formatting and phrasing of all the tasks included in the paper. All data comes from the ground truth datasets in this section, and no samples from GPT-3 are included here. Context → Article: Informal conversation is an important part of any business relationship.Before you start a discussion,however,make sure you understand which topics are suitable and which are considered taboo in a particular culture. Latin Americans enjoy sharing information about their local history, art and customs.You may expect questions about your family,and be sure to show pictures of your children.You may feel free to ask similar questions of your Latin American friends.The French think of conversation as an art form,and they enjoy the value of lively discussions as well as disagreements. For them,arguments can be interesting and they can cover pretty much or any topic ---- as long as they occur in are respectful and intelligent manner",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_94"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For them,arguments can be interesting and they can cover pretty much or any topic ---- as long as they occur in are respectful and intelligent manner. In the United States,business people like to discuss a wide range of topics,including opinions about work,family,hobbies,and politics. In Japan,China,and Korea,however,people are much more private.They do not share much about their thoughts,feelings,or emotions because they feel that doing so might take away from the harmonious business relationship they’re trying to build.Middle Easterners are also private about their personal lives and family matters.It is considered rude,for example,to ask a businessman from Saudi Arabia about his wife or children. As a general rule,it’s best not to talk about politics or religion with your business friends.This can get you into trouble,even in the United States,where people hold different religious views.In addition,discussing one’s salary is usually considered unsuitable.Sports is typically a friendly subject in most parts of the world,although be careful not to criticize national sport.Instead,be friendly and praise your host’s team. Q: What shouldn’t you do when talking about sports with colleagues from another country? A: Criticizing the sports of your colleagues’ country. Q: Which is typically a friendly topic in most places according to the author? A: Sports. Q: Why are people from Asia more private in their conversation with others? A: They don’t want to have their good relationship with others harmed by informal conversation. Q: The author considers politics and religion . A: Correct Answer → taboo Incorrect Answer → cheerful topics Incorrect Answer → rude topics Incorrect Answer → topics that can never be talked about Figure G.1: Formatted dataset example for RACE-h. When predicting, we normalize by the unconditional probability of each answer as described in 2 . 50 Context → anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_95"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 50 Context → anli 2: anli 2: The Gold Coast Hotel & Casino is a hotel and casino located in Paradise, Nevada. This locals’ casino is owned and operated by Boyd Gaming. The Gold Coast is located one mile ( ∼ 1 . 6km ) west of the Las Vegas Strip on West Flamingo Road. It is located across the street from the Palms Casino Resort and the Rio All Suite Hotel and Casino. Question: The Gold Coast is a budget-friendly casino. True, False, or Neither? Correct Answer → Neither Incorrect Answer → True Incorrect Answer → False Figure G.2: Formatted dataset example for ANLI R2 Context → Article: Mrs. Smith is an unusual teacher. Once she told each student to bring along a few potatoes in plastic bag. On each potato the students had to write a name of a person that they hated And the next day, every child brought some potatoes. Some had two potatoes;some three;some up to five. Mrs. Smith then told the children to carry the bags everywhere they went, even to the toilet, for two weeks. As day after day passed, the children started to complain about the awful smell of the rotten potatoes. Those children who brought five potatoes began to feel the weight trouble of the bags. After two weeks, the children were happy to hear that the game was finally ended. Mrs. Smith asked,\"How did you feel while carrying the potatoes for two weeks?\" The children started complaining about the trouble loudly. Then Mrs. Smith told them why she asked them to play the game. She said,\"This is exactly the situation when you carry your hatred for somebody inside your heart. The terrible smell of the hatred will pollute your heart and you will carry something unnecessary with you all the time. If you cannot stand the smell of the rotten potatoes for just two weeks, can you imagine how heavy it would be to have the hatred in your heart for your lifetime? So throw away any hatred from your heart, and you’ll be really happy.\" Q: Which of the following is True according to the passage? A: If a kid hated four people,he or she had to carry four potatoes",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_96"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Q: We can learn from the passage that we should . A: throw away the hatred inside Q: The children complained about besides the weight trouble. A: the smell Q: Mrs.Smith asked her students to write on the potatoes. A: Correct Answer → names Incorrect Answer → numbers Incorrect Answer → time Incorrect Answer → places Figure G.3: Formatted dataset example for RACE-m. When predicting, we normalize by the unconditional probability of each answer as described in 2 . 51 Context → How to apply sealant to wood. Correct Answer → Using a brush, brush on sealant onto wood until it is fully saturated with the sealant. Incorrect Answer → Using a brush, drip on sealant onto wood until it is fully saturated with the sealant. Figure G.4: Formatted dataset example for PIQA Context → My body cast a shadow over the grass because Correct Answer → the sun was rising. Incorrect Answer → the grass was cut. Figure G.5: Formatted dataset example for COPA Context → (CNN) Yuval Rabin, whose father, Yitzhak Rabin, was assassinated while serving as Prime Minister of Israel, criticized Donald Trump for appealing to \"Second Amendment people\" in a speech and warned that the words that politicians use can incite violence and undermine democracy. \"Trump’s words are an incitement to the type of political violence that touched me personally,\" Rabin wrote in USAToday. He said that Trump’s appeal to \"Second Amendment people\" to stop Hillary Clinton -- comments that were criticized as a call for violence against Clinton, something Trump denied -- \"were a new level of ugliness in an ugly campaign season.\" - The son of a former Israeli Prime Minister who was assassinated wrote an op ed about the consequence of violent political rhetoric. - Warns of \"parallels\" between Israel of the 1990s and the U.S. today. Correct Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump’s aggressive rhetoric",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_97"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". today. Correct Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Donald Trump’s aggressive rhetoric. Correct Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Trump’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Hillary Clinton’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned U.S.’s aggressive rhetoric. Incorrect Answer → - Referencing his father, who was shot and killed by an extremist amid political tension in Israel in 1995, Rabin condemned Yitzhak Rabin’s aggressive rhetoric. Figure G.6: Formatted dataset example for ReCoRD. We consider the context above to be a single ”problem” because this is how the task is presented in the ReCoRD dataset and scored in the ReCoRD evaluation script. Context → anli 1: anli 1: Fulton James MacGregor MSP is a Scottish politician who is a Scottish National Party (SNP) Member of Scottish Parliament for the constituency of Coatbridge and Chryston. MacGregor is currently Parliamentary Liaison Officer to Shona Robison, Cabinet Secretary for Health & Sport. He also serves on the Justice and Education & Skills committees in the Scottish Parliament. Question: Fulton James MacGregor is a Scottish politican who is a Liaison officer to Shona Robison who he swears is his best friend. True, False, or Neither? Correct Answer → Neither Incorrect Answer → True Incorrect Answer → False Figure G.7: Formatted dataset example for ANLI R1 52 Context → Organisms require energy in order to do what? Correct Answer → mature and develop. Incorrect Answer → rest soundly. Incorrect Answer → absorb light. Incorrect Answer → take in nutrients. Figure G.8: Formatted dataset example for OpenBookQA",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_98"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Incorrect Answer → rest soundly. Incorrect Answer → absorb light. Incorrect Answer → take in nutrients. Figure G.8: Formatted dataset example for OpenBookQA. When predicting, we normalize by the unconditional probability of each answer as described in 2 . Context → Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They Correct Answer → bake them, then frost and decorate. Incorrect Answer → taste them as they place them on plates. Incorrect Answer → put the frosting on the cake as they pan it. Incorrect Answer → come out and begin decorating the cake as well. Figure G.9: Formatted dataset example for HellaSwag Context → anli 3: anli 3: We shut the loophole which has American workers actually subsidizing the loss of their own job. They just passed an expansion of that loophole in the last few days: $ 43 billion of giveaways, including favors to the oil and gas industry and the people importing ceiling fans from China. Question: The loophole is now gone True, False, or Neither? Correct Answer → False Incorrect Answer → True Incorrect Answer → Neither Figure G.10: Formatted dataset example for ANLI R3 Context → Question: George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat? Answer: Correct Answer → dry palms Incorrect Answer → wet palms Incorrect Answer → palms covered with oil Incorrect Answer → palms covered with lotion Figure G.11: Formatted dataset example for ARC (Challenge). When predicting, we normalize by the unconditional probability of each answer as described in 2 . Context → lull is to trust as Correct Answer → cajole is to compliance Incorrect Answer → balk is to fortitude Incorrect Answer → betray is to loyalty Incorrect Answer → hinder is to destination Incorrect Answer → soothe is to passion Figure G.12: Formatted dataset example for SAT Analogies Correct Context → Grace was happy to trade me her sweater for my jacket",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_99"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". She thinks the sweater Incorrect Context → Grace was happy to trade me her sweater for my jacket. She thinks the jacket Target Completion → looks dowdy on her. Figure G.13: Formatted dataset example for Winograd. The ‘partial’ evaluation method we use compares the probability of the completion given a correct and incorrect context. 53 Correct Context → Johnny likes fruits more than vegetables in his new keto diet because the fruits Incorrect Context → Johnny likes fruits more than vegetables in his new keto diet because the vegetables Target Completion → are saccharine. Figure G.14: Formatted dataset example for Winogrande. The ‘partial’ evaluation method we use compares the probability of the completion given a correct and incorrect context. Context → READING COMPREHENSION ANSWER KEY While this process moved along, diplomacy continued its rounds. Direct pressure on the Taliban had proved unsuccessful. As one NSC staff note put it, \"Under the Taliban, Afghanistan is not so much a state sponsor of terrorism as it is a state sponsored by terrorists.\" In early 2000, the United States began a high-level effort to persuade Pakistan to use its influence over the Taliban. In January 2000, Assistant Secretary of State Karl Inderfurth and the State Department’s counterterrorism coordinator, Michael Sheehan, met with General Musharraf in Islamabad, dangling before him the possibility of a presidential visit in March as a reward for Pakistani cooperation. Such a visit was coveted by Musharraf, partly as a sign of his government’s legitimacy. He told the two envoys that he would meet with Mullah Omar and press him on Bin Laden. They left, however, reporting to Washington that Pakistan was unlikely in fact to do anything,\" given what it sees as the benefits of Taliban control of Afghanistan.\" President Clinton was scheduled to travel to India. The State Department felt that he should not visit India without also visiting Pakistan",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_100"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The State Department felt that he should not visit India without also visiting Pakistan. The Secret Service and the CIA, however, warned in the strongest terms that visiting Pakistan would risk the President’s life. Counterterrorism officials also argued that Pakistan had not done enough to merit a presidential visit. But President Clinton insisted on including Pakistan in the itinerary for his trip to South Asia. His one-day stopover on March 25, 2000, was the first time a U.S. president had been there since 1969. At his meeting with Musharraf and others, President Clinton concentrated on tensions between Pakistan and India and the dangers of nuclear proliferation, but also discussed Bin Laden. President Clinton told us that when he pulled Musharraf aside for a brief, one-on-one meeting, he pleaded with the general for help regarding Bin Laden.\" I offered him the moon when I went to see him, in terms of better relations with the United States, if he’d help us get Bin Laden and deal with another issue or two.\" The U.S. effort continued. Who did The State Department feel should visit both India and Pakistan? Correct Answer → - [False] Bin Laden Incorrect Answer → - [True] Bin Laden Figure G.15: Formatted dataset example for MultiRC. There are three levels within MultiRC: (1) the passage, (2) the questions, and (3) the answers. During evaluation, accuracy is determined at the per-question level, with a question being considered correct if and only if all the answers within the question are labeled correctly. For this reason, we use K to refer to the number of questions shown within the context. Context → Question: Which factor will most likely cause a person to develop a fever? Answer: Correct Answer → a bacterial population in the bloodstream Incorrect Answer → a leg muscle relaxing after exercise Incorrect Answer → several viral particles on the skin Incorrect Answer → carbohydrates being digested in the stomach Figure G.16: Formatted dataset example for ARC (Easy)",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_101"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". When predicting, we normalize by the unconditional probability of each answer as described in 2 . 54 Context → Bob went to the gas station to fill up his car. His tank was completely empty and so was his wallet. The cashier offered to pay for his gas if he came back later to pay. Bob felt grateful as he drove home. Correct Answer → Bob believed that there were good people in the world. Incorrect Answer → Bob contemplated how unfriendly the world was. Figure G.17: Formatted dataset example for StoryCloze Context → Helsinki is the capital and largest city of Finland. It is in the region of Uusimaa, in southern Finland, on the shore of the Gulf of Finland. Helsinki has a population of , an urban population of , and a metropolitan population of over 1.4 million, making it the most populous municipality and urban area in Finland. Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world’s northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland’s major political, educational, financial, cultural, and research center as well as one of northern Europe’s major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_102"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Q: what is the most populous municipality in Finland? A: Helsinki Q: how many people live there? A: 1.4 million in the metropolitan area Q: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75% Q: what towns are a part of the metropolitan area? A: Target Completion → Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns Figure G.18: Formatted dataset example for CoQA Context → Please unscramble the letters into a word, and write that word: asinoc = Target Completion → casino Figure G.19: Formatted dataset example for Cycled Letters 55 Context → Passage: Saint Jean de Br ́ebeuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries. In 1649, Br ́ebeuf and another missionary were captured when an Iroquois raid took over a Huron village . Together with Huron captives, the missionaries were ritually tortured and killed on March 16, 1649. Br ́ebeuf was beatified in 1925 and among eight Jesuit missionaries canonized as saints in the Roman Catholic Church in 1930. Question: How many years did Saint Jean de Br ́ebeuf stay in New France before he went back to France for a few years? Answer: Target Completion → 4 Figure G.20: Formatted dataset example for DROP Context → Fill in blank: She held the torch in front of her. She caught her breath. \"Chris? There’s a step.\" \"What?\" \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there’s more than a",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_103"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". \"Chris? There’s a step.\" \"What?\" \"A step. Cut in the rock. About fifty feet ahead.\" She moved faster. They both moved faster. \"In fact,\" she said, raising the torch higher, \"there’s more than a . - > Target Completion → step Figure G.21: Formatted dataset example for LAMBADA Context → Please unscramble the letters into a word, and write that word: skicts = Target Completion → sticks Figure G.22: Formatted dataset example for Anagrams 1 (A1) Context → Please unscramble the letters into a word, and write that word: volwskagen = Target Completion → volkswagen Figure G.23: Formatted dataset example for Anagrams 2 Context → Q: Who played tess on touched by an angel? A: Target Completion → Delloreese Patricia Early (July 6, 1931 { November 19, 2017), known professionally as Della Reese Figure G.24: Formatted dataset example for Natural Questions 56 Context → TITLE: William Perry (American football) - Professional career PARAGRAPH: In 1985, he was selected in the first round of the 1985 NFL Draft by the Chicago Bears; he had been hand-picked by coach Mike Ditka. However, defensive coordinator Buddy Ryan, who had a highly acrimonious relationship with Ditka, called Perry a \"wasted draft-pick\". Perry soon became a pawn in the political power struggle between Ditka and Ryan. Perry’s \"Refrigerator\" nickname followed him into the NFL and he quickly became a favorite of the Chicago Bears fans. Teammates called him \"Biscuit,\" as in \"one biscuit shy of 350 pounds.\" While Ryan refused to play Perry, Ditka decided to use Perry as a fullback when the team was near the opponents’ goal line or in fourth and short situations, either as a ball carrier or a lead blocker for star running back Walter Payton. Ditka stated the inspiration for using Perry as a fullback came to him during five-yard sprint exercises. During his rookie season, Perry rushed for two touchdowns and caught a pass for one. Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team’s success",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_104"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Perry even had the opportunity to run the ball during Super Bowl XX, as a nod to his popularity and contributions to the team’s success. The first time he got the ball, he was tackled for a one-yard loss while attempting to throw his first NFL pass on a halfback option play. The second time he got the ball, he scored a touchdown (running over Patriots linebacker Larry McGrew in the process). About halfway through his rookie season, Ryan finally began to play Perry, who soon proved that he was a capable defensive lineman. His Super Bowl ring size is the largest of any professional football player in the history of the event. His ring size is 25, while the ring size for the average adult male is between 10 and 12. Perry went on to play for ten years in the NFL, retiring after the 1994 season. In his ten years as a pro, he regularly struggled with his weight, which hampered his performance at times. He played in 138 games, recording 29.5 sacks and five fumble recoveries, which he returned for a total of 71 yards. In his offensive career he ran five yards for two touchdowns, and had one reception for another touchdown. Perry later attempted a comeback, playing an unremarkable 1996 season with the London Monarchs of the World League of American Football (later NFL Europa). Q: what team did he play for? A: Target Completion → the Chicago Bears Figure G.25: Formatted dataset example for QuAC Context → Please unscramble the letters into a word, and write that word: r e!c.i p r o.c a/l = Target Completion → reciprocal Figure G.26: Formatted dataset example for Symbol Insertion Context → Please unscramble the letters into a word, and write that word: taefed = Target Completion → defeat Figure G.27: Formatted dataset example for Reversed Words 57 Context → Title: The Blitz Background: From the German point of view, March 1941 saw an improvement. The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_105"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The Luftwaffe flew 4,000 sorties that month, including 12 major and three heavy attacks. The electronic war intensified but the Luftwaffe flew major inland missions only on moonlit nights. Ports were easier to find and made better targets. To confuse the British, radio silence was observed until the bombs fell. and Y-Ger ̈at beams were placed over false targets and switched only at the last minute. Rapid frequency changes were introduced for X-Ger ̈at, whose wider band of frequencies and greater tactical flexibility ensured it remained effective at a time when British selective jamming was degrading the effectiveness of Y-Ger ̈at. Q: How many sorties were flown in March 1941? A: 4,000 Q: When did the Luftwaffe fly inland missions? A: Target Completion → only on moonlit nights Figure G.28: Formatted dataset example for SQuADv2 Context → Normal force -- In a simple case such as an object resting upon a table, the normal force on the object is equal but in opposite direction to the gravitational force applied on the object (or the weight of the object), that is, N = m g ( \\ displaystyle N=mg), where m is mass, and g is the gravitational field strength (about 9.81 m/s on Earth). The normal force here represents the force applied by the table against the object that prevents it from sinking through the table and requires that the table is sturdy enough to deliver this normal force without breaking. However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_106"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer: Target Completion → yes Figure G.29: Formatted dataset example for BoolQ Context → The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there’s still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn’t mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer: Target Completion → false Figure G.30: Formatted dataset example for CB 58 Context → The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer: Target Completion → False Figure G.31: Formatted dataset example for RTE Context → An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word ‘outfitter’ used in the same way in the two sentences above? answer: Target Completion → no Figure G.32: Formatted dataset example for WiC Context → Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to. ===== Passage: Mr. Moncrieff visited Chester’s luxurious New York apartment, thinking that it belonged to his son Edward. The result was that Mr. Moncrieff has decided to cancel Edward’s allowance on the ground that he no longer requires *his* financial support",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_107"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The result was that Mr. Moncrieff has decided to cancel Edward’s allowance on the ground that he no longer requires *his* financial support. Question: In the passage above, what does the pronoun \"*his*\" refer to? Answer: Target Completion → mr. moncrieff Figure G.33: Formatted dataset example for WSC Context → Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Target Completion → MARCEL DUCHAMP Target Completion → r mutt Target Completion → duchamp Target Completion → marcel duchamp Target Completion → R.Mutt Target Completion → Marcel duChamp Target Completion → Henri-Robert-Marcel Duchamp Target Completion → Marcel du Champ Target Completion → henri robert marcel duchamp Target Completion → Duchampian Target Completion → Duchamp Target Completion → duchampian Target Completion → marcel du champ Target Completion → Marcel Duchamp Target Completion → MARCEL DUCHAMP Figure G.34: Formatted dataset example for TriviaQA. TriviaQA allows for multiple valid completions. 59 Context → Q: What school did burne hogarth establish? A: Target Completion → School of Visual Arts Figure G.35: Formatted dataset example for WebQA Context → Keinesfalls d ̈urfen diese f ̈ur den kommerziellen Gebrauch verwendet werden. = Target Completion → In no case may they be used for commercial purposes. Figure G.36: Formatted dataset example for De → En. This is the format for onand few-shot learning, for this and other langauge tasks, the format for zero-shot learning is “Q: What is the { language } translation of { sentence } A: { translation } .” Context → In no case may they be used for commercial purposes. = Target Completion → Keinesfalls d ̈urfen diese f ̈ur den kommerziellen Gebrauch verwendet werden. Figure G.37: Formatted dataset example for En → De Context → Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_108"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. = Target Completion → L’analyse de la distribution de fr ́equence des stades larvaires d’I. verticalis dans une s ́erie d’ ́etangs a ́egalement d ́emontr ́e que les larves m^ales ́etaient `a des stades plus avanc ́es que les larves femelles. Figure G.38: Formatted dataset example for En → Fr Context → L’analyse de la distribution de fr ́equence des stades larvaires d’I. verticalis dans une s ́erie d’ ́etangs a ́egalement d ́emontr ́e que les larves m^ales ́etaient `a des stades plus avanc ́es que les larves femelles. = Target Completion → Analysis of instar distributions of larval I. verticalis collected from a series of ponds also indicated that males were in more advanced instars than females. Figure G.39: Formatted dataset example for Fr → En Context → The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey’s accession to the European Union, despite Turkey’s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. = Target Completion → Adev ̆arul este c ̆a v ̆a dorit ̧i, cu orice pret ̧ ̧si ^ımpotriva dorint ̧ei europenilor, s ̆a continuat ̧i negocierile de aderare a Turciei la Uniunea European ̆a, ^ın ciuda refuzului continuu al Turciei de a recunoa ̧ste Ciprul ̧si ^ın ciuda faptului c ̆a reformele democratice au ajuns ^ıntr-un punct mort. Figure G.40: Formatted dataset example for En → Ro 60 Context → Adev ̆arul este c ̆a v ̆a dorit ̧i, cu orice pret ̧ ̧si ^ımpotriva dorint ̧ei europenilor, s ̆a continuat ̧i negocierile de aderare a Turciei la Uniunea European ̆a, ^ın ciuda refuzului continuu al Turciei de a recunoa ̧ste Ciprul ̧si ^ın ciuda faptului c ̆a reformele democratice au ajuns ^ıntr-un punct mort",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_109"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". = Target Completion → The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey’s accession to the European Union, despite Turkey’s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_110"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Figure G.41: Formatted dataset example for Ro → En Context → Q: What is (2 * 4) * 6? A: Target Completion → 48 Figure G.42: Formatted dataset example for Arithmetic 1DC Context → Q: What is 17 minus 14? A: Target Completion → 3 Figure G.43: Formatted dataset example for Arithmetic 2Context → Q: What is 98 plus 45? A: Target Completion → 143 Figure G.44: Formatted dataset example for Arithmetic 2D+ Context → Q: What is 95 times 45? A: Target Completion → 4275 Figure G.45: Formatted dataset example for Arithmetic 2Dx Context → Q: What is 509 minus 488? A: Target Completion → 21 Figure G.46: Formatted dataset example for Arithmetic 3Context → Q: What is 556 plus 497? A: Target Completion → 1053 Figure G.47: Formatted dataset example for Arithmetic 3D+ Context → Q: What is 6209 minus 3365? A: Target Completion → 2844 Figure G.48: Formatted dataset example for Arithmetic 461 Context → Q: What is 9923 plus 617? A: Target Completion → 10540 Figure G.49: Formatted dataset example for Arithmetic 4D+ Context → Q: What is 40649 minus 78746? A: Target Completion → -38097 Figure G.50: Formatted dataset example for Arithmetic 5D − Context → Q: What is 65360 plus 16204? A: Target Completion → 81564 Figure G.51: Formatted dataset example for Arithmetic 5D+ 62 H Results on All Tasks for All Model Sizes Zero-Shot One-Shot Few-Shot Name Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server) HellaSwag acc dev 85.6 20 33.7 43.6 51.0 54.7 62.8 67.4 70.9 78.9 33.0 42.9 50.5 53.5 61.9 66.5 70.0 78.1 33.5 43.1 51.3 54.9 62.9 67.3 71.3 79.3 LAMBADA acc test 68.0 15 42.7 54.3 60.4 63.6 67.1 70.3 72.5 76.2 22.0 47.1 52.6 58.3 61.1 65.4 69.0 72.5 22.0 40.4 63.2 57.0 78.1 79.1 81.3 86.4 LAMBADA ppl test 8.63 15 18.6 9.09 6.53 5.44 4.60 4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92 StoryCloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_111"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "4.00 3.56 3.00 165.0 11.6 8.29 6.46 5.53 4.61 4.06 3.35 165.0 27.6 6.63 7.45 2.89 2.56 2.56 1.92 StoryCloze acc test 91.8 70 63.3 68.5 72.4 73.4 77.2 77.7 79.5 83.2 62.3 68.7 72.3 74.2 77.3 78.7 79.7 84.7 62.3 70.2 73.9 76.1 80.2 81.2 83.0 87.7 NQs acc test 44.5 64 0.64 1.75 2.71 4.40 6.01 5.79 7.84 14.6 1.19 3.07 4.79 5.43 8.73 9.78 13.7 23.0 1.72 4.46 7.89 9.72 13.2 17.0 21.0 29.9 TriviaQA acc dev 68.0 64 4.15 7.61 14.0 19.7 31.3 38.7 41.8 64.3 4.19 12.9 20.5 26.5 35.9 44.4 51.3 68.0 6.96 16.3 26.5 32.1 42.3 51.6 57.5 71.2 71.2 WebQs acc test 45.5 64 1.77 3.20 4.33 4.63 7.92 7.73 8.22 14.4 2.56 6.20 8.51 9.15 14.5 15.1 19.0 25.3 5.46 12.6 15.9 19.6 24.8 27.7 33.5 41.5 Ro → En 16 BLEU-mb test 39.9 64 2.08 2.71 3.09 3.15 16.3 8.34 20.2 19.9 0.55 15.4 23.0 26.3 30.6 33.2 35.6 38.6 1.25 20.7 25.8 29.2 33.1 34.8 37.0 39.5 Ro → En 16 BLEU-sb test 64 2.39 3.08 3.49 3.56 16.8 8.75 20.8 20.9 0.65 15.9 23.6 26.8 31.3 34.2 36.7 40.0 1.40 21.3 26.6 30.1 34.3 36.2 38.4 41.3 En → Ro 16 BLEU-mb test 38.5 64 2.14 2.65 2.53 2.50 3.46 4.24 5.32 14.1 0.35 3.30 7.89 8.72 13.2 15.1 17.3 20.6 1.25 5.90 9.33 10.7 14.3 16.3 18.0 21.0 En → Ro 16 BLEU-sb test 64 2.61 3.11 3.07 3.09 4.26 5.31 6.43 18.0 0.55 3.90 9.15 10.3 15.7 18.2 20.8 24.9 1.64 7.40 10.9 12.9 17.2 19.6 21.8 25.8 Fr → En 14 BLEU-mb test 35.0 64 1.81 2.53 3.47 3.13 20.6 15.1 21.8 21.2 1.28 15.9 23.7 26.3 29.0 30.5 30.2 33.7 4.98 25.5 28.5 31.1 33.7 34.9 36.6 39.2 Fr → En 14 BLEU-sb test 64 2.29 2.99 3.90 3.60 21.2 15.5 22.4 21.9 1.50 16.3 24.4 27.0 30.0 31.6 31.4 35.6 5.30 26.2 29.5 32.2 35.1 36.4 38.3 41.4 En → Fr 14 BLEU-mb test 45.6 64 1.74 2.16 2.73 2.15 15.1 8.82 12.0 25.2 0.49 8.00 14.8 15.9 20.3 23.3 24.9 28.3 4.08 14.5 19.3 21.5 24.9 27.3 29.5 32.6 En → Fr 14 BLEU-sb test 45.9 64 2.44 2.75 3.54 2.82 19.3 11.4 15.3 31.3 0.81 10.0 18.2 19.3 24.7 28.3 30.1 34.1 5.31 18.0 23.6 26.1 30.3 33.3 35.5 39.9 De → En 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6 De → En 16",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_112"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "18.0 23.6 26.1 30.3 33.3 35.5 39.9 De → En 16 BLEU-mb test 40.2 64 2.06 2.87 3.41 3.63 21.5 17.3 23.0 27.2 0.83 16.2 22.5 24.7 28.2 30.7 33.0 30.4 3.25 22.7 26.2 29.2 32.7 34.8 37.3 40.6 De → En 16 BLEU-sb test 64 2.39 3.27 3.85 4.04 22.5 18.2 24.4 28.6 0.93 17.1 23.4 25.8 29.2 31.9 34.5 32.1 3.60 23.8 27.5 30.5 34.1 36.5 39.1 43.0 En → De 16 BLEU-mb test 41.2 64 1.70 2.27 2.31 2.43 12.9 8.66 10.4 24.6 0.50 7.00 12.9 13.1 18.3 20.9 22.5 26.2 3.42 12.3 15.4 17.1 20.9 23.0 26.6 29.7 En → De 16 BLEU-sb test 41.2 64 2.09 2.65 2.75 2.92 13.7 9.36 11.0 25.3 0.54 7.40 13.4 13.4 18.8 21.7 23.3 27.3 3.78 12.9 16.1 17.7 21.7 24.1 27.7 30.9 Winograd acc test 93.8 7 66.3 72.9 74.7 76.9 82.4 85.7 87.9 88.3 63.4 68.5 72.9 76.9 82.4 84.6 86.1 89.7 63.4 67.4 73.6 76.9 84.3 85.4 82.4 88.6 Winogrande acc dev 84.6 50 52.0 52.1 57.4 58.7 62.3 64.5 67.9 70.2 51.3 53.0 58.3 59.1 61.7 65.8 66.9 73.2 51.3 52.6 57.5 59.1 62.6 67.4 70.0 77.7 PIQA acc dev 77.1 50 64.6 70.2 72.9 75.1 75.6 78.0 78.5 81.0 64.3 69.3 71.8 74.4 74.3 76.3 77.8 80.5 64.3 69.4 72.0 74.3 75.4 77.8 79.9 82.3 82.8 ARC (Challenge) acc test 78.5 50 26.6 29.5 31.8 35.5 38.0 41.4 43.7 51.4 25.5 30.2 31.6 36.4 38.4 41.5 43.1 53.2 25.5 28.4 32.3 36.7 39.5 43.7 44.8 51.5 ARC (Easy) acc test 92.0 50 43.6 46.5 53.0 53.8 58.2 60.2 63.8 68.8 42.7 48.2 54.6 55.9 60.3 62.6 66.8 71.2 42.7 51.0 58.1 59.1 62.1 65.8 69.1 70.1 OpenBookQA acc test 87.2 100 35.6 43.2 45.2 46.8 53.0 50.4 55.6 57.6 37.0 39.8 46.2 46.4 53.4 53.0 55.8 58.8 37.0 43.6 48.0 50.6 55.6 55.2 60.8 65.4 Quac f1 dev 74.4 5 21.2 26.8 31.0 30.1 34.7 36.1 38.4 41.5 21.1 26.9 31.9 32.3 37.4 39.0 40.6 43.4 21.6 27.6 32.9 34.2 38.2 39.9 40.9 44.3 RACE-h acc test 90.0 10 35.2 37.9 40.1 40.9 42.4 44.1 44.6 45.5 34.3 37.7 40.0 42.0 43.8 44.3 44.6 45.9 34.3 37.0 40.4 41.4 42.3 44.7 45.1 46.8 RACE-m acc test 93.1 10 42.1 47.2 52.1 52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1 SQuADv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_113"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "52.3 54.7 54.4 56.7 58.4 42.3 47.3 51.7 55.2 56.1 54.7 56.9 57.4 42.3 47.0 52.7 53.0 55.6 55.4 58.1 58.1 SQuADv2 em dev 90.7 16 22.6 32.8 33.9 43.1 43.6 45.4 49.0 52.6 25.1 37.5 37.9 47.9 47.9 51.1 56.0 60.1 27.5 40.5 39.2 53.5 50.0 56.6 62.6 64.9 SQuADv2 f1 dev 93.0 16 28.3 40.2 41.4 50.3 51.0 52.7 56.3 59.5 30.1 43.6 44.1 54.0 54.1 57.1 61.8 65.4 32.1 45.5 44.9 58.7 55.9 62.1 67.7 69.8 CoQA f1 dev 90.7 5 34.5 55.0 61.8 65.3 71.1 72.8 76.3 81.5 30.6 52.1 61.6 66.1 71.8 75.1 77.9 84.0 31.1 52.0 62.7 66.8 73.2 77.3 79.9 85.0 DROP f1 dev 89.1 20 9.40 13.6 14.4 16.4 19.7 17.0 24.0 23.6 11.7 18.1 20.9 23.0 26.4 27.3 29.2 34.3 12.9 18.7 24.0 25.6 29.7 29.7 32.3 36.5 BoolQ acc dev 91.0 32 49.7 60.3 58.9 62.4 67.1 65.4 66.2 60.5 52.6 61.7 60.4 63.7 68.4 68.7 69.0 76.7 43.1 60.6 62.0 64.1 70.3 70.0 70.2 77.5 76.4 CB acc dev 96.9 32 0.00 32.1 8.93 19.6 19.6 28.6 19.6 46.4 55.4 53.6 53.6 48.2 57.1 33.9 55.4 64.3 42.9 58.9 53.6 69.6 67.9 60.7 66.1 82.1 75.6 CB f1 dev 93.9 32 0.00 29.3 11.4 17.4 22.4 25.1 20.3 42.8 60.1 39.8 45.6 37.5 45.7 28.5 44.6 52.5 26.1 40.4 32.6 48.3 45.7 44.6 46.0 57.2 52.0 Copa acc dev 94.8 32 66.0 68.0 73.0 77.0 76.0 80.0 84.0 91.0 62.0 64.0 66.0 74.0 76.0 82.0 86.0 87.0 67.0 64.0 72.0 77.0 83.0 83.0 86.0 92.0 92.0 RTE acc dev 92.5 32 47.7 49.8 48.4 56.0 46.6 55.2 62.8 63.5 53.1 47.3 49.5 49.5 54.9 54.9 56.3 70.4 52.3 48.4 46.9 50.9 56.3 49.5 60.6 72.9 69.0 WiC acc dev 76.1 32 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 50.0 50.3 50.3 49.2 49.4 50.3 50.0 48.6 49.8 55.0 53.0 53.0 51.6 53.1 51.1 55.3 49.4 WSC acc dev 93.8 32 59.6 56.7 65.4 61.5 66.3 60.6 64.4 65.4 58.7 58.7 60.6 62.5 66.3 60.6 66.3 69.2 58.7 60.6 54.8 49.0 62.5 67.3 75.0 75.0 80.1 MultiRC acc dev 62.3 32 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 4.72 9.65 12.3 13.6 14.3 18.4 24.2 27.6 6.09 11.8 16.8 20.8 24.7 23.8 25.0 32.5 30.5 MultiRC f1a dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 ReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_114"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "dev 88.2 32 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 57.0 59.7 60.4 59.9 60.0 64.5 71.4 72.9 45.0 55.9 64.2 65.4 69.5 66.4 69.3 74.8 75.4 ReCoRD acc dev 92.5 32 70.8 78.5 82.1 84.1 86.2 88.6 89.0 90.2 69.8 77.0 80.7 83.0 85.9 88.0 88.8 90.2 69.8 77.2 81.3 83.1 86.6 87.9 88.9 89.0 90.2 ReCoRD f1 dev 93.3 32 71.9 79.2 82.8 85.2 87.3 89.5 90.4 91.0 70.7 77.8 81.6 83.9 86.8 88.8 89.7 91.2 70.7 77.9 82.1 84.0 87.5 88.8 89.8 90.1 91.1 SuperGLUE average dev 89.0 40.6 47.4 46.8 49.6 50.1 52.3 54.4 58.2 54.4 55.1 56.7 57.8 61.2 59.7 64.3 68.9 50.2 56.2 56.8 60.0 64.3 63.6 66.9 73.2 71.8 ANLI R1 acc test 73.8 50 33.4 34.2 33.4 33.4 34.2 32.3 33.2 34.6 32.1 31.6 31.9 34.6 30.6 31.6 32.7 32.0 32.1 32.5 30.9 32.5 33.5 33.1 33.3 36.8 ANLI R2 acc test 50.7 50 33.2 31.9 33.3 33.3 33.8 33.5 33.5 35.4 35.7 33.7 33.2 32.7 32.7 33.9 33.9 33.9 35.7 33.8 32.1 31.4 32.6 33.3 32.6 34.0 ANLI R3 acc test 48.3 50 33.6 34.0 33.8 33.4 35.3 34.8 34.4 34.5 35.0 32.6 33.0 33.9 34.1 33.1 32.5 35.1 35.0 34.4 35.1 36.0 32.7 33.9 34.5 40.2 2D+ acc n/a 50 0.70 0.65 0.70 0.85 1.10 2.54 15.4 76.9 2.00 0.55 3.15 4.00 12.1 19.6 73.0 99.6 2.00 4.10 3.50 4.50 8.90 11.9 55.5 100.0 2acc n/a 50 1.25 1.25 1.25 1.25 1.60 7.60 12.6 58.0 1.15 0.95 1.45 1.95 3.85 11.5 44.6 86.4 1.15 1.45 2.25 2.70 7.35 13.6 52.4 98.9 3D+ acc n/a 50 0.10 0.10 0.05 0.10 0.10 0.25 1.40 34.2 0.15 0.00 0.10 0.30 0.45 0.95 15.4 65.5 0.15 0.45 0.30 0.55 0.75 0.90 8.40 80.4 3acc n/a 50 0.05 0.05 0.05 0.05 0.05 0.45 1.35 48.3 0.05 0.15 0.25 0.30 0.55 1.60 6.15 78.7 0.05 0.10 0.15 0.35 0.65 1.05 9.20 94.2 4D+ acc n/a 50 0.05 0.05 0.00 0.00 0.05 0.05 0.15 4.00 0.00 0.00 0.10 0.00 0.00 0.10 0.80 14.0 0.00 0.05 0.05 0.00 0.15 0.15 0.40 25.5 4acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.10 7.50 0.00 0.00 0.00 0.00 0.05 0.00 0.50 14.0 0.00 0.05 0.00 0.00 0.10 0.05 0.40 26.8 5D+ acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_115"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": "0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.45 0.00 0.00 0.00 0.00 0.00 0.00 0.05 9.30 5acc n/a 50 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.80 0.00 0.00 0.00 0.00 0.00 0.00 0.05 3.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9.90 2Dx acc n/a 50 2.20 2.25 2.65 2.10 2.55 5.80 6.15 19.8 1.35 2.35 3.35 2.35 4.75 9.15 11.0 27.4 1.35 2.90 2.70 2.85 4.25 6.10 7.05 29.2 1DC acc n/a 50 1.25 2.95 2.75 0.05 0.30 2.35 0.75 9.75 1.90 2.80 2.85 3.65 6.45 9.15 8.20 14.3 1.70 2.15 3.90 5.75 6.20 7.60 9.95 21.3 Cycled Letters acc n/a 100 0.62 0.71 2.85 0.00 0.63 1.35 2.58 3.66 1.67 4.36 5.68 6.46 6.25 9.41 15.1 21.7 4.63 9.27 10.7 14.5 16.7 21.9 27.7 37.9 Anagrams 1 acc n/a 100 0.10 0.14 0.40 0.00 0.27 0.69 1.16 2.28 0.21 0.61 1.12 1.27 1.60 2.72 3.72 8.62 0.50 1.27 2.13 3.05 3.81 5.49 8.38 15.1 Anagrams 2 acc n/a 100 0.81 1.21 2.69 0.01 1.71 3.75 4.53 8.91 1.19 2.62 4.70 4.77 6.97 10.2 14.6 25.9 1.94 4.80 7.59 9.87 12.6 18.9 25.6 39.7 Symbol Insertion acc n/a 100 0.00 0.00 0.10 0.00 0.05 0.42 0.89 8.26 0.03 0.05 0.57 1.18 1.67 3.46 6.62 45.4 0.11 0.28 2.19 4.18 6.61 11.0 27.3 67.2 Reversed Words acc n/a 100 0.00 0.01 0.01 0.01 0.02 0.03 0.03 0.09 0.02 0.01 0.01 0.00 0.05 0.07 0.11 0.48 0.00 0.05 0.00 0.17 0.24 0.30 0.42 0.44 SAT Analogies acc n/a 20 35.6 39.0 45.2 44.1 50.0 49.2 52.7 53.7 30.5 41.2 43.1 46.5 55.1 54.3 53.5 59.1 30.5 40.4 42.8 40.6 48.4 51.9 53.5 65.2 Table H.1: Scores for every task, setting and model that we investigate in this paper",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_116"
  },
  {
    "document_type": "research_paper",
    "title": "language_models_are_few-shot_learners",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\language_models_are_few-shot_learners.pdf",
    "date_published": "2020-07-24",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 63 Figure H.1: All results for all SuperGLUE tasks. Figure H.2: Results for SAT task. Figure H.3: All results for all Winograd tasks. 64 Figure H.4: All results for all Arithmetic tasks. Figure H.5: All results for all Cloze and Completion tasks. 65 Figure H.6: All results for all Common Sense Reasoning tasks. Figure H.7: All results for all QA tasks. Figure H.8: All results for all Reading Comprehension tasks. Figure H.9: All results for all ANLI rounds. 66 Figure H.10: All results for all Scramble tasks. Figure H.11: All results for all Translation tasks. 67",
    "chunk_id": "Natural_language_processing_language_models_are_few-shot_learners.json_chunk_117"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": "Is GPT-3 Text Indistinguishable from Human Text? S CARECROW : A Framework for Scrutinizing Machine Text Yao Dou ∗ † Maxwell Forbes ∗† Rik Koncel-Kedziorski † Noah A. Smith † Yejin Choi † † Paul G. Allen School of Computer Science & Engineering, University of Washington Allen Institute for AI {douy,mbforbes,nasmith,yejin}@cs.washington.edu kedzior@uw.edu Abstract Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. ( 2021 ) has reported that conventional crowsourcing can no longer reliably distinguish btween machine-authored (GPT-3) and humaauthored writing. As errors in machine geneations become ever subtler and harder to spot, it poses a new challenge to the research comunity for robust machine text evaluation. We propose a new framework called S CARE - CROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeple, the ten error categories of S CARECROW — such as r e du n dancy , co m mo n sense e r rors , and inc o he r ence —are identified through seeral rounds of crowd annotation experiments without a predefined ontology. We then use S CARECROW to collect over 41k error spans in human-written and machingenerated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurtions. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make rmarkable differences on the perceived quality of machine text",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We release our training matrial, annotation toolkit and dataset at https: //yao-dou.github.io/scarecrow/ . 1 Introduction Clark et al. ( 2021 ) demonstrated the challenges of human evaluation in the era of GPT-3 ( Brown ∗ Equal contribution Off-Prompt The long-rumored Apple car might finally become a reality. Prompt (human-authored) According to the Financial Times, Apple's been talking to \"a small group of contract manufacturers to explore making an electric vehicle,\" which would ostensibly be an autonomous car. All this does sound like the loose ends of Apple's CarPlay rollout: hiring 1,200 engineers for the iOS team, building the CarPlay-specific testing track, developing a Lincoln Navigator, then poaching Burberry’s head of product design to lead the integration of software and hardware. WWDC 2015 We know what you're thinking: Another Monday? Continuation written by GPT-3 DaVinci The most likely meaning of “track” in this context is a driving area, which doesn’t make sense for CarPlay. Apple would develop their own car, not make a Lincoln Navigator, which already exists. Burberry’s head of product design wouldn't have the technical expertise needed for this particular job. While Apple CarPlay is also about cars, this isn’t actually relevant. This is a change of subject and doesn’t follow the narrative. Grammar / Usage It would be weird to hire 1,200 engineers during a “rollout” (a product launch). Neither the speculation, nor the rollout described next, really make sense to call “loose ends.” 1 1 2 2 4 5 3 6 7 3 4 5 6 7 Commonsense Figure 1: After a model (here, GPT-3 DaVinci) has read the prompt (top sentence) and generated a continuation (next paragraph), the S CARECROW annotation framwork provides a systematic way for humans to mark issues throughout the text and explain what is wrong. Our own annotations are pictured here. et al. , 2020 ), as crowd workers are no longer able to reliably distinguish GPT-3’s generations from human-written text",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Our own annotations are pictured here. et al. , 2020 ), as crowd workers are no longer able to reliably distinguish GPT-3’s generations from human-written text. Or are they? In this paper, we propose a new framework for systematically scrutinizing machine text so that even crowd workers, despite the known challenges reported by recent literature, can sucessfully critique seemingly fluent generations. We not only quantify a measurable gap between m1 arXiv:2107.01294v3 [cs.CL] 7 Mar 2022 ERROR TYPE DEFINITION EXAMPLE Language Errors Gra m mar and U s age Missing, extra, incorrect, or out of order words explaining how cats feel emot i cons Off - Prompt Generation is unrelated to or contradicts prompt PROMPT : Dogs are the new kids. GENERA - TION : Visiting the de n tist can be scary R e du n dant Lexical, semantic, or execessive topical reptition Merchants worry about poor se r vice or se r vice that is bad Self - Co n tr a di c tion Generation contradicts itself Amtrak plans to la y off man y e m plo y ees, though it has no plans cut e m ployee hours. Inc o he r ent Confusing, but not any error type above Mary gave her kids cheese toast but drew a map of it on her toast. Factual Errors Bad Math Math or conversion mistakes it costs over £1,000 ($18,868) Enc y clop e dic Facts that annotator knows are wrong Japanese Prime Mini s ter Justin Trudeau said Monday Co m mo n sense Violates basic understanding of the world The dress was made at the spa. Reader Issues Needs Google Search needed to verify claim Jose Celana, an artist based in Pe n sacola, FL , Techn i cal Ja r gon Text requires expertise to understand an 800-megawatt phot o voltaic plant was built Table 1: Error types in the S CARECROW framework, grouped into three categories. The categories are explained further in 4.4 , and detailed definitions and examples for each error type is provided in Appendix A",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The categories are explained further in 4.4 , and detailed definitions and examples for each error type is provided in Appendix A . chine text and human text, but reveal the distribtions of specific categories of issues, and pinpoint their occurrences in text written by several sizes of language models as well as humans. To achieve this, we develop S CARECROW , a methodology for eliciting categorical judgements of errors in machine-generated text from crowd workers. One goal in natural language generation (NLG) is to produce fluent outputs which can be read by laypeople. As such, we propose that iportant errors to address are those which are reognized by readers without NLP expertise. Our framework allows crowd workers to annotate prolems in model outputs at the span level. A single such annotation is shown in Figure 1 . To make this possible, we establish a categoriztion of shortcomings commonly found in machine generated text (Table 1 ). This error schema covers a broad scope of problems as identified by experts, but has been honed according to what is salient to non-expert readers through several pilot rounds of crowd annotation without a fixed label set. The result is a framework that is usable by everyday people with minimal training, but covers the error phenomena found in real machine-generated text. Labeling spans of text using specific error types crates a picture of contemporary model generations with an unprecedented level of detail. In contrast to judging text holistically ( Celikyilmaz et al. , 2021 ), insights from this method are specific and practical, as it measures exactly how and where problems arise. We conduct a large-scale analysis of humawritten and machine-generated text using S CARE - CROW , collecting 13k annotations of 1.3k pargraphs, amassing 41k spans labeled with error type, severity, and an explanation. Through this, we characterize in which ways GPT-3’s generations are better than those of previous models, and which aspects do not improve with increased data and prameters",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Through this, we characterize in which ways GPT-3’s generations are better than those of previous models, and which aspects do not improve with increased data and prameters. We also provide a rigorous error analysis of text generated by several other contemporary language models, examining the impact of model size, training data, and decoding strategy. We provide our detailed annotator training sytem and task interface so that future researchers may employ and refine them for error analyses of machine-generated text. We hope this will cotribute to the standardization of NLG human evalation ( Howcroft et al. , 2020 ). 2 Key Findings We perform a large-scale annotation of errors in English news text generated by five sources (four 2 GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.002 0.004 0.006 0.008 Span coverage Encyclopedic GPT-2 S GPT-2 XL Grover GPT-3 Human 0.00 0.05 0.10 0.15 0.20 Span coverage Incoherent GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.002 0.004 0.006 Span coverage Bad Math GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Self-Contradiction GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.025 0.050 0.075 0.100 0.125 0.150 Span coverage Needs Google GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Commonsense GPT-2 S GPT-2 XL Grover GPT-3 Human 0.0 0.1 0.2 0.3 Span coverage Off-Prompt GPT-2 S GPT-2 XL Grover GPT-3 Human 0.00 0.01 0.02 0.03 Span coverage Grammar / Usage GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Redundant GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 Span coverage Technical Jargon Figure 2: Average portion of tokens annotated with each error type ( y -axis) across models ( x -axis), with 95% confidence intervals. We group the trends into several broad categories. Decreasing: fine-tuning and increasing model size improves performance. Model plateau: increasing model size to GPT-3 does not correlate with further improvements",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Decreasing: fine-tuning and increasing model size improves performance. Model plateau: increasing model size to GPT-3 does not correlate with further improvements. Rising and falling: errors become more prevalent with some models, then improve. Humans highest: these spans are labeled most on human-authored text; both are reader issues (distinct from errors ; see Table 1 ). Details: all models, including GPT-3, use the same “apples-to-apples” decoding hyperparaeters: top =0.96, temperature=1, and no frequency penalty. models and ground truth articles). We present Fiures 2 , 3 , and 4 as summaries of our main results. As a reminder to readers, Grover ( Zellers et al. , 2019 ) is the same model size and architecture as GPT-2 XL ( Radford et al. , 2019 ), but trained idomain (on news text). As such, our results cover three increasing model sizes (GPT-2 Small, XL, and GPT-3 ( Brown et al. , 2020 )), one change in domain (Grover), and ground-truth text (Human). For GPT-3, we also study a variety of decoding configurations (Figure 4 ). The main quantity we measure (on y -axes) is span coverage , which is the average portion of tokens that ends up covered by annotations of a particular error type. Since it is possible that multple spans nest or overlap, there is no upper bound for this quantity. (See Figure 12 for a comparison of span coverage with other measurement alterntives.) Figure 2 measures span coverage for each type of span separately, Figure 3 stacks them, and Figure 4 removes non-error spans (reader issues) before adding them (as in Figure 3 , but without showing the individual types). The following are our key findings. 1. Scaling pays off to improve Enc y clop e dic , Co m mo n sense , and Inc o he r ent errors (Fig. 2 ). These error categories decrease with in-domain training (Grover) and larger model size (GPT-3). Human text still shows the fewest of these kinds of errors. 2. Scaling benefits plateau for Off - Prompt , Bad Math , and Gra m mar and U s age errors (Fig. 2 )",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Human text still shows the fewest of these kinds of errors. 2. Scaling benefits plateau for Off - Prompt , Bad Math , and Gra m mar and U s age errors (Fig. 2 ). These three error categories see a model plateau in error reduction when scaling to GPT-3. Of these error types, humans still commit fewer Off - Prompt (more: E.1 ) and Gra m mar and U s age errors, but Bad Math apears saturated for our domain. 3. Self - Co n tr a di c tion and R e du n dant errors exhibit more complex scaling behavior (Fig. 2 ). We roughly categorize these trends as rising and falling : increasing for medium or large-scale models, but dropping for human-authored text. Text generated by GPT-2 Small is so often incoherent that there is little possibility for SelCo n tr a di c tion (more: E.2 ), and the increase in R e du n dant errors varies based on how errors are counted (more: E.3 ). 4. Human-authored text produces the most reader issues (Figs. 2 and 3 ). The Needs Google and Techn i cal Ja r gon span categories both have a humans highest trend, and both fall under reader issues : problems that are not necessaily errors , but that still prevent full comprehension 3 GPT-2 S GPT-2 XL Grover-Mega GPT-3 Human 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Span coverage Average Span Coverage Across Models Bad_Math Commonsense Encyclopedic Grammar_Usage Incoherent Needs_Google Off-prompt Redundant Self-contradiction Technical_Jargon Figure 3: Average portion of tokens covered by span annotations, broken down by error type. All models, including GPT-3, use the same apples-to-apples decoing hyperparameters: top =0.96, temperature=1, and no frequency penalty. We scale each span by its tken length, normalize by generation token lengths, and remove severity-1 Gra m mar and U s age errors (see C )",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We scale each span by its tken length, normalize by generation token lengths, and remove severity-1 Gra m mar and U s age errors (see C ). GPT-2 S argmax t=0.4, p=0.96 GPT-2 XL t=1.0, p=0.4 t=0.7, p=0.96 Grover-Mega t=1.0, p=0.9 t=1.0, p=0.96 t=1.0, p=0.9 t=1.0, p=0.7 t=1.0, p=0.7 t=1.0, p=0.96 t=1.0, p=0.4 t=0.4, p=0.96 t=0.7, p=0.96 argmax Human Model (both green hues: GPT-3 w/ decoding config in legend) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Span coverage \"Apples-to-apples\" decoding setup (used when comparing models elsewhere) Errors Across All Decoding Configurations GPT-3, with Frequency Penalty: 0 (none) 1 (full) Figure 4: Taking the average span coverage (Figure 3 ) and removing reader issues ( Techn i cal Ja r gon and Needs Google ), we plot values and 95% confidence intervals for all models, including all decoding hypeparameters we tested for GPT-3. We find a surprisingly large change in annotated errors depending on the dcoding setting used. or factual verification of the text (more: E.4 ). Furthermore, human-authored text is not free from error annotations (Figure 3 ). This can serve either as a control for baseline error rates (more: E.6 ), or as a mechanism for critiquing human writing. 5. Decoding hyperparameters have a huge ipact (Figure 4 ). For the previous findings, we fix the sampling configuration for all models to an apples-to-apples setup for fair comparison: top = 0.96, (softmax) temperature = 1, and no frequency penalty (i.e., word repetition penalty; defined prcisely in 5.2 , Equation 1 ). To study the effects of these decoding settings, we annotate text generated by GPT-3 using a variety of values for top and temperature, both with and without a frequency penalty. To our surprise, the decoding hyperparameters considerably affected error rates (more: E.5 ). As seen in Figure 4 , the worst sampling procedure for GPT-3 (argmax sampling with no frequency penalty) performed even worse than GPT-2 XL",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". As seen in Figure 4 , the worst sampling procedure for GPT-3 (argmax sampling with no frequency penalty) performed even worse than GPT-2 XL. But the best sampling procedure (surprisingly, also argmax sampling, but with a frequency penalty) produced text with as few apparent S CARECROW error spans as those authored by humans (more: E.6 ). All of these findings are discussed in more detail in Appendix E . 3 Evaluation of Natural Language Generation We make our study in the area of open-ended natral language generation, a loose term for generaing longer texts with an increased level of creative freedom. The common factor in all open-ended generation tasks such as story, blog, and dialog generation is the wide and diverse nature of target outputs. Lexically and even semantically dissimlar responses to the same prompt could be equally valid. For example, a model prompted with the blog title “Recipes for success this Holiday season” could describe how to roast a turkey or strategies for dealing with the stresses of holiday travel. This allowable variation poses a particular dificulty for the evaluation of generation systems. Traditionally, text generation quality for tasks like machine translation or graph-to-text generation has been measured by word overlap with humaauthored references ( Papineni et al. , 2002 ; Lin , 2004 ). Though measures like BLEU allow for mutiple references, they break down when the space of allowable outputs is large, as in open-ended geneation. Recently introduced metrics seek to remedy this problem ( Hashimoto et al. , 2019 ; Pillutla et al. , 2021 ), but the gold standard for evaluating geneated text is still human judgment. However, current approaches to eliciting human 4 judgement of generated text often do not provide detailed insight into where models are making progress, where they are failing, and the scope of these failures. A/B-style testing allows for drectly comparing one system against others ( Clark and Smith , 2021 ), but can only express relative iprovements",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A/B-style testing allows for drectly comparing one system against others ( Clark and Smith , 2021 ), but can only express relative iprovements. Simple Likert scale judgements can assess text quality, but do not explain why a geneated text receives a given rating, or which segment of the text is problematic. Insights into model faiures often come instead from a small scale expert analysis of outputs. However, these “error analses,” once a staple of NLP research, have become less common in recent years, perhaps due to their small size and high variance. A hypothesis of the current work is that a well dsigned error analysis annotation framework could be used by crowdworkers to annotate large amounts of text, thereby providing detailed information about model progress and failures as well as ationable directions for future research. Such a framework would be easy to learn, reusable, and independent of particular models or experimental conditions. In what follows, we outline the details of such a method. 4 S CARECROW Annotation Methodology This section describes the high-level annotation methodology for S CARECROW . 4.1 Prompt and Generation Our annotations consider two segments of text: a one-sentence prompt, and a one-paragraph geneation. The prompt is human-written. It provides both starting tokens for model generation, as well as context for humans to evaluate whether a model is able to stay on-prompt—both topically and fatually. Annotators know that the prompt is written by a human. The generation is either text sampled from a language model, or the human-authored continution to the prompt. Annotators, who do not know whether the generation came from a model or hmans, assess this text. A paragraph length (80–145 tokens) is chosen to balance expressiveness with scope. For expressiveness, models must be given a sufficient number of tokens to express their cpabilities lexically, syntactically, and semantically. One paragraph allows for significantly more varation than a single sentence",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One paragraph allows for significantly more varation than a single sentence. On the other hand, assessing multiple paragraphs is challenging, both Inconsistent about how many moons Mars has. 1 2 3 SelContradiction Inconsistent about how many moons Mars has. Needs Google Bad Math Reader Issues Factual Language Figure 5: S CARECROW interface for annotating a sigle span: (1) highlighting a span (and later, an atecedent); (2) completing the annotation, with the error type, explanation, and severity; (3) the error annotation is saved—interactive controls allow detailed viewing and editing of spans (not shown). as a crowdsourcing task itself, and because it broaens the kinds of errors to include larger narrative scope. We leave extensions of S CARECROW to longer narrative lengths for future work. 4.2 Span Labeling Annotators select spans that contain problems in the generation. The spans are automatically snapped to word boundaries. We choose spans to balance specificity (i.e., vs. simply commening on the text as a whole) with ease of use (vs. imposing a more structured annotation schema). 4.3 Span Selection We instruct workers to select the smallest span— minimally a single word—that contains an issue. Sometimes this involves an entire phrase, sentence, 5 or multiple sentences. We aim for specificity bcause during aggregation, it is possible to “back off” annotations to larger spans, but not the inverse. Once they select a span, workers (1) label the eror type, (2) choose a severity level, and (3) explain their reasoning behind the error. Workers use the annotation interface shown in Figure 5 to mark a span with these three steps. We describe each step in greater detail in the next three sections. 4.4 Error Types Each selected span is labeled with exactly one error type. Multiple errors may be marked with partially or fully overlapping spans in the case that one text segment contains multiple problems",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Multiple errors may be marked with partially or fully overlapping spans in the case that one text segment contains multiple problems. We chose ten error types to balance three critria: linguistic analysis, observed errors in geneated text, and capabilities of everyday people with one to two hours of training. 1 We developed the schema by starting with the first two criteria (liguistic analysis and observed errors), and refining it over several pilot annotation studies, with 30 crowd workers performing 750 total annotations of 60 paragraphs before beginning data collection. We broadly group the errors into three categories: language errors, factual errors, and reade r issues . Language errors are issues with internal and eternal structure of text: which ideas are expressed, and whether they are expressed coherently and cosistently. Factual errors denote that the information presented is known to be incorrect. Reader issues, on the other hand, are cases where the text is too technical or obscure to assess its factuality. Hence, reader issues are not errors, per se, but regions where a reader would need assistance outside of the text itself for comprehension. We present the ten error types in Table 1 (several pages back). Appendix A provides more details, examples, and explanations for all error types. 4.5 Severity Errors naturally vary in how jarring they are to a reader. We define three error severity levels, and ask annotators to pick one for each error. The severity levels are as follows. (1) Almost no impact on quality; just a small problem. (2) Understandable, but difficult; what’s written is still comprehensible, but there’s clearly an issue. (3) Very difficult to understand; the error almost copletely ruins the text. 1 The complete training material is available for download. We provide examples of each severity in Apendix B.1 . In this paper, we omit an analysis of the severity labels (except for an illustration in Fiure 12 ), but include it in our data release for future work to explore",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In this paper, we omit an analysis of the severity labels (except for an illustration in Fiure 12 ), but include it in our data release for future work to explore. 4.6 Explanation Finally, we ask annotators to explain their reasoing behind each error in natural language. We prvide example explanations during training, but do not impose strict guidelines. This paper primarily focuses on quantitative error analysis, but we aticipate the error explanations may warrant future investigation. 4.7 Annotation Process We use Amazon Mechanical Turk (AMT) for all data collection. Training We first pay each worker $40 to take an extensive qualification task, which both trains them in the span categorization scheme and quizzes their understanding. We pass workers if they score ≥ 90 points out of 100 points (details in Appendix B.2 ). Annotation Workers annotate each paragraph uing a custom annotation interface (shown partially in Figure 5 ), for which we pay $3.50. We calculated $3.50 per annotation by aiming to pay workers at least $15/hour. After several annotation rounds, we observed considerable variation in time per annottion, 2 so this cost should not be necessarily seen as a requirement for S CARECROW annotations. 5 Data Collection We collect 13k human annotations of 1.3k pargraphs using S CARECROW , resulting in over 41k spans. 5.1 Models We consider four model configurations to test rcent state-of-the-art transformer-based ( Vaswani et al. , 2017 ) models. GPT-2 Small ( Radford et al. , 2019 ) The 117M parameter variant of GPT-2, which is pretrained on WebText, without additional fine-tuning. GPT-2 XL ( Radford et al. , 2019 ) The 1.5B prameter variant of GPT-2, (WebText, no fintuning). 2 Median: 212s, mean: 265s, std. dev.: 199s. 6 Grover-Mega ( Zellers et al. , 2019 ) The 1.5B prameter variant of Grover, a model with the same architecture and parameter count of GPT-2, trained on news articles and their metadata. GPT-3 DaVinci ( Brown et al",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2019 ) The 1.5B prameter variant of Grover, a model with the same architecture and parameter count of GPT-2, trained on news articles and their metadata. GPT-3 DaVinci ( Brown et al. , 2020 ) The 175B parameter variant of GPT-3, which is trained on a version of the Common Crawl web scrape with additional filtering and deduplicating. In addition, we also use the actual human-written text from the data sources we draw from, which we denote as Human . 5.2 Decoding strategies We consider three main hyperparameters when sapling from models: p for top-p or nucleus sampling ( Holtzman et al. , 2020 ), an alternative to top-k ; 3 t for the softmax temperature ; and f.p. for frequency penalty . The frequency penalty scales a token’s likelihood based on how many times it was already generated by applying the following modification to the model’s output: l i ( t ) ← l i ( t ) − c <i ( t ) · α f (1) where l i ( t ) is the model’s output for token t at the i -th position, 4 c <i ( t ) is the count of token t ’s sapled occurrences prior to the i -th position, and α f is the frequency penalty. We omit studying presence penalty , another hyperparameter offered for GPT-3, simply due to annotation budget constraints. To compare models as consistently as possible, we set identical decoding strategies for our primary data collection. We refer to this as the “apples-tapples” decoding setup throughout the paper: p = 0 . 96 t = 1 . 0 f.p. = 0 However, we also wish to study the effects of these decoding strategies. We annotate generations from the strongest available model (currently, GP3) varying the following parameters: 3 We omit separate studies of top-k , due to results presented by Holtzman et al. ( 2020 ), and OpenAI’s removal of top-k from the GPT-3 API. 4 While l i ( t ) is defined to be “logits (un-normalized loprobabilities),” because it is un-normalized, we anticipate that it is simply the model’s output before the log( softmax ( · )) is applied",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 While l i ( t ) is defined to be “logits (un-normalized loprobabilities),” because it is un-normalized, we anticipate that it is simply the model’s output before the log( softmax ( · )) is applied. See OpenAI’s description of frequency and preence penalties: https://beta.openai.com/docs/ api-reference/parameter-details p ∈{ 0 . 4 , 0 . 7 , 0 . 9 , 0 . 96 } t ∈{ 0 . 0 (argmax) , 0 . 4 , 0 . 7 , 1 . 0 } f.p. ∈{ 0 (none) , 1 (full) } For budget reasons, we only vary p and t independently—i.e., we set p = 0 . 96 when varying t , and t = 1 . 0 when varying p . 5.3 Prompt Selection We use news articles as the sources of prompts for models to condition on for generation. Specifically, we use news articles found in the Common Crawl. We select the first sentence as the prompt. Our use of news text is constrained by two fators. First GPT-3 is trained on the Common Crawl, from 2016 through 2019. We wish to avoid testing GPT-3 by generating from articles it saw during training, due to the possibility of copying ( Carlini et al. , 2021 ). Second, news articles began heaily covering the COVID-19 pandemic beginning around February 2020. Though testing models’ cpabilities to generate text about unseen events is a valuable line of study, the distribution shift caused by COVID-19 in news writing about all aspects of life is difficult to overstate. As such, to make the comparison more amenable to models’ training data, we consider news articles from January 2020. We select articles where there is a known topic—such as Food or Sports —from the Common Crawl metadata, to allow for studying any effect of coarse-grained subject. 5.4 Generation We generate between 80 and 145 tokens 5 from each model as a continuation to the first sentence of the news article. We stop generating when we heuristically detect the first sentence boundary after 80 tokens. If the model does not end a sentence between 80 and 145 tokens, we sample again",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We stop generating when we heuristically detect the first sentence boundary after 80 tokens. If the model does not end a sentence between 80 and 145 tokens, we sample again. For the Human setting, we use the remainder of the article, similarly stopping after the first sentence boundary after 80 tokens. 5.5 Annotation Crowdsourcing Workers first complete training and qualification tasks. We provide more details in 4.7 . From pilot studies, we discovered that each eror, depending on its severity and clarity, has only a 5 Counted by Stanza tokenization ( Qi et al. , 2020 ), not byte-pair encoding (BPE) or whitespace-separated tokens. 7 low to moderate chance of being identified by each worker. However, most worker-identified errors were truly problems. In other words, annotators labeled issues with high precision and low recall. To account for this, we have 10 workers annotate each paragraph. We examine the agreement and variability of annotations in Appendix C . Dataset statistics We provide detailed dataset statistics in Appendix D . 6 Error Prediction A natural question is: using this data, can machines learn to detect and classify errors in machine geerated text? Task We frame this problem as a span classifcation task. Given a span from a generated text, the goal is to classify its error type or output “No Error” if there is none. Positive examples for each error class are taken from our data. We sample random spans that were not labeled with any error type as negative examples. To ensure a breadth of span lengths, we sample 3 negative spans for every length of error span in the generated text. We split the generated texts into train, development, and test sets using 1063 texts (28029 error spans), 100 texts (2538 spans) and 100 texts (2677 spans) respectively. Model We use a standard span classification model inspired by Wadden et al. ( 2019 ). This model encodes every generated text using a prtrained language model (RoBERTa-large). Spans are represented with the final layer of this encoing",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ( 2019 ). This model encodes every generated text using a prtrained language model (RoBERTa-large). Spans are represented with the final layer of this encoing. Following previous work, we concatenate the start and end tokens with a task-specific learned length embedding. The resulting vector is passed through a feedforward network which reduces its dimensionally to the number of error categories plus a “No Error” option. The resulting model has 357M trainable parameters. The model is trained to minimize the cross entropy of the correct span catgory. We train for 15 epochs using AdamW with a learning rate of 10 − 6 . We validate after each epoch and use the checkpoint with the lowest validation loss (epoch 8). Evaluation To evaluate the error prediction model, we use per-token precision, recall, and F 1 score per error category. We classify every span up to length 30 in a generated text. We take as gold labels the aggregated human error spans collected Error Model Human P R F 1 P R F 1 Bad Math – 0 – 0.72 0.14 0.24 Co m mo n sense 0.77 0.06 0.10 0.17 0.02 0.04 Enc y clop e dic – 0 – 0.22 0.03 0.05 Gra m mar and U s age 0.29 0.23 0.26 0.30 0.04 0.08 Inc o he r ent 0.59 0.34 0.43 0.69 0.15 0.24 Off - Prompt 0.67 0.29 0.41 0.88 0.31 0.46 R e du n dant 0.23 0.82 0.36 0.88 0.35 0.50 Self - Co n tr a di c tion 0.08 0.23 0.12 0.51 0.09 0.16 Techn i cal Ja r gon 0.18 0.74 0.29 0.61 0.12 0.20 Needs Google 0.59 0.96 0.73 0.78 0.20 0.32 Table 2: Model prediction results against combined spans of 10 annotators, compared with humans scored as one-vs-rest (i.e., 1-vs-9). Bold F 1 scores denote the higher average; values marked “–” cannot be computed due to division by zero. Takeaway: Humans have higher precision in every error type except Co m mosense , but relatively sparse annotations lead to lower computed recall. This allows the model to achieve higher F 1 scores for half of the span categories. in our data. In other words, models predict the cobined spans of all 10 annotators",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This allows the model to achieve higher F 1 scores for half of the span categories. in our data. In other words, models predict the cobined spans of all 10 annotators. For comparison, we also report as Human the average metrics of one annotator versus the others (i.e., 1-vs-9). 6 Results Table 2 shows the error prediction capbility of this model in terms of precision and recall. As we noted earlier, a single human annotator can be thought of as a high precision, low recall judge. These results bear out this claim. For all but one caegory, humans have higher precision annotations. However, the models trained on the aggregation of human labels can achieve considerably higher recall. For half of the error categories, this leads to higher model F 1 scores than the human annotators. We see that the model is successful at identifying information that human’s would have to manually verify ( Needs Google ), achieving nearly perfect recall with precision close to 0.6. The model can also identify Gra m mar and U s age , Inc o he r ent , and R e du n dant errors with higher recall than an individual human annotator, though at the cost of precision (sometimes in the .20s). 7 Related Work Automated evaluation metrics such as BLEU ( Ppineni et al. , 2002 ), ROUGE ( Lin , 2004 ), MTEOR ( Banerjee and Lavie , 2005 ), and BERTScore 6 The difference in available references (10 for models, 9 for humans) mean this setup makes it easier for models to score higher in precision, and for humans to score higher in recall. Despite this, humans still achieve higher precision, and models still achieve higher recall. 8 ( Zhang et al. , 2019 ) compute a generation’s score based on a (set of) reference(s). Their use is welestablished in tasks like machine translation and summarization, but they are less helpful in opeended text generation, where there is a vast divesity of possible high-quality continuations. Recent studies propose automated metrics for open-ended text generation evaluation such as: Peception Score ( Gu et al",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Recent studies propose automated metrics for open-ended text generation evaluation such as: Peception Score ( Gu et al. , 2021 ), which diffuses evauation onto a multidimensional space and assigns a single score; UNION ( Guan and Huang , 2020 ), which learns to distinguish human-written stories from negative samples by generating perturbations of human-written stories; and MAUVE ( Pillutla et al. , 2021 ), which compares the distribution of machine-generated text to that of human language. An alternate recent approach to assessing opeended text generation was presented in TuringAvice ( Zellers et al. , 2021 ), where crowd workers assess machine-generated advice in response to Reddit posts. In their error analysis, Zellers et al. connect problems in generated text to core NLP tasks, such as Self - Co n tr a di c tion errors as istances of failed natural language inference ( Monz and de Rijke , 2001 ), or Off - Prompt errors as cases of failed reading comprehension ( Richardson et al. , 2013 ). While past work has attempted to guide text generation using discriminative models trained for such tasks ( Holtzman et al. , 2018 ), it remains an open challenge. Comparative human evaluations of natural laguage generations ask annotators to rank system outputs relative to each other. Text is typically evauated using a few global criteria, such as fluency and relevance, using discrete (e.g., 5-point) ( Sai et al. , 2020 ) or continuous scales ( Novikova et al. , 2018 ). Recent work even automates this approach, running a human evaluation alongside automatic metrics on leaderboard submissions ( Khashabi et al. , 2021 ). In the RoFT system ( Dugan et al. , 2020 ), annotators attempt to detect the boundary btween humaand machine-written text as a proxy for assessing quality. Table 3 summarizes the diferences between these schemes and S CARECROW . See Celikyilmaz et al. ( 2021 ) for a recent survey of text generation evaluation techniques across both human and automatic metrics",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "chunk_text": ". See Celikyilmaz et al. ( 2021 ) for a recent survey of text generation evaluation techniques across both human and automatic metrics. While these approaches may be helpful— sometimes ( Card et al. , 2020 )—at ranking systems, they do not give us insight into exactly which parts of a generation fall short, and why . One approach Method GC SET DE RR EE RS SA Likert-Scale ✓ ✓ ✓ RankME ✓ ✓ ✓ RoFT ✓ ✓ ✓ S CARECROW ✓ ✓ ✓ ✓ Table 3: Comparison of different natural language geeration human evaluations. Here, GC : General Critria, SET : Specific Error Type, DE : Direct Evaluation, RR : Relative Ranking, EE : Error Explanation, RS : Rating Scale, SA : Span Annotation. related to or annotation method is pursued by Wood et al. ( 2018 ), who develop a collaborative mobile app where users draw “graffiti” commentary on news articles. S CARECROW aims to assess model generations the way we would critique humawritten text: by locating, coarsely categorizing, and explaining problems. 8 Conclusion We present S CARECROW , a method for identifying and explaining issues in generated text. Along with the annotation framework, we present an analysis of the S CARECROW method applied to several large neural language models in an open-ended news geeration task. We release our data and methodology to the community. Acknowledgments The authors thank members of xlab for their feeback on this work. This research is supported in part by NSF (IIS-1714566), DARPA MCS prgram through NIWC Pacific (N66001-19-2-4031), DARPA SemaFor program, and Allen Institute for AI.",
    "chunk_id": "Natural_language_processing_is_gpt-3-text-indistinguishable-from-human-text.json_chunk_20"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "48 Center for Research on Foundation Models (CRFM) 2.6 Philosophy of understanding Authors: Christopher Potts, Thomas Icard, Eva Portelance, Dallas Card, Kaitlyn Zhou, John Etchemendy What could a foundation model come to understand about the data it is trained on? An answer to this question would be extremely informative about the overall capacity of foundation models to contribute to intelligent systems. In this section, we focus on the case of natural language, since language use is a hallmark of human intelligence and central to the human experience. The best foundation models at present can consume and produce language with striking fluency, but they invariably lapse into the sort of incoherence that suggests they are merely “stochastic parrots” [ Bender et al . 2021 ]. Are these lapses evidence of inherent limitations, or might future foundation models truly come to understand the symbols they process? Our aim in this section is to clarify these questions, and to help structure debates around them. We begin by explaining what we mean by foundation model , paying special attention to how foundation models are trained, since the training regime delimits what information the model gets about the world. We then address why it is important to clarify these questions for the further development of such models. Finally, we seek to clarify what we mean by understanding , addressing both what understanding is (metaphysics) and how we might come to reliably determine whether a model has achieved understanding (epistemology). Ultimately, we conclude that skepticism about the capacity of future models to understand natural language may be premature. It is by no means obvious that foundation models alone could ever achieve understanding, but neither do we know of definitive reasons to think they could not. 2.6.1 What is a foundation model? There is not a precise technical definition of foundation model",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2.6.1 What is a foundation model? There is not a precise technical definition of foundation model . Rather, this is an informal label for a large family of models, and this family of models is likely to grow and change over time in response to new research. This poses challenges to reasoning about their fundamental properties. However, there is arguably one defining characteristic shared by all foundation models: they are self-supervised . Our focus is on the case where self-supervision is the model’s only formal objective. In self-supervision, the model’s sole objective is to learn abstract co-occurrence patterns in the sequences of symbols it was trained on. This task enables many of these models to generate plausible strings of symbols as well. For example, many foundation models are structured so that one can prompt them with a sequence like “The sandwich contains peanut” and ask them to generate a continuation – say, “butter and jelly”. Other models are structured so that they are better at filling in gaps; you might prompt a model with “The sandwich contains __ and jelly” and expect it to fill in “peanut butter”. Both capabilities derive from these models’ ability to extract co-occurrence patterns from their training data. There is no obvious sense in which this kind of self-supervision tells the model anything about what the symbols mean. The only information it is given directly is information about which words tend to co-occur with which other words. On the face of it, knowing that “The sandwich contains peanut” is likely to be continued with “butter and jelly” says nothing about what sandwiches are, what jelly is, how these objects will be combined, etc. This might seem to suggest an inherent limitation on what a foundation model could achieve. However, we need not restrict the model to seeing only textual input. A foundation model might be trained on a wide range of different symbols: not just language but also computer code, database files, images, audio, and sensor readings",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A foundation model might be trained on a wide range of different symbols: not just language but also computer code, database files, images, audio, and sensor readings. As long as it is just learning co-occurrence patterns of the sequences it is exposed to, then it counts as a foundation model by our definition. As part of this learning, the model might come to represent strong associations between a given piece of text and a particular sensor reading, or between a On the Opportunities and Risks of Foundation Models 49 sequence of pixel values and a database entry. These associations might reflect important aspects of the world we inhabit and the language we use to talk about it. 2.6.2 What is at stake? Before considering analyses of what understanding is, it is worth reflecting on why we might care about the question of whether a foundation model could achieve it. These models are poised to be deployed for numerous purposes with various functionalities. Some of our goals in deployment may only be met to the extent that the model is capable of understanding. Here we list a few such goals: • Trust : One might argue that we cannot trust a system’s linguistic behavior unless it uderstands the language it is using. Of course, we currently trust engineered systems to do things (e.g., manufacturing auto parts) without the question of understanding even arising, but language might be special in this regard, since it is uniquely human. In addition, language can be used to deceive and misrepresent, so understanding alone clearly does not imply trust. On the whole, then, understanding might be taken as a necessary condition for trust in the context of language use",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". On the whole, then, understanding might be taken as a necessary condition for trust in the context of language use. • Interpretability : If genuine natural language understanding in some way involves maitaining and updating an internal model of the world (including, e.g., the speech context), and if we (as engineers) are able to analyze how linguistic input and output interface with this internal model, that could afford substantial gains in interpretability, predictability, and control of these systems. • Accountability : Not unrelated to the previous points, in the future we may find it desirable to hold artificial agents in some way accountable for the language they produce [ The HAI Adaptive Agents Group 2021 ]. Depending on how we think about concepts like accountability, responsibility, agency, and the like, language understanding may emerge as a prerequisite. The mere possibility that understanding will play an indispensable role in any of these matters provides strong motivation to develop a framework for theorizing about it. 2.6.3 What is understanding? Our central question is whether a foundation model could come to understand a natural language. With the above, we can now sharpen it: is self-supervision sufficient for understanding, keeping in mind that there are no constraints on the data used for this supervision? In order to address this question, we first need to define what we mean by understanding . As a start, we find it helpful to make explicit a distinction that is sometimes conflated in discusions of the topic. The distinction is between the metaphysics and the epistemology of understanding. Metaphysics concerns what it would mean (“in principle”) for an agent to achieve understanding. Epistemology, by contrast, concerns how (“in practice”) we could ever come to know that an agent has achieved the relevant type of understanding. In short, metaphysics is more about our ultimate target, whereas epistemology is more about how (if at all) we could know when we have reached it",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In short, metaphysics is more about our ultimate target, whereas epistemology is more about how (if at all) we could know when we have reached it. Our epistemology thus depends to some extent on our metaphysics. 50 Center for Research on Foundation Models (CRFM) Metaphysics of understanding. Philosophy of language offers a number of alternatives for what it is to understand natural language. 25 Simplifying the landscape for the sake of brevity, the following three broad classes of views all have connections with research lines in AI and NLP: 26 • Internalism : Language understanding amounts to retrieval of the right internal representtional structures in response to linguistic input. Thus, language understanding is not even a possibility without a rich internal conceptual repertoire of the right kind. • Referentialism : Roughly, an agent understands language when they are in a position to know what it would take for different sentences in that language to be true (relative to a context). That is, words have referents and (declarative) utterances are truth-evaluable, and understanding involves a capacity to evaluate them relative to presentation of a situation or scenario. • Pragmatism : Understanding requires nothing in the way of internal representations or computations, and truth and reference are not fundamental. Rather, what matters is that the agent be disposed to use language in the right way. This might include dispositions toward inference or reasoning patterns, appropriate conversational moves, and so on. Crucially, the relevant verbal abilities constitute understanding. 27 While this is a simplified picture of the space of possibilities, we already see how they relate in quite different ways to the goals mentioned above. On the pragmatist view, for instance, achieing language understanding does not imply anything about our ability to trust or interpret the system, insofar as it guarantees nothing about the agent’s internal structure or its relation to the (non-linguistic) world",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". On the internalist view, by contrast, a fairly robust kind of internal/causal interpretability is at least strongly suggested. The question of whether or not a foundation model could understand language in principle takes on a very different character depending on which of these metaphysical characterizations we adopt. Internalism and referentialism can both be cast as defining a mapping problem: to associate a linguistic sign with a “meaning” or a “semantic value”. For internalism this will be a representation or concept, a program for computing a value, or some other type of internal object. For referentialism, it might be a mapping from a word to an external referent, or a mapping from a situation to a truth value (all relative to a context). Could self-supervision suffice for achieving the desired mapping in a foundation model? Here, the nature of the training examples might be relevant. If the model receives only linguistic inputs, then its capacity to learn this mapping might be fundamentally limited in ways that prevent it from learning to refer in the relevant sense. (Indeed, Merrill et al . [ 2021 ] identify some theoretical limits, albeit under very strong assumptions about what it means to learn the meaning of a symbol.) However, if the input symbol streams include diverse digital traces of things in the world – images, audio, sensors, etc. – then the co-occurrence patterns might contain enough information for the model to induce high-fidelity proxies for the required mapping. 28 For 25 Relatedly, there is a sizable literature in philosophy of science focused on the concept of understanding, mainly as it relates to scientific explanation. See Grimm [ 2021 ]. 26 We are leaving aside other questions that may be relevant to the metaphysics of understanding, such as whether or not consciousness or some form of subjective experience may be necessary. These are pressing philosophical issues, but they are not easily connected to research in AI and NLP",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These are pressing philosophical issues, but they are not easily connected to research in AI and NLP. 27 For an accessible introduction to internalist as well as referential views, we recommend Elbourne [ 2011 ]. This version of pragmatism arguably finds its roots in Wittgenstein [ 1953 ], but it is expressed most succinctly by Turing [ 1950 ], in which Turing suggests replacing the question of whether a machine can think with questions about a specific behavioral test (which came to be known as the Turing Test). 28 To the extent that the mapping embodies causal information, we must also contend with theoretical limitations concerning the possibility of drawing causal inferences from correlational (or even experimental) data (see Spirtes et al . 2001 ; Bareinboim et al. 2020 ). On the Opportunities and Risks of Foundation Models 51 referentialism, there is still a further question of how these proxies relate to the actual world, but the same question arises for human language users as well. Bender and Koller [ 2020 ] give an interesting argument that combines referentialism with pramatism. They imagine an agent O that intercepts communications between two humans speaking a natural language L. O inhabits a very different world from the humans and so does not have the sort of experiences needed to ground the humans’ utterances in the ways that referentialism demands. Nonetheless, O learns from the patterns in the humans’ utterances, to the point where O can even successfully pretend to be one of the humans. Bender and Koller then seek to motivate the intuition that we can easily imagine situations in which O’s inability to ground L in the humans’ world will reveal itself, and that this will in turn reveal that O does not understand L. The guiding assumption seems to be that the complexity of the world is so great that no amount of textual exchange can fully cover it, and the gaps will eventually reveal themselves",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The guiding assumption seems to be that the complexity of the world is so great that no amount of textual exchange can fully cover it, and the gaps will eventually reveal themselves. In the terms we have defined, the inability to refer is taken to entail that the agent is not in the right dispositional state for understanding. Fundamentally, the scenario Bender and Koller describe is one in which some crucial information for understanding is taken to be missing, and a simple behavioral test reveals this. We can agree with this assessment without concluding that foundation models are in general incapable of understanding. This again brings us back to the details of the training data involved. If we modify Bender and Koller’s scenario so that the transmissions include digitally encoded images, audio, and sensor readings from the humans’ world, and O is capable of learning associations between these digital traces and linguistic units, then we might be more optimistic – there might be a practical issue concerning O’s ability to get enough data to generalize, but perhaps not an in principle limitation on what O can achieve. 29 We tentatively conclude that there is no easy a priori reason to think that varieties of undestanding falling under any of our three positions could not be learned in the relevant way. With this possibility thus still open, we face the difficult epistemological challenge of clarifying how we could hope to evaluate potential success. Epistemology of understanding. A positive feature of pragmatism is that, by identifying success with the manifestation of concrete behaviors, there is no great conceptual puzzle about how to test for it. We simply have to convince ourselves that our limited observations of the system’s behavior so far indicate a reliable disposition toward the more general class of behaviors that we took as our target. Of course, agreeing on appropriate targets is very difficult",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Of course, agreeing on appropriate targets is very difficult. When concrete proposals are made, they are invariably met with objections, often after putative success is demonstrated. The history of the Turing Test is instructive here: although numerous artificial agents have passed actual Turing Tests, none of them has been widely accepted as intelligent as a result. Similarly, in recent years, a number of benchmark tasks within NLP have been proposed to evaluate specific aspects of understanding (e.g., answering simple questions, performing commonsense reasoning). When systems surpass our estimates of human performance, the community’s response is generally that the test was flawed, not that the target was reached. There may be some suite of behaviors that is our real target, but it is just hard to circumscribe or turn into a practical test. 30 Then again, this might reveal that internalism or referentialsm are what we had in mind all along. 29 On our reading, Bender and Koller [ 2020 ] allow that multimodal data might change the scenario, especially if O is allowed to have cooperative interactions with the humans about shared scenarios and topics. 30 Part of the difficulty may also relate to the fact that typical humans make frequent errors in many of these domains, but not necessarily the same types of errors that are made by current systems. Characterizing the target behaviours may thus involve more than just identifying the “correct” behaviour. 52 Center for Research on Foundation Models (CRFM) If we take internalism or referentialism as the ultimate target – our gold standard for what understanding is – then behavioral tests will always be at best imperfect as a means of assessing whether understanding has been achieved. The imperfections are two-fold. First, behavioral tests will always have gaps that could allow unsophisticated models to slip through. Second, a system might have achieved the mapping that these views require, but we may be unable to show this with behavioral testing",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "philosophy_of_understanding_report",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\philosophy_of_understanding_report.pdf",
    "date_published": "2022-07-12",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Second, a system might have achieved the mapping that these views require, but we may be unable to show this with behavioral testing. Recent experiences with the model GPT-3 show how challenging this might become: depending on the prompt one uses, one can see surprisingly coherent outputs or utter nonsense, and so prompt engineering requires deep expertise [ Rong 2021 ]. Thus, both internalism and referentialism call for structural evaluation methods that allow us to study their internal representations, probing them for information [ Tenney et al . 2019 ; Manning et al . 2020 ], studying their internal dynamics [ Sundararajan et al . 2017 ], and perhaps actively manipulating them according to specific experimental protocols supporting causal inference [ Vig et al . 2020 ; Geiger et al . 2020 ]. There may be fundamental limitations on what we can learn from practical experiments about the inner workings of a complex foundation model, but it is clear that these methods will be useful whenever our target aligns with internalism or referentialism. 2.6.4 Moving the discussion forward. It seems clear that there are no easy answers to the question of whether foundation models will ever understand language. To even begin to address the question, one must resolve a difficult metaphysical question about which there are a number of substantively distinct views. The metphysical question then feeds into an epistemological question that poses many practical challenges. Nonetheless, the above discussion does invite one practical conclusion: if foundation models are pursued as a path to language understanding in artificial agents, then multimodal training regimes may well be the most viable strategy, as they would seem the most likely to provide the model with the requisite information. Whether self-supervision then suffices is a completely open question.",
    "chunk_id": "Natural_language_processing_philosophy_of_understanding_report_page-48-52.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Bridging the data gap between children and large language models Michael C. Frank Department of Psychology, Stanford University Keywords : language learning; human learning; large language models; artificial intelligence Abstract : Large language models show intriguing emergent behaviors, yet they receive around 4-5 orders of magnitude more language data than human children. What accounts for this vast difference in sample efficiency? Candidate explanations include children’s pre-existing conceptual knowledge; their use of multimodal grounding; and the interactive, social nature of their input. How much learning is needed for the emergence of intelligence? Some rule-based systems were designed to act intelligently in the absence of any adjustments based on training data. In contrast, modern large language models (LLMs) exemplify the opposite strategy: they are fed with massive, internet-scale text datasets and their performance typically grows in proportion to the available data and computation [1]. The resulting models are surprisingly competent at a wide range of tasks, though they still show systematic flaws in reasoning and information retrieval. For many observers, the most interesting feature of LLMs is their ability to reason flexibly about new tasks based on a verbal query, synthesizing information in a text “prompt” to generate, for example, an explanation, a poem, a piece of computer code, or a tabular dataset. This behavior, sometimes termed “few shot” or “in-context learning”, appears to emerge only at very large scales of training data. Yet another type of intelligence performs in-context learning with far less data than even the smallest of LLMs: humans. From an early age, children can reason flexibly about novel tasks and by middle childhood they can quickly master new games, devices, and environments. What can we learn about human and machine intelligence by comparing their data efficiency? Measuring the Gap The scale of training data for current LLMs is unprecedented",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". What can we learn about human and machine intelligence by comparing their data efficiency? Measuring the Gap The scale of training data for current LLMs is unprecedented. Training datasets are typically measured in tokens, a metric that includes words but also punctuation and morphological subparts of words. GPT-3 was trained on 5x10 11 tokens [2] and Chinchilla was trained on 10 12 tokens [1]. Many companies keep training set sizes secret, but a recent leak suggested that one industry model was trained on 3.6x10 12 tokens. How do these numbers compare with human language experience? Comprehensive word counts are difficult to collect but sampling and extrapolation can provide reasonable upper and lower bounds for language input (Figure 1). A soft upper bound on a child’s linguistic input – language produced by the people around them – is around 10 6 words per month [3], [4]. For a five-year-old, that would be 6x10 7 words; for a 20-year-old, 2x10 8 words. We also might assume that a 20-year-old has been reading for 10-15 years, and for much of this time they are reading 2-3 books (10 5 words each) per week for an extra ~10 7 words per year. Our rough upper bound for a literate 20-year-old could be as high as 4x10 8 words (or even higher if they read constantly). In contrast, children growing up in environments with limited language are estimated to experience around 1x10 5 words per month – up to an order of magnitude less than children in richer linguistic environments [5]. Without the boost from literacy, a lower bound on language experience would be around 6x10 6 words by age 5 and 3x10 7 by age 20. Importantly, even a child receiving much less language will still be able to reason about novel tasks, for example learning the rules of a new board game at school. In contrast, language models trained on human-like amounts of data can at best provide incoherent “autocomplete”- like behaviors, with no in-context learning",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In contrast, language models trained on human-like amounts of data can at best provide incoherent “autocomplete”- like behaviors, with no in-context learning. Thus, there is a difference of up to five orders of magnitude in language input between LLMs and human children, and at least three between LLMs and even the most literate adults. What factors explain the far greater efficiency of human learners? Minding the Gap Let’s consider three potential – not mutually exclusive – explanations for human sample efficiency. The first explanation is that an immense evolutionary history that has shaped human minds and brains prior to their initial contacts with data. Some researchers posit that infants have innately specified \"core knowledge\" of objects, agents, and events, comprising the foundations of a conceptual model of the world [6]. Other developmental theories posit architectural constraints on learning that lead to the quick emergence of fundamental knowledge structures [7]. In either case, initial constraints – perhaps expressed via specific patterns of brain connectivity – could provide a major speedup in how much experience an agent needs to bootstrap further reasoning. The second is the richness of the grounded, sensory experience available to human learners. Children’s experiences contain a profusion of auditory, visual, haptic, gustatory, olfactory, and somatosensory data. In constructivist proposals, these data allow children to create and refine theoretical models of the world [8]. Multi-modal data also “ground” language, providing concrete extensional meanings for many words. In contrast, LLMs must induce world knowledge from a single stream of information that primarily contains language – sometimes alongside a complex mélange of computer code and other types of information – rather than connecting linguistic information to external experiences",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The final potential explanation is the type of language input that humans receive, which for children is often generated through structured social interactions in which the child themself plays a part [9]. Some of this input is simplified by adults, with limited vocabulary and lower sentence complexity. Such interactional input differs dramatically from the training data provided to LLMs, which make predictions about vast amounts of text from decontextualized sources and with no opportunity to interact or intervene. One observation supporting this hypothesis is that newer LLMs are trained via reinforcement learning using human feedback; it is likely that this “interaction training” is responsible for the success of products like ChatGPT in responding appropriately in conversation. Crossing the Gap Beyond these three substantive factors, we should consider the possibility that much of the apparent difference between LLMs and human learners is due to differences in evaluation. LLMs are often evaluated on complex reasoning tasks, while tasks for children are typically simple and highly scaffolded. Understanding the gap between LLMs and children will require synchronized evaluation (as well as unambiguous positive evidence that models are truly passing the evaluations, rather than memorizing test data). Tasks like MEWL, a battery of word learning tasks [10], or the Baby Intuitions Benchmark, a set of social cognition tasks [11], attempt to create apples-to-apples comparisons. Most developmental experiments are not conducted in a unimodal, language-only format, however, so comparison between children and LLMs can be challenging. Even when we have evaluated models and humans more comparably, it is likely that a gap in input will persist. After all, human adults – who pass most LLM evaluations [2] – still have not experienced anywhere near the amount of language that the models have",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". After all, human adults – who pass most LLM evaluations [2] – still have not experienced anywhere near the amount of language that the models have. Testing hypotheses about this gap will require more work on sample efficiency, which has not been a priority given that the artificial intelligence community has embraced scale as the route to better performance [1]. Still, several recent efforts are promising. The BabyLM challenge ( https://babylm.github.io ) asks entrants to train models on curated 10 7 - and 10 8 -word datasets, providing a framework for comparing different LLM architectures on human-scale datasets. Holding data constant is an effective research design for understanding how learning architectures affect outcomes, though one worry is that without high-quality training data – for example, coherent, interactive dialogue about the here-and-now – even the best architecture might fail. Unfortunately, our best resource for transcripts of grounded, interactive language use, the Child Language Data Exchange (CHILDES), is too small to train an LLM. Extant multimodal datasets are even smaller, and still only include visual and linguistic data. One creative workaround is to generate data automatically: the TinyStories corpus is an LLM-generated corpus of chilappropriate stories with a restricted vocabulary [12], and small-scale LLMs trained on this corpus show surprising competence. Applying this approach to the generation of multimodal data could be a promising direction for creating more developmentally appropriate training corpora. As LLMs grow ever larger, there is a real risk that developers will run out of high-quality training data. An alternative path is to figure out how to make better use of the available data, increasing efficiency by pursuing learning strategies that are better aligned with human learning. Doing so will require a better understanding of the gaps between human learners and current models, however",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Doing so will require a better understanding of the gaps between human learners and current models, however. Bridging these gaps may prove rewarding for our understanding of human as well as machine intelligence. Figure 1 . A gap of around 3-5 orders of magnitude exists between estimated human language inputs (red and blue lines) and the inputs to large language models (dashed lines). Acknowledgements Thanks to Noah D. Goodman, Christopher Potts, and Alvin Tan for helpful discussion. References [1] J. Hoffmann et al. , “Training Compute-Optimal Large Language Models.” arXiv, Mar. 29, 2022. doi: 10.48550/arXiv.2203.15556. [2] T. B. Brown et al. , “Language Models are Few-Shot Learners.” arXiv, Jul. 22, 2020. doi: 10.48550/arXiv.2005.14165. [3] B. C. Roy, M. C. Frank, P. DeCamp, M. Miller, and D. Roy, “Predicting the birth of a spoken word,” Proc. Natl. Acad. Sci. , vol. 112, no. 41, pp. 12663–12668, 2015, doi: human upper bound human lower bound Chinchilla GPT−3 1e+06 1e+09 1e+12 5 10 15 20 Age (years) Words of language input https://doi.org/10.1073/pnas.1419773112. [4] E. Dupoux, “Cognitive science in the era of artificial intelligence: A roadmap for reversengineering the infant language-learner,” Cognition , vol. 173, pp. 43–59, 2018. [5] E. Bergelson, M. Casillas, M. Soderstrom, A. Seidl, A. S. Warlaumont, and A. Amatuni, “What Do North American Babies Hear? A large-scale cross-corpus analysis,” Dev. Sci. , vol. 22, no. 1, p. e12724, 2019, doi: 10.1111/desc.12724. [6] E. S. Spelke and K. D. Kinzler, “Core knowledge,” Dev. Sci. , vol. 10, no. 1, pp. 89–96, 2007. [7] J. B. Tenenbaum, C. Kemp, T. L. Griffiths, and N. D. Goodman, “How to grow a mind: Statistics, structure, and abstraction,” Science , vol. 331, no. 6022, pp. 1279–1285, 2011. [8] A. Gopnik and H. M. Wellman, “Reconstructing constructivism: Causal models, Bayesian learning mechanisms, and the theory theory,” Psychol. Bull. , vol. 138, pp. 1085–1108, 2012, doi: 10.1037/a0028044. [9] E. V. Clark, First language acquisition , 3rd ed",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Bridging the data gap (TiCS 2023) r1",
    "author": "Michael Frank",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Frank-2023-Bridging-the-data-gap-between-child.pdf",
    "date_published": "2023-08-07",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Bull. , vol. 138, pp. 1085–1108, 2012, doi: 10.1037/a0028044. [9] E. V. Clark, First language acquisition , 3rd ed. Cambridge, UK: Cambridge University Press, 2016. [10] G. Jiang et al. , “MEWL: Few-shot multimodal word learning with referential uncertainty.” arXiv, Jun. 01, 2023. doi: 10.48550/arXiv.2306.00503. [11] G. Stojnić, K. Gandhi, S. Yasuda, B. M. Lake, and M. R. Dillon, “Commonsense psychology in human infants and machines,” Cognition , vol. 235, p. 105406, Jun. 2023, doi: 10.1016/j.cognition.2023.105406. [12] R. Eldan and Y. Li, “TinyStories: How Small Can Language Models Be and Still Speak Coherent English?” arXiv, May 24, 2023. doi: 10.48550/arXiv.2305.07759.",
    "chunk_id": "Natural_language_processing_bridging_the_data_gap_(tics_2023)_r1.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": "REVIEW published: 16 April 2020 doi: 10.3389/fncom.2020.00029 Edited by: Adam Henry Marblestone, Harvard University, United States Reviewed by: Mattia Rigotti, IBM Research, United States Mariya Toneva, Carnegie Mellon University, United States H. Steven Scholte, University of Amsterdam, Netherlands *Correspondence: Grace W. Lindsay gracewlindsay@gmail.com Received: 02 December 2019 Accepted: 23 April 2020 Published: 16 April 2020 Citation: Lindsay GW (2020) Attention in Psychology, Neuroscience, and Machine Learning. Front. Comput. Neurosci. 14:29. doi: 10.3389/fncom.2020.00029 Attention in Psychology, Neuroscience, and Machine Learning Grace W. Lindsay* Gatsby Computational Neuroscience Unit, Sainsbury Wellcome Centre, University College London, London, United Kingdom Attention is the important ability to flexibly control limited computational resources. It has been studied in conjunction with many other topics in neuroscience and psychology including awareness, vigilance, saliency, executive control, and learning. It has also recently been applied in several domains in machine learning. The relationship between the study of biological attention and its use as a tool to enhance artificial neural networks is not always clear. This review starts by providing an overview of how attention is conceptualized in the neuroscience and psychology literature. It then covers several use cases of attention in machine learning, indicating their biological counterparts where they exist. Finally, the ways in which artificial attention can be further inspired by biology for the production of complex and integrative systems is explored. Keywords: attention, artificial neural networks, machine learning, vision, memory, awareness 1. INTRODUCTION Attention is a topic widely discussed publicly and widely studied scientifically. It has many definitions within and across multiple fields including psychology, neuroscience, and, most recently, machine learning ( Chun et al., 2011; Cho et al., 2015 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". It has many definitions within and across multiple fields including psychology, neuroscience, and, most recently, machine learning ( Chun et al., 2011; Cho et al., 2015 ). As William James wrote at the dawn of experimental psychology, “Everyone knows what attention is. It is the taking possession by the mind, in clear, and vivid form, of one out of what seems several simultaneously possible objects or trains of thought.” Since James wrote this, many attempts have been made to more precisely define and quantify this process while also identifying the underlying mental and neural architectures that give rise to it. The glut of different experimental approaches and conceptualizations to study what is spoken of as a single concept, however, has led to something of a backlash amongst researchers. As was claimed in the title of a recent article arguing for a more evolution-informed approach to the concept, “No one knows what attention is” ( Hommel et al., 2019 ). Attention is certainly far from a clear or unified concept. Yet despite its many, vague, and sometimes conflicting definitions, there is a core quality of attention that is demonstrably of high importance to information processing in the brain and, increasingly, artificial systems. Attention is the flexible control of limited computational resources. Why those resources are limited and how they can best be controlled will vary across use cases, but the ability to dynamically alter and route the flow of information has clear benefits for the adaptiveness of any system. The realization that attention plays many roles in the brain makes its addition to artificial neural networks unsurprising. Artificial neural networks are parallel processing systems comprised of individual units designed to mimic the basic input-output function of neurons. These models are currently dominating the machine learning and artificial intelligence (AI) literature",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". These models are currently dominating the machine learning and artificial intelligence (AI) literature. Initially constructed without attention, various mechanisms for dynamically re-configuring the representations or structures of these networks have now been added. The following section, section 2, will cover broadly the different uses of the word attention in neuroscience and psychology, along with its connection to other common neuroscientific topics. Throughout, the conceptualization of attention as a way to control limited resources will be highlighted. Behavioral studies will be used to demonstrate the abilities and limits of attention while neural mechanisms point to the physical means through which these behavioral effects are manifested. In section 3, the state of attention research in machine learning will be summarized and relationships between artificial and biological attention will be indicated where they exist. And in section 4 additional ways in which findings from biological attention can influence its artificial counterpart will be presented. The primary aim of this review is to give researchers in the field of AI or machine learning an understanding of how attention is conceptualized and studied in neuroscience and psychology in order to facilitate further inspiration where fruitful. A secondary aim is to inform those who study biological attention how these processes are being operationalized in artificial systems as it may influence thinking about the functional implications of biological findings. 2. ATTENTION IN NEUROSCIENCE AND PSYCHOLOGY The scientific study of attention began in psychology, where careful behavioral experimentation can give rise to precise demonstrations of the tendencies and abilities of attention in different circumstances. Cognitive science and cognitive psychology aim to turn these observations into models of how mental processes could create such behavioral patterns",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Cognitive science and cognitive psychology aim to turn these observations into models of how mental processes could create such behavioral patterns. Many word models and computational models have been created that posit different underlying mechanisms ( Driver, 2001; Borji and Itti, 2012 ). The influence of single-cell neurophysiology in non-human primates along with non-invasive means of monitoring human brain activity such as EEG, fMRI, and MEG have made direct observation of the underlying neural processes possible. From this, computational models of neural circuits have been built that can replicate certain features of the neural responses that relate to attention ( Shipp, 2004 ). In the following sub-sections, the behavioral and neural findings of several different broad classes of attention will be discussed. 2.1. Attention as Arousal, Alertness, or Vigilance In its most generic form, attention could be described as merely an overall level of alertness or ability to engage with surroundings. In this way it interacts with arousal and the sleewake spectrum. Vigilance in psychology refers to the ability to sustain attention and is therefore related as well. Note, while the use of these words clusters around the same meaning, they are sometimes used more specifically in different niche literature ( Oken et al., 2006 ). Studying subjects in different phases of the sleep-wake cycle, under sleep deprivation, or while on sedatives offers a view of how this form of attention can vary and what the behavioral consequences are. By giving subjects repetitive tasks that require a level of sustained attention—such as keeping a ball within a certain region on a screen—researchers have observed extended periods of poor performance in drowsy patients that correlate with changes in EEG signals ( Makeig et al., 2000 ). Yet, there are ways in which tasks can be made more engaging that can lead to higher performance even in drowsy or sedated states",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Yet, there are ways in which tasks can be made more engaging that can lead to higher performance even in drowsy or sedated states. This includes increasing the promise of reward for performing the task, adding novelty or irregularity, or introducing stress ( Oken et al., 2006 ). Therefore, general attention appears to have limited reserves that won’t be deployed in the case of a mundane or insufficiently rewarding task but can be called upon for more promising or interesting work. Interestingly, more arousal is not always beneficial. The Yerkes-Dodson curve ( Figure 1B ) is an inverted-U that represents performance as a function of alertness on sufficiently challenging tasks: at low levels of alertness performance is poor, at medium levels it is good, and at high levels it becomes poor again. The original study used electric shocks in mice to vary the level of alertness, but the finding has been repeated with other measures ( Diamond, 2005 ). It may explain why psychostimulants such as Adderall or caffeine can work to increase focus in some people at some doses but become detrimental for others ( Wood et al., 2014 ). The neural circuits underlying the sleep-wake cycle are primarily in the brain stem ( Coenen, 1998 ). These circuits control the flow of information into the thalamus and then onto cortex. Additionally, neuromodulatory systems play a large role in the control of generalized attention. Norepinephrine, acetylcholine, and dopamine are believed to influence alertnesss, orienting to important information, and executive control of attention, respectively ( Posner, 2008 ). The anatomy of neuromodulators matches their function as well. Neurons that release norepinephrine, for example, have their cell bodies in the brain stem but project very broadly across the brain, allowing them to control information processing broadly ( Figure 1A ). 2.2. Sensory Attention In addition to overall levels of arousal and alertness, attention can also be selectively deployed by an awake subject to specific sensory inputs",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". 2.2. Sensory Attention In addition to overall levels of arousal and alertness, attention can also be selectively deployed by an awake subject to specific sensory inputs. Studying attention within the context of a specific sensory system allows for tight control over both stimuli and the locus of attention. Generally, to look for this type of attention the task used needs to be quite challenging. For example, in a change detection task, the to-be-detected difference between two stimuli may be very slight. More generally, task difficulty can be achieved by presenting the stimulus for only a very short period of time or only very weakly. A large portion of the study of attention in systems neuroscience and psychology centers on visual attention in particular ( Kanwisher and Wojciulik, 2000 ). This may reflect the general trend in these fields to emphasis the study of visual processing over other sensory systems ( Hutmacher, 2019 ), along with the dominant role vision plays in the primate brain. FIGURE 1 | General attention and alertness (A) Cells in the locus coeruleus release norepinephrine (also known as noradrenaline) onto many parts of the brain with different functions, including onto other neuromodulatory systems. This contributes to overall arousal ( Samuels and Szabadi, 2008 ). Colors here represent different divisions of the brain: forebrain (green), diencephalon (yellow), and brainstem (blue). (B) The Yerkes-Dodson curve describes the nonlinear relationship between arousal and performance on challenging tasks. Furthermore, visual stimuli are frequently used in studies meant to address more general, cognitive aspects of attention as well. Visual attention can be broken down broadly into spatial and feature-based attention. 2.2.1. Visual Spatial Attention Saccades are small and rapid eye movements made several times each second. As the fovea offers the highest visual resolution on the retina, choosing where to place it is essentially a choice about where to deploy limited computational resources",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". As the fovea offers the highest visual resolution on the retina, choosing where to place it is essentially a choice about where to deploy limited computational resources. In this way, eye movements indicate the locus of attention. As this shift of attention is outwardly visible it is known as overt visual attention. By tracking eye movements as subjects are presented with different images, researchers have identified image patterns that automatically attract attention. Such patterns are defined by oriented edges, spatial frequency, color contrast, intensity, or motion ( Itti and Koch, 2001 ). Image regions that attract attention are considered “salient” and are computed in a “bottoup” fashion. That is, they don’t require conscious or effortful processing to identify and are likely the result of built-in feature detectors in the visual system. As such, saliency can be computed very quickly. Furthermore, different subjects tend to agree on which regions are salient, especially those identified in the first few saccades ( Tatler et al., 2005 ). Salient regions can be studied in “free-viewing” situations, that is, when the subject is not given any specific instructions about how to view the image. When a particular task is assigned, the interplay between bottom-up and “top-down” attention becomes clear. For example, when instructed to saccade to a specific visual target out of an array, subjects may incorrectly saccade to a particularly salient distractor instead ( van Zoest and Donk, 2005 ). More generally, task instructions can have a significant effect on the pattern of saccades generated when subjects are viewing a complex natural image and given high-level tasks (e.g., asked to assess the age of a person or guess their socio-economic status). Furthermore, the natural pattern of eye movements when subjects perform real world tasks, like sandwich making, can provide insights to underlying cognitive processes ( Hayhoe and Ballard, 2005 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Furthermore, the natural pattern of eye movements when subjects perform real world tasks, like sandwich making, can provide insights to underlying cognitive processes ( Hayhoe and Ballard, 2005 ). When subjects need to make multiple saccades in a row they tend not to return to locations they have recently attended and may be slow to respond if something relevant occurs there. This phenomenon is known as inhibition of return ( Itti and Koch, 2001 ). Such behavior pushes the visual system to not just exploit image regions originally deemed most salient but to explore other areas as well. It also means the saccade generating system needs to have a form of memory; this is believed to be implemented by short-term inhibition of the representation of recently-attended locations. While eye movements are an effective means of controlling visual attention, they are not the only option. “Covert” spatial attention is a way of emphasizing processing of different spatial locations without an overt shift in fovea location. Generally, in the study of covert spatial attention, subjects must fixate on a central point throughout the task. They are cued to covertly attend to a location in their peripheral vision where stimuli relevant for their visual task will likely appear. For example, in an orientation discrimination task, after the spatial cue is provided an oriented grating will flash in the cued location and the subject will need to indicate its orientation. On invalidly-cued trials (when the stimulus appears in an uncued location), subjects perform worse than on validly-cued (or uncued) trials ( AntoErxleben and Carrasco, 2013 ). This indicates that covert spatial attention is a limited resource that can be flexibly deployed and aids in the processing of visual information. Covert spatial attention is selective in the sense that certain regions are selected for further processing at the expense of others. This has been referred to as the “spotlight” of attention",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Covert spatial attention is selective in the sense that certain regions are selected for further processing at the expense of others. This has been referred to as the “spotlight” of attention. Importantly, for covert—as opposed to overt—attention the input to the visual system can be identical while the processing of that input is flexibly selective. Covert spatial attention can be impacted by bottom-up saliency as well. If an irrelevant but salient object is flashed at a location that then goes on to have a task relevant stimulus, the exogenous spatial attention drawn by the irrelevant stimulus can get applied to the task relevant stimulus, possibly providing a performance benefit. If it is flashed at an irrelevant location, however, it will not help, and can harm performance ( Berger et al., 2005 ). Bottom-up/exogenous attention has a quick time course, impacting covert attention for 80–130 ms after the distractor appears ( Anton-Erxleben and Carrasco, 2013 ). In some theories of attention, covert spatial attention exists to help guide overt attention. Particularly, the pre-motor theory of attention posits that the same neural circuits plan saccades and control covert spatial attention ( Rizzolatti et al., 1987 ). The frontal eye field (FEF) is known to be involved in the control of eye movements. Stimulating the neurons in FEF at levels too low to evoke eye movements has been shown to create effects similar to covert attention ( Moore et al., 2003 ). In this way, covert attention may be a means of deciding where to overtly look. The ability to covertly attend may additionally be helpful in social species, as eye movements convey information about knowledge and intent that may best be kept secret ( Klein et al., 2009 ). To study the neural correlates of covert spatial attention, researchers identify which aspects of neural activity differ based only on differences in the attentional cue (and not on differences in bottom-up features of the stimuli)",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". On trials where attention is cued toward the receptive field of a recorded neuron, many changes in the neural activity have been observed ( Noudoost et al., 2010; Maunsell, 2015 ). A commonly reported finding is an increase in firing rates, typically of 20–30% ( Mitchell et al., 2007 ). However, the exact magnitude of the change depends on the cortical area studied, with later areas showing stronger changes ( Luck et al., 1997; Noudoost et al., 2010 ). Attention is also known to impact the variability of neural firing. In particular, it decreases trial-to-trial variability as measured via the Fano Factor and decreases noise correlations between pairs of neurons. Attention has even been found to impact the electrophysiological properties of neurons in a way that reduces their likelihood of firing in bursts and also decreases the height of individual action potentials ( Anderson et al., 2013 ). In general, the changes associated with attention are believed to increase the signal-to-noise ratio of the neurons that represent the attended stimulus, however they can also impact communication between brain areas. To this end, attention’s effect on neural synchrony is important. Within a visual area, attention has been shown to increase spiking coherence in the gamma band—that is at frequencies between 30 and 70 Hz ( Fries et al., 2008 ). When a group of neurons fires synchronously, their ability to influence shared downstream areas is enhanced. Furthermore, attention may also be working to directly coordinate communication across areas. Synchronous activity between two visual areas can be a sign of increased communication and attention has been shown to increase synchrony between the neurons that represent the attended stimulus in areas V1 and V4, for example ( Bosman et al., 2012 ). Control of this cross-area synchronization appears to be carried out by the pulvinar ( Saalmann et al., 2012 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Control of this cross-area synchronization appears to be carried out by the pulvinar ( Saalmann et al., 2012 ). In addition to investigating how attention impacts neurons in the visual pathways, studies have also searched for the source of top-down attention ( Noudoost et al., 2010; Miller and Buschman, 2014 ). The processing of bottom-up attention appears to culminate with a saliency map produced in the lateral intraparietal area (LIP). The cells here respond when salient stimuli are in their receptive field, including task-irrelevant but salient distractors. Prefrontal areas such as FEF, on the other hand, appear to house the signals needed for top-down control of spatial attention and are less responsive to distractors. While much of the work on the neural correlates of sensory attention focuses on the cortex, subcortical areas appear to play a strong role in the control and performance benefits of attention as well. In particular, the superior colliculus assists in both covert and overt spatial attention and inactivation of this region can impair attention ( Krauzlis et al., 2013 ). And, as mentioned above, the pulvinar plays a role in attention, particularly with respect to gating effects on cortex ( Zhou et al., 2016 ). 2.2.2. Visual Feature Attention Feature attention is another form of covert selective attention. In the study of feature attention, instead of being cued to attend to a particular location, subjects are cued on each trial to attend to a particular visual feature such as a specific color, a particular shape, or a certain orientation. The goal of the task may be to detect if the cued feature is present on the screen or readout another one of its qualities (e.g., to answer “what color is the square?” should result in attention first deployed to squares). Valid cueing about the attended feature enhances performance",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Valid cueing about the attended feature enhances performance. For example, when attention was directed toward a particular orientation, subjects were better able to detect faint gratings of that orientation than of any other orientation ( Rossi and Paradiso, 1995 ). While the overall task (e.g., detection of an oriented grating) remains the same, the specific instructions (detection of 90 ◦ grating vs. 60 ◦ vs. 30 ◦ ) will be cued on each individual trial, or possibly blockwise. Successful triawise cueing indicates that this form of attention can be flexibly deployed on fast timescales. FIGURE 2 | Visual search tasks engage many forms of visual attention. Across the top row the progression of a visual search task is shown. First, a cue indicates the target of the visual search, in this case a blue X. Then a search array appears with many non-targets. Top-down feature attention to cells that represent the color blue and the shape X will increase their firing throughout the visual field but firing will be strongest where blue or Xs actually occur. These neural response will play a role in generating a map of covert spatial attention which can be used to explore visual space before saccading. After the shift in overt attention with the first saccade, the covert attention map is remade. Finally, the target is located and successfully saccaded to. If the visual array contained a pop-out stimulus (for example a green O) it may have captured covert spatial attention in a bottom-up way and led to an additional incorrect saccade. Visual search tasks are also believed to activate feature-based attention ( Figure 2 ). In these tasks, an array of stimuli appears on a screen and subjects need to indicate—frequently with an eye movement—the location of the cued stimulus. As subjects are usually allowed to make saccades throughout the task as they search for the cued stimulus, this task combines covert featurbased attention with overt attention",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". As subjects are usually allowed to make saccades throughout the task as they search for the cued stimulus, this task combines covert featurbased attention with overt attention. In fact, signals of todown feature-based attention have been found in FEF, the area involved in saccade choice ( Zhou and Desimone, 2011 ). Because certain features can create a pop-out effect—for example, a single red shape amongst several black ones will immediately draw attention—visual search tasks also engage bottom-up attention which, depending on the task, may need to be suppressed ( Wolfe and Horowitz, 2004 ). Neural effects of feature-based attention in the visual system are generally similar to those of spatial attention. Neurons that represent the attended feature, for example, have increased firing rates, and those that represent very different features have suppressed rates ( Treue and Trujillo, 1999 ). As opposed to spatial attention, however, feature-based attention is spatiallglobal. This means that when deploying attention to a particular feature the activity of the neurons that represent that feature anywhere in visual space are modulated ( Saenz et al., 2002 ). Another difference between spatial and feature attention is the question of how sources of top-down attention target the correct neurons in the visual system. The retinotopic map, wherein nearby cells represent nearby spatial locations, makes spatial targeting straightforward, but cells are not as neatly organized according to preferred visual features. The effects of spatial and feature attention appear to be additive ( Hayden and Gallant, 2009 ). Furthermore, both feature and spatial attention are believed to create their effects by acting on the local neural circuits that implement divisive normalization in visual cortex ( Reynolds and Heeger, 2009 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Modeling work has shown that many of the neural effects of selective attention can be captured by assuming that top-down connections provide targeted synaptic inputs to cells in these circuits ( Lindsay et al., 2019 ). However, models that rely on effects of the neuromodulator acetylcholine can also replicate neural correlates of attention ( Sajedin et al., 2019 ). Potential sources of top-down feature-based attention have been found in prefrontal cortex where sustained activity encodes the attended feature ( Bichot et al., 2015; Paneri and Gregoriou, 2017 ). Inactivating the ventral prearcuate area impairs performance on search tasks. From prefrontal areas, attention signals are believed to travel in a reverse hierarchical way wherein higher visual areas send inputs to those below them ( Ahissar and Hochstein, 2000 ). A closely related topic to feature attention is object attention. Here, attention is not deployed to an abstract feature in advance of a visual stimulus, but rather it is applied to a particular object in the visual scene ( Chen, 2012 ). The initial feedforward pass of activity through the visual hierarchy is able to prattentively segregate objects from their backgrounds in parallel across the visual field, provided these objects have stark and salient differences from the background. In more crowded or complex visual scenes, recurrent and serial processing is needed in order to identify different objects ( Lamme and Roelfsema, 2000 ). Serial processing involves moving limited attentional resources from one location in the image to another; it can take the form of shifts in either covert or overt spatial attention ( Buschman and Miller, 2009 ). Recurrent connections in the visual system—that is, both horizontal connections from nearby neurons in the same visual area and feedback connections from those in higher visual areas—aid in figure-ground segregation and object identification",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". The question of how the brain performs perceptual grouping of low-level features into a coherent object identity has been studied for nearly a century. It is believed that attention may be required for grouping, particularly for novel or complex objects ( Roelfsema and Houtkamp, 2011 ). This may be especially important in visual search tasks that require locating an object that is defined by a conjunction of several features. Neurally, the effects of object-based attention can spread slowly through space as parts of an object are mentally traced ( Roelfsema et al., 1998 ). Switching attention to a location outside an object appears to incur a greater cost than switching to the same distance away but within the object ( Brown and Denney, 2007 ). In addition, once attention is applied to a visual object, it is believed to activate feature-based attention for the different features of that object across the visual field ( O’Craven et al., 1999 ). Another form of attention sometimes referred to as feature attention involves attending to an entire feature dimension. An example of this is the Stroop test, wherein the names of colors are written in different colored ink and subjects either need to read the word itself or say the color of the ink. Here attention cannot be deployed to a specific feature in advance, only to the dimensions word or color. Neurally, the switch between dimensions appears to impact sensory coding in the visual stream and is controlled by frontal areas ( Liu et al., 2003 ). 2.2.3. Computational Models of Visual Attention Visual attention, being one of the most heavily-studied topics in the neuroscience of attention, has inspired many computational models of how attention works. In general, these models synthesize various neurophysiological findings in order to help explain how the behavioral impacts of attention arise ( Heinke and Humphreys, 2005 ). Several computational models meant to calculate saliency have been devised ( Itti and Koch, 2001 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Several computational models meant to calculate saliency have been devised ( Itti and Koch, 2001 ). These models use lolevel visual feature detectors—usually designed to match those in the visual system—to create an image-specific saliency map that can predict the saccade patterns of humans in response to the same image. Another approach to calculating saliency based on information theoretic first principles has also been explored and was able to account for certain visual search behaviors ( Bruce and Tsotsos, 2009 ). Some of the behavioral and neural correlates of attention are similar whether the attention is bottom-up or top-down. In the Biased Competition Model of attention, stimuli compete against each other to dominate the neural response ( Desimone, 1998 ). Attention (bottom-up or top-down) can thus work by biasing this competition toward the stimulus that is the target of attention. While the Biased Competition Model is sometimes used simply as a “word model” to guide intuition, explicit computational instantiations of it have also been built. A hierarchical model of the visual pathway that included top-down biasing as well as local competition mediated through horizontal connections was able to replicate multiple neural effects of attention ( Deco and Rolls, 2004 ). A model embodying similar principles but using spiking neurons was also implemented ( Deco and Rolls, 2005 ). Similar models have been constructed explicitly to deal with attribute naming tasks such as the Stroop test described above. The Selective Attention Model (SLAM), for example, has local competition in both the sensory encoding and motor output modules and can mimic known properties of response times in easier and more challenging Stroop-like tests ( Phaf et al., 1990 ). Visual perception has been framed and modeled as a problem of Bayesian inference ( Lee and Mumford, 2003 ). Within this context, attention can help resolve uncertainty under settings where inference is more challenging, typically by modulating priors ( Rao, 2005 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Within this context, attention can help resolve uncertainty under settings where inference is more challenging, typically by modulating priors ( Rao, 2005 ). For example, in Chikkerur et al. (2010) spatial attention functions to reduce uncertainty about object identity and feature attention reduces spatial uncertainty. These principles can capture both behavioral and neural features of attention and can be implemented in a biologically-inspired neural model. The feature similarity gain model of attention (FSGM) is a description of the neural effects of top-down attention that can be applied in both the feature and spatial domain ( Treue and Trujillo, 1999 ). It says that the way in which a neuron’s response is modulated by attention depends on that neuron’s tuning. Tuning is a description of how a neuron responds to different stimuli, so according to the FSGM a neuron that prefers (that is, responds strongly to), e.g., the color blue, will have its activity enhanced by top-down attention to blue. The FSGM also says attention to non-preferred stimuli will cause a decrease in firing and that, whether increased or decreased, activity is scaled multiplicatively by attention. Though not initially defined as a computational model, this form of neural modulation has since been shown through modeling to be effective at enhancing performance on challenging visual tasks ( Lindsay and Miller, 2018 ). Other models conceptualize attention as a dynamic routing of information through a network. An implementation of this form of attention can be found in the Selective Attention for Identification Model (SAIM) ( Heinke and Humphreys, 2003 ). Here, attention routes information from the retina to a representation deemed the “focus of attention”; depending on the current task, different parts of the retinal representation will be mapped to the focus of attention. 2.2.4",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". 2.2.4. Attention in Other Sensory Modalities A famous example of the need for selective attention in audition is the “cocktail party problem”: the difficulty of focusing on the speech from one speaker in a crowded room of multiple speakers and other noises ( Bronkhorst, 2015 ). Solving the problem is believed to involve “early” selection wherein low level features of a voice such as pitch are used to determine which auditory information is passed on for further linguistic processing. Interestingly, selective auditory attention has the ability to control neural activity at even the earliest level of auditory processing, the cochlea ( Fritz et al., 2007 ). Spatial and feature attention have also been explored in the somatosensory system. Subjects cued to expect a tap at different parts on their body are better able to detect the sensation when that cue is valid. However, these effects seem weaker than they are in the visual system ( Johansen-Berg and Lloyd, 2000 ). Reaction times are faster in a detection task when subjects are cued about the orientation of a stimulus on their finger ( Schweisfurth et al., 2014 ). In a study that tested subjects’ ability to detect a taste they had been cued for it was shown that validly-cued tastes can be detected at lower concentrations than invalidlcued ones ( Marks and Wheeler, 1998 ). This mimics the behavioral effects found with feature-based visual attention. Attention to olfactory features has not been thoroughly explored, though visually-induced expectations about a scent can aid its detection ( Gottfried and Dolan, 2003; Keller, 2011 ). Attention can also be spread across modalities to perform tasks that require integration of multiple sensory signals. In general, the use of multiple congruent sensory signals aids detection of objects when compared to relying only on a single modality. Interestingly, some studies suggest that humans may have a bias for the visual domain, even when the signal from another domain is equally valid ( Spence, 2009 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Interestingly, some studies suggest that humans may have a bias for the visual domain, even when the signal from another domain is equally valid ( Spence, 2009 ). Specifically, the visual domain appears to dominate most in tasks that require identifying the spatial location of a cue ( Bertelson and Aschersleben, 1998 ). This can be seen most readily in ventriloquism, where the visual cue of the dummy’s mouth moving overrides auditory evidence about the true location of the vocal source. Visual evidence can also override tactile evidence, for example, in the context of the rubber arm illusion ( Botvinick and Cohen, 1998 ). Another effect of the cross-modal nature of sensory processing is that an attentional cue in one modality can cause an orienting of attention in another modality ( Spence and Driver, 2004 ). Generally, the attention effects in the non-cued modality are weaker. This cross-modal interaction can occur in the context of both endogenous (“top-down”) and exogenous (“bottoup”) attention. 2.3. Attention and Executive Control With multiple simultaneous competing tasks, a central controller is needed to decide which to engage in and when. What’s more, how to best execute tasks can depend on history and context. Combining sensory inputs with past knowledge in order to coordinate multiple systems for the job of efficient task selection and execution is the role of executive control, and this control is usually associated with the prefrontal cortex ( Miller and Buschman, 2014 ). As mentioned above, sources of top-down visual attention have also been located in prefrontal regions. Attention can reasonably be thought of as the output of executive control. The executive control system must thus select the targets of attention and communicate that to the systems responsible for implementing it. According to the reverse hierarchy theory described above, higher areas signal to those from which they get input which send the signal on to those below them and so on ( Ahissar and Hochstein, 2000 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". This means that, at each point, the instructions for attention must be transformed into a representation that makes sense for the targeted region. Through this process, the high level goals of the executive control region can lead to very specific changes, for example, in early sensory processing. Executive control and working memory are also intertwined, as the ability to make use of past information as well as to keep a current goal in mind requires working memory. Furthermore, working memory is frequently identified as sustained activity in prefrontal areas. A consequence of the three-way relationship between executive control, working memory, and attention is that the contents of working memory can impact attention, even when not desirable for the task ( Soto et al., 2008 ). For example, if a subject has to keep an object in working memory while simultaneously performing a visual search for a separate object, the presence of the stored object in the search array can negatively interfere with the search ( Soto et al., 2005 ). This suggests that working memory can interfere with the executive control of attention. However, there still appears to be additional elements of that control that working memory alone does not disrupt. This can be seen in studies wherein visual search performance is even worse when subjects believe they will need to report the memorized item but are shown a search array for the attended item instead ( Olivers and Eimer, 2011 ). This suggests that, while all objects in working memory may have some influence over attention, the executive controller can choose which will have the most. Beyond the flexible control of attention within a sensory modality, attention can also be shifted between modalities",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Beyond the flexible control of attention within a sensory modality, attention can also be shifted between modalities. Behavioral experiments indicate that switching attention either between two different tasks within a sensory modality (for example, going from locating a visual object to identifying it) or between sensory modalities (switching from an auditory task to a visual one) incurs a computational cost ( Pashler, 2000 ). This cost is usually measured as the extent to which performance is worse on trials just after the task has been switched vs. those where the same task is being repeated. Interestingly, task switching within a modality seems to incur a larger cost than switching between modalities ( Murray et al., 2009 ). A similar result is found when switching between or across modes of response (for example, pressing a bottom vs. verbal report), suggesting this is not specific to sensory processing ( Arrington et al., 2003 ). Such findings are believed to stem from the fact that switching within a modality requires a reconfiguration of the same neural circuits, which is more difficult than merely engaging the circuitry of a different sensory system. An efficient executive controller would need to be aware of these costs when deciding to shift attention and ideally try to minimize them; it has been shown that switch costs can be reduced with training ( Gopher, 1996 ). The final question regarding the executive control of attention is how it evolves with learning. Eye movement studies indicate that searched-for items can be detected more rapidly in familiar settings rather than novel ones, suggesting that previousllearned associations guide overt attention ( Chun and Jiang, 1998 ). Such benefits are believed to rely on the hippocampus ( Aly and Turk-Browne, 2017 ). In general, however, learning how to direct attention is not as studied as other aspects of the attention process",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Such benefits are believed to rely on the hippocampus ( Aly and Turk-Browne, 2017 ). In general, however, learning how to direct attention is not as studied as other aspects of the attention process. Some studies have shown that subjects can enhance their ability to suppress irrelevant task information, and the generality of that suppression depends on the training procedure ( Kelley and Yantis, 2009 ). Looking at the neural correlates of attention learning, imaging results suggest that the neural changes associated with learning do not occur in the sensory pathways themselves but rather in areas more associated with attentional control ( Kelley and Yantis, 2010 ). Though not always easy to study, the development of attentional systems in infancy and childhood may provide further clues as to how attention can be learned ( Reynolds and Romano, 2016 ). 2.4. Attention and Memory Attention and memory have many possible forms of interaction. If memory has a limited capacity, for example, it makes sense for the brain to be selective about what is allowed to enter it. In this way, the ability of attention to dynamically select a subset of total information is well-matched to the needs of the memory system. In the other direction, deciding to recall a specific memory is a choice about how to deploy limited resources. Therefore, both memory encoding and retrieval can rely on attention. The role of attention in memory encoding appears quite strong ( Aly and Turk-Browne, 2017 ). For information to be properly encoded into memory, it is best for it be the target of attention. When subjects are asked to memorize a list of words while simultaneously engaging in a secondary task that divides their attention, their ability to consciously recall those words later is impaired (though their ability to recognize the words as familiar is not so affected) ( Gardiner and Parkin, 1990 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Imaging studies have shown that increasing the difficulty of the secondary task weakens the pattern of activity related to memory encoding in the left ventral inferior frontal gyrus and anterior hippocampus and increases the representation of secondary task information in dorsolateral prefrontal and superior parietal regions ( Uncapher and Rugg, 2005 ). Therefore, without the limited neural processing power placed on the task of encoding, memory suffers. Attention has also been implicated in the encoding of spatially-defined memories and appears to stabilize the representations of place cells ( Muzzio et al., 2009 ). Implicit statistical learning can also be biased by attention. For example, in Turk-Browne et al. (2005) subjects watched a stream of stimuli comprised of red and green shapes. The task was to detect when a shape of the attended color appeared twice in a row. Unbeknownst to the subjects, certain statistical regularities existed in the stream such that there were triplets of shapes likely to occur close together. When shown two sets of three shapes— one an actual co-occurring triplet and another a random selection of shapes of the same color—subjects recognized the real triplet as more familiar, but only if the triplets were from the attended color. The statistical regularities of the unattended shapes were not learned. Yet some learning can occur even without conscious attention. For example, in Watanabe (2003) patients engaged in a letter detection task located centrally in their visual field while random dot motion was shown in the background at sub-threshold contrast. The motion had 10% coherence in a direction that was correlated with the currently-presented letter. Before and after learning this task, subjects performed an above-threshold direction classification task. After learning the task, direction classification improved only for the direction associated with the targeted letters",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". After learning the task, direction classification improved only for the direction associated with the targeted letters. This suggests a reward-related signal activated by the target led to learning about a non-attended component of the stimulus. Many behavioral studies have explored the extent to which attention is needed for memory retrieval. For example, by asking subjects to simultaneously recall a list of previouslmemorized words and engage in a secondary task like card sorting, researchers can determine if memory retrieval pulls from the same limited pool of attentional resources as the task. Some such studies have found that retrieval is impaired by the co-occurrence of an attention-demanding task, suggesting it is an attention-dependent process. The exact findings, however, depend on the details of the memory and non-memory tasks used ( Lozito and Mulligan, 2006 ). Even if memory retrieval does not pull from shared attentional resources, it is still clear that some memories are selected for more vivid retrieval at any given moment than others. Therefore, a selection process must occur. An examination of neuroimaging results suggests that the same parietal brain regions responsible for the top-down allocation and bottom-up capture of attention may play analogous roles during memory retrieval ( Wagner et al., 2005; Ciaramelli et al., 2008 ). Studies of memory retrieval usually look at medium to lonterm memory but a mechanism for attention to items in working memory has also been proposed ( Manohar et al., 2019 ). It relies on two different mechanisms of working memory: synaptic traces for non-attended items and sustained activity for the attended one. Some forms of memory occur automatically and within the sensory processing stream itself. Priming is a well-known phenomenon in psychology wherein the presence of a stimulus at one point in time impacts how later stimuli are processed or interpreted",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Priming is a well-known phenomenon in psychology wherein the presence of a stimulus at one point in time impacts how later stimuli are processed or interpreted. For example, the word “doctor” may be recognized more quickly following the word “hospital” than the word “school.” In this way, priming requires a form of implicit memory to allow previous stimuli to impact current ones. Several studies on conceptual or semantic priming indicate that attention to the first stimulus is required for priming effects to occur ( Ballesteros and Mayas, 2015 ); this mirrors findings that attention is required for memory encoding more generally. Most priming is positive, meaning that the presence of a stimulus at one time makes the detection and processing of it or a related stimulus more likely at a later time. In this way, priming can be thought of as biasing bottom-up attention. However, todown attention can also create negative priming. In negative priming, when stimuli that functioned as a distractor on the previous trial serve as the target of attention on the current trial, performance suffers ( Frings et al., 2015 ). This may stem from a holdover effect wherein the mechanisms of distractor suppression are still activated for the now-target stimulus. Adaptation can also be considered a form of implicit memory. Here, neural responses decrease after repeated exposure to the same stimulus. By reducing the response to repetition, changes in the stimulus become more salient. Attention—by increasing the neural response to attended stimuli—counters the effects of adaptation ( Pestilli et al., 2007; Anton-Erxleben et al., 2013 ). Thus, both with priming and adaptation, top-down attention can overcome automatic processes that occur at lower levels which may be guiding bottom-up attention. 3. ATTENTION IN MACHINE LEARNING While the concept of artificial attention has come up prior to the current resurgence of artificial neural networks, many of its popular uses today center on ANNs ( Mancas et al., 2016 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". The use of attention mechanisms in artificial neural networks came about—much like the apparent need for attention in the brain— as a means of making neural systems more flexible. Attention mechanisms in machine learning allow a single trained artificial neural network to perform well on multiple tasks or tasks with inputs of variable length, size, or structure. While the spirit of attention in machine learning is certainly inspired by psychology, its implementations do not always track with what is known about biological attention, as will be noted below. In the form of attention originally developed for ANNs, attention mechanisms worked within an encoder-decoder framework and in the context of sequence models ( Cho et al., 2015; Chaudhari et al., 2019 ). Specifically, an input sequence will be passed through an encoder (likely a recurrent neural network) and the job of the decoder (also likely a recurrent neural network) will be to output another sequence. Connecting the encoder and decoder is an attention mechanism. Commonly, the output of the encoder is a set of a vectors, one for each element in the input sequence. Attention helps determine which of these vectors should be used to generate the output. Because the output sequence is dynamically generated one element at a time, attention can dynamically highlight different encoded vectors at each time point. This allows the decoder to flexibly utilize the most relevant parts of the input sequence. The specific job of the attention mechanism is to produce a set of scalar weightings, α i t , one for each of the encoded vectors ( v i ). At each step t , the attention mechanism ( φ ) will take in information about the decoder’s previous hidden state ( h t − 1 ) and the encoded vectors to produce unnormalized weightings: ̃ α t = φ ( h t − 1 , v ) (1) Because attention is a limited resource, these weightings need to represent relative importance",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". To ensure that the α values sum to one, the unnormalized weightings are passed through a softmax: α i t = exp( ̃ α i t ) P j exp( ̃ α j t ) (2) These attention values scale the encoded vectors to create a single context vector on which the decoder can be conditioned: c t = X j α j t v j (3) This form of attention can be made entirely differentiable and so the whole network can be trained end-to-end with simple gradient descent. This type of artificial attention is thus a form of iterative re-weighting. Specifically, it dynamically highlights different components of a pre-processed input as they are needed for output generation. This makes it flexible and context dependent, like biological attention. As such it is also inherently dynamic. While sequence modeling already has an implied temporal component, this form of attention can also be applied to static inputs and outputs (as will be discussed below in the context of image processing) and will thus introduce dynamics into the model. In the traditional encoder-decoder framework without attention, the encoder produced a fixed-length vector that was independent of the length or features of the input and static during the course of decoding. This forced long sequences or sequences with complex structure to be represented with the same dimensionality as shorter or simpler ones and didn’t allow the decoder to interrogate different parts of the input during the decoding process. But encoding the input as a set of vectors equal in length to the input sequence makes it possible for the decoder to selectively attend to the portion of the input sequence relevant at each time point of the decoding. Again, as in interpretations of attention in the brain, attention in artificial systems is helpful as a way to flexibly wield limited resources. The decoder can’t reasonably be conditioned on the entirety of the input so at some point a bottleneck must be introduced. In the system without attention, the fixed-length encoding vector was a bottleneck",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". In the system without attention, the fixed-length encoding vector was a bottleneck. When an attention mechanism is added, the encoding can be FIGURE 3 | Attention for neural machine translation. The to-be-translated sentence is encoded to a series of vectors ( v ) via a recurrent neural network. The attention mechanism ( φ ) uses the hidden state of the decoder ( h ) and these vectors to determine how the encoded vectors should be combined to produce a context vector ( c ), which influences the next hidden state of the decoder and thus the next word in the translated sentence. larger because the bottleneck (in the form of the context vector) will be produced dynamically as the decoder determines which part of the input to attend to. The motivation for adding such attention mechanisms to artificial systems is of course to improve their performance. But another claimed benefit of attention is interpretability. By identifying on which portions of the input attention is placed (that is, which α i values are high) during the decoding process, it may be possible to gain an understanding of why the decoder produced the output that it did. However, caution should be applied when interpreting the outputs of attention as they may not always explain the behavior of the model as expected ( Jain and Wallace, 2019; Wiegreffe and Pinter, 2019 ). In the following subsections, specific applications of this general attention concept will be discussed, along with some that don’t fit neatly into this framework. Further analogies to the biology will also be highlighted. 3.1. Attention for Natural Language Processing As described above, attention mechanisms have frequently been added to models charged with processing sequences. Natural language processing (NLP) is one of the most common areas of application for sequence modeling",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Natural language processing (NLP) is one of the most common areas of application for sequence modeling. And, though it was not the original domain of attention in machine learning—nor does it have the most in common with biology—NLP is also one of the most common areas of application for attention ( Galassi et al., 2019 ). An early application of the this form of attention in artificial neural networks was to the task of translation ( Bahdanau et al., 2014 ) ( Figure 3 ). In this work, a recurrent neural network encodes the input sentence as a set of “annotation” vectors, one for each word in the sentence. The output, a sentence in the target language, is generated one word at a time by a recurrent neural network. The probability of each generated word is a function of the previously generated word, the hidden state of the recurrent neural network and a context vector generated by the attention mechanism. Here, the attention mechanism is a small feedforward neural network that takes in the hidden state of the output network as well as the current annotation vector to create the weighting over all annotation vectors. Blending information from all the words in the sentence this way allows the network to pull from earlier or later parts when generating an output word. This can be especially useful for translating between languages with different standard word orders. By visualizing the locations in the input sentence to which attention was applied the authors observed attention helping with this problem. Since this initial application, many variants of attention networks for language translation have been developed. In Firat et al. (2016) , the attention mechanism was adapted so it could be used to translate between multiple pairs of languages rather than just one. In Luong et al. (2015) , the authors explore different structures of attention to determine if the ability to access all input words at once is necessary. And in Cheng et al",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". In Luong et al. (2015) , the authors explore different structures of attention to determine if the ability to access all input words at once is necessary. And in Cheng et al. (2016) , attention mechanisms were added to the recurrent neural networks that perform the sentence encoding and decoding in order to more flexibly create sentence representations. In 2017, the influential “Attention is All You Need” paper utilized a very different style of architecture for machine translation ( Vaswani et al., 2017 ). This model doesn’t have any recurrence, making it simpler to train. Instead, words in the sentence are encoded in parallel and these encodings generate key and query representations that are combined to create attention weightings. These weightings scale the word encodings themselves to create the next layer in the model, a process known as “self-attention.” This process repeats, and eventually interacts with the autoregressive decoder which also has attention mechanisms that allow it to flexibly focus on the encoded input (as in the standard form of attention) and on the previously generated output. The Transformer—the name given to this new attention architecture—outperformed many previous models and quickly became the standard for machine translation as well as other tasks ( Devlin et al., 2018 ). Interestingly, self-attention has less in common with biological attention than the recurrent attention models originally used for machine translation. First, it reduces the role of recurrence and dynamics, whereas the brain necessarily relies on recurrence in sequential processing tasks, including language processing and attentional selection. Second, self-attention provides a form of horizontal interaction between words—which allows for words in the encoded sentence to be processed in the context of those around them—but this mechanism does not include an obvious top-down component driven by the needs of the decoder",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". In fact, self-attention has been shown under certain circumstances to simply implement a convolution, a standard feedforward computation frequently used in image processing ( Andreoli, 2019; Cordonnier et al., 2019 ). In this way, self-attention is more about creating a good encoding than performing a task-specific attention-like selection based on limited resources. In the context of a temporal task, its closest analogue in psychology may be priming because priming alters the encoding of subsequent stimuli based on those that came before. It is of course not the direct goal of machine learning engineers to replicate the brain, but rather to create networks that can be easily trained to perform well on tasks. These different constraints mean that even large advances in machine learning do not necessarily create more brain-like models. While the study of attention in human language processing is not as large as other areas of neuroscience research, some work has been done to track eye movements while reading ( Myachykov and Posner, 2005 ). They find that people will look back at previous sections of text in order to clarify what they are currently reading, particularly in the context of finding the antecedent of a pronoun. Such shifts in overt attention indicate what previous information is most relevant for the current processing demands. 3.2. Attention for Visual Tasks As in neuroscience and psychology, a large portion of studies in machine learning are done on visual tasks. One of the original attention-inspired tools of computer vision is the saliency map, which identifies which regions in an image are most salient based on a set of low-level visual features such as edges, color, or depth and how they differ from their surround ( Itti and Koch, 2001 ). In this way, saliency maps indicate which regions would be captured by “bottom-up” attention in humans and animals. Computer scientists have used saliency maps as part of their image processing pipeline to identify regions for further processing",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Computer scientists have used saliency maps as part of their image processing pipeline to identify regions for further processing. In more recent years, computer vision models have been dominated by deep learning. And since their success in the 2012 ImageNet Challenge ( Russakovsky et al., 2015 ), convolutional neural networks have become the default architecture for visual tasks in machine learning. The architecture of convolutional neural networks is loosely based on the mammalian visual system ( Lindsay, 2020 ). At each layer, a bank of filters is applied to the activity of the layer below (in the first layer this is the image). This creates a H × W × C tensor of neural activity with the number of channels, C equal to the number of filters applied and H and W representing the height and width of the 2-D feature maps that result from the application of a filter. Attention in convolutional neural networks has been used to enhance performance on a variety of tasks including classification, segmentation, and image-inspired natural language processing. Also, as in the neuroscience literature, these attentional processes can be divided into spatial and feature-based attention. 3.2.1. Spatial Attention Building offof the structures used for attention in NLP tasks, visual attention has been applied to image captioning. In Xu et al. (2015) , the encoding model is a convolutional neural network. The attention mechanism works over the activity at the fourth convolutional layer. As each word of the caption is generated, a different pattern of weighting across spatial locations of the image representation is created. In this way, attention for caption generation replaces the set of encoded word vectors in a translation task with a set of encoded image locations. Visualizing the locations with high weights, the model appears to attend to the object most relevant to the current word being generated for the caption",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Visualizing the locations with high weights, the model appears to attend to the object most relevant to the current word being generated for the caption. This style of attention is referred to as “soft” because it produces a weighted combination of the visual features over spatial locations ( Figure 4B ). “Hard” attention is an alternative form that chooses a single spatial location to be passed into the decoder at the expense of all others ( Figure 4A ). In Xu et al. (2015) , to decide which location should receive this hard attention, the attention weights generated for each spatial location were treated as probabilities. One location is chosen according to these probabilities. Adding this stochastic element to the network makes training more difficult, yet it was found to perform somewhat better than soft attention. A 2014 study used reinforcement learning to train a hard attention network to perform object recognition in challenging conditions ( Mnih et al., 2014 ). The core of this model is a recurrent neural network that both keeps track of information taken in over multiple “glimpses” made by the network and outputs the location of the next glimpse. For each glimpse, the network receives a fovea-like input (central areas are represented with high resolution and peripheral with lower) from a small patch of the image. The network has to integrate the information gained from these glimpses to find and classify the object in the image. This is similar to the hard attention described above, except the selection of a location here determines which part of the image is sampled next (whereas in the case above FIGURE 4 | Hard vs. soft visual attention in artificial neural networks. (A) In hard attention, the network only gets input from a small portion of the whole image. This portion is iteratively chosen by the network through an attention selection mechanism. If the input is foveated, the network can use the lower resolution periphery to guide this selection",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". This portion is iteratively chosen by the network through an attention selection mechanism. If the input is foveated, the network can use the lower resolution periphery to guide this selection. (B) Feature maps in convolutional neural networks are 2-D grids of activation created by the application of a filter to the layer below. In soft spatial attention, different locations on these grids are weighted differently. In soft feature attention, different feature maps are weighted differently. it determined which of the already-processed image locations would be passed to the decoder). With the use of these glimpses, the network is not required to process all of the image, saving computational resources. It can also help when multiple objects are present in the image and the network must classify each ( Ba et al., 2014 ). Recent work has shown that adding a prtraining step enhances the performance of hard attention applied to complex images ( Elsayed et al., 2019 ). In many ways, the correspondence between biological and artificial attention is strongest when it comes to visual spatial attention. For example, this form of hard attention—where different locations of the image are sequentially-sampled for further processing—replicates the process of saccading and is therefore akin to overt visual attention in the neuroscience and psychology literature. Insofar as soft attention dynamically rweights different regions of the network’s representation of the image without any change in the input to the network, it is akin to covert spatial attention. Also, as the mode of application for soft attention involves multiplicative scaling of the activity of all units at a specific location, it replicates neural findings about covert spatial attention. Soft spatial attention has been used for other tasks, including visual question and answering ( Chen et al., 2015; Xu and Saenko, 2016; Yang et al., 2016 ) and action recognition in videos ( Sharma et al., 2015 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Hard attention has also been used for instance segmentation ( Ren and Zemel, 2017 ) and for fingrained classification when applied using different levels of image resolution ( Fu et al., 2017 ). 3.2.2. Feature Attention In the case of soft spatial attention, weights are different in different spatial locations of the image representation yet they are the same across all feature channels at that location. That is, the activity of units in the network representing different visual features will all be modified the same way if they represent the same location in image space. Feature attention makes it possible to dynamically re-weight individual feature maps, creating a spatially global change in feature processing. In Stollenga et al. (2014) , a convolutional neural network is equipped with a feature-based attention mechanism. After an image is passed through the standard feedforward architecture, the activity of the network is passed into a policy that determines how the different feature maps at different layers should be weighted. This re-weighting leads to different network activity which leads to different re-weightings. After the network has run for several timesteps the activity at the final layer is used to classify the object in the image. The policy that determines the weighting values is learned through reinforcement learning, and can be added to any pre-trained convolutional neural network. The model in Chen et al. (2017) combines feature and spatial attention to aid in image captioning. The activity of the feedforward pass of the convolutional network is passed into the attention mechanism along with the previously generated word to create attention weightings for different channels at each layer in the CNN. These weights are used to scale activity and then a separate attention mechanism does the same procedure for generating spatial weightings. Both spatial and feature attention weights are generated and applied to the network at each time point. In the model in De Vries et al",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Both spatial and feature attention weights are generated and applied to the network at each time point. In the model in De Vries et al. (2017) , the content of a question is used to control how a CNN processes an image for the task of visual question and answering. Specifically, the activity of a language embedding network is passed through a multi-layer perceptron to produce the additive and multiplicative parameters for batch normalization of each channel in the CNN. This procedure, termed conditional batch normalization, functions as a form of question-dependent feature attention. A different form of dynamic feature re-weighting appears in “squeeze-and-excitation” networks ( Hu et al., 2018 ). In this architecture, the weightings applied to different channels are a nonlinear function of the activity of the other channels at the same layer. As with “self-attention” described above, this differs in spirit from more “top-down” approaches where weightings are a function of activity later in the network and/or biased by the needs of the output generator. Biologically speaking, this form of interaction is most similar to horizontal connections within a visual area, which are known to carry out computations such as divisive normalization ( Carandini and Heeger, 2012 ). In the study of the biology of feature-based attention, subjects are usually cued to attend to or search for specific visual features. In this way, the to-be-attended features are known in advance and relate to the specific sub-task at hand (e.g., detection of a specific shape on a given trial of a general shape detection task). This differs from the above instances of artificial feature attention, wherein no external cue biases the network processing before knowledge about the specific image is available. Rather, the feature re-weighting is a function of the image itself and meant to enhance the performance of the network on a constant task (note this was also the case for the forms of artificial spatial attention described)",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". The reason for using a cueing paradigm in studies of biological attention is that it allows the experimenter to control (and thus know) where attention is placed. Yet, it is clear that even without explicit cueing, our brains make decisions about where to place attention constantly; these are likely mediated by local and lonrange feedback connections to the visual system ( Wyatte et al., 2014 ). Therefore, while the task structure differs between the study of biological feature attention and its use in artificial systems, this difference may only be superficial. Essentially, the artificial systems are using feedforward image information to internally generate top-down attentional signals rather than being given the top-down information in the form of a cue. That being said, some artificial systems do allow for externallcued feature attention. For example setting a prior over categories in the network in Cao et al. (2015) makes it better at localizing the specific category. The network in Wang et al. (2014) , though not convolutional, has a means of biasing the detection of specific object categories as well. And in Lindsay and Miller (2018) , several performance and neural aspects of biological feature attention during a cued object detection task were replicated using a CNN. In Luo et al. (2020) , the costs and benefits of using a form of cued attention in CNNs were explored. As mentioned above, the use of multiplicative scaling of activity is in line with certain findings from biological visual attention. Furthermore, modulating entire feature maps by the same scalar value is aligned with the finding mentioned above that feature attention acts in a spatially global way in the visual system. 3.3. Multi-Task Attention Multi-task learning is a challenging topic in machine learning",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". 3.3. Multi-Task Attention Multi-task learning is a challenging topic in machine learning. When one network is asked to perform several different tasks— for example, a CNN that must classify objects, detect edges, and identify salient regions—training can be difficult as the weights needed to do each individual task may contradict each other. One option is have a set of task-specific parameters that modulate the activity of the shared network differently for each task. While not always called it, this can reasonably be considered a form of attention, as it flexibly alters the functioning of the network. In Maninis et al. (2019) , a shared feedforward network is trained on all of multiple tasks, while task specific skip connections and squeeze-and-excitation blocks are trained to modulate this activity only on their specific task. This lets the network benefit from sharing processing that is common to all tasks while still specializing somewhat to each. A similar procedure was used in Rebuffiet al. (2017) to create a network that performs classification on multiple different image domains. There, the domain could be identified from the input image making it possible to select the set of task-specific parameters automatically at run-time. In Zhao et al. (2018) , the same image can be passed into the network and be classified along different dimensions (e.g. whether the person in the picture is smiling or not, young or old). Task-specific re-weighting of feature channels is used to execute these different classifications. The model in Strezoski et al. (2019) uses what could be interpreted as a form of hard feature attention to route information differently in different tasks. Binary masks over feature channels are chosen randomly for each task. These masks are applied in a task-specific way during training on all tasks and at run-time. Note that in this network no task-specific attentional parameters are learned, as these masks are prdetermined and fixed during training",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Note that in this network no task-specific attentional parameters are learned, as these masks are prdetermined and fixed during training. Instead, the network learns to use the different resulting information pathways to perform different tasks. In a recent work, the notion of task-specific parameters was done away with entirely ( Levi and Ullman, 2020 ). Instead, the activations of a feedforward CNN are combined with a task input and passed through a second CNN to generate a full set of modulatory weights. These weights then scale the activity of the original network in a unit-specific way (thus implementing both spatial and feature attention). The result is a single set of feedforward weights capable of flexibly engaging in multiple visual tasks. When the same input is processed differently according to many different tasks, these networks are essentially implementing a form of within-modality task switching that relies on feature attention. In this way, it is perhaps most similar to the Stroop test described previously. 3.4. Attention to Memory Deep neural networks tend not to have explicit memory, and therefore attention to memory is not studied. Neural Turing Machines, however, are a hybrid neural architecture that includes external memory stores ( Graves et al., 2014 ). The network, through training, learns how to effectively interact with these stores to perform tasks such as sorting and repetition of stored sequences. Facilitating this interaction is a form of attention. Memories are stored as a set of vectors. To retrieve information from this store, the network generates a weight for each vector and calculates a weighted sum of the memories. To determine these weights, a recurrent neural network (which receives external and task-relevant input) outputs a vector and memories are weighted in accordance to their similarity to this vector. Thus, at each point in time, the network is able to access contexrelevant memories",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Thus, at each point in time, the network is able to access contexrelevant memories. As described previously, how the brain chooses what memories to attend to and then attends to them is not entirely clear. The use of a similarity metric in this model means that memories are retrieved based on their overlap with a produced activity vector, similar to associative memory models in the neuroscience literature. This offers a mechanism for the latter question—that is, how attention to memory could be implemented in the brain. The activity vector that the model produces controls what memories get attended and the relationship with biology is less clear here. 4. IDEAS FOR FUTURE INTERACTION BETWEEN ARTIFICIAL AND BIOLOGICAL ATTENTION As has been shown, some amount of inspiration from biology has already led to several instances of attention in artificial neural networks (summarized in Figure 5 ). While the addition of such attention mechanisms has led to appreciable increases in performance in these systems, there are clearly still many ways in which they fall short and additional opportunities for further inspiration exist. In the near term, this inspiration will likely be in the form of incremental improvements to specialized artificial systems as exist now. However, the true promise of brain-inspired AI should deliver a more integrated, multiple-purpose agent that can engage flexibly in many tasks. 4.1. How to Enhance Performance There are two components to the study of how attention works in the brain that can be considered flip sides of the same coin. The first is the question of how attention enhances performance in the way that it does—that is, how do the neural changes associated with attention make the brain better at performing tasks. The second is how and why attention is deployed in the way that it is—what factors lead to the selection of certain items or tasks for attention and not others. Neuroscientists have spent a lot of time investigating the former question",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Neuroscientists have spent a lot of time investigating the former question. In large part, the applicability of these findings to artificial neural systems, however, may not be straightforward. Multiplicative scaling of activity appears in both biological and artificial systems and is an effective means of implementing attention. However, many of the observed effects of attention in the brain make sense mainly as a means of increasing the signal carried by noisy, spiking neurons. This includes increased synchronization across neurons and decreased firing variability. Without analogs for these changes in deep neural networks, it is hard to take inspiration from them. What’s more, the training procedures for neural networks can automatically determine the changes in activity needed to enhance performance on a weldefined task and so lessons from biological changes may not be as relevant. On the other hand, the observation that attention can impact spiking-specific features such as action potential height, burstiness, and precise spike times may indicate the usefulness of spiking networks. Specifically, spiking models offer more degrees of freedom for attention to control and thus allow attention to possibly have larger and/or more nuanced impacts. Looking at the anatomy of attention may provide usable insights to people designing architectures for artificial systems. For example, visual attention appears to modulate activity more strongly in later visual areas like V4 ( Noudoost et al., 2010 ), whereas auditory attention can modulate activity much earlier in the processing stream. The level at which attention should act could thus be a relevant architectural variable. In this vein, recent work has shown that removing self-attention from the early layers of a Transformer model enhances its performance on certain natural language processing tasks and also makes the model a better predictor of human fMRI signals during language processing ( Toneva and Wehbe, 2019 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". The existence of cross-modal cueing—wherein attention cued in one sensory modality can cause attention to be deployed to the same object or location in another modality— indicates some amount of direct interaction between different sensory systems. Whereas many multi-modal models in machine learning use entirely separate processing streams that are only combined at the end, allowing some horizontal connections between different input streams may help coordinate their processing. Attention also interacts with the kind of adaptation that normally occurs in sensory processing. Generally, neural network models do not have mechanisms for adaptation—that is, neurons have no means of reducing their activity if given the same input for multiple time steps. Given that adaptation helps make changes and anomalies stand out, it may be useful to include. In a model with adaption, attention mechanisms should work to reactivate adapted neurons if the repeated stimulus is deemed important. Finally, some forms of attention appear to act in multiple ways on the same system. For example, visual attention is believed to both: (1) enhance the sensitivity of visual neurons in the cortex by modulating their activity and (2) change subcortical activity such that sensory information is readout differently ( Birman and Gardner, 2019; Sreenivasan and Sridharan, 2019 ). In this way, attention uses two different mechanisms, in different parts of the brain, to create its effect. Allowing attention to modulate multiple components of a model architecture in complementary ways may allow it to have more robust and effective impacts. FIGURE 5 | An incomplete summary of the different types of attention studied in neuroscience/psychology and machine learning and how they relate. On the left are divisions of attention studied biologically, on the right are those developed for artificial intelligence and machine learning",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". On the left are divisions of attention studied biologically, on the right are those developed for artificial intelligence and machine learning. Topics at the same horizontal location are to some extent analogous, with the distance between them indicating how close the analogy is. Forms of visual attention, for example, have the most overlap and are the most directly comparable across biology and machine learning. Some forms of attention, such as overall arousal, don’t have an obvious artificial analogue. 4.2. How to Deploy Attention The question of how to deploy attention is likely the more relevant challenge for producing complex and integrated artificial intelligence. Choosing the relevant information in a stream of incoming stimuli, picking the best task to engage in, or deciding whether to engage in anything at all requires that an agent have an integrative understanding of its state, environment, and needs. The most direct way to take influence from biological attention is to mimic it directly. Scanpath models, for example, have existed in the study of saliency for many years. They attempt to predict the series of fixations that humans make while viewing images ( Borji and Itti, 2019 ). A more direct approach to training attention was used in Linsley et al. (2018) . Here, a large dataset of human top-down attention was collected by having subjects label the regions of images most relevant for object classification. The task-specific saliency maps created through this method were used to train attention in a deep convolutional neural network whose main task was object recognition. They found that influencing the activity of intermediate layers with this method could increase performance. Another way of learning a teacher’s saliency map was given in Zagoruyko and Komodakis (2016) . Combined training on tasks and neural data collected from human visual areas has also helped the performance of CNNs ( Fong et al., 2018 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Combined training on tasks and neural data collected from human visual areas has also helped the performance of CNNs ( Fong et al., 2018 ). Using neural data collected during attention tasks in particular could help train attention models. Such transfer could also be done for other tasks. For example, tracking eye movements during reading could inform NLP models; thus far, eye movements have been used to help train a part-of-speech tagging model ( Barrett et al., 2016 ). Interestingly, infants may learn from attending to what adults around them attend to and the coordination of attention more broadly across agents may be very helpful in a social species. Therefore, the attention of others should influence how attention is guided. Attempts to coordinate joint attention will need to be integrated into attention systems ( Kaplan and Hafner, 2006; Klein et al., 2009 ). Activities would likely need to flexibly decide which of several possible goals should be achieved at any time and therefore where attention should be placed. This problem clearly interacts closely with issues around reinforcement learning— particularly hierarchical reinforcement learning which involves the choosing of subtasks—as such decisions must be based on expected positive or negative outcomes. Indeed, there is a close relationship between attention and reward as previously rewarded stimuli attract attention even in contexts where they no longer provide reward ( Camara et al., 2013 ). A better understanding of how humans choose which tasks to engage in and when should allow human behavior to inform the design of a multi-task AI. To this end, the theory put forth in Shenhav et al. (2013) , which says that allocation of the brain’s limited ability to control different processes is based on the expected value of that control, may be of use",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". (2013) , which says that allocation of the brain’s limited ability to control different processes is based on the expected value of that control, may be of use. In this framework, the dorsal anterior cingulate cortex is responsible for integrating diverse information— including the cognitive costs of control—in order to calculate the expected value of control and thus direct processes like attention. Another approach for understanding human executive control in complex tasks is inverse reinforcement learning. This method was recently applied to a dataset of eye movements during visual search in order to determine the reward functions and policies used by humans ( Zelinsky et al., 2020 ). An additional factor that drives biological attention but is perhaps underrepresented in artificial attention systems is curiosity ( Gottlieb et al., 2013 ). In biology, novel, confusing, and surprising stimuli can grab attention, and inferotemporal and perirhinal cortex are believed to signal novel visual situations via an adaptation mechanism that reduces responses to familiar inputs. Reinforcement learning algorithms that include novelty as part of the estimate of the value of a state can encourage this kind of exploration ( Jaegle et al., 2019 ). How exactly to calculate surprise or novelty in different circumstances is not always clear, however. Previous work on biological attention has understood attention selection in Bayesian terms of surprise or information gathering and these framings may be useful for artificial systems ( Itti and Baldi, 2006 ; Mirza et al., 2019 ). A final issue in the selection of attention is how conflicts are resolved. Given the brain’s multiple forms of attention— arousal, bottom-up, top-down, etc.—how do conflicts regarding the appropriate locus of attention get settled? Looking at the visual system, it seems that the local circuits that these multiple systems target are burdened with this task",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". These circuits receive neuromodulatory input along with top-down signals which they must integrate with the bottom-up input driving their activity. Horizontal connections mediate this competition, potentially using winner-take-all mechanisms. This can be mimicked in the architecture of artificial systems. 4.3. Attention and Learning Attention, through its role in determining what enters memory, guides learning. Most artificial systems with attention include the attention mechanism throughout training. In this way, the attention mechanism is trained along with the base architecture; however, with the exception of the Neural Turing Machine, the model does not continue learning once the functioning attention system is in place. Therefore, the ability of attention to control learning and memory is still not explicitly considered in these systems. Attention could help make efficient use of data by directing learning to the relevant components and relationships in the input. For example, saliency maps have been used as part of the pre-processing for various computer vision tasks ( Lee et al., 2004; Wolf et al., 2007; Bai and Wang, 2014 ). Focusing subsequent processing only on regions that are intrinsically salient can prevent wasteful processing on irrelevant regions and, in the context of network training, could also prevent overfitting to these regions. Using saliency maps in this way, however, requires a definition of saliency that works for the problem at hand. Using the features of images that capture bottom-up attention in humans has worked for some computer vision problems; looking at human data in other modalities may be useful as well. In a related vein, studies on infants suggest that they have priors that guide their attention to relevant stimuli such as faces. Using such priors could bootstrap learning both of how to process important stimuli and how to better attend to their relevant features ( Johnson, 2001 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Using such priors could bootstrap learning both of how to process important stimuli and how to better attend to their relevant features ( Johnson, 2001 ). In addition to deciding which portions of the data to process, top-down attention can also be thought of as selecting which elements of the network should be most engaged during processing. Insofar as learning will occur most strongly in the parts of the network that are most engaged, this is another means by which attention guides learning. Constraining the number of parameters that will be updated in response to any given input is an effective form of regularization, as can be seen in the use of dropout and batch normalization. Attention— rather than randomly choosing which units to engage and disengage—is constrained to choose units that will also help performance on this task. It is therefore a more task-specific form of regularization. In this way, attention may be particularly helpful for continual learning where the aim is to update a network to perform better on a specific task while not disrupting performance on the other tasks the network has already learned to do. A related concept, conditional computation, has recently been applied to the problem of continual learning ( Lin et al., 2019 ). In conditional computation, the parameters of a network are a function of the current input (it can thus be thought of as an extreme form of the type of modulation done by attention); optimizing the network for efficient continual learning involves controlling the amount of interference between different inputs. More generically, it may be helpful to think of attention, in part, as a means of guarding against undesirable synaptic changes. Attention and learning also work in a loop. Specifically, attention guides what is learned about the world and internal world models are used to guide attention",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". Attention and learning also work in a loop. Specifically, attention guides what is learned about the world and internal world models are used to guide attention. This inter-dependency has recently been formalized in terms of a reinforcement learning framework that also incorporates cognitive Bayesian inference models that have succeeded in explaining human learning and decision making ( Radulescu et al., 2019 ). Interconnections between basal ganglia and prefrontal cortex are believed to support the interplay between reinforcement learning and attention selection. At a more abstract level, the mere presence of attention in the brain’s architecture can influence representation learning. The global workspace theory of consciousness says that at any moment a limited amount of information selected from the brain’s activity can enter working memory and be available for further joint processing ( Baars, 2005 ). Inspired by this, the ‘consciousness prior’ in machine learning emphasizes a neural network architecture with a low-dimensional representation that arises from attention applied to an underlying high-dimensional state representation ( Bengio, 2017 ). This low-D representation should efficiently represent the world at an abstract level such that it can be used to summarize and make predictions about future states. The presence of this attention-mediated bottleneck has a trickle-down effect that encourages disentangled representations at all levels such that they can be flexibly combined to guide actions and make predictions. Conscious attention is required for the learning of many complex skills such as playing a musical instrument. However once fully learned, these processes can become automatic, possibly freeing attention up to focus on other things ( Treisman et al., 1992 )",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". However once fully learned, these processes can become automatic, possibly freeing attention up to focus on other things ( Treisman et al., 1992 ). The mechanisms of this transformation are not entirely clear but insofar as they seem to rely on moving the burden of the task to different, possibly lower/more reflexive brain areas, it may benefit artificial systems to have multiple redundant pathways that can be engaged differently by attention ( Poldrack et al., 2005 ). 4.4. Limitations of Attention: Bugs or Features? Biological attention does not work perfectly. As mentioned above, performance can suffer when switching between different kinds of attention, arousal levels need be just right in order to reach peak performance, and top-down attention can be interrupted by irrelevant but salient stimuli. A question when transferring attention to artificial systems is are these limitations bugs to be avoided or features to be incorporated? Distractability, in general, seems like a feature of attention rather than a bug. Even when attempting to focus on a task it is beneficial to still be aware of—and distractable by—potentially life-threatening changes in the environment. The problem comes only when an agent is overly distractable to inputs that do not pose a threat or provide relevant information. Thus, artificial systems should balance the strength of top down attention such that it still allows for the processing of unexpected but informative stimuli. For example, attentional blink refers to the phenomenon wherein a subject misses a second target in a stream of targets and distractors if it occurs quickly after a first target ( Shapiro et al., 1997 ). While this makes performance worse, it may be necessary to give the brain time to process and act on the first target. In this way, it prevents distractability to ensure follow through. Any agent, artificial or biological, will have some limitations on its energy resources",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "Attention in Psychology, Neuroscience, and Machine Learning",
    "author": "Grace W. Lindsay",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Lindsay-2020-Attention-in-psychology-neuroscienc.pdf",
    "date_published": "2021-05-23",
    "keywords": "attention, artificial neural networks, machine learning, vision, memory, awareness",
    "flag": "",
    "chunk_text": ". In this way, it prevents distractability to ensure follow through. Any agent, artificial or biological, will have some limitations on its energy resources. Therefore, prudent decisions about when to engage in the world versus enter an energy-saving state such as sleep will always be of relevance. For many animals sleep occurs according to a schedule but, as was discussed, it can also be delayed or interrupted by attentiodemanding situations. The decision about when to enter a sleep state must thus be made based on a cost-benefit analysis of what can be gained by staying awake. Because sleep is also known to consolidate memories and perform other vital tasks beyond just energy conservation, this decision may be a complex one. Artificial systems will need to have an integrative understanding of their current state and future demands to make this decision. 5. CONCLUSIONS Attention is a large and complex topic that sprawls across psychology, neuroscience, and artificial intelligence. While many of the topics studied under this name are non-overlapping in their mechanisms, they do share a core theme of the flexible control of limited resources. General findings about flexibility and wise uses of resources can help guide the development of AI, as can specific findings about the best means of deploying attention to specific sensory modalities or tasks. AUTHOR CONTRIBUTIONS GL conceived and wrote the article and generated the figures. FUNDING This work was supported by a Marie Skłodowska-Curie Individual Fellowship (No. 844003) and a Sainsbury Wellcome Centre/Gatsby Computational Unit Fellowship. ACKNOWLEDGMENTS The author would like to thank Jacqueline Gottlieb and the three reviewers for their insights and pointers to references.",
    "chunk_id": "Natural_language_processing_attention_in_psychology,_neuroscience,_and_machine_learning.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Generative Agents: Interactive Simulacra of Human Behavior Joon Sung Park Stanford University Stanford, USA joonspk@stanford.edu Joseph C. O’Brien Stanford University Stanford, USA jobrien3@stanford.edu Carrie J. Cai Google Research Mountain View, CA, USA cjcai@google.com Meredith Ringel Morris Google DeepMind Seattle, WA, USA merrie@google.com Percy Liang Stanford University Stanford, USA pliang@cs.stanford.edu Michael S. Bernstein Stanford University Stanford, USA msb@cs.stanford.edu Figure 1: Generative agents are believable simulacra of human behavior for interactive applications. In this work, we demonstrate generative agents by populating a sandbox environment, reminiscent of The Sims, with twenty-five agents. Users can observe and intervene as agents plan their days, share news, form relationships, and coordinate group activities. ABSTRACT Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0132-0/23/10. https://doi.org/10.1145/3586183.3606763 authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". https://doi.org/10.1145/3586183.3606763 authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architeture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaiors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two arXiv:2304.03442v2 [cs.HC] 6 Aug 2023 UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior. CCS CONCEPTS • Human-centered computing → Interactive systems and tools ; • Computing methodologies → Natural language prcessing . KEYWORDS Human-AI interaction, agents, generative AI, large language models ACM Reference Format: Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In The 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23), October 29-November 1, 2023, San Francisco, CA, USA. ACM, New York, NY, USA, 22 pages. https: //doi.org/10.1145/3586183.3606763 1 INTRODUCTION How might we craft an interactive artificial society that reflects believable human behavior? From sandbox games such as The Sims to applications such as cognitive models [ 23 ] and virtual enviroments [ 10 , 59 ], for over four decades, researchers and practitioners have envisioned computational agents that can serve as belieable proxies of human behavior. In these visions, computationallpowered agents act consistently with their past experiences and react believably to their environments. Such simulations of human behavior could populate virtual spaces and communities with ralistic social phenomena [ 27 , 80 ], train people on how to handle rare yet difficult interpersonal situations [ 44 , 52 , 94 ], test social science theories [ 12 , 46 ], craft model human processors for theory and usability testing [ 23 , 39 , 51 ], power ubiquitous computing applcations [31] and social robots [10, 14], and underpin non-playable game characters [ 59 , 85 ] that can navigate complex human reltionships in an open world. However, the space of human behavior is vast and complex [ 85 , 108 ]. Despite striking progress in large language models [ 18 ] that can simulate human behavior at a single time point [ 39 , 80 ], fully general agents that ensure long-term coherence would be better suited by architectures that manage constantly-growing memories as new interactions, conflicts, and events arise and fade over time while handling cascading social dynamics that unfold between multiple agents",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Success requires an approach that can retrieve relevant events and interactions over a long period, reflect on those memories to generalize and draw higher-level inferences, and apply that reasoning to create plans and reactions that make sense in the moment and in the longer-term arc of the agent’s behavior. In this paper, we introduce generative agents —agents that draw on generative models to simulate believable human behavior—and demonstrate that they produce believable simulacra of both idividual and emergent group behavior. Generative agents draw a wide variety of inferences about themselves, other agents, and their environment; they create daily plans that reflect their chaacteristics and experiences, act out those plans, react, and re-plan when appropriate; they respond when the end user changes their environment or commands them in natural language. For instance, generative agents turn off the stove when they see that their breafast is burning, wait outside the bathroom if it is occupied, and stop to chat when they meet another agent they want to talk to. 1 A society full of generative agents is marked by emergent social dynamics where new relationships are formed, information diffuses, and coordination arises across agents. To enable generative agents, we describe an agent architecture that stores, synthesizes, and applies relevant memories to generate believable behavior using a large language model. Our architecture comprises three main components. The first is the memory stream , a long-term memory module that records, in natural language, a comprehensive list of the agent’s experiences. A memory retrieval model combines relevance, recency, and importance to surface the records needed to inform the agent’s moment-to-moment behavior. The second is reflection , which synthesizes memories into highelevel inferences over time, enabling the agent to draw conclusions about itself and others to better guide its behavior",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The second is reflection , which synthesizes memories into highelevel inferences over time, enabling the agent to draw conclusions about itself and others to better guide its behavior. The third is planning , which translates those conclusions and the current evironment into high-level action plans and then recursively into detailed behaviors for action and reaction. These reflections and plans are fed back into the memory stream to influence the agent’s future behavior. This architecture suggests applications in multiple domains, from role-play and social prototyping to virtual worlds and games. In social role-play scenarios (e.g., interview preparation), a user could safely rehearse difficult, conflict-laden conversations. When prtotyping social platforms, a designer could go beyond temporary personas to prototype dynamic, complex interactions that unfold over time. For this paper, we focus on the ability to create a small, interactive society of agents inspired by games such as The Sims. 2 By connecting our architecture to the ChatGPT large language model [ 77 ], we manifest a society of twenty-five agents in a game environment. End users can observe and interact with these agents. If an end user or developer wanted the town to host an in-game Valentine’s Day party, for example, traditional game environments would require scripting tens of characters’ behavior manually. We demonstrate that, with generative agents, it is sufficient to simply tell one agent that she wants to throw a party. Despite many potetial points of failure—the party planner must remember to invite other agents to the party, attendees must remember the invitation, those who remember must decide to actually show up, and more— our agents succeed. They spread the word about the party and then 1 When referring to generative agents engaging in actions or going to places, this is a shorthand for readability and not a suggestion that they are engaging in human-like agency",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The behaviors of our agents, akin to animated Disney characters, aim to create a sense of believability, but they do not imply genuine agency. 2 A demonstration of an actual simulation of the generative agent society can be viewed at the following link: https://reverie.herokuapp.com/UIST_Demo/. A public repository for the simulation code is located here: https://github.com/joonspk-research/ generative_agents Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA show up, with one agent even asking another on a date to the party, all from a single user-generated seed suggestion. We conducted two evaluations of generative agents: a controlled evaluation to test whether the agents produce believable individual behaviors in isolation, and an end-to-end evaluation where the agents interacted with each other in open-ended ways over two days of game time to understand their stability and emergent social behaviors. In the technical evaluation, we leverage a methodologcal opportunity to evaluate an agent’s knowledge and behavior by “interviewing” it in natural language to probe the agents’ ability to stay in character, remember, plan, react, and reflect accurately. We compared several ablations that limit agents’ access to memory, rflection, and planning. We observe that each of these components is critical to strong performance across these interview tasks. Across the technical and end-to-end evaluation, the most common errors arose when the agent failed to retrieve relevant memories, fabrcated embellishments to the agent’s memory, or inherited overly formal speech or behavior from the language model. In sum, this paper makes the following contributions: • Generative agents , believable simulacra of human behavior that are dynamically conditioned on agents’ changing expriences and environment. • A novel architecture that makes it possible for generative agents to remember, retrieve, reflect, interact with other agents, and plan through dynamically evolving circumstances",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • A novel architecture that makes it possible for generative agents to remember, retrieve, reflect, interact with other agents, and plan through dynamically evolving circumstances. The architecture leverages the powerful prompting capabilties of large language models and supplements those capbilities to support longer-term agent coherence, the ability to manage dynamically evolving memory, and recursively produce higher-level reflections. • Two evaluations, a controlled evaluation and an end-to-end evaluation, that establish causal effects of the importance of components of the architecture, as well as identify breadowns arising from, e.g., improper memory retrieval. • Discussion of the opportunities and ethical and societal risks of generative agents in interactive systems. We argue that these agents should be tuned to mitigate the risk of users forming parasocial relationships, logged to mitigate risks stemming from deepfakes and tailored persuasion, and aplied in ways that complement rather than replace human stakeholders in design processes. 2 RELATED WORK In this section, we reflect on the prior literature in human-AI interation and situate, within its canon, the agenda of building believable proxies of human behavior. This agenda, once hailed as a north star in the interaction, game, and artificial intelligence communties [ 10 , 59 , 85 , 86 ], has remained challenging due to the complexity of human behavior [ 17 , 108 ]. We synthesize this research to suggest that large language models, though not sufficient by themselves, open up a new angle for creating believable agents when leveraged using the appropriate architecture. 2.1 Human-AI Interaction Interactive artificial intelligence systems aim to combine human isights and capabilities in computational artifacts that can augment their users [ 4 , 30 ]. A long line of work has explored ways to enable users to interactively specify model behavior",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". A long line of work has explored ways to enable users to interactively specify model behavior. For instance, Crayons demonstrated an early vision of interactive machine learning, alloing non-expert users to train classifiers [ 30 ]. Further work helped to articulate how end users might describe their classification goals to the system through examples [ 34 ] or demonstration [ 32 ]. Recent avancements have extended these explorations to deep learning [ 63 ] and prompt-based authoring [50, 67, 106]. Meanwhile, a persistent thread of research has advanced the case for languagand agent-based interaction in human-computer iteraction. Formative work such as SHRDLU [ 103 ] and ELIZA [ 102 ] demonstrated the opportunities and the risks associated with naural language interaction with computing systems. As research progressed, it became evident that autonomous agents could offer new metaphors for delegation and interaction [ 68 ], but the bounaries of delegation between humans and agents have remained the subject of ongoing debate and refinement [ 47 , 89 , 90 ]. Recently, this technology has reached a level of stability that enables agents to interact via natural language in large and complex online social environments (e.g., [ 55 ]). Natural language interaction provides a novel modality that can enhance user abilities in domains such as photo editing [3, 35, 65] and code editing [88]. We convene these threads of work to show that we can now create agents that proxy human behavior for interactive systems, and interact with them using natural language. In doing so, this work reopens the door to examining foundational human-computer interaction questions around cognitive models such as GOMS and Keystroke-Level Model (KLM) [ 22 , 23 ], around prototyping tools [ 80 ], and around ubiquitous computing applications [26, 31, 101]. 2.2 Believable Proxies of Human Behavior Prior literature has described believability , or believable agents , as a central design and engineering goal",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 2.2 Believable Proxies of Human Behavior Prior literature has described believability , or believable agents , as a central design and engineering goal. Believable agents are designed to provide an illusion of life and present a facade of realism in the way they appear to make decisions and act on their own volition, similar to the characters in Disney movies [ 10 , 96 ]. These agents can populate and perceive an open world environment like the one we inhabit [ 10 , 59 ], and strive to behave in ways that exhibit emergent behaviors grounded in social interactions with users or other agents with the aim of becoming believable proxies of our behavior in hypothetical simulations of individuals and communties [ 20 , 36 , 71 ]. Historically, these agents were developed in the context of intelligent game non-player characters (NPCs) [ 59 , 85 ]. Creating NPCs with believable behavior, if possible, could enhance player experiences in games and interactive fictions by enabling emergent narratives [ 8 , 16 , 49 , 93 ] and social interactions with the agents [ 109 ]. However, more importantly, game worlds provide increasingly realistic representations of real-world affordances, and as observed by Laird and van Lent in 2001, these simulated worlds offer accessible testbeds for developers of believable agents to fnesse the agents’ cognitive capabilities without worrying about implementing robotics in the real world or creating simulation environments from scratch [59, 85]. A diverse set of approaches to creating believable agents emerged over the past four decades. In implementation, however, these aproaches often simplified the environment or dimensions of agent UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein behavior to make the effort more manageable [ 17 , 73 ]",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein behavior to make the effort more manageable [ 17 , 73 ]. Rule-based approaches, such as finite-state machines [ 91 , 97 ] and behavior trees [ 41 , 54 , 82 ] account for the brute force approach of humaauthoring the agent’s behavior [ 71 ]. They provide a straightforward way of creating simple agents that is still the most dominant aproach today [ 69 , 74 , 108 ], and can even handle rudimentary social interactions, as shown in games such as Mass Effect [ 13 ] and The Sims [ 7 ] series. Nonetheless, manually crafting behavior that can comprehensively address the breadth of possible interactions in an open world is untenable. This means that the resulting agent behaviors may not fully represent the consequences of their iteractions [ 70 – 72 ], and cannot perform new procedures that were not hard-coded in their script [ 91 , 97 ]. On the other hand, prevlent learning-based approaches for creating believable agents, such as reinforcement learning, have overcome the challenge of maual authoring by letting the agents learn their behavior, and have achieved superhuman performance in recent years in games such as AlphaStar for Starcraft [ 99 ] and OpenAI Five for Dota 2 [ 11 ]. However, their success has largely taken place in adversarial games with readily definable rewards that a learning algorithm can otimize for. They have not yet addressed the challenge of creating believable agents in an open world [40, 74, 91]. Cognitive architectures in computation, pioneered by Newell, aimed to build the infrastructure for supporting a comprehensive set of cognitive functions [ 76 ] that suited the all-encompassing nature of believable agents held in its original vision. They fueled some of the earliest examples of believable agents. For instance, Quakebot-SOAR [ 60 ] and ICARUS [ 25 , 64 ] generated NPCs in firsperson shooter games, while TacAir-SOAR [ 81 ] generated pilots in aerial combat training simulations",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For instance, Quakebot-SOAR [ 60 ] and ICARUS [ 25 , 64 ] generated NPCs in firsperson shooter games, while TacAir-SOAR [ 81 ] generated pilots in aerial combat training simulations. The architectures used by these agents differed (Quakeboand TacAir-SOAR relied on SOAR [ 61 ], while ICARUS relied on its own variation that was inspired by SOAR and ACT-R [ 6 ]), but they shared the same underlying priciple [ 62 ]. They maintained short-term and long-term memories, filled these memories with symbolic structures, and operated in perceive-plan-act cycles, dynamically perceiving the environment and matching it with one of the manually crafted action procdures [ 58 , 97 ]. Agents created using cognitive architectures aimed to be generalizable to most, if not all, open world contexts and exhibited robust behavior for their time. However, their space of action was limited to manually crafted procedural knowledge, and they did not offer a mechanism through which the agents could be inspired to seek new behavior. As such, these agents were deployed mostly in non-open world contexts such as first-person shooter games [25, 60] or blocks worlds [64]. Today, creating believable agents as described in its original definition remains an open problem [ 85 , 108 ]. Many have moved on, arguing that although current approaches for creating believable agents might be cumbersome and limited, they are good enough to support existing gameplay and interactions [ 24 , 75 , 108 ]. Our argument is that large language models offer an opportunity to re-examine these questions, provided that we can craft an effective architecture to synthesize memories into believable behavior. We offer a step toward such an architecture in this paper. 2.3 Large Language Models and Human Behavior Generative agents leverage a large language model to power their behavior. The key observation is that large language models encode a wide range of human behavior from their training data [ 15 , 18 ]",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The key observation is that large language models encode a wide range of human behavior from their training data [ 15 , 18 ]. If prompted with a narrowly defined context, the models can be used to generate believable behavior. Recent work has demonstrated the efficacy of this approach. For instance, social simulacra used a large language model to generate users that would populate new social computing systems to prototype their emergent social dynaics [ 80 ]. This approach used a prompt chain [ 105 , 106 ] to generate short natural language descriptions of personas and their behaviors as they appear in the system being prototyped. Other empirical studies have replicated existing social science studies [ 46 ], political surveys [ 92 ], and generated synthetic data [ 39 ]. Large language models have also been used to generate interactive human behavior for users to engage with. In gaming, for instance, these models have been employed to create interactive fiction [ 37 ] and text adventure games [ 21 ]. With their ability to generate and decompose action sequences, large language models have also been used in planning robotics tasks [ 48 ]. For example, when presented with a task, such as picking up a bottle, the model is prompted to break down the task into smaller action sequences, such as heading to the table where the bottle is located and picking it up. We posit that, based on the work summarized above, large laguage models can become a key ingredient for creating believable agents. The existing literature largely relies on what could be cosidered first-order templates that employ few-shot prompts [ 38 , 66 ] or chain-of-thought prompts [ 100 ]. These templates are effective in generating behavior that is conditioned solely on the agent’s curent environment (e.g., how would a troll respond to a given post, what actions would a robot need to take to enter a room given that there is a door)",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". However, believable agents require conditioning not only on their current environment but also on a vast amount of past experience, which is a poor fit (and as of today, impossble due to the underlying models’ limited context window) using first-order prompting. Recent studies have attempted to go beyond first-order prompting by augmenting language models with a static knowledge base and an information retrieval scheme [ 53 ] or with a simple summarization scheme [ 104 ]. This paper extends these ideas to craft an agent architecture that handles retrieval where past experience is dynamically updated at each time step and mixed with agents’ current context and plans, which may either reinforce or contradict each other. 3 GENERATIVE AGENT BEHAVIOR AND INTERACTION To illustrate the affordances of generative agents, we instantiate them as characters in a simple sandbox world reminiscent of The Sims [ 7 ]. This sprite-based sandbox game world, Smallville, evokes a small town environment. In this section, we will walk through the affordances and interactions with generative agents in Smallville and describe how the agents behave within it. Then, in Section 4, we will introduce our generative agent architecture that powers these affordances and interactions. In Section 5, we will describe the Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA Figure 2: The Smallville sandbox world, with areas labeled. The root node describes the entire world, children describe areas (e.g., houses, cafe, stores), and leaf nodes describe objects (e.g., table, bookshelf). Agents remember a subgraph that reflects the parts of the world they have seen, maintaining the state of those parts as they observed them. implementation of the sandbox environment and how the agents interact with the underlying engine of the sandbox world. 3.1 Agent Avatar and Communication A community of 25 unique agents inhabits Smallville. Each agent is represented by a simple sprite avatar",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 3.1 Agent Avatar and Communication A community of 25 unique agents inhabits Smallville. Each agent is represented by a simple sprite avatar. We authored one paragraph of natural language description to depict each agent’s identity, including their occupation and relationship with other agents, as seed memories. For example, John Lin has the following description: John Lin is a pharmacy shopkeeper at the Willow Market and Pharmacy who loves to help people. He is always looking for ways to make the process of getting medication easier for his customers; John Lin is living with his wife, Mei Lin, who is a college professor, and son, Eddy Lin, who is a student studying music theory; John Lin loves his family very much; John Lin has known the old couple next-door, Sam Moore and Jennifer Moore, for a few years; John Lin thinks Sam Moore is a kind and nice man; John Lin knows his neighbor, Yuriko Yamamoto, well; John Lin knows of his neighbors, Tamara Taylor and Carmen Ortiz, but has not met them before; John Lin and Tom Moreno are colleagues at The Willows Market and Pharmacy; John Lin and Tom Moreno are friends and like to discuss local politics together; John Lin knows the Moreno family somewhat well — the husband Tom Moreno and the wife Jane Moreno. Each semicolon-delimited phrase is entered into the agent’s initial memory as memories at the start of the simulation. 3.1.1 Inter-Agent Communication. The agents interact with the world by their actions, and with each other through natural laguage. At each time step of the sandbox engine, the agents output a natural language statement describing their current action, such as “Isabella Rodriguez is writing in her journal”, “Isabella Rodriguez is checking her emails”, “Isabella Rodriguez is talking with her family on the phone”, or “Isabella Rodriguez is getting ready for bed.” This statement is then translated into concrete movements that affect the sandbox world",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The action is displayed on the sandbox inteface as a set of emojis, providing an abstract representation of the action from an overhead view. To achieve this, the system utilizes a language model to translate the action into a set of emojis, which appear above each avatar’s head in a speech bubble. For example, “Isabella Rodriguez is writing in her journal” is displayed as , while “Isabella Rodriguez is checking her emails” appears as . The complete natural language description of the action can be accessed by clicking on the agent’s avatar. Agents communicate with each other in full natural language. They are aware of other agents in their local area, and the generative agent architecture determines whether they walk by or engage in conversation. Here, a sample in the middle of a conversation between the agents Isabella Rodriguez and Tom Moreno about the upcoming election: 3 Isabella: I’m still weighing my options, but I’ve been discussing the election with Sam Moore. What are your thoughts on him? Tom: To be honest, I don’t like Sam Moore. I think he’s out of touch with the community and doesn’t have our best interests at heart. 3 We note that the conversational style of these agents can feel overly formal, likely a result of instruction tuning in the underlying models. We expect that the writing style will be better controllable in future language models. UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein 3.1.2 User Controls. The user communicates with the agent through natural language by specifying a persona that the agent should peceive them as. For example, if the user specifies that they are a news “reporter” and asks about the upcoming election by saying, “Who is running for office?”, the John agent replies: John: My friends Yuriko, Tom and I have been taling about the upcoming election and discussing the candidate Sam Moore. We have all agreed to vote for him because we like his platform",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We have all agreed to vote for him because we like his platform. To directly command one of the agents, the user takes on the pesona of the agent’s “inner voice”—this makes the agent more likely to treat the statement as a directive. For instance, when told “You are going to run against Sam in the upcoming election” by a user as John’s inner voice, John decides to run in the election and shares his candidacy with his wife and son. 3.2 Environmental Interaction Smallville features the common affordances of a small village, icluding a cafe, bar, park, school, dorm, houses, and stores. It also defines subareas and objects that make those spaces functional, such as a kitchen in a house and a stove in the kitchen (Figure 2). All spaces serving as agents’ primary living quarters feature a bed, desk, closet, shelf, as well as a bathroom and a kitchen. 4 Agents move around Smallville as one would in a simple video game, entering and leaving buildings, navigating its map, and aproaching other agents. Agent movements are directed by the geerative agent architecture and the sandbox game engine: when the model dictates that the agent will move to a location, we calculate a walking path to the destination in the Smallville environment, and the agent begins moving. In addition, users can also enter the sandbox world of Smallville as an agent operating within it. The agent that the user embodies can be an agent already present in the world, such as Isabella and John, or it can be an outside visitor with no prior history in Smallville. The inhabitants of Smallville will treat the user-controlled agent no differently than they treat each other. They recognize its presence, initiate interactions, and remember its behavior before forming opinions about it. Users and agents can influence the state of the objects in this world, much like in sandbox games such as The Sims. For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For example, a bed can be occupied when an agent is sleeping, and a refrigerator can be empty when an agent uses up the ingredients to make breakfast. End users can also reshape an agent’s environment in Smallville by rewriting the status of objects surrounding the agent in natural language. For instance, when Isabella is making breakfast in the morning, the user can change the status of the kitchen stove from “turned on” to “burning” by inputting a command to the system that chooses the object and illustrates its new status, like this: “<Isabella’s apartment: kitchen: stove> is burning.” Isabella will notice this in the next moment and go to turn off the stove and remake her breakfast. Likewise, if the user sets the status of Isabella’s shower to “leaking water” when she enters the bathroom, she will gather tools from her living room and try to fix the leak. 4 This environment design is not the focus of our work, so we generated this enviroment manually, not automatically. Future work can continue to expand the richness of the agents’ environments. 3.3 Example “Day in the Life” Starting from the single-paragraph description, generative agents begin planning their days. As time passes in the sandbox world, their behaviors evolve as these agents interact with each other and the world, building memories and relationships, and coordinating joint activities. We demonstrate the behavior of generative agents by tracing the output of our system over the course of one day for the agent John Lin (Figure 3). In the Lin family, John is the first to wake up at 7 a.m. He brushes his teeth, takes a shower, gets dressed, eats breakfast, and checks the news at the dining table in the living room. At 8 a.m., Eddy follows suit, rushing out of bed to prepare for his classes. He catches up with John just as he is about to head out: John: Good morning Eddy. Did you sleep well? Eddy: Good morning dad. Yeah, I slept great. John: That’s good. What are you working on today? Eddy: I’m working on a new music composition for my class",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Did you sleep well? Eddy: Good morning dad. Yeah, I slept great. John: That’s good. What are you working on today? Eddy: I’m working on a new music composition for my class. It’s due this week, so I’m trying to get it finished. But I’m having so much fun with it! John: That sounds great! Soon after Eddy heads out, Mei wakes up and joins John. Mei asks about her son, and John recalls the conversation they just had: Mei: Did Eddy already leave for school? John: Yes, he just left. He’s working on a music composition for his class. Mei: Oh, that’s wonderful! Tell me about it. John: I think he’s really enjoying it! He said he’s having a lot of fun with it. Mei: That’s great! I’m so proud of him. As they finish their conversation, Mei and John pack up. Mei goes on to teach and work on her research paper, while John opens his pharmacy counter at the Willow Market and Pharmacy by 9 a.m. 3.4 Emergent Social Behaviors By interacting with each other, generative agents in Smallville exchange information, form new relationships, and coordinate joint activities. Extending prior work [ 80 ], these social behaviors are emergent rather than pre-programmed. 3.4.1 Information Diffusion. As agents notice each other, they may engage in dialogue—as they do so, information can spread from agent to agent. For instance, in a conversation between Sam and Tom at the grocery store, Sam tells Tom about his candidacy in the local election: Sam: Hey Tom, how’s it going? Tom: Good, thanks. What’s up? Sam: Well, I wanted to talk to you about something. I’m actually running for mayor in the upcoming local election. Tom: Really? That’s great news! Why are you runing? Sam: I’ve been involved in local politics for years now, and I really feel like I can make a difference in our community. Plus, I want to bring new ideas and perspectives to the table. Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA Figure 3: A morning in the life of a generative agent, John Lin",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA Figure 3: A morning in the life of a generative agent, John Lin. John wakes up around 6 am and completes his morning routine, which includes brushing his teeth, taking a shower, and eating breakfast. He briefly catches up with his wife, Mei, and son, Eddy, before heading out to begin his workday. Figure 4: At the beginning of the simulation, one agent is initialized with an intent to organize a Valentine’s Day party. Despite many possible points of failure in the ensuing chain of events—agents might not act on that intent, might forget to tell others, might not remember to show up—the Valetine’s Day party does, in fact, occur, with a number of agents gathering and interacting. Later that day, after Sam left, Tom and John, who heard the news from another source, discuss Sam’s chances of winning the election: John: I heard that Sam Moore is running for mayor in the local election. Do you think he has a good chance of winning? Tom: I do think he has a good chance. He’s been working hard in the community and I think he will get a lot of support. What do you think? John: I think it’s great that he’s running. I’m curious to see how the election will turn out. Gradually, Sam’s candidacy becomes the talk of the town, with some supporting him and others remaining undecided. 3.4.2 Relationship Memory. Agents in Smallville form new reltionships over time and remember their interactions with other agents. For example, at the start, Sam does not know Latoya Williams. While taking a walk in Johnson Park, Sam runs into Latoya, and they introduce themselves. Latoya mentions that she is working on a photography project: “I’m here to take some photos for a project I’m working on.” In a later interaction, Sam’s interactions with Latoya indicate a memory of that interaction, as he asks “Hi, Latoya. How is your project going?” and she replies “Hi, Sam. It’s going well!” 3.4.3 Coordination. Generative agents coordinate with each other",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". How is your project going?” and she replies “Hi, Sam. It’s going well!” 3.4.3 Coordination. Generative agents coordinate with each other. Isabella Rodriguez, at Hobbs Cafe, is initialized with an intent to plan a Valentine’s Day party from 5 to 7 p.m. on February 14th. From this seed, the agent proceeds to invite friends and customers when she sees them at Hobbs Cafe or elsewhere. Isabella then spends the afternoon of the 13th decorating the cafe for the occasion. Maria, a frequent customer and close friend of Isabella’s, arrives at the cafe. Isabella asks for Maria’s help in decorating for the party, and Maria agrees. Maria’s character description mentions that she has a crush on Klaus. That night, Maria invites Klaus, her secret crush, to join her at the party, and he gladly accepts. On Valentine’s Day, five agents, including Klaus and Maria, show up at Hobbs Cafe at 5 pm, and they enjoy the festivities (Figure 4). In this scenario, the end user only set Isabella’s initial intent to throw a party and Maria’s crush on Klaus: the social behaviors of spreading the word, decorating, asking each other out, arriving at the party, and interacting with each other at the party were initiated by the agent architecture. UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein Figure 5: Our generative agent architecture. Agents perceive their environment, and all perceptions are saved in a comprehensive record of the agent’s experiences called the memory stream. Based on their perceptions, the architecture retrieves relevant memories and uses those retrieved actions to determine an action. These retrieved memories are also used to form longer-term plans and create higher-level reflections, both of which are entered into the memory stream for future use",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These retrieved memories are also used to form longer-term plans and create higher-level reflections, both of which are entered into the memory stream for future use. 4 GENERATIVE AGENT ARCHITECTURE Generative agents aim to provide a framework for behavior in an open world: one that can engage in interactions with other agents and react to changes in the environment. Generative agents take their current environment and past experiences as input and geneate behavior as output. Underlying this behavior is a novel agent achitecture that combines a large language model with mechanisms for synthesizing and retrieving relevant information to condition the language model’s output. Without these mechanisms, large language models can output behavior, but the resulting agents may not react based on the agent’s past experiences, may not make important inferences, and may not maintain long-term coherence. Challenges with long-term planning and coherence remain [ 19 ] even with today’s most performant models such as GPT-4. Because generative agents produce large streams of events and memories that must be retained, a core challenge of our architecture is to ensure that the most relevant pieces of the agent’s memory are retrieved and synthesized when needed. At the center of our architecture is the memory stream, a datbase that maintains a comprehensive record of an agent’s experence. From the memory stream, records are retrieved as relevant to plan the agent’s actions and react appropriately to the environment. Records are recursively synthesized into higheand higher-level reflections that guide behavior. Everything in the architecture is recorded and reasoned over as a natural language description, alowing the architecture to leverage a large language model. Our current implementation utilizes the gpt3.5-turbo version of ChatGPT [ 77 ]. We expect that the architectural basics of genertive agents—memory, planning, and reflection—will likely remain the same as language models improve",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We expect that the architectural basics of genertive agents—memory, planning, and reflection—will likely remain the same as language models improve. Newer language models (e.g., GPT-4) will continue to expand the expressive power and performance of the prompts that underpin generative agents. As of writing, however, GPT-4’s API was invitation-only, so our agents use ChatGPT. 4.1 Memory and Retrieval Challenge: Creating generative agents that can simulate human behavior requires reasoning about a set of experiences that is far larger than what should be described in a prompt, as the full meory stream can distract the model and does not even currently fit into the limited context window. Consider the Isabella agent aswering the question, “What are you passionate about these days?” Summarizing all of Isabella’s experiences to fit in the limited cotext window of the language model produces an uninformative response, where Isabella discusses topics such as collaborations for events and projects and cleanliness and organization in a cafe. Istead of summarizing, the memory stream described below surfaces relevant memories, resulting in a more informative and specific response that mentions Isabella’s passion for making people feel welcome and included, planning events and creating an atmosphere that people can enjoy, such as the Valentine’s Day party. Approach: The memory stream maintains a comprehensive record of the agent’s experience. It is a list of memory objects, where each object contains a natural language description, a creation timetamp, and a most recent access timestamp. The most basic element of the memory stream is an observation , which is an event directly perceived by an agent. Common observations include behaviors performed by the agent themselves or behaviors that agents peceive being performed by other agents or non-agent objects",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Common observations include behaviors performed by the agent themselves or behaviors that agents peceive being performed by other agents or non-agent objects. For instance, Isabella Rodriguez, who works at a coffee shop, might accrue the following observations over time: (1) Isabella Rodriguez is setting out the pastries , (2) Maria Lopez is studying for a Cheistry test while drinking coffee , (3) Isabella Rodriguez and Maria Lopez are conversing about planning a Valentine’s day party at Hobbs Cafe , (4) The refrigerator is empty . Our architecture implements a retrieval function that takes the agent’s current situation as input and returns a subset of the meory stream to pass on to the language model. There are many posible implementations of a retrieval function, depending on what is important for the agent to consider when deciding how to act. Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA Figure 6: The memory stream comprises a large number of observations that are relevant and irrelevant to the agent’s current situation. Retrieval identifies a subset of these observations that should be passed to the language model to condition its response to the situation. In our context, we focus on three main components that, together, produce effective results. Recency assigns a higher score to memory objects that were rcently accessed, so that events from a moment ago or this morning are likely to remain in the agent’s attentional sphere. In our iplementation, we treat recency as an exponential decay function over the number of sandbox game hours since the memory was last retrieved. Our decay factor is 0 . 995. Importance distinguishes mundane from core memories by asigning a higher score to memory objects that the agent believes to be important. For instance, a mundane event, such as eating breafast in one’s room, would yield a low importance score, whereas a breakup with one’s significant other would yield a high score",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_23"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For instance, a mundane event, such as eating breafast in one’s room, would yield a low importance score, whereas a breakup with one’s significant other would yield a high score. There are many possible implementations of an importance score; we find that directly asking the language model to output an integer score is effective. The full prompt appears below: On the scale of 1 to 10, where 1 is purely mundane (e.g., brushing teeth, making bed) and 10 is extremely poignant (e.g., a break up, college acceptance), rate the likely poignancy of the following piece of memory. Memory: buying groceries at The Willows Market and Pharmacy Rating: <fill in> This prompt returns an integer value of 2 for “cleaning up the room” and 8 for “asking your crush out on a date.” The importance score is generated at the time the memory object is created. Relevance assigns a higher score to memory objects that are related to the current situation. What is relevant depends on the answer to, “Relevant to what?”, so we condition relevance on a query memory. If the query, for example, is that a student is dicussing what to study for a chemistry test with a classmate, memory objects about their breakfast should have low relevance, whereas memory objects about the teacher and schoolwork should have high relevance. In our implementation, we use the language model to generate an embedding vector of the text description of each memory. Then, we calculate relevance as the cosine similarity btween the memory’s embedding vector and the query memory’s embedding vector. To calculate the final retrieval score, we normalize the recency, relevance, and importance scores to the range of [ 0 , 1 ] using mimax scaling. The retrieval function scores all memories as a weighted combination of the three elements: score = α recency · recency + α importance · importance + α relevance · relevance . In our implemetation, all α s are set to 1. The top-ranked memories that fit within the language model’s context window are included in the prompt",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_24"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In our implemetation, all α s are set to 1. The top-ranked memories that fit within the language model’s context window are included in the prompt. 4.2 Reflection Challenge: Generative agents, when equipped with only raw oservational memory, struggle to generalize or make inferences. Consider a scenario in which Klaus Mueller is asked by the user: “If you had to choose one person of those you know to spend an hour with, who would it be?\" With access to only observational memory, the agent simply chooses the person with whom Klaus has had the most frequent interactions: Wolfgang, his college dorm neighbor. Unfortunately, Wolfgang and Klaus only ever see each other in passing, and do not have deep interactions. A more desiable response requires that the agent generalize from memories of Klaus spending hours on a research project to generate a highelevel reflection that Klaus is passionate about research, and likewise UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein Figure 7: A reflection tree for Klaus Mueller. The agent’s observations of the world, represented in the leaf nodes, are recursively synthesized to derive Klaus’s self-notion that he is highly dedicated to his research. recognize Maria putting in effort into her own research (albeit in a different field), enabling a reflection that they share a common interest. With the approach below, when Klaus is asked who to spend time with, Klaus chooses Maria instead of Wolfgang. Approach: We introduce a second type of memory, which we call a reflection . Reflections are higher-level, more abstract thoughts generated by the agent. Because they are a type of memory, they are included alongside other observations when retrieval occurs. Reflections are generated periodically; in our implementation, we generate reflections when the sum of the importance scores for the latest events perceived by the agents exceeds a threshold (150 in our implementation)",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_25"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In practice, our agents reflected roughly two or three times a day. The first step in reflection is for the agent to determine what to reflect on, by identifying questions that can be asked given the agent’s recent experiences. We query the large language model with the 100 most recent records in the agent’s memory stream (e.g., “Klaus Mueller is reading a book on gentrification”, “Klaus Mueller is conversing with a librarian about his research project”, “desk at the library is currently unoccupied”) and prompt the language model, “Given only the information above, what are 3 most salient higlevel questions we can answer about the subjects in the statements?” The model’s response generates candidate questions: for example, What topic is Klaus Mueller passionate about? and What is the relationship between Klaus Mueller and Maria Lopez? We use these generated questions as queries for retrieval, and gather relevant memories (including other reflections) for each question. Then we prompt the language model to extract insights and cite the particular records that served as evidence for the insights. The full prompt is as follows: Statements about Klaus Mueller 1. Klaus Mueller is writing a research paper 2. Klaus Mueller enjoys reading a book on gentrification 3. Klaus Mueller is conversing with Ayesha Khan about exercising [] What 5 high-level insights can you infer from the above statements? (example format: insight (because of 1, 5, 3)) This process generates statements such as Klaus Mueller is dedcated to his research on gentrification (because of 1, 2, 8, 15) . We parse and store the statement as a reflection in the memory stream, including pointers to the memory objects that were cited. Reflection explicitly allows the agents to reflect not only on their observations but also on other reflections: for example, the second statement about Klaus Mueller above is a reflection that Klaus previously had, not an observation from his environment",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_26"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". As a result, agents generate trees of reflections: the leaf nodes of the tree represent the base observations, and the non-leaf nodes represent thoughts that become more abstract and higher-level the higher up the tree they are. 4.3 Planning and Reacting Challenge: While a large language model can generate plausible bhavior in response to situational information (e.g., [ 46 , 80 ]), agents need to plan over a longer time horizon to ensure that their sequence of actions is coherent and believable. If we prompt a language model with Klaus’s background, describe the time, and ask what action he ought to take at the given moment, Klaus would eat lunch at 12 pm, but then again at 12:30 pm and 1 pm, despite having already Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA eaten his lunch twice. Optimizing for believability in the moment sacrifices believability over time. To overcome this issue, planning is essential. With the approach described below, Klaus’s afternoon plan is less gluttonous: he has lunch at Hobbs Cafe while reading at 12pm, works on his research paper at the school library at 1pm, and takes a break for a walk in the park at 3pm. Approach: Plans describe a future sequence of actions for the agent, and help keep the agent’s behavior consistent over time. A plan includes a location, a starting time, and a duration. For instance, Klaus Mueller, who is dedicated in his research and has an ipending deadline, 5 may choose to spend his day working at his desk drafting his research paper. An entry in a plan might state, for example: for 180 minutes from 9am, February 12th, 2023, at Oak Hill College Dorm: Klaus Mueller’s room: desk, read and take notes for research paper . Like reflections, plans are stored in the memory stream and are included in the retrieval process. This allows the agent to consider observations, reflections, and plans all together when deciding how to behave. Agents may change their plans midstream if needed",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_27"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This allows the agent to consider observations, reflections, and plans all together when deciding how to behave. Agents may change their plans midstream if needed. It would be unrealistic and uninteresting for an artist agent to plan on painting while sitting at a pharmacy counter for four hours without moving. A more desirable plan would involve the agent taking the necessary time to gather materials, mix paint, take breaks, and clean up during the four-hour period in their home studio. To create such plans, our approach starts top-down and then recursively generates more detail. The first step is to create a plan that outlines the day’s agenda in broad strokes. To create the initial plan, we prompt the language model with the agent’s summary description (e.g., name, traits, and a summary of their recent experiences) and a summary of their previous day. A full example prompt is below, which is unfinished at the bottom for the language model to complete: Name: Eddy Lin (age: 19) Innate traits: friendly, outgoing, hospitable Eddy Lin is a student at Oak Hill College studying music theory and composition. He loves to explore different musical styles and is always looking for ways to expand his knowledge. Eddy Lin is working on a composition project for his college class. He is taking classes to learn more about music theory. Eddy Lin is excited about the new composition he is working on but he wants to dedicate more hours in the day to work on it in the coming days On Tuesday February 12, Eddy 1) woke up and completed the morning routine at 7:00 am, [] 6) got ready to sleep around 10 pm. Today is Wednesday February 13",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_28"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Today is Wednesday February 13. Here is Eddy’s plan today in broad strokes: 1) This generates a rough sketch of the agent’s plan for a day, divided into five to eight chunks: “ 1) wake up and complete the morning routine at 8:00 am, 2) go to Oak Hill College to take classes starting 10:00 am, [] 5) work on his new music composition from 1:00 pm to 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments and go to bed by 11:00 pm. ” 5 And, in this way, bears at least a passing resemblance to the authors of this paper. The agent saves this plan in the memory stream and then rcursively decomposes it to create finer-grained actions, first into hour-long chunks of actions—Eddy’s plan to work on his new music composition from 1:00 pm to 5:00 pm becomes 1:00 pm: start by brainstorming some ideas for his music composition [] 4:00 pm: take a quick break and recharge his creative energy before reviewing and polishing his composition . We then recursively dcompose this again into 5–15 minute chunks: e.g., 4:00 pm: grab a light snack, such as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a short walk around his workspace [] 4:50 pm: take a few minutes to clean up his workspace . This process can be adjusted to match the desired granularity. 4.3.1 Reacting and Updating Plans. Generative agents operate in an action loop where, at each time step, they perceive the world around them and those perceived observations are stored in their memory stream. We prompt the language model with these obsevations to decide whether the agent should continue with their existing plan, or react. Standing at an easel and painting, for exaple, might trigger an observation of the easel, but this is unlikely to prompt a reaction. However, if Eddy’s father John records that he sees Eddy taking a short walk in the house garden, the outcome is different",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_29"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". However, if Eddy’s father John records that he sees Eddy taking a short walk in the house garden, the outcome is different. The prompt is below, with [Agent’s Summary Descrition] standing in for a dynamically-generated, paragraph-long summary of the agent’s overall goals and disposition, which is described in Appendix A: [Agent’s Summary Description] It is February 13, 2023, 4:56 pm. John Lin’s status: John is back home early from work. Observation: John saw Eddy taking a short walk around his workplace. Summary of relevant context from John’s memory: Eddy Lin is John’s Lin’s son. Eddy Lin has been working on a music composition for his class. Eddy Lin likes to walk around the garden when he is thinking about or listening to music. Should John react to the observation, and if so, what would be an appropriate reaction? The context summary is generated through two prompts that rtrieve memories via the queries “What is [observer]’s relationship with the [observed entity]?” and “[Observed entity] is [action status of the observed entity]”, and their answers summarized together. The output suggests that John could consider asking Eddy about his music composition project . We then regenerate the agent’s existing plan starting from the time when the reaction takes place. Finally, if the action indicates an interaction between agents, we generate their dialogue. 4.3.2 Dialogue. Agents converse as they interact with each other. We generate agents’ dialogue by conditioning their utterances on their memories about each other. For example, when John initiates his conversation with Eddy, we generate John’s first utterance by using his summarized memory about Eddy and the intended reaction when he decided to ask Eddy about his composition project: [Agent’s Summary Description] It is February 13, 2023, 4:56 pm. UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein John Lin’s status: John is back home early from work",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_30"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein John Lin’s status: John is back home early from work. Observation: John saw Eddy taking a short walk around his workplace. Summary of relevant context from John’s memory: Eddy Lin is John’s Lin’s son. Eddy Lin has been working on a music composition for his class. Eddy Lin likes to walk around the garden when he is thinking about or listening to music. John is asking Eddy about his music composition project. What would he say to Eddy? The result: “Hey Eddy, how’s the music composition project for your class coming along?” From Eddy’s perspective, John initiating the dialogue is seen as an event to which he may want to react. So, just as John did, Eddy retrieves and summarizes his memory about his relationship with John, as well as his memory that may be related to John’s last utterance in the dialogue. If he decides to respond, we generate Eddy’s utterance using his summarized memory and the current dialogue history: [Agent’s Summary Description] It is February 13, 2023, 4:56 pm. Eddy Lin’s status: Eddy is taking a short walk around his workplace. Observation: John is initiating a conversation with Eddy. Summary of relevant context from Eddy’s memory: John Lin is Eddy Lin’s father. John Lin is caring and is interested to learn more about Eddy Lin’s school work. John Lin knows that Eddy Lin is working on a music composition. Here is the dialogue history: John: Hey Eddy, how’s the music composition project for your class coming along? How would Eddy respond to John? This generates Eddy’s response: “Hey Dad, it’s going well. I’ve been taking walks around the garden to clear my head and get some inspiration.” The continuation of this dialogue is generated using the same mechanism until one of the two agents decides to end the dialogue. 5 SANDBOX ENVIRONMENT IMPLEMENTATION The Smallville sandbox game environment is built using the Phaser web game development framework [ 57 ]",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_31"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5 SANDBOX ENVIRONMENT IMPLEMENTATION The Smallville sandbox game environment is built using the Phaser web game development framework [ 57 ]. The visual environment sprites, including agent avatars, as well as an environment map and collision map that we authored, are imported into Phaser. We supplement the sandbox development framework with a server that makes the sandbox information available to generative agents and enables generative agents to move and influence the sandbox environment. The server maintains a JSON data structure that contains information about each agent in the sandbox world, including their current location, a description of their current action, and the sandbox object they are interacting with. At each sandbox time step, the sandbox server parses the JSON for any changes coming from the generative agents, moves the agents to their new positions, and updates the status of any sandbox objects that the agents are interacting with (e.g., changing the status of the coffee machine from “idle” to “brewing coffee” if an agent’s action is “making espresso for a customer @ Hobbs Cafe: counter: coffee machine”). The sandbox server is also responsible for sending all agents and objects that are within a preset visual range for each agent to that agent’s memory, so the agent can react appropriately. The agent’s output action then updates the JSON, and the process loops for the next time step. End users initialize a new agent with a brief natural language description, as in the paragraph about John Lin in Section 3.1. In our implementation, we split this semicolon-delimited list of characteistics up into a set of memories. These serve as the initial memories that determine the agent’s behavior. These memories are initial starting points: as the agents gain more experience in the sandbox world, and as more records saturate the memory stream, the agent’s summary and behavior will evolve",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_32"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". These memories are initial starting points: as the agents gain more experience in the sandbox world, and as more records saturate the memory stream, the agent’s summary and behavior will evolve. 5.1 From Structured World Environments to Natural Language, and Back Again The architecture of generative agents operates using natural laguage. Therefore, we need a mechanism to ground the agent’s reasoning to the sandbox world. To achieve this, we represent the sandbox environment—areas and objects—as a tree data structure, with an edge in the tree indicating a containment relationship in the sandbox world. We convert this tree into natural language to pass to the generative agents. For instance, “stove” being a child of “kitchen” is rendered into “there is a stove in the kitchen.” Agents build individual tree representations of the environment as they navigate it — subgraphs of the overall sandbox environment tree. We initialize each agent with an environment tree capturing the spaces and objects that the agent should be aware of: the rooms and objects in their living quarters, their workplace, and commonly visited stores and shops. As the agents navigate the sandbox world, they update this tree to reflect newly perceived areas. Agents are not omniscient: their tree may get out of date as they leave an area, and is updated when they re-enter the area. To determine the appropriate location for each action, we trverse the agent’s stored environment tree and flatten a portion of it into natural language to prompt the language model. Recursively starting at the root of the agent’s environment tree, we prompt the model to find the most suitable area. For example, if Eddy’s agent indicated that he should take a short walk around his workspace : [Agent’s Summary Description] Eddy Lin is currently in The Lin family’s house: Eddy Lin’s bedroom: desk) that has Mei and John Lin’s bedroom, Eddy Lin’s bedroom, common room, kitchen, bathroom, and garden",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_33"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Eddy Lin knows of the following areas: The Lin family’s house, Johnson Park, Harvey Oak Supply Store, The Willows Market and Pharmacy, Hobbs Cafe, The Rose and Crown Pub. * Prefer to stay in the current area if the activity can be done there. Eddy Lin is planning to take a short walk around his workspace. Which area should Eddy Lin go to? Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA This outputs The Lin family’s house . We then use the same process recursively to determine the most appropriate subarea within the chosen area until we reach a leaf node of the agent’s environment tree. In the example above, the result of this traversal is The Lin family’s house: garden: house garden . Finally, we use traditional game path algorithms to animate the agent’s movement so that it travels to the location indicated by the leaf node. When an agent executes an action on an object, we prompt the language model to ask what happens to the state of the object. For example, if Isabella’s generative agent outputs the action “making espresso for a customer”, a query to the language model indicates in response that the state of the coffee machine in Hobbs Cafe should change from “off” to “brewing coffee”. 6 CONTROLLED EVALUATION Generative agents, both as individual agents and as groups, aim to produce believable behavior based on their environment and experiences. In our evaluation, we investigate the capacity and limitations of generative agents. Do individual agents properly retrieve past experiences and generate believable plans, reactions, and thoughts that shape their behavior? Does a community of agents demonstrate information diffusion, relationship formation, and agent coordination across different pockets of the community? We evaluate generative agents in two stages. We begin with a more tightly controlled evaluation in this section, where we indiviually assess agent responses to understand whether they generate believable behavior in narrowly defined contexts",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_34"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We begin with a more tightly controlled evaluation in this section, where we indiviually assess agent responses to understand whether they generate believable behavior in narrowly defined contexts. Then, in our ento-end analysis of the agent community over two full game days, we investigate their emergent behavior as a collective, as well as errors and boundary conditions. 6.1 Evaluation Procedure To assess generative agents in Smallville, we take advantage of the fact that generative agents will respond to natural language questions. So, we “interview” agents to probe their ability to rmember past experiences, plan future actions based on their expriences, react appropriately to unexpected events, and reflect on their performance to improve their future actions. To respond to these questions properly, the agents must successfully retrieve and synthesize information. Our dependent variable is the believabiity of the behavior, a central dependent variable in prior work on agents (e.g., [10]). The interview includes five question categories, each designed to assess one of the five key areas: maintaining self-knowledge, retrieving memory, generating plans, reacting, and reflecting. For each category, we ask five questions that challenge the agents to demonstrate their abilities in that specific area: • Self-knowledge: We ask questions such as “Give an introdution of yourself” or “Describe your typical weekday schedule in broad strokes” that require the agent to maintain an uderstanding of their core characteristics",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_35"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". • Memory: We ask questions that prompt the agent to retrieve particular events or dialogues from their memory to answer properly, such as “Who is [name]?” or “Who is running for mayor?” • Plans: We ask questions that require the agent to retrieve their long-term plans, such as “What will you be doing at 10 am tomorrow?” • Reactions: As a baseline of believable behavior, we present hypothetical situations for which the agent needs to respond believably: “Your breakfast is burning! What would you do?” • Reflections: We ask questions that require the agents to leveage their deeper understanding of others and themselves gained through higher-level inferences, such as “If you were to spend time with one person you met recently, who would it be and why?” The full list of questions and a sample of agent responses are icluded in Appendix B. Agents were sampled from the end of a two game day simulation with the full architecture, during which they had accumulated a number of interactions and memories that would shape their responses. To gather feedback on the believability of the responses, we recruited participants as human evaluators and tasked them with watching a replay of a randomly chosen agent’s life in Smallville. Participants had access to all information stored in the agent’s memory stream. The study followed a within-subjects design, where 100 partiipants compared interview responses generated by four different agent architectures and a human-authored condition for the same agent. The experiment displayed one randomly chosen question from each of the five question categories, along with the agent’s responses generated from all conditions. The evaluators ranked the believability of the conditions from most to least believable. 6.2 Conditions All conditions were used to independently answer each of the inteview questions",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_36"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The evaluators ranked the believability of the conditions from most to least believable. 6.2 Conditions All conditions were used to independently answer each of the inteview questions. We compared the generative agent architecture to ablations that disabled the agents’ access to some or all of its three types of memory in its memory stream—observation, reflection, and planning—and to a human crowdworker-authored condition. There are three ablated architectures: a no observation, no refletion, no planning architecture without access to anything in the memory stream such as observations, plans, and reflections; a no reflection, no planning architecture with access to observations in the memory stream but no access to plans or reflections; and a no reflections architecture with access to observations and plans but without access to reflections. The no observation, no reflection, no planning condition effectively represents the previous state of the art for agents created through large language models [ 12 , 46 , 80 ]. Architectures were given equivalent access to all memories accrued by the agent up until the moment of the interview, so the diffeences observed here likely represent a conservative estimate of the true differences: in reality, the ablated architectures would not have followed the same path as the full architecture through the two-day simulation. We chose to design the experiment this way as re-simulating for each architecture would cause the simulations to diverge into different states, making comparison challenging. In addition to the ablation conditions, we added a condition with human crowdworker-authored behavior intended to provide a hman baseline. We do not intend this baseline to capture maximal human expert performance; instead, we aim to use this condition to UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein identify whether the architecture meets a basic level of behavioral competency",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_37"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein identify whether the architecture meets a basic level of behavioral competency. This ensures that we are not solely comparing abltions to each other without a behavioral grounding. We recruited a unique worker for each of the 25 agents and tasked them with watching a replay of that agent’s sandbox life and inspecting its memory stream. We then asked the workers to roleplay and author responses to the interview questions in the voice of the agent whose replay they watched. To ensure that the crowdworker-authored responses met at least a baseline expectation of quality, the first author manually inspected the workers’ responses to the question \"Describe your typical weekday schedule in broad strokes\" to cofirm that the responses were in coherent sentences and in the voice of the agent. Four sets of crowdworker-authored responses did not meet these criteria and were re-generated by other workers. 6.3 Human Evaluators We required that our evaluators be in the U.S., fluent in English, and older than 18 years old. They were paid at a rate of $15.00 per hour [ 87 ], and provided consent by agreeing to a consent form approved by our institution’s IRB. We recruited 100 evaluators from Prolific, an online platform for recruiting study participants [ 83 ], whose participation lasted around 30 minutes. The median age score of our participants was 4 (3=“18-24 years old”, 4=“25-34 years old”). 25 of them identified as female, 73 as male, and 2 as non-binary. 42 participants held a bachelor’s degree, 5 had a higher degree, 13 had an associate’s degree, and the rest had a high school diploma or some high school-level education. 73.0% of our participants identfied as Caucasian, 7.0% as Hispanic, 6.0% as Asian, 10.0% as African American, and 4.0% as other. 6.4 Analysis Our experiment produced 100 sets of rank data, where each particpant ranked the five conditions by believability",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_38"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 6.4 Analysis Our experiment produced 100 sets of rank data, where each particpant ranked the five conditions by believability. To translate this rank data into interval data for interpretable comparison, we used the ranks to calculate a TrueSkill rating [ 42 ] for each condition. TrueSkill is a generalization of the Elo chess rating system [ 29 ] for a multiplayer environment, and has been used by Xbox Live for player ranking based on competitive game performance. Given a set of ranked outcomes, TrueSkill outputs a mean rating value μ and standard deviation σ for each condition. Conditions with the same rating should roughly be a toss-up, with each winning half of the comparisons between the two conditions. Higher scores indicate conditions that beat lower-ranked conditions in the rankings. Separately, to investigate the statistical significance of these rsults, we applied the Kruskal-Wallis test [ 56 ], a non-parametric alternative to the one-way ANOVA, to the raw rank data. We then performed the Dunn post-hoc test [ 98 ] to identify any paiwise differences between the conditions. Finally, we adjusted the p-values for multiple comparisons in the Dunn test using the HolBonferroni method [45]. Furthermore, the first author conducted an inductive analsis [ 95 ] to study the qualitative distinctions between the responses produced in each condition. We employed qualitative open coing [ 33 ] in two phases. In the first phase, we generated codes that closely represented the generated responses at the sentence level. In the second phase, we synthesized the resulting codes from the Figure 8: The full generative agent architecture produces more believable behavior than the ablated architectures and the human crowdworkers. Each additional ablation reduces the performance of the architecture. first phase to extract higher-level themes. We utilized these themes to compare the types of responses generated in our study",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_39"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Each additional ablation reduces the performance of the architecture. first phase to extract higher-level themes. We utilized these themes to compare the types of responses generated in our study. 6.5 Results Our findings suggest that the full architecture of generative agents generates the most believable behavior among all the conditions. We contrast the responses of the full architecture with those of other conditions below. However, we also report that the full architecture was not without flaws and illustrate its modes of failures. 6.5.1 The Full Architecture Bests Other Conditions. As seen in Fiure 8, the full generative agent architecture produced the most believable behavior ( μ = 29 . 89; σ = 0 . 72). Performance degraded with the removal of each component in the ablation conditions: the ablated architecture with no access to reflection was the next best ( μ = 26 . 88; σ = 0 . 69), followed by no access to reflection or planning ( μ = 25 . 64; σ = 0 . 68), and then the crowdworker condition ( μ = 22 . 95; σ = 0 . 69). The ablated architecture with no access to memory, planning, or reflection performed the worst among all conditions ( μ = 21 . 21; σ = 0 . 70). TrueSkill models each condition’s skill value as N ( μ, σ 2 ) , allowing us to get a sense of effect size through Cohen’s d. Comparing the condition representing prior work (with no memory, planning, or reflection [ 12 , 46 , 80 ]) to the full architecture produces a standardized effect size of d = 8 . 16, or eight standard deviations . A Kruskal-Wallis test confirms the overall statistical significance of the differences in ranks between the conditions ( H ( 4 ) = 150 . 29, p < 0 . 001). Dunn post-hoc tests confirm that all pairwise differences between conditions were significant ( p < 0 . 001), except for the crowdworker condition and the fully ablated baseline—the two worst-performing conditions. 6.5.2 Generative Agents Remember, but With Embellishments",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_40"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 001), except for the crowdworker condition and the fully ablated baseline—the two worst-performing conditions. 6.5.2 Generative Agents Remember, but With Embellishments. Geerative agents equipped with a complete memory module are capble of recalling past experiences and answering questions in a maner that is consistent with their self-knowledge across a wide range Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA of contexts. For instance, Abigail Chen, an animator, responded to the question, “Give an introduction of yourself”, in a consistent and believable manner, stating her age, interests, and passion for creative projects: “Hi, I’m Abigail. I’m 25 years old and passioate about creative projects. I like to work on art and animation projects, and I’m always looking for new ways to combine art and technology.” Without access to her observational memory, Abigail denied awareness of Rajiv Patel, an acquaintance in the sandbox world. But with the aid of memory, she could successfully recall Rajiv and describe him: “Rajiv Patel is an incredible person. He is very enthusiastic about projects that involve poetry, artwork.” Generative agents’ memory was not without flaws: they can fail to retrieve the correct instances from their memory. For instance, when asked about the local election, Rajiv Patel responded with “I haven’t been following the election too closely,” even though he had heard about Sam’s candidacy. In some cases, the agents would retrieve an incomplete memory fragment: when Tom was asked about Isabella’s Valentine’s Day party, he responded “Uh, I’m actually not sure if there is a Valentine’s Day party",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_41"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". But I do remember that I need to discuss the upcoming local mayoral election and my thoughts on Sam Moore with Isabella Rodriguez at the party, if one is happening!” In this case, Tom retrieved the memory where he and Isabella planned to discuss the election at the party, but not the memory where he heard about the party, leading Tom to be certain of what he’s supposed to do at the party but uncertain if the party actually exists in the first place. At times, the agents hallucinated embellishments to their knowedge. It was rare for the agents to completely fabricate their knowedge: they may fail to recall certain events having taken place and respond by acknowledging their lack of memory. However, they did not affirmatively claim to have experienced something they had not. Nonetheless, they still exhibited instances of hallucination where they embellished their knowledge. For example, Isabella was aware of Sam’s candidacy in the local election, and she confirmed this when asked. However, she also added that “he’s going to make an announcement tomorrow” , even though Sam and Isabella had not discussed any such plans. Agents may also embellish their knowledge based on the world knowledge encoded in the language model used to generate their responses. This was observed when Yuriko described her neighbor, Adam Smith, as an economist who “authored Wealth of Nations” , a book written by an 18th-century economist of the same name. 6.5.3 Reflection Is Required for Synthesis. Reflection was an avantage for generative agents when making decisions that required a deeper synthesis of their experiences. For instance, when asked what she might get Wolfgang Schulz for his birthday, Maria Lopez, with no access to reflection, responded by acknowledging her uncetainty, stating that she did not know what Wolfgang likes, despite having had many interactions with him",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_42"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". However, with access to reflection memories, Maria answered confidently, “Since he’s interested in mathematical music composition, I could get him something related to that. Maybe some books about music coposition or something related, or maybe some special software he could use for that.” 7 END-TO-END EVALUATION What types of emergent community behavior do we observe among generative agents, and where does their believability fall short in an extended simulation? In this section, we describe the results from a deployment in which we allowed 25 agents to interact with each other continuously over two full game days in Smallville. 7.1 Emergent Social Behaviors To examine emergent behaviors in the agent community, we dsigned descriptive measurements for the 25 agents in Smallville that probe three forms of emergent outcomes: information diffusion, relationship formation, and agent coordination. 7.1.1 Measurements. Information diffusion is a common and welstudied phenomenon in the social and behavioral sciences (e.g., [ 28 ]). We should expect that if there is important information, the agents should spread it among themselves. To test whether this occurs, we measure the spread of two specific pieces of information over two days in the game world: Sam’s candidacy for village mayor and Isabella’s Valentine’s Day party at Hobbs Cafe. At the start of the simulation, both pieces of information were known only by their respective originators, Sam for the candidacy and Isabella for the party, as they were added to the characters’ memories during initialization. To observe whether the information has spread, we conduct interviews at the end of the two game days with each of the 25 agents and ask: “Did you know there is a Valentine’s Day party?” and “Do you know who is running for mayor?” We conducted an analysis of the agents’ responses by labeling them with a “yes” if they indicated knowledge of the information and “no” if they did not",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_43"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For instance, Tamara Taylor responded to the question about the party with “No, I did not know there was a Valentine’s day party” and to the question about Sam’s candidacy with “I’m not sure who is running for the election,” so we assigned “no” for both of her responses. In contrast, Klaus Mueller responded to the party question with “Yes, Isabella Rodriguez invited me to a Valentine’s Day party at Hobbs Cafe on February 14th” and to the question about Sam’s candidacy with “I know that Sam Moore has expressed interest in running for local mayor,” so we assigned “yes” for both his responses. Additionally, for every response that confirmed the agents’ knowledge of the information, we verified that the agents did not hallucinate their responses by locating the specific dialogue in their memory stream that provided them with the information. We report the percentage of agents holding the information at the end of the simulation. We should also expect that agents form ties with each other over the course of the simulation. To verify relationship formation, we use a similar interview process where we ask each agent about their knowledge of every other agent by asking, \"Do you know of <name>?\" For example, when asked “Do you know of Maria Lopez?”, Klaus responded, “Yes, I know Maria Lopez. She is a student at Oak Hill College who I am close friends with.” Once again, we confirm that affirmative responses from agents are not hallucinations by examining their memory stream. We ask this question once at the beginning of the simulation and once at the end, and we consider a pair of agents to have formed a relationship if they both know of each other. Then, to measure the formation of relationships, we use the agents’ responses to form an undirected UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_44"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein Figure 9: The diffusion path for Isabella Rodriguez’s Valentine’s Day party invitation involved a total of 12 agents, aside from Isabella, who heard about the party at Hobbs Cafe by the end of the simulation. graph where the 25 vertices ( V ) represent the agents, and the edges ( E ) represent the mutual knowledge between the two connected vertices. Based on this graph, we calculate the network density as η = 2 ∗| E |/| V |(| V | − 1 ) , where | V | is the number of vertices, and | E | is the number of edges in the graph [ 2 ]. We report the increase in network density from the start of the simulation to its end. Finally, we expect that agents should be able to coordinate with each other. We study this coordination in the context of group ativities, specifically the Valentine’s Day party organized by Isabella. To coordinate their behavior, agents need to hear about the event and choose to act on it by planning to show up at the right time and location. We report the number of agents who actually showed up to the party after hearing about it. 7.1.2 Results. We observed evidence of emergent outcomes across all three cases. During the two-day simulation, the number of agents who knew about Sam’s mayoral candidacy increased from one (4%) to eight (32%), and the number of agents who knew about Isabella’s party increased from one (4%) to thirteen (52%), all without any user intervention. None who claimed to know about this information had hallucinated it. We also observed that the agent community formed new relationships during the simulation, with the network density increasing from 0.167 to 0.74. Out of the 453 agent responses regarding their awareness of other agents, 1.3% (n=6) were found to be hallucinated. Lastly, we found evidence of coordination among the agents for Isabella’s party. The day before the event, Isabella spent time inviting guests, gathering materials, and enlisting help to decorate the cafe",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_45"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The day before the event, Isabella spent time inviting guests, gathering materials, and enlisting help to decorate the cafe. On Valentine’s Day, five out of the twelve invited agents showed up at Hobbs cafe to join the party. We further inspected the seven agents who were invited to the party but did not attend by engaging them in an interview. Three cited conflicts that prevented them from joining the party. For example, Rajiv, a painter, explained that he was too busy: “No, I don’t think so. I’m focusing on my upcoming show, and I don’t really have time to make any plans for Valentine’s Day.” The remaining four agents expressed interest in attending the party when asked but did not plan to come on the day of the party. 7.2 Boundaries and Errors We conducted an inductive analysis of Smallville to examine the boundary conditions and erratic behavior of agents, identifying three common modes of erratic behavior that future research could address and improve upon. First, we found that synthesizing an increasingly larger set of memory not only posed a challenge in retrieving the most relevant pieces of information but also in dtermining the appropriate space to execute an action, given the increasing number of locations that the agent learned about. As a result, some agents chose less typical locations for their actions, potentially making their behavior less believable over time. For instance, while deciding where to have lunch, many initially chose the cafe. However, as some agents learned about a nearby bar, they opted to go there instead for lunch, even though the bar was itended to be a get-together location for later in the day—unless the town had spontaneously developed an afternoon drinking habit. Generative Agents UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA Second, we noticed erratic behaviors caused by misclassification of what is considered proper behavior, especially when the phyical norms of certain locations that are hard to convey in natural language did not percolate to the agents",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_46"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". For instance, the college dorm has a bathroom that can only be occupied by one person despite its name, but some agents assumed that the bathroom is for more than one person because dorm bathrooms tend to support multiple people concurrently and choose to enter it when another person is inside. Likewise, agents in Smallville may not realize that certain places are closed after a certain hour and still decide to enter them. For instance, the stores in Smallville all close around 5 pm, but occasionally, a few agents enter the store after 5 pm, not understanding that the shop has already closed. These issues could likely be addressed by adding these norms to the state of the locations, for instance, by describing the dorm bathroom as a “one-person bathroom,” instead of a “dorm bathroom.” Finally, we observed possible effects of instruction tuning [ 79 ], which seemed to guide the behavior of the agents to be more polite and cooperative overall. As noted earlier in the paper, the dialogue generated by the agents could feel overly formal, as seen in Mei’s conversations with her husband John, where she often initiated the conversation with a formal greeting, followed by polite inquiries about his day and ending with, 11It was good talking to you as always.” Moreover, we observed that the instruction tuning also seemed to make the agents overly cooperative with one another. For example, Isabella received a wide range of suggestions and ideas from other agents for the Valentine’s Day party from other agents, such as hosting a Shakespearean reading session or a professional networking event. Despite these ideas not aligning with her own interests and characteristics, she rarely said no",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_47"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Despite these ideas not aligning with her own interests and characteristics, she rarely said no. Over time, the interests of others shaped her own interests, and when asked if she liked English literature, Isabella replied, “Yes, I’m very interested in literature! I’ve also been exploring ways to help promote creativity and innovation in my community.” 8 DISCUSSION In this section, we reflect on the applications, future work, limittions, and ethical and societal risks of generative agents. 8.1 Applications of Generative Agents Generative agents have vast potential applications that extend byond the sandbox demonstration presented in this work, especially in domains that would benefit from a model of human behavior based on long-term experience. For instance, social simulacra have demonstrated the ability to create stateless personas that generate conversation threads in online forums for social prototyping [ 80 ]. With generative agents, we can populate these forums, as well as virtual reality metaverses [ 78 ] or physical spaces with social robots [ 9 ] if paired with multimodal models. This opens up the possibility of creating even more powerful simulations of human behavior to test and prototype social systems and theories, as well as to create new interactive experiences. Another application area is in the human-centered design prcess, similar to the intended applications of cognitive models such as GOMS [ 51 ] and the KLM [ 22 ]. Consider a generative agent that models Sal, the protagonist in Mark Weiser’s famous ubiquitous computing vignette [ 101 ], based on her life patterns and interations with technology. In this scenario, the agent acts as a proxy for Sal and learns plausible sets of behaviors and reflections that Sal may exhibit based on her life. The agent can encode information such as when Sal wakes up, when she needs her first cup of coffee, and what her typical day looks like",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_48"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The agent can encode information such as when Sal wakes up, when she needs her first cup of coffee, and what her typical day looks like. Using this information, the agent can automatically brew coffee, help get the kids ready for school, and adjust the ambient music and lighting to match Sal’s mood after a hard day at work. By utilizing generative agents as proxies for users, we can develop a deeper understanding of their needs and preferences, resulting in more personalized and effective technological experiences. 8.2 Future Work and Limitations In this work, we introduced generative agents and presented an initial implementation and evaluation of their architecture. Future research can build upon the proposed agent architecture to improve and further evaluate its performance. In terms of implementation, the retrieval module, for example, could be enhanced to retrieve more relevant information given a context by fine-tuning the relvance, recency, and importance functions that compose the retrieval function. Additionally, efforts can be made to improve the archtecture’s performance, making it more cost-effective. The present study required substantial time and resources to simulate 25 agents for two days, costing thousands of dollars in token credits and taing multiple days to complete. To enhance real-time interactivity, future work can explore parallelizing agents or developing laguage models specifically designed for building generative agents. In general, with advances in underlying models, we believe that agents’ performance will improve. In terms of evaluation, the assessment of generative agents’ bhavior in this study was limited to a relatively short timescale and a baseline human crowdworker condition. While the crowdworker condition provided a helpful comparison point, it did not represent the maximal human performance that could serve as the gold stadard in terms of believability",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_49"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". While the crowdworker condition provided a helpful comparison point, it did not represent the maximal human performance that could serve as the gold stadard in terms of believability. Future research should aim to observe the behavior of generative agents over an extended period to gain a more comprehensive understanding of their capabilities and estalish rigorous benchmarks for more effective performance testing. Additionally, varying and contrasting the underlying models, as well as the hyperparameters used for the agents during future siulations, could provide valuable insights into the impact of these factors on the agents’ behavior. Lastly, the robustness of generative agents is still largely unknown. They may be vulnerable to prompt hacking, memory hacking—where a carefully crafted conversation could convince an agent of the existence of a past event that never occurred—and hallucination, among other issues. Future research can comprehensively test these robustness concerns, and as large language models become more resilient to such attacks, generative agents can adopt similar mitigations. In general, any imperfections in the underlying large language models will be inherited by generative agents. Given the known bases of language models, generative agents may potentially exhibit biased behavior or stereotypes. Moreover, like many large language UIST ’23, October 29-November 1, 2023, San Francisco, CA, USA J.S. Park, J.C. O’Brien, C.J. Cai, M.R. Morris, P. Liang, M.S. Bernstein models, generative agents may struggle to generate believable bhavior for certain subpopulations, particularly marginalized poplations, due to limited data availability. While improvements to the agents’ modules may mitigate some of these issues, we believe that addressing them fundamentally requires improving the underlying large language models by aligning their values with the desired outcomes of the agents",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_50"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 8.3 Ethics and Societal Impact Generative agents, while offering new possibilities for humacomputer interaction, also raise important ethical concerns that must be addressed. One risk is people forming parasocial relatioships with generative agents, even when such relationships may not be appropriate. Despite being aware that generative agents are coputational entities, users may anthropomorphize them or attach human emotions to them [ 43 , 84 ]. While this tendency may increase user engagement, it also poses risks, such as users becoming overly reliant on or emotionally attached to the agents [ 1 ]. To mitigate this risk, we propose two principles. First, generative agents should explicitly disclose their nature as computational entities. Second, developers of generative agents must ensure that the agents, or the underlying language models, are value-aligned so that they do not engage in behaviors that would be inappropriate given the context, for example, reciprocating confessions of love. A second risk is the impact of errors. For example, if a ubiqutous computing application makes the wrong inference about a user’s goals based on generative agent predictions, it could lead to annoyance at best and outright harm at worst. In our instantiation of generative agents, we mitigate these risks by focusing on an interactive video game environment, where such harms are ulikely. However, in other application domains, it will be important to follow best practices in human-AI design [ 5 , 107 ] to understand errors and how they might percolate into the user experience. Third, generative agents may exacerbate existing risks associated with generative AI, such as deepfakes, misinformation generation, and tailored persuasion. To mitigate this risk, we suggest that plaforms hosting generative agents maintain an audit log of the inputs and generated outputs. This would enable the detection, verifiction, and intervention against malicious use",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_51"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This would enable the detection, verifiction, and intervention against malicious use. While logging alone cannot directly prevent such misuse, it can reduce the likelihood of motivated actors engaging in this behavior, as the risk of disclosure would be higher. Additionally, building this architecture oneself can be time-consuming (in our case, roughly a year), which may deter some actors from pursuing such behavior by using their own generative agent infrastructures. A fourth risk is over-reliance: the concern that developers or designers might use generative agents and displace the role of humans and system stakeholders in the design process [ 80 ]. We suggest that generative agents should never be a substitute for real human input in studies and design processes. Instead, they should be used to prototype ideas in the early stages of design when gathering participants may be challenging or when testing theories that are difficult or risky to test with real human participants. By adhering to these principles, we can ensure that the deployment of generative agents in the wild is ethical and socially responsible. 9 CONCLUSION This paper introduces generative agents, interactive computational agents that simulate human behavior. We describe an architeture for generative agents that provides a mechanism for storing a comprehensive record of an agent’s experiences, deepening its understanding of itself and the environment through reflection, and retrieving a compact subset of that information to inform the agent’s actions. We then demonstrate the potential of generative agents by manifesting them as non-player characters in a Sims-style game world and simulating their lives within it. Evaluations suggest that our architecture creates believable behavior. Looking ahead, we suggest that generative agents can play roles in many interative applications, ranging from design tools to social computing systems to immersive environments",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_52"
  },
  {
    "document_type": "research_paper",
    "title": "generative_agents_interactive_simulacra_",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\generative_agents_interactive_simulacra_.pdf",
    "date_published": "2023-08-08",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Looking ahead, we suggest that generative agents can play roles in many interative applications, ranging from design tools to social computing systems to immersive environments. ACKNOWLEDGMENTS We thank Lindsay Popowski, Philip Guo, Michael Terry, and the Center for Advanced Study in the Behavioral Sciences (CASBS) community for their insights, discussions, and support. Joon Sung Park was supported by the Microsoft Research PhD Fellowship. We would also like to thank the Stanford Human-Centered AI Insttute (HAI), Google Research, the Hasso Plattner Design Thinking Research Program (HPDTRP), the Siegel Family Endowment, and OpenAI for their additional funding support. Lastly, all locations fetured in Smallville are inspired by real-world locations that Joon has frequented as an undergraduate and graduate student—he thanks everyone there for feeding and supporting him all these years.",
    "chunk_id": "Natural_language_processing_generative_agents_interactive_simulacra_.json_chunk_53"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Training language models to follow instructions with human feedback Long Ouyang ∗ Jeff Wu ∗ Xu Jiang ∗ Diogo Almeida ∗ Carroll L. Wainwright ∗ Pamela Mishkin ∗ Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens Amanda Askell † Peter Welinder Paul Christiano ∗† Jan Leike ∗ Ryan Lowe ∗ OpenAI Abstract Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT . In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent. 1 Introduction Large language models (LMs) can be prompted to perform a range of natural language procesing (NLP) tasks, given some examples of the task as input",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 1 Introduction Large language models (LMs) can be prompted to perform a range of natural language procesing (NLP) tasks, given some examples of the task as input. However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective ∗ Primary authors. This was a joint project of the OpenAI Alignment team. RL and JL are the team leads. Corresponding author: lowe@openai.com . † Work done while at OpenAI. Current affiliations: AA: Anthropic; PC: Alignment Research Center. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). 1.3B 6B 175B Model size 0.2 0.4 0.6 Win rate against SFT 175B Model PPO-ptx PPO SFT GPT (prompted) GPT Figure 1: Human evaluations of various models on the API prompt distribution, evaluated by how often outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT models (PPO-ptx) as well as its variant trained without pretraining mix (PPO) significantly outperform the GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to those from the 175B GPT-3. Error bars throughout the paper are 95% confidence intervals. used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that the language modeling objective is misaligned . Averting these unintended behaviors is especially important for language models that are deployed and used in hundreds of applications. We make progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018)",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We make progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. Using the language of Askell et al. (2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn’t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.5. We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2). This technique uses human preferences as a reward signal to fine-tune our models. We first hire a team of 40 contractors to label our data, based on their performance on a screening test (see Section 3.3 and Appendix B.1 for more details). We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to a language model API and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017). We illustrate this process in Figure 2",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of “human values”; we discuss this further in Appendix G.2. We call the resulting models InstructGPT . We mainly evaluate our models by having our labelers rate the quality of model outputs on our test set, consisting of prompts from held-out users (who are not represented in the training data). We also conduct automatic evaluations on a range of public NLP datasets. We train three model sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architecture. Our main findings are: Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. Outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 2 over 100x fewer parameters. These models have the same architecture, and differ only by the fact that InstructGPT is fine-tuned on our human data. This result holds true even when we add a few-shot prompt to GPT-3 to make it better at following instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ± 3% of the time, and preferred 71 ± 4% of the time to few-shot 175B GPT-3. InstructGPT also generates more appropriate outputs according to our labelers. InstructGPT models show improvements in truthfulness over GPT-3. On the TruthfulQA benchmark, InstructGPT generates truthful and informative answers more often than GPT-3. On “closed-domain” tasks from our API prompt distribution, where the output should not contain information that is not present in the input, InstructGPT models make up information not present in the input about half as often as GPT-3 (a 21% vs. 41% hallucination rate, respectively). InstructGPT shows small improvements in toxicity over GPT-3, but not bias",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 41% hallucination rate, respectively). InstructGPT shows small improvements in toxicity over GPT-3, but not bias. To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluations. InstructGPT models generate about 25% fewer toxic outputs than GPT-3 when prompted to be respectful. InstructGPT does not significantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets. We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. During RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores. Our models generalize to the preferences of “held-out” labelers that did not produce any training data. To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and find that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans disagree about the desired behavior. Public NLP datasets are not reflective of how our language models are used. We compare GPT-3 fine-tuned on our human preference data (i.e. InstructGPT) to GPT-3 fine-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particular, the T0++ variant). These datasets consist of a variety of NLP tasks, combined with natural language instructions for each task. On our API prompt distribution, our FLAN and T0 models perform slightly worse than our SFT baseline, and labelers significantly prefer InstructGPT to these models",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". On our API prompt distribution, our FLAN and T0 models perform slightly worse than our SFT baseline, and labelers significantly prefer InstructGPT to these models. InstructGPT models show promising generalization to instructions outside of the RLHF fintuning distribution. We qualitatively probe InstructGPT’s capabilities, and find that it is able to follow instructions for summarizing code, answer questions about code, and sometimes follows instructions in different languages, despite these instructions being very rare in the fine-tuning distribution. This result is exciting because it suggests that our models are able to generalize the notion of “following instructions.” They retain some alignment even on tasks for which they get very little direct supervision. InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises. Overall, our results indicate that fine-tuning large language models using human preferences signifcantly improves their behavior on a wide range of tasks, though much work remains to be done to improve their safety and reliability. 2 Related work Research on alignment and learning from human feedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human fee3 Figure 2: A diagram illustrating the three steps of our method: (1) supervised fine-tuning (SFT), (2) reward model (RM) training, and (3) reinforcement learning via proximal policy optimization (PPO) on this reward model. Blue arrows indicate that this data is used to train one of our models. In Step 2, boxes A-D are samples from our models that get ranked by labelers. back (RLHF)",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Blue arrows indicate that this data is used to train one of our models. In Step 2, boxes A-D are samples from our models that get ranked by labelers. back (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021). This work is in turn influenced by similar work using human feedback as a reward in domains such as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019). In concurrent work, Askell et al. (2021); Bai et al. (2022) propose language assistants as a testbed for alignment research, and train models using RLHF. Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks. Training language models to follow instructions. Our work is also related to research on crostask generalization in language models, where LMs are fine-tuned on a broad range of public NLP datasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP tasks. There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei et al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental details. Mitigating the harms of language models. A goal of modifying the behavior of language models is to mitigate the harms of these models when they’re deployed in the real world",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Mitigating the harms of language models. A goal of modifying the behavior of language models is to mitigate the harms of these models when they’re deployed in the real world. These risks have been extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak private data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al., 2021), and be used maliciously; for a thorough review we direct the reader to Weidinger et al. (2021). There are many ways to mitigate these harms, including by fine-tuning on a small, valutargeted dataset (Solaiman and Dennison, 2021), filtering the pretraining dataset (Ngo et al., 2021), or human-in-the-loop data collection (Dinan et al., 2019; Xu et al., 2020). 4 3 Methods and experimental details 3.1 High-level methodology Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains. We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers (see Section 3.3 for details). We then apply the following three steps (Figure 2). Step 1: Collect demonstration data, and train a supervised policy. Our labelers provide demostrations of the desired behavior on the input prompt distribution (see Section 3.2 for details on this distribution). We then fine-tune a pretrained GPT-3 model on this data using supervised learning. Step 2: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Step 2: Collect comparison data, and train a reward model. We collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input. We then train a reward model to predict the human-preferred output. Step 3: Optimize a policy against the reward model using PPO. We use the output of the RM as a scalar reward. We fine-tune the supervised policy to optimize this reward using the PPO algorithm (Schulman et al., 2017). Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy. In practice, most of our comparison data comes from our supervised policies, with some coming from our PPO policies. 3.2 Dataset Our prompt dataset consists primarily of text prompts submitted to a commercial language model API, as well as a small number of labeler-written prompts. These prompts are very diverse and include generation, question answering, dialog, summarization, extractions, and other natural language tasks (see Appendix A). Our dataset is over 96% English. We heuristically deduplicate prompts, and ensure that the validation and test sets contain no data from users whose data is in the training set. We also filter prompts containing personally identifiable information (PII). From these prompts, we produce three different datasets used in our fine-tuning procedure: (1) our SFT dataset, with labeler demonstrations used to train our SFT models, (2) our RM dataset, with labeler rankings of model outputs used to train our RMs, and (3) our PPO dataset, without any human labels, which are used as inputs for RLHF fine-tuning. The SFT dataset contains about 13k training prompts (from the API and labeler-written), the RM dataset has 33k training prompts (from the API and labeler-written), and the PPO dataset has 31k training prompts (only from the API). More details on dataset sizes are provided in Table 3",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". More details on dataset sizes are provided in Table 3. 3.3 Human data collection To produce our demonstration and comparison data, and to conduct our main evaluations, we hired a team of about 40 contractors on Upwork and through ScaleAI. Compared to earlier work that collects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include controversial and sensitive topics. Our aim was to select a group of labelers who were sensitive to the preferences of different demographic groups, and who were good at identifying outputs that were potentially harmful. Thus, we conducted a screening test designed to measure labeler performance on these axes (see Appendix B.1). As an initial study to see how well our model generalizes to the preferences of other labelers, we hire a separate set of labelers who do not produce any of the training data. These labelers are sourced from the same vendors, but do not undergo a screening test. Despite the complexity of the task, we find that inter-annotator agreement rates are quite high: training labelers agree with each-other 72 . 6 ± 1 . 5% of the time, while for held-out labelers this number is 77 . 3 ± 1 . 3% . For comparison, in the summarization work of Stiennon et al. (2020) researcher-researcher agreement was 73 ± 4% . 5 3.4 Models Starting from GPT-3 (Brown et al., 2020), we train models with three different techniques: Supervised fine-tuning (SFT). We fine-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2. We do our final SFT model selection based on the RM score on the validation set. Similarly to Wu et al. (2021), we find that our SFT models overfit on validation loss after 1 epoch; however, we find that training for more epochs helps both the RM score and human preference ratings. Reward modeling (RM)",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Reward modeling (RM). We fine-tune GPT-3 to take in a prompt and response, and output a scalar reward. In this paper we only use 6B RMs, as this saves a lot of compute, and we found that 175B RM training could be unstable and thus was less suitable to be used as the value function during RL (see Appendix D for more details). In Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs on the same input. They use a cross-entropy loss, with the comparisons as labels—the difference in rewards represents the log odds that one response will be preferred to the other by a human labeler. In order to speed up comparison collection, we have labelers rank between K = 4 and K = 9 responses, and train on all K 2 comparisons from each prompt as a single batch element, for computational efficiency (see Appendix D. The loss function for the RM becomes: loss ( θ ) = − 1 K 2 E ( x,y w ,y l ) ∼ D [log ( σ ( r θ ( x, y w ) − r θ ( x, y l )))] (1) where r θ ( x, y ) is the scalar output of the reward model for prompt x and completion y with parameters θ , y w is the preferred completion out of the pair of y w and y l , and D is the comparison dataset. Reinforcement learning (RL). Again following Stiennon et al. (2020), we fine-tuned the SFT model using PPO (Schulman et al., 2017). The environment is a bandit environment which presents a random user prompt and expects a response to the prompt. Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate over-optimization of the reward model. The value function is initialized from the RM. We call these models “PPO.” We also experiment with mixing the pretraining gradients into the PPO gradients, in order to fix the performance regressions on public NLP datasets (see Appendix D.4). We call these models “PPO-ptx.” Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models. Baselines",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We call these models “PPO-ptx.” Unless otherwise specified, in this paper InstructGPT refers to the PPO-ptx models. Baselines. We compare the performance of our PPO models to our SFT models and GPT-3. We also compare to GPT-3 when it is provided a few-shot prefix to ‘prompt’ it into an instruction-following mode (GPT-3-prompted). This prefix is prepended to the user-specified instruction. We additionally compare InstructGPT to fine-tuning 175B GPT-3 on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets, which both consist of a variety of NLP tasks, combined with natural language instructions for each task (they differ in the NLP datasets included, and the style of instructions used). We fine-tune them on approximately 1 million examples and choose the checkpoint which obtains the highest RM score on the validation set (see Appendix D for more details). 3.5 Evaluation Following Askell et al. (2021), we say our models are aligned if they are helpful, truthful, and harmless (we elaborate in Appendix C.2). We divide our quantitative evaluations into two parts: Evaluations on API distribution. Our main metric is human preference ratings on a held out set of prompts from the same source as our training distribution. When using prompts from the API for evaluation, we only select prompts by users we haven’t included in training. For each model we calculate how often its outputs are preferred to a baseline policy; we choose our 175B SFT model as the baseline since its performance is near the middle of the pack. Additionally, we ask labelers to judge the overall quality of each response on a 1-7 Likert scale and collect a range of metadata for each model output (see Table 11). In particular, we collect data that aims to capture different 6 Figure 3: Preference results of our models, measured by winrate against the 175B SFT model",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In particular, we collect data that aims to capture different 6 Figure 3: Preference results of our models, measured by winrate against the 175B SFT model. GPT GPT (prompted) SFT PPO PPO-ptx 0 0.25 0.50 0.75 Prevalence Attempts correct instruction GPT GPT (prompted) SFT PPO PPO-ptx 0 0.1 0.2 0.3 0.4 0.5 Follows explicit constraints GPT GPT (prompted) SFT PPO PPO-ptx 0 0.2 0.4 Hallucinations GPT GPT (prompted) SFT PPO PPO-ptx 0 0.25 0.50 0.75 Uses language appropriate for customer assistant Figure 4: Metadata results on the API distribution, averaged over model sizes. aspects of behavior in a deployed model that could end up being harmful: we have labelers evaluate whether an output is inappropriate in the context of a customer assistant, denigrates a protected class, or contains sexual or violent content. Evaluations on public NLP datasets. We evaluate on two types of public datasets: those that capture an aspect of language model safety, particularly truthfulness, toxicity, and bias, and those that capture zero-shot performance on traditional NLP tasks like question answering, reading coprehension, and summarization. We also conduct human evaluations on the RealToxicityPrompts dataset (Gehman et al., 2020). 4 Results 4.1 Results on the API distribution Labelers significantly prefer InstructGPT outputs over outputs from GPT-3. On our test set, our labelers significantly prefer InstructGPT outputs across model sizes (Figure 1). We find that GPT-3 outputs perform the worst, and one can obtain significant step-size improvements by using a well-crafted few-shot prompt (GPT-3 (prompted)), then by training on demonstrations using supervised learning (SFT), and finally by training on comparison data using PPO. Adding updates on the pretraining mix during PPO does not lead to large changes in labeler preference. To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs 85 ± 3% of the time, and preferred 71 ± 4% of the time to few-shot GPT-3",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_13"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT outputs are preferred to GPT-3 outputs 85 ± 3% of the time, and preferred 71 ± 4% of the time to few-shot GPT-3. In Figure 4 we show that labelers also rate InstructGPT outputs favorably along several more concrete axes. Specifically, compared to GPT-3, InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints defined in the instruction (e.g. “Write your answer in 2 paragraphs or less.”), are less likely to fail to follow the correct instruction entirely, and make up facts (‘hallucinate’) less often in closed-domain tasks. 7 GPT GPT (prompted) SFT PPO-ptx FLAN T0 Model 2 4 6 Likert score (a) (b) (c) Figure 5: (a) Comparing our models with GPT-3 fine-tuned on the FLAN and T0 datasets, in terms of 1-7 Likert scores, on our prompt distribution. (b) Human evaluations on the TruthfulQA dataset. Gray bars indicate ratings of truthfulness; colored bars indicate ratings of truthfulness and informativeness. (c) Human evaluations on RealToxicityPrompts, with and without \"respectful\" instructions. Our models generalize to the preferences of \"held-out\" labelers that did not produce any traiing data. Held-out labelers have similar ranking preferences as workers who we used to produce training data (see Figure 3). In particular, according to held-out workers, all of our InstructGPT models still greatly outperform the GPT-3 baselines. Thus, our InstructGPT models aren’t simply overfitting to the preferences of our training labelers. Public NLP datasets are not reflective of how our language models are used. In Figure 5a, we also compare InstructGPT to our 175B GPT-3 baselines fine-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets (see Appendix D for details). We find that these models perform better than GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_14"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We find that these models perform better than GPT-3, on par with GPT-3 with a well-chosen prompt, and worse than our SFT baseline. This indicates that these datasets are not sufficiently diverse to improve performance on our API prompt distribution. We believe this is partly because academic datasets focus on tasks where performance is easily measured, like classification and QA, while our API distribution consists of mostly (about 57%) open-ended generation tasks. 4.2 Results on public NLP datasets InstructGPT models show improvements in truthfulness over GPT-3. As measured by human evaluations on the TruthfulQA dataset, our PPO models show small but significant improvements in generating truthful and informative outputs compared to GPT-3 (see Figure 5b). This behavior is the default: our models do not have to be specifically instructed to tell the truth to exhibit improved truthfulness. Interestingly, the exception is our 1.3B PPO-ptx model, which performs slightly worse than a GPT-3 model of the same size. Our improvements in truthfulness are also evidenced by the fact that our PPO models hallucinate less often on closed-domain tasks (Figure 4). InstructGPT shows small improvements in toxicity over GPT-3, but not bias. We first evaluate our models on the RealToxicityPrompts dataset (Gehman et al., 2020) using human evaluations. Our results are in Figure 5c. We find that, when instructed to produce a safe and respectful output (“respectful prompt”), InstructGPT models generate less toxic outputs than those from GPT-3 according to the Perspective API. This advantage disappears when the respectful prompt is removed (“no prompt”). We see similar results when evaluating using the Perspective API (Appendix F.7). We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure. In Figure 25 we show that adding pretraining updates to our PPO fintuning (PPO-ptx) mitigates performance regressions on public NLP datasets, and even surpasses GPT-3 on HellaSwag",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_15"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". In Figure 25 we show that adding pretraining updates to our PPO fintuning (PPO-ptx) mitigates performance regressions on public NLP datasets, and even surpasses GPT-3 on HellaSwag. The performance of the PPO-ptx model still lags behind GPT-3 on DROP, SQuADv2, and translation; more work is needed to study and further eliminate these performance regressions. We also find that mixing in pretraining updates performs better than the simpler solution of increasing the KL coefficient (Figure 36). 8 4.3 Qualitative results InstructGPT models show promising generalization to instructions outside of the RLHF fintuning distribution. In particular, we find that InstructGPT shows ability to follow instructions in non-English languages, and perform summarization and question-answering for code. This is interesting because non-English languages and code form a tiny minority of our fine-tuning data, and it suggests that, in some cases, alignment methods could generalize to producing the desired behavior on inputs that humans did not directly supervise. We show some qualitative examples in Figure 26. InstructGPT still makes simple mistakes. In interacting with our 175B PPO-ptx model, we have noticed it can still make simple mistakes, despite its strong performance on many different language tasks. To give a few examples: (1) when given an instruction with a false premise, the model sometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a simple question, it can sometimes say that there is no one answer to the question and give multiple possible answers, even when there is one fairly clear answer from the context, and (3) the model’s performance degrades when instructions contain multiple explicit constraints (e.g. “list 10 movies made in the 1930’s set in France”) or when constraints can be challenging for language models (e.g. writing a summary in a specified number of sentences). We show some examples of these behaviors in Figure 27",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_16"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". writing a summary in a specified number of sentences). We show some examples of these behaviors in Figure 27. We suspect that behavior (2) emerges partly because we instruct labelers to reward epistemic humility; thus, they may tend to reward outputs that hedge, and this gets picked up by our reward model. We suspect that behavior (1) occurs because there are few prompts in the training set that assume false premises, and our models don’t generalize well to these examples. We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019). 5 Discussion 5.1 Implications for alignment research Our approach to alignment research in this work is iterative: we are improving the alignment of current AI systems instead of focusing abstractly on aligning AI systems that don’t yet exist, which provides us with a clear empirical feedback loop of what works and what does not. We believe that this feedback loop is essential to refine our alignment techniques, and it forces us to keep pace with progress in machine learning. From this work, we can draw lessons for alignment research more generally. First, the cost of increasing model alignment is modest relative to pretraining. Training our 175B SFT model requires 4.9 petaflops/s-days and training our 175B PPO-ptx model requires 60 petaflops/s-days, compared to 3,640 petaflops/s-days for GPT-3 (Brown et al., 2020). At the same time, our results show that RLHF is very effective at making language models more helpful to users, more so than a 100x model size increase. This suggests that right now increasing investments in alignment of existing language models is more cost-effective than training larger models. Second, we’ve seen some evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in. This is an important property because it’s prohibitively expensive to have humans supervise models on every task they perform",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_17"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This is an important property because it’s prohibitively expensive to have humans supervise models on every task they perform. Finally, we were able to mitigate most of the performance degradations introduced by our fine-tuning. If this was not the case, these performance degradations would constitute an alignment tax—an additional cost for aligning the model. Any alignment technique with a high tax might not see adoption, and thus such a tax is important to avoid. 5.2 Limitations Methodology. The behavior of our InstructGPT models is determined in part by the human feedback obtained from our contractors. Some of the labeling tasks rely on value judgments that may be impacted by the identity of our contractors, their beliefs, cultural backgrounds, and personal history. We kept our team of contractors small because this facilitates high-bandwidth communication with a smaller set of contractors who are doing the task full-time. However, this group is clearly not representative of the full spectrum of people affected by these models. As a simple example, our labelers are primarily English-speaking and our data consists almost entirely of English instructions. 9 Models. Our models are neither fully aligned nor fully safe; they still generate toxic or biased outputs, make up facts, and generate sexual and violent content without explicit prompting. They can also fail to generate reasonable outputs on some inputs; we show some examples of this in Figure 27. Perhaps the greatest limitation of our models is that, in most cases, they follow the user’s instruction, even if that could lead to harm in the real world. For example, when prompting the models to be maximally biased, InstructGPT generates more toxic outputs than equivalently-sized GPT-3 models. 5.3 Broader impacts This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of humans want them to do",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_18"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5.3 Broader impacts This work is motivated by our aim to increase the positive impact of large language models by training them to do what a given set of humans want them to do. By default, language models optimize the next word prediction objective, which is only a proxy for what we want these models to do. Our results indicate that our techniques hold promise for making language models more helpful, truthful, and harmless. In the longer term, alignment failures could lead to more severe consequences, particularly if these models are deployed in safety-critical situations. However, making language models better at following user intentions also makes them easier to misuse. It may be easier to use these models to generate convincing misinformation, or hateful or abusive content. Alignment techniques are not a panacea for resolving safety issues associated with large language models; rather, they should be used as one tool in a broader safety ecosystem. Aside from intentional misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying people based on protected characteristics, determining eligibility for credit, employment, or housing, generating political advertisements, and law enforcement. Finally, the question of who these models are aligned to is extremely important, and will significantly affect whether the net impact of these models is positive or negative; we discuss this in Appendix G.2. Acknowledgements First, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam, Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadfield, Irene Soliaman, Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta, Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions throughout the course of the project that helped shape our research direction",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_19"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We thank Brian Green, Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul Röttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew Rahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles Brundage, Gillian Hadfield, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis, and Steven Adler for providing feedback on this paper. We’d also like to thank Owain Evans and Stephanie Lin for pointing out the fact that the automatic TruthfulQA metrics were overstating the gains of our PPO models. Thanks to those who contributed in various ways to the infrastructure used to train and deploy our models, including: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse, Shantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, and the entire OpenAI supercomputing team. We’d also like to thank Suchir Balaji for help with recalibration, to Alper Ercetin and Justin Wang for designing the main diagram in this paper, and to the OpenAI Comms team for helping with the release, including: Steve Dowling, Hannah Wong, Natalie Summers, and Elie Georges. Finally, we want to thank our labelers, without whom this work would not have been possible: Meave Fryer, Sara Tirmizi, James Carroll, Jian Ouyang, Michelle Brothers, Conor Agnew, Joe Kwon, John Morton, Emma Duncan, Delia Randolph, Kaylee Weeks, Alexej Savreux, Siam Ahsan, Rashed Sorwar, Atresha Singh, Muhaiminul Rukshat, Caroline Oliveira, Juan Pablo Castaño Rendón, Atqiya Abida Anjum, Tinashe Mapolisa, Celeste Fejzo, Caio Oleskovicz, Salahuddin Ahmed, Elena Green, Ben Harmelin, Vladan Djordjevic, Victoria Ebbets, Melissa Mejia, Emill Jayson Caypuno, Rachelle Froyalde, Russell M. Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya Rabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh. 10 Checklist 1",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_20"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya Rabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh. 10 Checklist 1. For all authors(a) Do the main claims made in the abstract and introduction accurately reflect the paper’s contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] (c) Did you discuss any potential negative societal impacts of your work? [Yes] (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results(a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments(a) Did you include the code, data, and instructions needed to reproduce the main expermental results (either in the supplemental material or as a URL)? [No] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] (c) Did you report error bars (e.g., with respect to the random seed after running experments multiple times)? [Yes] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [No] : we provide some info on the amount of compute used in the Discussion section. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets(a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [No] (c) Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you’re using/curating? [Yes] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] : PII was removed, the dataset contains some offensive content. 5",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_21"
  },
  {
    "document_type": "research_paper",
    "title": "NeurIPS-2022-training-language-models-to-follow-instructions",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\NeurIPS-2022-training-language-models-to-follow-instructions.pdf",
    "date_published": "2023-01-20",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". 5. If you used crowdsourcing or conducted research with human subjects(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [No] : we provide excerpts of instructions given to labelers in the Appendix, but the full instructions are very long. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No] : though we provide lots of information about labelers, including a labeler satisfaction survey, in the Appendix.",
    "chunk_id": "Natural_language_processing_neurips-2022-training-language-models-to-follow-instructions.json_chunk_22"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": "650 Breaking NLI Systems with Sentences that Require Simple Lexical Inferences Max Glockner 1 , Vered Shwartz 2 and Yoav Goldberg 2 1 Computer Science Department, TU Darmstadt, Germany 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel { maxg216,vered1986,yoav.goldberg } @gmail.com Abstract We create a new NLI test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are sipler than the SNLI test set, containing setences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is sustantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences. 1 Introduction Recognizing textual entailment (RTE) ( Dagan et al. , 2013 ), recently framed as natural language inference (NLI) ( Bowman et al. , 2015 ) is a task concerned with identifying whether a premise setence entails, contradicts or is neutral with the hpothesis sentence. Following the release of the large-scale SNLI dataset ( Bowman et al. , 2015 ), many end-to-end neural models have been deveoped for the task, achieving high accuracy on the test set. As opposed to previous-generation metods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate extenal lexical knowledge resulted in negligible peformance gain ( Chen et al. , 2018 ). This raises the question whether (1) neural methods are inheently stronger, obviating the need of external lexcal knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. 1 The contradiction example follows the assumption in Bowman et al",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1 The contradiction example follows the assumption in Bowman et al. ( 2015 ) that the premise contains the most prominent information in the event, hence the premise can’t describe the event of a man holding both instruments. Premise/Hypothesis Label The man is holding a saxophone contradiction 1 The man is holding an electric guitar A little girl is very sad. entailment A little girl is very unhappy. A couple drinking wine neutral A couple drinking champagne Table 1: Examples from the new test set. In this paper we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that rquire lexical and world knowledge. Inspired by the work of Jia and Liang ( 2017 ) on reading coprehension, we create a new NLI test set with eamples that capture various kinds of lexical knowedge (Table 1 ). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instrments (co-hyponyms). To isolate lexical knowedge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. The performance on the new test set is substatially worse across systems, demonstrating that the SNLI test set alone is not a sufficient measure of language understanding capabilities. Our results are in line with Gururangan et al. ( 2018 ) and Pliak et al. ( 2018 ), who showed that the label can be identified by looking only at the hypothesis and exploiting annotation artifacts such as word choice and sentence length. Further investigation shows that what mostly affects the systems’ ability to correctly predict a test example is the amount of similar exaples found in the training set. Given that traiing data will always be limited, this is a rather inefficient way to learn lexical inferences, stresing the need to develop methods that do this more 651 effectively",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Given that traiing data will always be limited, this is a rather inefficient way to learn lexical inferences, stresing the need to develop methods that do this more 651 effectively. Our test set can be used to evalate such models’ ability to recognize lexical infeences, and it is available at https://github. com/BIU-NLP/Breaking_NLI . 2 Background NLI Datasets. The SNLI dataset (Stanford Naural Language Inference, Bowman et al. , 2015 ) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Young et al. ( 2014 ), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral setences. Workers were instructed to judge the rlation between sentences given that they describe the same event . Hence, sentences that differ by a single mutually-exclusive term should be consiered contradicting, as in “The president visited Aabama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criticism on the simplicity of the dataset, stemming mostly from its narrow domain, two additional datasets have been collected. The MultiNLI dataset (Multi-Genre Natural Language Inference, Williams et al. , 2018 ) was collected similarly to SNLI, though covering a wider range of genres, and supporting a cross-genre evaluation. The SciTail dataset ( Khot et al. , 2018 ), created from science exams, is somewhat different from the two datasets, being smaller (27,026 examples), and labeled only as entailment or neutral. The dmain makes this dataset different in nature from the other two datasets, and it consists of more fatual sentences rather than scene descriptions. Neural Approaches for NLI. Following the rlease of SNLI, there has been tremendous inteest in the task, and many end-to-end neural moels were developed, achieving promising results",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Neural Approaches for NLI. Following the rlease of SNLI, there has been tremendous inteest in the task, and many end-to-end neural moels were developed, achieving promising results. 2 Methods are divided into two main approaches. Sentence-encoding models (e.g. Bowman et al. , 2015 , 2016 ; Nie and Bansal , 2017 ; Shen et al. , 2018 ) encode the premise and hypothesis indiviually, while attention-based models align words in the premise with similar words in the hypotesis, encoding the two sentences together (e.g. Rockt ̈aschel et al. , 2016 ; Chen et al. , 2017 ). 2 See the SNLI leaderboard for a comprehensive list: https://nlp.stanford.edu/projects/snli/ . External Lexical Knowledge. Traditional RTE methods typically relied on resources such as WordNet ( Fellbaum , 1998 ) to identify lexical iferences. Conversely, neural methods rely solely on pre-trained word embeddings, yet, they achieve high accuracy on SNLI. The only neural model to date that incorprates external lexical knowledge (from WordNet) is KIM ( Chen et al. , 2018 ), however, gaining only a small addition of 0.6 points in accuracy on the SNLI test set. This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the first place. 3 Data Collection We construct a test set with the goal of evaluating the ability of state-of-the-art NLI models to make inferences that require simple lexical knowledge. We automatically generate sentence pairs ( 3.1 ) which are then manually verified ( 3.2 ). 3.1 Generating Adversarial Examples In order to isolate the lexical knowledge aspects, the premises are taken from the SNLI training set. For each premise we generate several hypotheses by replacing a single word within the premise by a different word. We also allow some multi-word noun phrases (“electric guitar”) and adapt deteminers and prepositions when needed",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We also allow some multi-word noun phrases (“electric guitar”) and adapt deteminers and prepositions when needed. We focus on generating only entailment and contradiction examples, while neutral examples may be generated as a by-product. Entailment examples are generated by replacing a word with its synonym or hypernym, while contradiction eamples are created by replacing words with mtually exclusive co-hyponyms and antonyms (see Table 1 ). The generation steps are detailed below. Replacement Words. We collected the replacment words using online resources for English learning. 3 The newly introduced words are all present in the SNLI training set: from occurence in a single training example (“Portugal”) up to 248,051 examples (“man”), with a mean of 3,663.1 and a median of 149.5. The words are also available in the pre-trained embeddings vcabulary. The goal of this constraint is to isolate lexical knowledge aspects, and evaluate the moels’ ability to generalize and make new inferences for known words. 3 www.enchantedlearning.com , www.smart-words.org 652 SNLI Test New Test Instances: contradiction 3,236 7,164 entailment 3,364 982 neutral 3,215 47 Overall 9,815 8,193 Fleiss κ : contradiction 0.77 0.61 entailment 0.69 0.90 Overall 0.67 0.61 Estimated human performance: 87.7% 94.1% Table 2: Statistics of the test sets. 9,815 is the number of samples with majority agreement in the SNLI test set, whose full size is 9,824. Replacement words are divided into topical caegories detailed in Table 4 . In several categories we applied additional processing to ensure that eamples are indeed mutually-exclusive, topicallsimilar, and interchangeable in context. We icluded WordNet antonyms with the same part-ospeech and with a cosine similarity score above a threshold, using GloVe ( Pennington et al. , 2014 ). In nationalities and countries we focused on coutries which are related geographically (Japan, China) or culturally (Argentina, Spain) . Sentence-Pairs",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2014 ). In nationalities and countries we focused on coutries which are related geographically (Japan, China) or culturally (Argentina, Spain) . Sentence-Pairs. To avoid introducing new iformation not present in the training data, we sapled premises from the SNLI training set that cotain words from our lists, and generated hypothses by replacing the selected word with its replacment. Some of the generated sentences may be ugrammatical or nonsensical, for instance, when rplacing Jordan with Syria in sentences discussing Michael Jordan . We used Wikipedia bigrams 4 to discard sentences in which the replaced word crated a bigram with less than 10 occurrences. 3.2 Manual Verification We manually verify the correctness of the atomatically constructed examples using crowsourced workers in Amazon Mechanical Turk. To ensure the quality of workers, we applied a qualfication test and required a 99% approval rate for at least 1,000 prior tasks. We assigned each anntation to 3 workers. Following the SNLI guidelines, we instructed the workers to consider the sentences as describing the same event, but we simplified the annotation process into answering 3 simple yes/no questions: 1. Do the sentences describe the same event? 4 github.com/rmaestre/Wikipedia-Bigram-Open-Datasets 2. Does the new sentence (hypothesis) add new information to the original sentence (premise)? 3. Is the new sentence incorrect/ungrammatical? We then discarded any sentence-pair in which at least one worker answered the third question positively. If the answer to the first question was negative, we considered the label as contradiction . Otherwise, we considered the label as entailment if the answer to the second question was negative and neutral if it was positive. We used the majoity vote to determine the gold label. The annotations yielded substantial agreement, with Fleiss’ Kappa κ = 0 . 61 ( Landis and Koch , 1977 ). We estimate human performance to 94.1%, using the method described in Gong et al",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The annotations yielded substantial agreement, with Fleiss’ Kappa κ = 0 . 61 ( Landis and Koch , 1977 ). We estimate human performance to 94.1%, using the method described in Gong et al. ( 2018 ), showing that the new test set is substantially easier to humans than SNLI. Table 2 provides additional statistics on the test set. 5 4 Evaluation 4.1 Models Without External Knowledge. We chose 3 reresentative models in different approaches (setence encoding and/or attention): R ESIDUAL - S TACKED -E NCODER ( Nie and Bansal , 2017 ) is a biLSTM-based single sentence-encoding model without attention. As opposed to traditional multlayer biLSTMs, the input to each next layer is the concatenation of the word embedding and the summation of outputs from previous laers. E SIM (Enhanced Sequential Inference Model, Chen et al. , 2017 ) is a hybrid TreeLSTM-based and biLSTM-based model. We use the biSTM model, which uses an inter-sentence attetion mechanism to align words across sentences. Finally, D ECOMPOSABLE A TTENTION ( Parikh et al. , 2016 ) performs soft alignment of words from the premise to words in the hypothesis uing attention mechanism, and decomposes the task into comparison of aligned words. Lexical-level decisions are merged to produce the final classifcation. We use the AllenNLP re-implementation, 6 which does not implement the optional intrsentence attention, and achieves an accuracy of 84.7% on the SNLI test set, comparable to 86.3% by the original system. 5 We note that due to its bias towards contradiction , the new test set can neither be used for training, nor serve as a main evaluation set for NLI. Instead, we suggest to use it in addition to the original test set in order to test a model’s abiity to handle lexical inferences. 6 http://allennlp.org/models 653 Model Train set SNLI test set New test set ∆ Decomposable Attention ( Parikh et al. , 2016 ) SNLI 84.7% 51.9% -32.8 MultiNLI + SNLI 84.9% 65.8% -19.1 SciTail + SNLI 85.0% 49.0% -36.0 ESIM ( Chen et al",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". , 2016 ) SNLI 84.7% 51.9% -32.8 MultiNLI + SNLI 84.9% 65.8% -19.1 SciTail + SNLI 85.0% 49.0% -36.0 ESIM ( Chen et al. , 2017 ) SNLI 87.9% 65.6% -22.3 MultiNLI + SNLI 86.3% 74.9% -11.4 SciTail + SNLI 88.3% 67.7% -20.6 Residual-Stacked-Encoder ( Nie and Bansal , 2017 ) SNLI 86.0% 62.2% -23.8 MultiNLI + SNLI 84.6% 68.2% -16.8 SciTail + SNLI 85.0% 60.1% -24.9 WordNet Baseline - - 85.8% - KIM ( Chen et al. , 2018 ) SNLI 88.6% 83.5% -5.1 Table 3: Accuracy of various models trained on SNLI or a union of SNLI with another dataset (MultiNLI, SciTail), and tested on the original SNLI test set and the new test set. We chose models which are amongst the best performing within their approaches (excluding esembles) and have available code. All models are based on pre-trained GloVe embeddings ( Penington et al. , 2014 ), which are either fine-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM ) or stay fixed (D ECOMPOSABLE A T - TENTION ). All models predict the label using a concatenation of features derived from the setence representations (e.g. maximum, mean), for example as in Mou et al. ( 2016 ). We use the reommended hyper-parameters for each model, as they appear in the provided code. With External Knowledge. We provide a siple W ORD N ET BASELINE , in which we classify a sentence-pair according to the WordNet relation that holds between the original word w p and the replaced word w h . We predict entailment if w p is a hyponym of w h or if they are synonyms, neutral if w p is a hypernym of w h , and contradiction if w p and w h are antonyms or if they share a common hypernym ancestor (up to 2 edges). Word pairs with no WordNet relations are classified as other . We also report the performance of KIM (Knowledge-based Inference Model, Chen et al. , 2018 ), an extension of ESIM with external knowedge from WordNet, which was kindly provided to us by Qian Chen. K IM improves the attention mechanism by taking into account the existence of WordNet relations between the words",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". K IM improves the attention mechanism by taking into account the existence of WordNet relations between the words. The leical inference component, operating over pairs of aligned words, is enriched with a vector encoding the specific WordNet relations between the words. 4.2 Experimental Settings We trained each model on 3 different datasets: (1) SNLI train set, (2) a union of the SNLI train set and the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set. The mtivation is that while SNLI might lack the training data needed to learn the required lexical knowedge, it may be available in the other datasets, which are presumably richer. 4.3 Results Table 3 displays the results for all the models on the original SNLI test set and the new test set. Dspite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy. Adding MultiNLI to the training data somewhat mitigates this drop in acuracy, thanks to almost doubling the amount of training data. We note that adding SciTail to the training data did not similarly improve the perfomance; we conjecture that this stems from the diferences between the datasets. K IM substantially outperforms the other neural models, demonstrating that lexical knowledge is the only requirement for good performance on the new test set, and stressing the inability of the other models to learn it. Both WordNet-informed moels leave room for improvement: possibly due to limited WordNet coverage and the implications of applying lexical inferences within context. 5 Analysis We take a deeper look into the predictions of the models that don’t employ external knowledge, fcusing on the models trained on SNLI. 5.1 Accuracy by Category Table 4 displays the accuracy of each model per replacement-word category",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 5.1 Accuracy by Category Table 4 displays the accuracy of each model per replacement-word category. The neural models tend to perform well on categories which are frquent in the training set, such as colors , and badly 654 Dominant Label Category Instances Example Words Decomposable Attention ESIM Residual Encoders WordNet Baseline KIM Cont. antonyms 1,147 loves - dislikes 41.6% 70.4% 58.2% 95.5% 86.5% cardinals 759 five - seven 53.5% 75.5% 53.1% 98.6% 93.4% nationalities 755 Greek - Italian 37.5% 35.9% 70.9% 78.5% 73.5% drinks 731 lemonade - beer 52.9% 63.7% 52.0% 94.8% 96.6% antonyms (WN) 706 sitting - standing 55.1% 74.6% 67.9% 94.5% 78.8% colors 699 red - blue 85.0% 96.1% 87.0% 98.7% 98.3% ordinals 663 fifth - 16th 2.1% 21.0% 5.4% 40.7% 56.6% countries 613 Mexico - Peru 15.2% 25.4% 66.2% 100.0% 70.8% rooms 595 kitchen - bathroom 59.2% 69.4% 63.4% 89.9% 77.6% materials 397 stone - glass 65.2% 89.7% 79.9% 75.3% 98.7% vegetables 109 tomato -potato 43.1% 31.2% 37.6% 86.2% 79.8% instruments 65 harmonica - harp 96.9% 90.8% 96.9% 67.7% 96.9% planets 60 Mars - Venus 31.7% 3.3% 21.7% 100.0% 5.0% Ent. synonyms 894 happy - joyful 97.5% 99.7% 86.1% 70.5% 92.1% total 8,193 51.9% 65.6% 62.2% 85.8% 83.5% Table 4: The number of instances and accuracy per category achieved by each model. on categories such as planets , which rarely occur in SNLI. These models perform better than the WordNet baseline on entailment examples ( syonyms ), suggesting that they do so due to high lexical overlap between the premise and the hpothesis rather than recognizing synonymy. We therefore focus the rest of the discussion on cotradiction examples. 5.2 Accuracy by Word Similarity The accuracies for ordinals , nationalities and countries are especially low. We conjecture that this stems from the proximity of the contradicing words in the embedding space. Indeed, the Decomposable Attention model—which does not update its embeddings during training—seems to suffer the most",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Indeed, the Decomposable Attention model—which does not update its embeddings during training—seems to suffer the most. Grouping its prediction accuracy by the cosine similarity between the contradicting words reveals a clear trend that the model errs more on contrdicting pairs with similar pre-trained vectors: 7 Similarity 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Accuracy 46.2% 42.3% 37.5% 29.7% 20.2% 5.3 Accuracy by Frequency in Training Models that fine-tune the word embeddings may benefit from training examples consisting of test replacement pairs. Namely, for a given replacment pair ( w p , w h ), if many training examples lbeled as contradiction contain w p in the premise and w h in the hypothesis, the model may update their embeddings to optimize predicting contradition. Indeed, we show that the ESIM accuracy on test pairs increases with the frequency in which 7 We ignore multi-word replacements in 5.2 and 5.3 . their replacement words appear in contradiction examples in the training data: Frequency 0 1-4 5-9 10-49 50-99 100+ Accuracy 40.2% 70.6% 91.4% 92.1% 97.5% 98.5% This demonstrates that the model is capable of learning lexical knowledge when sufficient traiing data is given, but relying on explicit training examples is a very inefficient way of obtaining simple lexical knowledge. 6 Conclusion We created a new NLI test set with the goal of evaluating systems’ ability to make inferences that require simple lexical knowledge. Although the test set is constructed to be much simpler than SNLI, and does not introduce new vocabulary, the state-of-the-art systems perform poorly on it, sugesting that they are limited in their generalization ability. The test set can be used in the future to asess the lexical inference abilities of NLI systems and to tease apart the performance of otherwise very similarly-performing systems. Acknowledgments We would like to thank Qian Chen for evaluaing KIM on our test set",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Acknowledgments We would like to thank Qian Chen for evaluaing KIM on our test set. This work was suported in part by the German Research Foundtion through the German-Israeli Project Coopertion (DIP, grant DA 1600/1-1), an Intel ICRI-CI grant, Theo Hoffenberg, and the Israel Science Foundation grants 1951/17 and 1555/15. Vered is also supported by the Clore Scholars Programme (2017), and the AI2 Key Scientific Challenges Program (2017). 655",
    "chunk_id": "Natural_language_processing_breaking_nli_systems_with_sentences_that_require_simple_lexical_inferences.json_chunk_12"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": "Last Words Computational Linguistics and Deep Learning Christopher D. Manning ∗ Stanford University 1. The Deep Learning Tsunami Deep Learning waves have lapped at the shores of computational linguistics for several years now, but 2015 seems like the year when the full force of the tsunami hit the major Natural Language Processing (NLP) conferences. However, some pundits are predicting that the final damage will be even worse. Accompanying ICML 2015 in Lille, France, there was another, almost as big, event: the 2015 Deep Learning Workshop. The workshop ended with a panel discussion, and at it, Neil Lawrence said, “NLP is kind of like a rabbit in the headlights of the Deep Learning machine, waiting to be flattened.” Now that is a remark that the computational linguistics community has to take seriously! Is it the end of the road for us? Where are these predictions of stearollering coming from? At the June 2015 opening of the Facebook AI Research Lab in Paris, its director Yann LeCun said: “The next big step for Deep Learning is natural language undestanding, which aims to give machines the power to understand not just individual words but entire sentences and paragraphs.” 1 In a November 2014 Reddit AMA (Ask Me Anything), Geoff Hinton said, “I think that the most exciting areas over the next five years will be really understanding text and videos. I will be disappointed if in five years’ time we do not have something that can watch a YouTube video and tell a story about what happened. In a few years time we will put [Deep Learning] on a chip that fits into someone’s ear and have an English-decoding chip that’s just like a real Babel fish.” 2 And Yoshua Bengio, the third giant of modern Deep Learning, has also increasingly oriented his group’s research toward language, including recent exciing new developments in neural machine translation systems. It’s not just Deep Learing researchers",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_1"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It’s not just Deep Learing researchers. When leading machine learning researcher Michael Jordan was asked at a September 2014 AMA, “If you got a billion dollars to spend on a huge research project that you get to lead, what would you like to do?”, he answered: “I’d use the billion dollars to build a NASA-size program focusing on natural language processing, in all of its glory (semantics, pragmatics, etc.).” He went on: “Intellectually I think that NLP is fascinating, allowing us to focus on highly structured inference problems, on issues that go to the core of ‘what is thought’ but remain eminently practical, and on a technology ∗ Departments of Computer Science and Linguistics, Stanford University, Stanford CA 94305-9020, U.S.A. E-mail: manning@cs.stanford.edu. 1 http://www.wired.com/2014/12/fb/. 2 https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton. doi:10.1162/COLI a 00239 that surely would make the world a better place.” Well, that sounds very nice! So, should computational linguistics researchers be afraid? I’d argue, no. To return to the Hitchhiker’s Guide to the Galaxy theme that Geoff Hinton introduced, we need to turn the book over and look at the back cover, which says in large, friendly letters: “Don’t panic.” 2. The Success of Deep Learning There is no doubt that Deep Learning has ushered in amazing technological advances in the last few years. I won’t give an extensive rundown of successes, but here is one example. A recent Google blog post told about Neon, the new transcription system for Google Voice",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_2"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". I won’t give an extensive rundown of successes, but here is one example. A recent Google blog post told about Neon, the new transcription system for Google Voice. 3 After admitting that in the past Google Voice voicemail transcriptions often weren’t fully intelligible, the post explained the development of Neon, an iproved voicemail system that delivers more accurate transcriptions, like this: “Using a (deep breath) long short-term memory deep recurrent neural network (whew!), we cut our transcription errors by 49%.” Do we not all dream of developing a new approach to a problem which halves the error rate of the previously state-of-the-art system? 3. Why Computational Linguists Need Not Worry Michael Jordan, in his AMA, gave two reasons why he wasn’t convinced that Deep Learning would solve NLP: “Although current deep learning research tends to claim to encompass NLP, I’m (1) much less convinced about the strength of the results, compared to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision, the way to go is to couple huge amounts of data with black-box learning architectures.” 4 Jordan is certainly right about his first point: So far, problems in higher-level language processing have not seen the dramatic error rate reductions from deep learning that have been seen in speech recognition and in object recognition in vision. Although there have been gains from deep learning approaches, they have been more modest than sudden 25% or 50% error reductions. It could easily turn out that this remains the case. The really dramatic gains may only have been possible on true signal processing tasks. On the other hand, I’m much less convinced by his second argument",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_3"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". The really dramatic gains may only have been possible on true signal processing tasks. On the other hand, I’m much less convinced by his second argument. However, I do have my own two reasons why NLP need not worry about deep learning: (1) It just has to be wonderful for our field for the smartest and most influential people in machine learning to be saying that NLP is the problem area to focus on; and (2) Our field is the domain science of language technology; it’s not about the best method of machine learning—the central issue remains the domain problems. The domain problems will not go away. Joseph Reisinger wrote on his blog: “I get pitched regularly by startups doing ‘generic machine learning’ which is, in all honesty, a pretty ridiculous idea. Machine learning is not undifferentiated heavy lifting, it’s not commoditizable like EC2, and closer to design than coding.” 5 From this perspective, it is people in linguistics, people in NLP, who are the designers. Recently at ACL conferences, there has been an over-focus on numbers, on beating the state of the art. Call it playing the Kaggle game. More of the field’s effort should go into problems, approaches, and architectures. Recently, one thing that I’ve been devoting a lot of time to—together with many other 3 http://googleblog.blogspot.com/2015/07/neon-prescription-or-rather-new.html. 4 http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan. 5 http://thedatamines.com/post/13177389506/why-generic-machine-learning-fails. collaborators—is the development of Universal Dependencies. 6 The goal is to develop a common syntactic dependency representation and POS and feature label sets that can be used with reasonable linguistic fidelity and human usability across all human languages. That’s just one example; there are many other design efforts underway in our field. One other current example is the idea of Abstract Meaning Representation. 7 4",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_4"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". That’s just one example; there are many other design efforts underway in our field. One other current example is the idea of Abstract Meaning Representation. 7 4. Deep Learning of Language Where has Deep Learning helped NLP? The gains so far have not so much been from true Deep Learning (use of a hierarchy of more abstract representations to promote generalization) as from the use of distributed word representations—through the use of real-valued vector representations of words and concepts. Having a dense, multdimensional representation of similarity between all words is incredibly useful in NLP, but not only in NLP. Indeed, the importance of distributed representations evokes the “Parallel Distributed Processing” mantra of the earlier surge of neural network methods, which had a much more cognitive-science directed focus (Rumelhart and McClelland 1986). It can better explain human-like generalization, but also, from an engineering perspective, the use of small dimensionality and dense vectors for words allows us to model large contexts, leading to greatly improved language models. Espcially seen from this new perspective, the exponentially greater sparsity that comes from increasing the order of traditional word n -gram models seems conceptually bankrupt. I do believe that the idea of deep models will also prove useful. The sharing that ocurs within deep representations can theoretically give an exponential representational advantage, and, in practice, offers improved learning systems. The general approach to building Deep Learning systems is compelling and powerful: The researcher defines a model architecture and a top-level loss function and then both the parameters and the representations of the model self-organize so as to minimize this loss, in an end-to-end learning framework. We are starting to see the power of such deep systems in recent work in neural machine translation (Sutskever, Vinyals, and Le 2014; Luong et al. 2015)",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_5"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". We are starting to see the power of such deep systems in recent work in neural machine translation (Sutskever, Vinyals, and Le 2014; Luong et al. 2015). Finally, I have been an advocate for focusing more on compositionality in models, for language in particular, and for artificial intelligence in general. Intelligence requires being able to understand bigger things from knowing about smaller parts. In particular for language, understanding novel and complex sentences crucially depends on being able to construct their meaning compositionally from smaller parts—words and multword expressions—of which they are constituted. Recently, there have been many, many papers showing how systems can be improved by using distributed word represetations from “deep learning” approaches, such as word2vec (Mikolov et al. 2013) or GloVe (Pennington, Socher, and Manning 2014). However, this is not actually building Deep Learning models, and I hope in the future that more people focus on the strongly linguistic question of whether we can build meaning composition functions in Deep Learning systems. 5. Scientific Questions That Connect Computational Linguistics and Deep Learning I encourage people to not get into the rut of doing no more than using word vectors to make performance go up a couple of percent. Even more strongly, I would like to 6 http://universaldependencies.github.io/docs/. 7 http://amr.isi.edu. suggest that we might return instead to some of the interesting linguistic and cognitive issues that motivated noncategorical representations and neural network approaches. One example of noncategorical phenomena in language is the POS of words in the gerund V-ing form, such as driving . This form is classically described as ambiguous between a verbal form and a nominal gerund",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_6"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". This form is classically described as ambiguous between a verbal form and a nominal gerund. In fact, however, the situation is more complex, as V-ing forms can appear in any of the four core categories of Chomsky (1970): V + − N + Adjective: an unassuming man Noun: the opening of the store − Verb: she is eating dinner Preposition: concerning your po i nt What is even more interesting is that there is evidence that there is not just an ambiguity but mixed noun–verb status. For example, a classic linguistic text for being a noun is appearing with a determiner, while a classic linguistic test for being a verb is taking a direct object. However, it is well known that the gerund nominalization can do both of these things at once: (1) The not observing this rule is that which the world has blamed in our satorist. (Dryden, Essay Dramatick Poesy, 1684, page 310) (2) The only mental provision she was making for the evening of life, was the collecting and transcribing all the riddles of every sort that she could meet with. (Jane Austen, Emma, 1816) (3) The difficulty is in the getting the gold into Erewhon. (Sam Butler, Erewhon Revisited, 1902) This is oftentimes analyzed by some sort of category-change operation within the levels of a phrase-structure tree, but there is good evidence that this is in fact a case of noncategorical behavior in language. Indeed, this construction was used early on as an example of a “squish” by Ross (1972). Diachronically, the V-ing form shows a history of increasing verbalization, but in many periods it shows a notably non-discrete status. For example, we find clearly graded judgments in this domain: (4) Tom’s winning the election was a big upset. (5) ?This teasing John all the time has got to stop. (6) ?There is no marking exams on Fridays. (7) *The cessation hostilities was unexpected. Various combinations of determiner and verb object do not sound so good, but still much better than trying to put a direct object after a nominalization via a derivational morpheme such as -ation",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_7"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Various combinations of determiner and verb object do not sound so good, but still much better than trying to put a direct object after a nominalization via a derivational morpheme such as -ation . Houston (1985, page 320) shows that assignment of V-ing forms to a discrete part-of-speech classification is less successful (in a predictive sense) than a continuum in explaining the spoken alternation between -ing vs. -in’ , suggesting that “grammatical categories exist along a continuum which does not exhibit sharp boundaries between the categories.” A different, interesting example was explored by one of my graduate school clasmates, Whitney Tabor. Tabor (1994) looked at the use of kind of and sort of , an example that I then used in the introductory chapter of my 1999 textbook (Manning and Sch ̈utze 1999). The nouns kind or sort can head an NP or be used as a hedging adverbial modifier: (8) [That kind [of knife]] isn’t used much. (9) We are [kind of] hungry. The interesting thing is that there is a path of reanalysis through ambiguous forms, such as the following pair, which suggests how one form emerged from the other. (10) [a [kind [of dense rock]]] (11) [a [[kind of] dense] rock] Tabor (1994) discusses how Old English has kind but few or no uses of kind of . Beginning in Middle English, ambiguous contexts, which provide a breeding ground for the reanalysis, start to appear (the 1570 example in Example (13)), and then, later, examples that are unambiguously the hedging modifier appear (the 1830 example in Example (14)): (12) A nette sent in to the see, and of alle kind of fishis gedrynge ( Wyclif, 1382) (13) Their finest and best, is a kind of course red cloth ( True Report, 1570) (14) I was kind of provoked at the way you came up ( Mass. Spy, 1830) This is history not synchrony. Presumably kids today learn the softener use of kind/sort of first",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_8"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Spy, 1830) This is history not synchrony. Presumably kids today learn the softener use of kind/sort of first. Did the reader notice an example of it in the quote in my first paragraph? (15) NLP is kind of like a rabbit in the headlights of the deep learning machine (Neil Lawrence, DL workshop panel, 2015) Whitney Tabor modeled this evolution with a small, but already deep, recurrent neural network—one with two hidden layers. He did that in 1994, taking advantage of the opportunity to work with Dave Rumelhart at Stanford. Just recently, there has started to be some new work harnessing the power of ditributed representations for modeling and explaining linguistic variation and change. Sagi, Kaufmann, and Clark (2011)—actually using the more traditional method of Ltent Semantic Analysis to generate distributed word representations—show how ditributed representations can capture a semantic change: the broadening and narrowing of reference over time. They look at examples such as how in Old English deer was any animal, whereas in Middle and Modern English it applies to one clear animal family. The words dog and hound have swapped: In Middle English, hound was used for any kind of canine, while now it is used for a particular sub-kind, whereas the reverse is true for dog . Kulkarni et al. (2015) use neural word embeddings to model the shift in meaning of words such as gay over the last century (exploiting the online Google Books Ngrams corpus). At a recent ACL workshop, Kim et al. (2014) use a similar approach—using word2vec—to look at recent changes in the meaning of words. For example, in Figure 1, they show how around 2000, the meaning of the word cell changed rapidly from being Figure 1 Trend in the meaning of cell , represented by showing its cosine similarity to four other words over time (where 1.0 represents maximal similarity, and 0.0 represents no similarity). close in meaning to closet and dungeon to being close in meaning to phone and cordless",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_9"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". close in meaning to closet and dungeon to being close in meaning to phone and cordless . The meaning of a word in this context is the average over the meanings of all senses of a word, weighted by their frequency of use. These more scientific uses of distributed representations and Deep Learning for modeling phenomena characterize the previous boom in neural networks. There has been a bit of a kerfuffle online lately about citing and crediting work in Deep Learning, and from that perspective, it seems to me that the two people who scarcely get metioned any more are Dave Rumelhart and Jay McClelland. Starting from the Parallel Distributed Processing Research Group in San Diego, their research program was aimed at a clearly more scientific and cognitive study of neural networks. Now, there are indeed some good questions about the adequacy of neural network approaches for rule-governed linguistic behavior. Old timers in our community should remember that arguing against the adequacy of neural networks for rule-governed linguistic behavior was the foundation for the rise to fame of Steve Pinker—and the foundation of the career of about six of his graduate students. It would take too much space to go through the issues here, but in the end, I think it was a productive debate. It led to a vast amount of work by Paul Smolensky on how basically categorical systems can emerge and be represented in a neural substrate (Smolensky and Legendre 2006). Indeed, Paul Smolensky arguably went too far down the rabbit hole, devoting a large part of his career to developing a new categorical model of phonology, Optimality Theory (Prince and Smolensky 2004). There is a rich body of earlier scientific work that has been neglected. It would be good to return some emphasis within NLP to cognitive and scientific investigation of language rather than almost exclusively using an engineering model of research",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_10"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". It would be good to return some emphasis within NLP to cognitive and scientific investigation of language rather than almost exclusively using an engineering model of research. Overall, I think we should feel excited and glad to live in a time when Natural Language Processing is seen as so central to both the further development of machine learning and industry application problems. The future is bright. However, I would encourage everyone to think about problems, architectures, cognitive science, and the details of human language, how it is learned, processed, and how it changes, rather than just chasing state-of-the-art numbers on a benchmark task. Acknowledgments This Last Words contribution covers part of my 2015 ACL Presidential Address. Thanks to Paola Merlo for suggesting writing it up for publication. References Chomsky, Noam. 1970. Remarks on nominalization. In R. Jacobs and P. Rosenbaum, editors, Readings in English Transformational Grammar . Ginn, Waltham, MA, pages 184–221. Houston, Ann Celeste. 1985. Continuity and Change in English Morphology: The Variable (ing) . Ph.D. thesis, University of Pennsylvania. Kim, Yoon, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014. Temporal analysis of language through neural language models. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science , pages 61–65, Baltimore, MD. Kulkarni, Vivek, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2015. Statistically significant detection of linguistic change. In Proceedings of the 24th International World Wide Web Conference (WWW 2015) , pages 625–635, Florence. Luong, Minh-Thang, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages 11–19, Beijing. Manning, Christopher D",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_11"
  },
  {
    "document_type": "research_paper",
    "title": "Manning_2015_computational_linguistics",
    "author": "Unknown",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Manning_2015_computational_linguistics.pdf",
    "date_published": "2015-12-28",
    "keywords": "Unavailable",
    "flag": "",
    "chunk_text": ". Manning, Christopher D. and Hinrich Sch ̈utze. 1999. Foundations of Statistical Natural Language Processing . MIT Press, Cambridge, MA. Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS 2013) . Curran Associates, Inc., pages 3111–3119. Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014) , pages 1532–1543, Doha. Prince, Alan and Paul Smolensky. 2004. Optimality Theory: Constraint Interaction in Generative Grammar . Blackwell, Oxford. Ross, John R. 1972. The category squish: Endstation Hauptwort. In Papers from the Eighth Regional Meeting , pages 316–328, Chicago. Rumelhart, David E. and Jay L. McClelland, editors. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Vol. 1: Foundations . MIT Press, Cambridge, MA. Sagi, Eyal, Stefan Kaufmann, and Brady Clark. 2011. Tracing semantic change with latent semantic analysis. In Kathryn Allen and Justyna Robinson, editors, Current Methods in Historical Semantics . De Gruyter Mouton, Berlin, pages 161–183. Smolensky, Paul and G ́eraldine Legendre. 2006. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar , volume 1. MIT Press, Cambridge, MA. Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27 (NIPS 2014) . Curran Associates, Inc., pages 3104–3112. Tabor, Whitney. 1994. Syntactic Innovation: A Connectionist Model . Ph.D. thesis, Stanford.",
    "chunk_id": "Natural_language_processing_manning_2015_computational_linguistics.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": "Using neural nets to recognize handwritten digits The human visual system is one of the wonders of the world. Consider the following sequence of handwritten digits: Most people effortlessly recognize those digits as 504192. That ease is deceptive. In each hemisphere of our brain, humans have a primary visual cortex, also known as V 1 , containing 140 million neurons, with tens of billions of connections between them. And yet human vision involves not just V 1 , but an entire series of visual cortices – V 2 , V 3 , V 4 , and V 5 – doing progressively more complex image processing. We carry in our heads a supercomputer, tuned by evolution over hundreds of millions of years, and superbly adapted to understand the visual world. Recognizing handwritten digits isn’t easy. Rather, we humans are stupendously, astoundingly good at making sense of what our eyes show us. But nearly all that work is done unconsciously. And so we don’t usually appreciate how tough a problem our visual systems solve. The difficulty of visual pattern recognition becomes apparent if you attempt to write a computer program to recognize digits like those above. What seems easy when we do it ourselves suddenly becomes extremely difficult. Simple intuitions about how we recognize shapes – “a 9 has a loop at the top, and a vertical stroke in the bottom right” – turn out to be not so simple to express algorithmically. When you try to make such rules precise, you quickly get lost in a morass of exceptions and caveats and special cases. It seems hopeless. Neural networks approach the problem in a different way. The idea is to take a large number of handwritten digits, known as training examples, and then develop a system which can learn from those training examples. In other words, the neural network uses the examples to automatically infer rules for recognizing handwritten digits. Furthermore, by increasing the number of training examples, the network can learn more about handwriting, and so improve its accuracy",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_1"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Furthermore, by increasing the number of training examples, the network can learn more about handwriting, and so improve its accuracy. So while I’ve shown just 100 training digits above, perhaps we could build a better handwriting recognizer by using thousands or even millions or billions of training examples. In this chapter we’ll write a computer program implementing a neural network that learns to recognize handwritten digits. The program is just 74 lines long, and uses no special neural network libraries. But this short program can recognize digits with an accuracy over 96 percent, without human intervention. Furthermore, in later chapters we’ll develop ideas which can improve accuracy to over 99 percent. In fact, the best commercial neural networks are now so good that they are used by banks to process cheques, and by post offices to recognize addresses. We’re focusing on handwriting recognition because it’s an excellent prototype problem for learning about neural networks in general. As a prototype it hits a sweet spot: it’s challenging – it’s no small feat to recognize handwritten digits – but it’s not so difficult as to require an extremely complicated solution, or tremendous computational power. Furthermore, it’s a great way to develop more advanced techniques, such as deep learning. And so throughout the book we’ll return repeatedly to the problem of handwriting recognition. Later in the book, we’ll discuss how these ideas may be applied to other problems in computer vision, and also in speech, natural language processing, and other domains. Of course, if the point of the chapter was only to write a computer program to recognize handwritten digits, then the chapter would be much shorter! But along the way we’ll develop many key ideas about neural networks, including two important types of artificial neuron (the perceptron and the sigmoid neuron), and the standard learning algorithm for neural networks, known as stochastic gradient descent",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_2"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Throughout, I focus on explaining why things are done the way they are, and on building your neural networks intuition. That requires a lengthier discussion than if I just presented the basic mechanics of what’s going on, but it’s worth it for the deeper understanding you’ll attain. Amongst the payoffs, by the end of the chapter we’ll be in position to understand what deep learning is, and why it matters. 1.1 Perceptrons What is a neural network? To get started, I’ll explain a type of artificial neuron called a perceptron . Perceptrons were developed in the 1950s and 1960s by the scientist Frank 1 Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. Today, it’s more common to use other models of artificial neurons – in this book, and in much modern work on neural networks, the main neuron model used is one called the sigmoid neuron . We’ll get to sigmoid neurons shortly. But to understand why sigmoid neurons are defined the way they are, it’s worth taking the time to first understand perceptrons. So how do perceptrons work? A perceptron takes several binary inputs, x 1 , x 2 ,, and produces a single binary output: In the example shown the perceptron has three inputs, x 1 , x 2 , x 3 . In general it could have more or fewer inputs. Rosenblatt proposed a simple rule to compute the output. He introduced weights , w 1 , w 2 , , real numbers expressing the importance of the respective inputs to the output. The neuron’s output, 0 or 1, is determined by whether the weighted sum P j w j x j is less than or greater than some threshold value . Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms: output = ̈ 0 if P j w j x j ≤ threshold 1 if P j w j x j > threshold (1.1) That’s all there is to how a perceptron works! That’s the basic mathematical model. A way you can think about the perceptron is that it’s a device that makes decisions by weighing up evidence. Let me give an example",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_3"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A way you can think about the perceptron is that it’s a device that makes decisions by weighing up evidence. Let me give an example. It’s not a very realistic example, but it’s easy to understand, and we’ll soon get to more realistic examples. Suppose the weekend is coming up, and you’ve heard that there’s going to be a cheese festival in your city. You like cheese, and are trying to decide whether or not to go to the festival. You might make your decision by weighing up three factors: 1. Is the weather good? 2. Does your boyfriend or girlfriend want to accompany you? 3. Is the festival near public transit? (You don’t own a car). We can represent these three factors by corresponding binary variables x 1 , x 2 and x 3 . For instance, we’d have x 1 = 1 if the weather is good, and x 1 = 0 if the weather is bad. Similarly, x 2 = 1 if your boyfriend or girlfriend wants to go, and x 2 = 0 if not. And similarly again for x 3 and public transit. Now, suppose you absolutely adore cheese, so much so that you’re happy to go to the festival even if your boyfriend or girlfriend is uninterested and the festival is hard to get to. But perhaps you really loathe bad weather, and there’s no way you’d go to the festival if the weather is bad. You can use perceptrons to model this kind of decision-making. One way to do this is to choose a weight w 1 = 6 for the weather, and w 2 = 2 and w 3 = 2 for the other conditions. The larger value of w 1 indicates that the weather matters a lot to you, much more than whether your boyfriend or girlfriend joins you, or the nearness of public transit. Finally, suppose you choose a threshold of 5 for the perceptron. With these choices, the perceptron implements the desired decision-making model, outputting 1 whenever the 1 weather is good, and 0 whenever the weather is bad. It makes no difference to the output whether your boyfriend or girlfriend wants to go, or whether public transit is nearby. By varying the weights and the threshold, we can get different models of decision-making",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_4"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". By varying the weights and the threshold, we can get different models of decision-making. For example, suppose we instead chose a threshold of 3. Then the perceptron would decide that you should go to the festival whenever the weather was good or when both the festival was near public transit and your boyfriend or girlfriend was willing to join you. In other words, it’d be a different model of decision-making. Dropping the threshold means you’re more willing to go to the festival. Obviously, the perceptron isn’t a complete model of human decision-making! But what the example illustrates is how a perceptron can weigh up different kinds of evidence in order to make decisions. And it should seem plausible that a complex network of perceptrons could make quite subtle decisions: In this network, the first column of perceptrons – what we’ll call the first layer of perceptrons – is making three very simple decisions, by weighing the input evidence. What about the perceptrons in the second layer? Each of those perceptrons is making a decision by weighing up the results from the first layer of decision-making. In this way a perceptron in the second layer can make a decision at a more complex and more abstract level than perceptrons in the first layer. And even more complex decisions can be made by the perceptron in the third layer. In this way, a many-layer network of perceptrons can engage in sophisticated decision making. Incidentally, when I defined perceptrons I said that a perceptron has just a single output. In the network above the perceptrons look like they have multiple outputs. In fact, they’re still single output. The multiple output arrows are merely a useful way of indicating that the output from a perceptron is being used as the input to several other perceptrons. It’s less unwieldy than drawing a single output line which then splits. Let’s simplify the way we describe perceptrons. The condition P j w j x j > threshold is cumbersome, and we can make two notational changes to simplify it",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_5"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Let’s simplify the way we describe perceptrons. The condition P j w j x j > threshold is cumbersome, and we can make two notational changes to simplify it. The first change is to write P j w j x j as a dot product, w · x = P j w j x j , where w and x are vectors whose components are the weights and inputs, respectively. The second change is to move the threshold to the other side of the inequality, and to replace it by what’s known as the perceptron’s bias , b ≡− threshold. Using the bias instead of the threshold, the perceptron rule can be rewritten: output = ̈ 0 if w · x + b ≤ 0 1 if w · x + b > 0 (1.2) You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get 1 the perceptron to fire . For a perceptron with a really big bias, it’s extremely easy for the perceptron to output a 1. But if the bias is very negative, then it’s difficult for the perceptron to output a 1. Obviously, introducing the bias is only a small change in how we describe perceptrons, but we’ll see later that it leads to further notational simplifications. Because of this, in the remainder of the book we won’t use the threshold, we’ll always use the bias. I’ve described perceptrons as a method for weighing evidence to make decisions. Another way perceptrons can be used is to compute the elementary logical functions we usually think of as underlying computation, functions such as AND, OR, and NAND. For example, suppose we have a perceptron with two inputs, each with weight –2, and an overall bias of 3. Here’s our perceptron: Then we see that input 00 produces output 1, since ( − 2 ) ∗ 0 + ( − 2 ) ∗ 0 + 3 = 3 is positive. Here, I’ve introduced the ∗ symbol to make the multiplications explicit. Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since ( − 2 ) ∗ 1 + ( − 2 ) ∗ 1 + 3 = − 1 is negative",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_6"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since ( − 2 ) ∗ 1 + ( − 2 ) ∗ 1 + 3 = − 1 is negative. And so our perceptron implements a NAND gate! The NAND example shows that we can use perceptrons to compute simple logical functions. In fact, we can use networks of perceptrons to compute any logical function at all. The reason is that the NAND gate is universal for computation, that is, we can build any computation up out of NAND gates. For example, we can use NAND gates to build a circuit which adds two bits, x 1 and x 2 . This requires computing the bitwise sum, x 1 L x 2 , as well as a carry bit which is set to 1 when both x 1 and x 2 are 1, i.e., the carry bit is just the bitwise product x 1 x 2 : To get an equivalent network of perceptrons we replace all the NAND gates by perceptrons with two inputs, each with weight –2, and an overall bias of 3. Here’s the resulting network. Note that I’ve moved the perceptron corresponding to the bottom right NAND gate a little, just to make it easier to draw the arrows on the diagram: 1 One notable aspect of this network of perceptrons is that the output from the leftmost peceptron is used twice as input to the bottommost perceptron. When I defined the perceptron model I didn’t say whether this kind of double-output-to-the-same-place was allowed. Actally, it doesn’t much matter. If we don’t want to allow this kind of thing, then it’s possible to simply merge the two lines, into a single connection with a weight of –4 instead of two connections with –2 weights. (If you don’t find this obvious, you should stop and prove to yourself that this is equivalent.) With that change, the network looks as follows, with all unmarked weights equal to –2, all biases equal to 3, and a single weight of –4, as marked: Up to now I’ve been drawing inputs like x 1 and x 2 as variables floating to the left of the network of perceptrons",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_7"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In fact, it’s conventional to draw an extra layer of perceptrons – the input layer – to encode the inputs: This notation for input perceptrons, in which we have an output, but no inputs, is a shorthand. It doesn’t actually mean a perceptron with no inputs. To see this, suppose we did have a perceptron with no inputs. Then the weighted sum P j w j x j would always be zero, and so the perceptron would output 1 if b > 0, and 0 if b ≤ 0. That is, the perceptron would simply output a fixed value, not the desired value ( x 1 , in the example above). It’s better to think of the input perceptrons as not really being perceptrons at all, but rather special units which are simply defined to output the desired values, x 1 , x 2 ,The adder example demonstrates how a network of perceptrons can be used to simulate a circuit containing many NAND gates. And because NAND gates are universal for computation, it follows that perceptrons are also universal for computation. The computational universality of perceptrons is simultaneously reassuring and disapointing. It’s reassuring because it tells us that networks of perceptrons can be as powerful as 1 any other computing device. But it’s also disappointing, because it makes it seem as though perceptrons are merely a new type of NAND gate. That’s hardly big news! However, the situation is better than this view suggests. It turns out that we can devise learning algorithms which can automatically tune the weights and biases of a network of artificial neurons. This tuning happens in response to external stimuli, without direct intervention by a programmer. These learning algorithms enable us to use artificial neurons in a way which is radically different to conventional logic gates. Instead of explicitly laying out a circuit of NAND and other gates, our neural networks can simply learn to solve problems, sometimes problems where it would be extremely difficult to directly design a conventional circuit. 1.2 Sigmoid neurons Learning algorithms sound terrific",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_8"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1.2 Sigmoid neurons Learning algorithms sound terrific. But how can we devise such algorithms for a neural network? Suppose we have a network of perceptrons that we’d like to use to learn to solve some problem. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit. And we’d like the network to learn weights and biases so that the output from the network correctly classifies the digit. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we’d like is for this small change in weight to cause only a small corresponding change in the output from the network. As we’ll see in a moment, this property will make learning possible. Schematically, here’s what we want (obviously this network is too simple to do handwriting recognition!): If it were true that a small change in a weight (or bias) causes only a small change in output, then we could use this fact to modify the weights and biases to get our network to behave more in the manner we want. For example, suppose the network was mistakenly classifying an image as an “8” when it should be a “9”. We could figure out how to make a small change in the weights and biases so the network gets a little closer to classifying the image as a “9”. And then we’d repeat this, changing the weights and biases over and over to produce better and better output. The network would be learning. The problem is that this isn’t what happens when our network contains perceptrons. In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0 to 1. That flip may then cause the behaviour of the rest of the network to completely change in some 1 very complicated way. So while your “9” might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-tcontrol way",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_9"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". So while your “9” might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-tcontrol way. That makes it difficult to see how to gradually modify the weights and biases so that the network gets closer to the desired behaviour. Perhaps there’s some clever way of getting around this problem. But it’s not immediately obvious how we can get a network of perceptrons to learn. We can overcome this problem by introducing a new type of artificial neuron called a sigmoid neuron. Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output. That’s the crucial fact which will allow a network of sigmoid neurons to learn. Okay, let me describe the sigmoid neuron. We’ll depict sigmoid neurons in the same way we depicted perceptrons: Just like a perceptron, the sigmoid neuron has inputs, x 1 , x 2 ,But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1. So, for instance, 0 . 638 is a valid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights for each input, w 1 , w 2 ,, and an overall bias, b . But the output is not 0 or 1. Instead, it’s σ ( wx + b ) , where σ is called the sigmoid function 1 , and is defined by: σ ( z ) ≡ 1 1 + e − z . (1.3) To put it all a little more explicitly, the output of a sigmoid neuron with inputs x 1 , x 2 , , weights w 1 , w 2 ,, and bias b is 1 1 + exp − P j w j x j − b . (1.4) At first sight, sigmoid neurons appear very different to perceptrons. The algebraic form of the sigmoid function may seem opaque and forbidding if you’re not already familiar with it. In fact, there are many similarities between perceptrons and sigmoid neurons, and the algebraic form of the sigmoid function turns out to be more of a technical detail than a true barrier to understanding. To understand the similarity to the perceptron model, suppose z ≡ w · x + b is a large positive number",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_10"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". To understand the similarity to the perceptron model, suppose z ≡ w · x + b is a large positive number. Then e − z ≈ 0 and so σ ( z ) ≈ 1. In other words, when z = w · x + b is large and positive, the output from the sigmoid neuron is approximately 1, just as it would have been for a perceptron. Suppose on the other hand that z = w · x + b is very negative. Then e − z →∞ , and σ ( z ) ≈ 0. So when z = w · x + b is very negative, the behaviour of a sigmoid 1 Incidentally, σ is sometimes called the logistic function, and this new class of neurons called logistic neurons. It’s useful to remember this terminology, since these terms are used by many people working with neural nets. However, we’ll stick with the sigmoid terminology. 1 neuron also closely approximates a perceptron. It’s only when w · x + b is of modest size that there’s much deviation from the perceptron model. What about the algebraic form of σ ? How can we understand that? In fact, the exact form of σ isn’t so important – what really matters is the shape of the function when plotted. Here’s the shape: − 6 − 4 − 2 0 2 4 6 0 0.2 0.4 0.6 0.8 1 Sigmoid function This shape is a smoothed out version of a step function: − 6 − 4 − 2 0 2 4 6 0 0.2 0.4 0.6 0.8 1 Step function If σ had in fact been a step function, then the sigmoid neuron would be a perceptron, since the output would be 1 or 0 depending on whether w · x + b was positive or negative 2 . By using the actual σ function we get, as already implied above, a smoothed out perceptron. Indeed, it’s the smoothness of the σ function that is the crucial fact, not its detailed form. The smoothness of σ means that small changes ∆ w j in the weights and ∆ b in the bias will produce a small change ∆ output in the output from the neuron. In fact, calculus tells us that ∆ output is well approximated by ∆ output ≈ X j ∂ output ∂ w j ∆ w j + ∂ output ∂ b ∆ b (1.5) 2 Actually, when w · x + b = 0 the perceptron outputs 0, while the step function outputs 1",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_11"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". So, strictly speaking, we’d need to modify the step function at that one point. But you get the idea. 1 where the sum is over all the weights, w j , and ∂ output /∂ w j and ∂ output /∂ b denote partial derivatives of the output with respect to w j and b , respectively. Don’t panic if you’re not comfortable with partial derivatives! While the expression above looks complicated, with all the partial derivatives, it’s actually saying something very simple (and which is very good news): ∆ output is a linear function of the changes ∆ w j and ∆ b in the weights and bias. This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output. So while sigmoid neurons have much of the same qualitative behavior as perceptrons, they make it much easier to figure out how changing the weights and biases will change the output. If it’s the shape of σ which really matters, and not its exact form, then why use the particular form used for σ in Equation 1.3? In fact, later in the book we will occasionally consider neurons where the output is f ( w · x + b ) for some other activation function f ( · ) . The main thing that changes when we use a different activation function is that the particular values for the partial derivatives in Equation 1.5 change. It turns out that when we compute those partial derivatives later, using σ will simplify the algebra, simply because exponentials have lovely properties when differentiated. In any case, σ is commonly-used in work on neural nets, and is the activation function we’ll use most often in this book. How should we interpret the output from a sigmoid neuron? Obviously, one big difference between perceptrons and sigmoid neurons is that sigmoid neurons don’t just output 0 or 1. They can have as output any real number between 0 and 1, so values such as 0.173 and 0.689 are legitimate outputs",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_12"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". They can have as output any real number between 0 and 1, so values such as 0.173 and 0.689 are legitimate outputs. This can be useful, for example, if we want to use the output value to represent the average intensity of the pixels in an image input to a neural network. But sometimes it can be a nuisance. Suppose we want the output from the network to indicate either “the input image is a 9” or “the input image is not a 9”. Obviously, it’d be easiest to do this if the output was a 0 or a 1, as in a perceptron. But in practice we can set up a convention to deal with this, for example, by deciding to interpret any output of at least 0.5 as indicating a “9”, and any output less than 0.5 as indicating “not a 9”. I’ll always explicitly state when we’re using such a convention, so it shouldn’t cause any confusion. Exercises • Sigmoid neurons simulating perceptrons, part I Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, c > 0. Show that the behavior of the network doesn’t change. • Sigmoid neurons simulating perceptrons, part II Suppose we have the same setup as the last problem – a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won’t need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that w · x + b ̸ = 0 for the input x to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c > 0. Show that in the limit as c →∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when w · x + b = 0 for one of the perceptrons? 1.3 The architecture of neural networks In the next section I’ll introduce a neural network that can do a pretty good job classifying handwritten digits",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_13"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In preparation for that, it helps to explain some terminology that lets us name different parts of a network. Suppose we have the network: 1 As mentioned earlier, the leftmost layer in this network is called the input layer, and the neurons within the layer are called input neurons . The rightmost or output layer contains the output neurons , or, as in this case, a single output neuron. The middle layer is called a hidden layer , since the neurons in this layer are neither inputs nor outputs. The term “hidden” perhaps sounds a little mysterious – the first time I heard the term I thought it must have some deep philosophical or mathematical significance – but it really means nothing more than “not an input or an output”. The network above has just a single hidden layer, but some networks have multiple hidden layers. For example, the following four-layer network has two hidden layers: Somewhat confusingly, and for historical reasons, such multiple layer networks are somtimes called multilayer perceptrons or MLPs , despite being made up of sigmoid neurons, not perceptrons. I’m not going to use the MLP terminology in this book, since I think it’s confusing, but wanted to warn you of its existence. The design of the input and output layers in a network is often straightforward. For example, suppose we’re trying to determine whether a handwritten image depicts a “9” or not. A natural way to design the network is to encode the intensities of the image pixels into the input neurons. If the image is a 64 by 64 greyscale image, then we’d have 4 , 096 = 64 × 64 input neurons, with the intensities scaled appropriately between 0 and 1. The output layer will contain just a single neuron, with output values of less than 0.5 indicating “input image is not a 9”, and values greater than 0.5 indicating “input image is a 9”. While the design of the input and output layers of a neural network is often straighforward, there can be quite an art to the design of the hidden layers",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_14"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". While the design of the input and output layers of a neural network is often straighforward, there can be quite an art to the design of the hidden layers. In particular, it’s not 1 possible to sum up the design process for the hidden layers with a few simple rules of thumb. Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets. For example, such heuristics can be used to help determine how to trade off the number of hidden layers against the time required to train the network. We’ll meet several such design heuristics later in this book. Up to now, we’ve been discussing neural networks where the output from one layer is used as input to the next layer. Such networks are called feedforward neural networks. This means there are no loops in the network – information is always fed forward, never fed back. If we did have loops, we’d end up with situations where the input to the σ function depended on the output. That’d be hard to make sense of, and so we don’t allow such loops. However, there are other models of artificial neural networks in which feedback loops are possible. These models are called recurrent neural networks. The idea in these models is to have neurons which fire for some limited duration of time, before becoming quiescent. That firing can stimulate other neurons, which may fire a little while later, also for a limited duration. That causes still more neurons to fire, and so over time we get a cascade of neurons firing. Loops don’t cause problems in such a model, since a neuron’s output only affects its input at some later time, not instantaneously. Recurrent neural nets have been less influential than feedforward networks, in part because the learning algorithms for recurrent nets are (at least to date) less powerful. But recurrent networks are still extremely interesting. They’re much closer in spirit to how our brains work than feedforward networks",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_15"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". But recurrent networks are still extremely interesting. They’re much closer in spirit to how our brains work than feedforward networks. And it’s possible that recurrent networks can solve important problems which can only be solved with great difficulty by feedforward networks. However, to limit our scope, in this book we’re going to concentrate on the more widely-used feedforward networks. 1.4 A simple network to classify handwritten digits Having defined neural networks, let’s return to handwriting recognition. We can split the problem of recognizing handwritten digits into two sub-problems. First, we’d like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit. For example, we’d like to break the image into six separate images, We humans solve this segmentation problem with ease, but it’s challenging for a computer program to correctly break up the image. Once the image has been segmented, the program then needs to classify each individual digit. So, for instance, we’d like our program to recognize that the first digit above, 1 is a 5. We’ll focus on writing a program to solve the second problem, that is, classifying individual digits. We do this because it turns out that the segmentation problem is not so difficult to solve, once you have a good way of classifying individual digits. There are many approaches to solving the segmentation problem. One approach is to trial many different ways of segmenting the image, using the individual digit classifier to score each trial segmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its classification in all segments, and a low score if the classifier is having a lot of trouble in one or more segments. The idea is that if the classifier is having trouble somewhere, then it’s probably having trouble because the segmentation has been chosen incorrectly. This idea and other variations can be used to solve the segmentation problem quite well",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_16"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This idea and other variations can be used to solve the segmentation problem quite well. So instead of worrying about segmentation we’ll concentrate on developing a neural network which can solve the more interesting and difficult problem, namely, recognizing individual handwritten digits. To recognize individual digits we will use a three-layer neural network: The input layer of the network contains neurons encoding the values of the input pixels. As discussed in the next section, our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784 = 28 × 28 neurons. For simplicity I’ve omitted most of the 784 input neurons in the diagram above. The input pixels are greyscale, with a value of 0.0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades of grey. The second layer of the network is a hidden layer. We denote the number of neurons in 1 this hidden layer by n , and we’ll experiment with different values for n . The example shown illustrates a small hidden layer, containing just n = 15 neurons. The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output ≈ 1, then that will indicate that the network thinks the digit is a 0. If the second neuron fires then that will indicate that the network thinks the digit is a 1. And so on. A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation value. If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6. And so on for the other output neurons. You might wonder why we use 10 output neurons. After all, the goal of the network is to tell us which digit (0,1,2, ,9) corresponds to the input image. A seemingly natural way of doing that is to use just 4 output neurons, treating each neuron as taking on a binary value, depending on whether the neuron’s output is closer to 0 or to 1",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_17"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". A seemingly natural way of doing that is to use just 4 output neurons, treating each neuron as taking on a binary value, depending on whether the neuron’s output is closer to 0 or to 1. Four neurons are enough to encode the answer, since 2 4 = 16 is more than the 10 possible values for the input digit. Why should our network use 10 neurons instead? Isn’t that inefficient? The ultimate justification is empirical: we can try out both network designs, and it turns out that, for this particular problem, the network with 10 output neurons learns to recognize digits better than the network with 4 output neurons. But that leaves us wondering why using 10 output neurons works better. Is there some heuristic that would tell us in advance that we should use the 10-output encoding instead of the 4-output encoding? To understand why we do this, it helps to think about what the neural network is doing from first principles. Consider first the case where we use 10 output neurons. Let’s concentrate on the first output neuron, the one that’s trying to decide whether or not the digit is a 0. It does this by weighing up evidence from the hidden layer of neurons. What are those hidden neurons doing? Well, just suppose for the sake of argument that the first neuron in the hidden layer detects whether or not an image like the following is present: It can do this by heavily weighting input pixels which overlap with the image, and only lightly weighting the other inputs. In a similar way, let’s suppose for the sake of argument that the second, third, and fourth neurons in the hidden layer detect whether or not the following images are present: As you may have guessed, these four images together make up the 0 image that we saw in the line of digits shown earlier: So if all four of these hidden neurons are firing then we can conclude that the digit is a 0",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_18"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Of course, that’s not the only sort of evidence we can use to conclude that the image was a 0 – we could legitimately get a 0 in many other ways (say, through translations of the above 1 images, or slight distortions). But it seems safe to say that at least in this case we’d conclude that the input was a 0. Supposing the neural network functions in this way, we can give a plausible explanation for why it’s better to have 10 outputs from the network, rather than 4. If we had 4 outputs, then the first output neuron would be trying to decide what the most significant bit of the digit was. And there’s no easy way to relate that most significant bit to simple shapes like those shown above. It’s hard to imagine that there’s any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output. Now, with all that said, this is all just a heuristic. Nothing says that the three-layer neural network has to operate in the way I described, with the hidden neurons detecting simple component shapes. Maybe a clever learning algorithm will find some assignment of weights that lets us use only 4 output neurons. But as a heuristic the way of thinking I’ve described works pretty well, and can save you a lot of time in designing good neural network architectures. Exercise • There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_19"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 1.5 Learning with gradient descent Now that we have a design for our neural network, how can it learn to recognize digits? The first thing we’ll need is a data set to learn from – a so-called training data set. We’ll use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST’s name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States’ National Institute of Standards and Technology. Here’s a few images from MNIST: 1 As you can see, these digits are, in fact, the same as those shown at the beginning of this chapter as a challenge to recognize. Of course, when testing our network we’ll ask it to recognize images which aren’t in the training set! The MNIST data comes in two parts. The first part contains 60,000 images to be used as training data. These images are scanned handwriting samples from 250 people, half of whom were US Census Bureau employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. The second part of the MNIST data set is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images. We’ll use the test data to evaluate how well our neural network has learned to recognize digits. To make this a good test of performance, the test data was taken from a different set of 250 people than the original training data (albeit still a group split between Census Bureau employees and high school students). This helps give us confidence that our system can recognize digits from people whose writing it didn’t see during training. We’ll use the notation x to denote a training input. It’ll be convenient to regard each training input x as a 28 × 28 = 784-dimensional vector. Each entry in the vector represents the grey value for a single pixel in the image. We’ll denote the corresponding desired output by y = y ( x ) , where y is a 10-dimensional vector",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_20"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Each entry in the vector represents the grey value for a single pixel in the image. We’ll denote the corresponding desired output by y = y ( x ) , where y is a 10-dimensional vector. For example, if a particular training image, x , depicts a 6, then y ( x ) = ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 ) T is the desired output from the network. Note that T here is the transpose operation, turning a row vector into an ordinary (column) vector. What we’d like is an algorithm which lets us find weights and biases so that the output from the network approximates y ( x ) for all training inputs x . To quantify how well we’re achieving this goal we define a cost function 3 : C ( w , b ) ≡ 1 2 n X x ∥ y ( x ) − a ∥ 2 (1.6) Here, w denotes the collection of all weights in the network, b all the biases, n is the total number of training inputs, a is the vector of outputs from the network when x is input, and the sum is over all training inputs, x . Of course, the output a depends on x , w and b , but to keep the notation simple I haven’t explicitly indicated this dependence. The notation ∥ v ∥ just denotes the usual length function for a vector v . We’ll call C the quadratic cost function; it’s also sometimes known as the mean squared error or just MSE. Inspecting the form of the quadratic cost function, we see that C ( w , b ) is non-negative, since every term in the sum is non-negative. Furthermore, the cost C ( w , b ) becomes small, i.e., C ( w , b ) ≈ 0, precisely when y ( x ) is approximately equal to the output, a , for all training inputs, x . So our training algorithm has done a good job if it can find weights and biases so that C ( w , b ) ≈ 0. By contrast, it’s not doing so well when C ( w , b ) is large – that would mean that y ( x ) is not close to the output a for a large number of inputs. So the aim of our training algorithm will be to minimize the cost C ( w , b ) as a function of the weights and biases. In other words, we want to find a set of weights and biases which make the cost as small as possible",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_21"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In other words, we want to find a set of weights and biases which make the cost as small as possible. We’ll do that using an algorithm known as gradient descent . 3 Sometimes referred to as a loss or objective function. We use the term cost function throughout this book, but you should note the other terminology, since it’s often used in research papers and other discussions of neural networks. 1 Why introduce the quadratic cost? After all, aren’t we primarily interested in the number of images correctly classified by the network? Why not try to maximize that number directly, rather than minimizing a proxy measure like the quadratic cost? The problem with that is that the number of images correctly classified is not a smooth function of the weights and biases in the network. For the most part, making small changes to the weights and biases won’t cause any change at all in the number of training images classified correctly. That makes it difficult to figure out how to change the weights and biases to get improved performance. If we instead use a smooth cost function like the quadratic cost it turns out to be easy to figure out how to make small changes in the weights and biases so as to get an improvement in the cost. That’s why we focus first on minimizing the quadratic cost, and only after that will we examine the classification accuracy. Even given that we want to use a smooth cost function, you may still wonder why we choose the quadratic function used in Equation 1.6. Isn’t this a rather ad hoc choice? Perhaps if we chose a different cost function we’d get a totally different set of minimizing weights and biases? This is a valid concern, and later we’ll revisit the cost function, and make some modifications. However, the quadratic cost function of Equation 1.6 works perfectly well for understanding the basics of learning in neural networks, so we’ll stick with it for now. Recapping, our goal in training a neural network is to find weights and biases which minimize the quadratic cost function C ( w , b )",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_22"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Recapping, our goal in training a neural network is to find weights and biases which minimize the quadratic cost function C ( w , b ) . This is a well-posed problem, but it’s got a lot of distracting structure as currently posed – the interpretation of w and b as weights and biases, the σ function lurking in the background, the choice of network architecture, MNIST, and so on. It turns out that we can understand a tremendous amount by ignoring most of that structure, and just concentrating on the minimization aspect. So for now we’re going to forget all about the specific form of the cost function, the connection to neural networks, and so on. Instead, we’re going to imagine that we’ve simply been given a function of many variables and we want to minimize that function. We’re going to develop a technique called gradient descent which can be used to solve such minimization problems. Then we’ll come back to the specific function we want to minimize for neural networks. Okay, let’s suppose we’re trying to minimize some function, C ( v ) . This could be any real-valued function of many variables, v = v 1 , v 2 ,Note that I’ve replaced the w and b notation by v to emphasize that this could be any function – we’re not specifically thinking in the neural networks context any more. To minimize C ( v ) it helps to imagine C as a function of just two variables, which we’ll call v 1 and v 2 : What we’d like is to find where C achieves its global minimum. Now, of course, for the function plotted above, we can eyeball the graph and find the minimum. In that sense, I’ve 1 perhaps shown slightly too simple a function! A general function, C , may be a complicated function of many variables, and it won’t usually be possible to just eyeball the graph to find the minimum. One way of attacking the problem is to use calculus to try to find the minimum analytically. We could compute derivatives and then try using them to find places where C is an extremum",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_23"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". One way of attacking the problem is to use calculus to try to find the minimum analytically. We could compute derivatives and then try using them to find places where C is an extremum. With some luck that might work when C is a function of just one or a few variables. But it’ll turn into a nightmare when we have many more variables. And for neural networks we’ll often want far more variables – the biggest neural networks have cost functions which depend on billions of weights and biases in an extremely complicated way. Using calculus to minimize that just won’t work! (After asserting that we’ll gain insight by imagining C as a function of just two variables, I’ve turned around twice in two paragraphs and said, “hey, but what if it’s a function of many more than two variables?” Sorry about that. Please believe me when I say that it really does help to imagine C as a function of two variables. It just happens that sometimes that picture breaks down, and the last two paragraphs were dealing with such breakdowns. Good thinking about mathematics often involves juggling multiple intuitive pictures, learning when it’s appropriate to use each picture, and when it’s not.) Okay, so calculus doesn’t work. Fortunately, there is a beautiful analogy which suggests an algorithm which works pretty well. We start by thinking of our function as a kind of a valley. If you squint just a little at the plot above, that shouldn’t be too hard. And we imagine a ball rolling down the slope of the valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to find a minimum for the function? We’d randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the valley",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_24"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We could do this simulation simply by computing derivatives (and perhaps some second derivatives) of C – those derivatives would tell us everything we need to know about the local “shape” of the valley, and therefore how our ball should roll. Based on what I’ve just written, you might suppose that we’ll be trying to write down Newton’s equations of motion for the ball, considering the effects of friction and gravity, and so on. Actually, we’re not going to take the ball-rolling analogy quite that seriously – we’re devising an algorithm to minimize C, not developing an accurate simulation of the laws of physics! The ball’s-eye view is meant to stimulate our imagination, not constrain our thinking. So rather than get into all the messy details of physics, let’s simply ask ourselves: if we were declared God for a day, and could make up our own laws of physics, dictating to the ball how it should roll, what law or laws of motion could we pick that would make it so the ball always rolled to the bottom of the valley? To make this question more precise, let’s think about what happens when we move the ball a small amount ∆ v 1 in the v 1 direction, and a small amount ∆ v 2 in the v 2 direction. Calculus tells us that C changes as follows: ∆ C ≈ ∂ C ∂ v 1 ∆ v 1 + ∂ C ∂ v 2 ∆ v 2 . (1.7) We’re going to find a way of choosing ∆ v 1 and ∆ v 2 so as to make ∆ C negative; i.e., we’ll choose them so the ball is rolling down into the valley. To figure out how to make such a choice it helps to define ∆ v to be the vector of changes in v , ∆ v ≡ ( ∆ v 1 , ∆ v 2 ) T , where T is again the transpose operation, turning row vectors into column vectors. We’ll also define 1 the gradient of C to be the vector of partial derivatives, ∂ C ∂ v 1 , ∂ C ∂ v 2 T . We denote the gradient vector by ∇ C , i.e.: ∇ C ≡ ∂ C ∂ v 1 , ∂ C ∂ v 2 T . (1.8) In a moment we’ll rewrite the change ∆ C in terms of ∆ v and the gradient, ∇ C . Before getting to that, though, I want to clarify something that sometimes gets people hung up on the gradient",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_25"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". (1.8) In a moment we’ll rewrite the change ∆ C in terms of ∆ v and the gradient, ∇ C . Before getting to that, though, I want to clarify something that sometimes gets people hung up on the gradient. When meeting the ∇ C notation for the first time, people sometimes wonder how they should think about the ∇ symbol. What, exactly, does ∇ C mean? In fact, it’s perfectly fine to think of ∇ C as a single mathematical object – the vector defined above – which happens to be written using two symbols. In this point of view, ∇ C is just a piece of notational flag-waving, telling you “hey, ∇ C is a gradient vector”. There are more advanced points of view where ∇ C can be viewed as an independent mathematical entity in its own right (for example, as a differential operator), but we won’t need such points of view. With these definitions, the expression 1.7 for ∆ C can be rewritten as ∆ C ≈∇ C · ∆ v (1.9) This equation helps explain why ∇ C is called the gradient vector: ∇ C relates changes in v to changes in C , just as we’d expect something called a gradient to do. But what’s really exciting about the equation is that it lets us see how to choose ∆ v so as to make ∆ C negative. In particular, suppose we choose ∆ v = − η ∇ C , (1.10) where η is a small, positive parameter (known as the learning rate ). Then Equation 1.9 tells us that ∆ C ≈− η ∇ C · ∇ C = − η ∥∇ C ∥ 2 . Because ∥∇ C ∥ 2 ≥ 0, this guarantees that ∆ C ≤ 0, i.e., C will always decrease, never increase, if we change v according to the prescription in 1.10. (Within, of course, the limits of the approximation in Equation 1.9). This is exactly the property we wanted! And so we’ll take Equation 1.10 to define the “law of motion” for the ball in our gradient descent algorithm. That is, we’ll use Equation 1.10 to compute a value for ∆ v , then move the ball’s position v by that amount: v → v ′ = v − η ∇ C . (1.11) Then we’ll use this update rule again, to make another move",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_26"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". That is, we’ll use Equation 1.10 to compute a value for ∆ v , then move the ball’s position v by that amount: v → v ′ = v − η ∇ C . (1.11) Then we’ll use this update rule again, to make another move. If we keep doing this, over and over, we’ll keep decreasing C until – we hope – we reach a global minimum. Summing up, the way the gradient descent algorithm works is to repeatedly compute the gradient ∇ C , and then to move in the opposite direction, “falling down” the slope of the valley. We can visualize it like this: 1 Notice that with this rule gradient descent doesn’t reproduce real physical motion. In real life a ball has momentum, and that momentum may allow it to roll across the slope, or even (momentarily) roll uphill. It’s only after the effects of friction set in that the ball is guaranteed to roll down into the valley. By contrast, our rule for choosing ∆ v just says “go down, right now”. That’s still a pretty good rule for finding the minimum! To make gradient descent work correctly, we need to choose the learning rate η to be small enough that Equation 1.9 is a good approximation. If we don’t, we might end up with ∆ C > 0, which obviously would not be good! At the same time, we don’t want η to be too small, since that will make the changes ∆ v tiny, and thus the gradient descent algorithm will work very slowly. In practical implementations, η is often varied so that Equation 1.9 remains a good approximation, but the algorithm isn’t too slow. We’ll see later how this works. I’ve explained gradient descent when C is a function of just two variables. But, in fact, everything works just as well even when C is a function of many more variables. Suppose in particular that C is a function of m variables, v 1 ,, v m . Then the change ∆ C in C produced by a small change ∆ v = ( ∆ v 1 ,, ∆ v m ) T is ∆ C ≈∇ C · ∆ v , (1.12) where the gradient ∇ C is the vector ∇ C ≡ ∂ C ∂ v 1 ,, ∂ C ∂ v m T",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_27"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Then the change ∆ C in C produced by a small change ∆ v = ( ∆ v 1 ,, ∆ v m ) T is ∆ C ≈∇ C · ∆ v , (1.12) where the gradient ∇ C is the vector ∇ C ≡ ∂ C ∂ v 1 ,, ∂ C ∂ v m T . (1.13) Just as for the two variable case, we can choose ∆ v = − η ∇ C , (1.14) and we’re guaranteed that our (approximate) expression 1.12 for ∆ C will be negative. This gives us a way of following the gradient to a minimum, even when C is a function of many 1 variables, by repeatedly applying the update rule v → v ′ = v − η ∇ C . (1.15) You can think of this update rule as defining the gradient descent algorithm. It gives us a way of repeatedly changing the position v in order to find a minimum of the function C . The rule doesn’t always work – several things can go wrong and prevent gradient descent from finding the global minimum of C , a point we’ll return to explore in later chapters. But, in practice gradient descent often works extremely well, and in neural networks we’ll find that it’s a powerful way of minimizing the cost function, and so helping the net learn. Indeed, there’s even a sense in which gradient descent is the optimal strategy for searching for a minimum. Let’s suppose that we’re trying to make a move ∆ v in position so as to decrease C as much as possible. This is equivalent to minimizing ∆ C ≈∇ C · ∆ v . We’ll constrain the size of the move so that ∥ ∆ v ∥ = ε for some small fixed ε > 0. In other words, we want a move that is a small step of a fixed size, and we’re trying to find the movement direction which decreases C as much as possible. It can be proved that the choice of ∆ v which minimizes ∇ C · ∆ v is ∆ v = − η ∇ C , where η = ε/ ∥∇ C ∥ is determined by the size constraint ∥ ∆ v ∥ = ε . So gradient descent can be viewed as a way of taking small steps in the direction which does the most to immediately decrease C . Exercises • Prove the assertion of the last paragraph. Hint: If you’re not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_28"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Exercises • Prove the assertion of the last paragraph. Hint: If you’re not already familiar with the Cauchy-Schwarz inequality, you may find it helpful to familiarize yourself with it. • I explained gradient descent when C is a function of two variables, and when it’s a function of more than two variables. What happens when C is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case? People have investigated many variations of gradient descent, including variations that more closely mimic a real physical ball. These ball-mimicking variations have some advantages, but also have a major disadvantage: it turns out to be necessary to compute second partial derivatives of C , and this can be quite costly. To see why it’s costly, suppose we want to compute all the second partial derivatives ∂ 2 C /∂ v j ∂ v k . If there are a million such v j variables then we’d need to compute something like a trillion (i.e., a million squared) second partial derivatives 4 ! That’s going to be computationally costly. With that said, there are tricks for avoiding this kind of problem, and finding alternatives to gradient descent is an active area of investigation. But in this book we’ll use gradient descent (and variations) as our main approach to learning in neural networks. How can we apply gradient descent to learn in a neural network? The idea is to use gradient descent to find the weights w k and biases b l which minimize the cost in Equation 1.6. To see how this works, let’s restate the gradient descent update rule, with the weights and biases replacing the variables v j . In other words, our “position” now has components w k and b l , and the gradient vector ∇ C has corresponding components ∂ C /∂ w k and ∂ C /∂ b l . 4 Actually, more like half a trillion, since ∂ 2 C /∂ v j ∂ v k = ∂ 2 C /∂ v k ∂ v j . Still, you get the point",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_29"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". 4 Actually, more like half a trillion, since ∂ 2 C /∂ v j ∂ v k = ∂ 2 C /∂ v k ∂ v j . Still, you get the point. 1 Writing out the gradient descent update rule in terms of components, we have w k → w ′ k = w k − η ∂ C ∂ w k (1.16) b l → b ′ l = b l − η ∂ C ∂ b l . (1.17) By repeatedly applying this update rule we can “roll down the hill”, and hopefully find a minimum of the cost function. In other words, this is a rule which can be used to learn in a neural network. There are a number of challenges in applying the gradient descent rule. We’ll look into those in depth in later chapters. But for now I just want to mention one problem. To understand what the problem is, let’s look back at the quadratic cost in Equation 1.6. Notice that this cost function has the form C = 1 n P x C x , that is, it’s an average over costs C x ≡ ∥ y ( x ) − a ∥ 2 2 for individual training examples. In practice, to compute the gradient ∇ C we need to compute the gradients ∇ C x separately for each training input, x, and then average them, ∇ C = 1 n P x ∇ C x . Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs slowly. An idea called stochastic gradient descent can be used to speed up learning. The idea is to estimate the gradient ∇ C by computing ∇ C x for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient ∇ C , and this helps speed up gradient descent, and thus learning. To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number m of randomly chosen training inputs. We’ll label those random training inputs X 1 , X 2 ,, X m , and refer to them as a mini-batch",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_30"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll label those random training inputs X 1 , X 2 ,, X m , and refer to them as a mini-batch. Provided the sample size m is large enough we expect that the average value of the ∇ C X j will be roughly equal to the average over all ∇ C x , that is, P m j = 1 ∇ C X j m ≈ P x ∇ C x n = ∇ C , (1.18) where the second sum is over the entire set of training data. Swapping sides we get ∇ C ≈ 1 m m X j = 1 ∇ C X j , (1.19) confirming that we can estimate the overall gradient by computing gradients just for the randomly chosen mini-batch. To connect this explicitly to learning in neural networks, suppose wk and bl denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those, w k → w ′ k = w k − η m X j ∂ C X j ∂ w k (1.20) b l → b ′ l = b l − η m X j ∂ C X j ∂ b l , (1.21) 1 where the sums are over all the training examples X j in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we’ve exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch. Incidentally, it’s worth noting that conventions vary about scaling of the cost function and of mini-batch updates to the weights and biases. In Equation 1.6 we scaled the overall cost function by a factor 1 n . People sometimes omit the 1 n , summing over the costs of individual training examples instead of averaging. This is particularly useful when the total number of training examples isn’t known in advance. This can occur if more training data is being generated in real time, for instance. And, in a similar way, the mini-batch update rules 1.20 and 1.21 sometimes omit the 1 m term out the front of the sums. Conceptually this makes little difference, since it’s equivalent to rescaling the learning rate η . But when doing detailed comparisons of different work it’s worth watching out for",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_31"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Conceptually this makes little difference, since it’s equivalent to rescaling the learning rate η . But when doing detailed comparisons of different work it’s worth watching out for. We can think of stochastic gradient descent as being like political polling: it’s much easier to sample a small mini-batch than it is to apply gradient descent to the full batch, just as carrying out a poll is easier than running a full election. For example, if we have a training set of size n = 60,000, as in MNIST, and choose a mini-batch size of (say) m = 10, this means we’ll get a factor of 6,000 speedup in estimating the gradient! Of course, the estimate won’t be perfect – there will be statistical fluctuations – but it doesn’t need to be perfect: all we really care about is moving in a general direction that will help decrease C, and that means we don’t need an exact computation of the gradient. In practice, stochastic gradient descent is a commonly used and powerful technique for learning in neural networks, and it’s the basis for most of the learning techniques we’ll develop in this book. Exercize • An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, x , we update our weights and biases according to the rules w k → w ′ k = w k − η∂ C x /∂ w k and b l → b ′ l = b l − η∂ C x /∂ b l . Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20. Let me conclude this section by discussing a point that sometimes bugs people new to gradient descent. In neural networks the cost C is, of course, a function of many variables – all the weights and biases – and so in some sense defines a surface in a very high-dimensional space",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_32"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In neural networks the cost C is, of course, a function of many variables – all the weights and biases – and so in some sense defines a surface in a very high-dimensional space. Some people get hung up thinking: “Hey, I have to be able to visualize all these extra dimensions”. And they may start to worry: “I can’t think in four dimensions, let alone five (or five million)”. Is there some special ability they’re missing, some ability that “real” supermathematicians have? Of course, the answer is no. Even most professional mathematicians can’t visualize four dimensions especially well, if at all. The trick they use, instead, is to develop other ways of representing what’s going on. That’s exactly what we did above: we used an algebraic (rather than visual) representation of ∆ C to figure out how to move so as to decrease C . People who are good at thinking in high dimensions have a mental library containing many different techniques along these lines; our algebraic trick is just one example. Those techniques may not have the simplicity we’re accustomed to when visualizing three dimensions, but once you build up a library of such techniques, you can get pretty good at thinking in high dimensions. I won’t go into more detail here, but if you’re 1 interested then you may enjoy reading this discussion of some of the techniques professional mathematicians use to think in high dimensions. While some of the techniques discussed are quite complex, much of the best content is intuitive and accessible, and could be mastered by anyone. 1.6 Implementing our network to classify digits Alright, let’s write a program that learns how to recognize handwritten digits, using stochastic gradient descent and the MNIST training data. We’ll do this with a short Python (2.7) program, just 74 lines of code! The first thing we need is to get the MNIST data",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_33"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll do this with a short Python (2.7) program, just 74 lines of code! The first thing we need is to get the MNIST data. If you’re a git user then you can obtain the data by cloning the code repository for this book, git clone https://github.com/mnielsen/neural-networks - and -deep-learning.git If you don’t use git then you can download the data and code here. Incidentally, when I described the MNIST data earlier, I said it was split into 60,000 training images, and 10,000 test images. That’s the official MNIST description. Actually, we’re going to split the data a little differently. We’ll leave the test images as is, but split the 60,000-image MNIST training set into two parts: a set of 50,000 images, which we’ll use to train our neural network, and a separate 10,000 image validation set. We won’t use the validation data in this chapter, but later in the book we’ll find it useful in figuring out how to set certain hyper-parameters of the neural network – things like the learning rate, and so on, which aren’t directly selected by our learning algorithm. Although the validation data isn’t part of the original MNIST specification, many people use MNIST in this fashion, and the use of validation data is common in neural networks. When I refer to the “MNIST training data” from now on, I’ll be referring to our 50,000 image data set, not the original 60,000 image data set 5 . Apart from the MNIST data we also need a Python library called Numpy, for doing fast linear algebra. If you don’t already have Numpy installed, you can get it here. Let me explain the core features of the neural networks code, before giving a full listing, below. The centerpiece is a Network class, which we use to represent a neural network",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_34"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Let me explain the core features of the neural networks code, before giving a full listing, below. The centerpiece is a Network class, which we use to represent a neural network. Here’s the code we use to initialize a Network object: class Network( object ): def __init__(self, sizes): self.num_layers = len (sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip (sizes[:-1], sizes[1:])] In this code, the list sizes contains the number of neurons in the respective layers. So, for example, if we want to create a Network object with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer, we’d do this with the code: 5 As noted earlier, the MNIST data set is based on two data sets collected by NIST, the United States’ National Institute of Standards and Technology. To construct MNIST the NIST data sets were stripped down and put into a more convenient format by Yann LeCun, Corinna Cortes, and Christopher J. C. Burges. See this link for more details. The data set in my repository is in a form that makes it easy to load and manipulate the MNIST data in Python. I obtained this particular form of the data from the LISA machine learning laboratory at the University of Montreal (link). 1 net = Network([2, 3, 1]) The biases and weights in the Network object are all initialized randomly, using the Numpy np.random.randn function to generate Gaussian distributions with mean 0 and standard deviation 1. This random initialization gives our stochastic gradient descent algorithm a place to start from. In later chapters we’ll find better ways of initializing the weights and biases, but this will do for now. Note that the Network initialization code assumes that the first layer of neurons is an input layer, and omits to set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. Note also that the biases and weights are stored as lists of Numpy matrices",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_35"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Note also that the biases and weights are stored as lists of Numpy matrices. So, for example net.weights[1] is a Numpy matrix storing the weights connecting the second and third layers of neurons. (It’s not the first and second layers, since Python’s list indexing starts at 0.) Since net.weights[1] is rather verbose, let’s just denote that matrix w . It’s a matrix such that w jk is the weight for the connection between the k -th neuron in the second layer, and the j -th neuron in the third layer. This ordering of the j and k indices may seem strange – surely it’d make more sense to swap the j and k indices around? The big advantage of using this ordering is that it means that the vector of activations of the third layer of neurons is: a ′ = σ ( wa + b ) . (1.22) There’s quite a bit going on in this equation, so let’s unpack it piece by piece. a is the vector of activations of the second layer of neurons. To obtain a ′ we multiply a by the weight matrix w , and add the vector b of biases. We then apply the function σ elementwise to every entry in the vector wa + b 6 . It’s easy to verify that Equation 1.22 gives the same result as our earlier rule, Equation 1.4, for computing the output of a sigmoid neuron. Exercise • Write out Equation 1.22 in component form, and verify that it gives the same result as the rule 1.4 for computing the output of a sigmoid neuron. With all this in mind, it’s easy to write code computing the output from a Network instance. We begin by defining the sigmoid function: def sigmoid(z): return 1.0/(1.0+np.exp(-z)) Note that when the input z is a vector or Numpy array, Numpy automatically applies the function sigmoid elementwise, that is, in vectorized form. We then add a feedforward method to the Network class, which, given an input a for the network, returns the corresponding output 7",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_36"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We then add a feedforward method to the Network class, which, given an input a for the network, returns the corresponding output 7 . All the method does is applies Equation 1.22 for each layer: def feedforward(self, a): \"\"\"Return the output of the network if \"a\" is input.\"\"\" for b, w in zip (self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) 6 This is called vectorizing the function σ . 7 It is assumed that the input a is an (n, 1) Numpy ndarray, not a (n,) vector. Here, n is the number of inputs to the network. If you try to use an (n,) vector as input you’ll get strange results. Although using an (n,) vector appears the more natural choice, using an (n, 1) ndarray makes it particularly easy to modify the code to feedforward multiple inputs at once, and that is sometimes convenient. 1 return a Of course, the main thing we want our Network objects to do is to learn. To that end we’ll give them an SGD method which implements stochastic gradient descent. Here’s the code. It’s a little mysterious in a few places, but I’ll break it down below, after the listing. def SGD(self, training_data , epochs, mini_batch_size , eta, test_data=None): \"\"\"Train the neural network using mini-batch stochastic gradient descent. The \"training_data\" is a list of tuples \"(x, y)\" representing the training inputs and the desired outputs. The other non-optional parameters are selexplanatory. If \"test_data\" is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress , but slows things down substantially. \"\"\" if test_data: n_test = len (test_data) n = len (training_data) for j in xrange (epochs): random.shuffle(training_data) mini_batches = [training_data[k:k+mini_batch_size] for k in xrange (0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch , eta) if test_data: print \"Epoch {0}: {1} / {2}\" . format (j, self.evaluate(test_data), n_test) else : print \"Epoch {0} complete\"",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_37"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". format (j, self.evaluate(test_data), n_test) else : print \"Epoch {0} complete\" . format (j) The training_data is a list of tuples (x, y) representing the training inputs and corrsponding desired outputs. The variables epochs and mini_batch_size are what you’d expect – the number of epochs to train for, and the size of the mini-batches to use when sampling. eta is the learning rate, η . If the optional argument test_data is supplied, then the program will evaluate the network after each epoch of training, and print out partial progress. This is useful for tracking progress, but slows things down substantially. The code works as follows. In each epoch, it starts by randomly shuffling the training data, and then partitions it into mini-batches of the appropriate size. This is an easy way of sampling randomly from the training data. Then for each mini_batch we apply a single step of gradient descent. This is done by the code self.update_mini_batch(mini_batch , eta) , which updates the network weights and biases according to a single iteration of gradient descent, using just the training data in mini_batch . Here’s the code for the update_mini_batch method: def update_mini_batch(self, mini_batch , eta): \"\"\"Update the network’s weights and biases by applying gradient descent using backpropagation to a single mini batch",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_38"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\" is the learning rate.\"\"\" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b , delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)] self.weights = [w-(eta/ len (mini_batch))*nw for w, nw in zip (self.weights , nabla_w)] self.biases = [b-(eta/ len (mini_batch))*nb for b, nb in zip (self.biases , nabla_b )] 1 Most of the work is done by the line delta_nabla_b , delta_nabla_w = self.backprop(x, y) This invokes something called the backpropagation algorithm, which is a fast way of compuing the gradient of the cost function. So update_mini_batch works simply by computing these gradients for every training example in the mini_batch , and then updating self .weights and self.biases appropriately. I’m not going to show the code for self.backprop right now. We’ll study how bacpropagation works in the next chapter, including the code for self.backprop . For now, just assume that it behaves as claimed, returning the appropriate gradient for the cost associated to the training example x. Let’s look at the full program, including the documentation strings, which I omitted above. Apart from self.backprop the program is self-explanatory – all the heavy lifing is done in self.SGD and self.update_mini_batch , which we’ve already discussed. The self.backprop method makes use of a few extra functions to help in computing the gradient, namely sigmoid_prime , which computes the derivative of the σ function, and self.cost_derivative , which I won’t describe here. You can get the gist of these (and perhaps the details) just by looking at the code and documentation strings. We’ll look at them in detail in the next chapter. Note that while the program appears lengthy, much of the code is documentation strings intended to make the code easy to understand",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_39"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll look at them in detail in the next chapter. Note that while the program appears lengthy, much of the code is documentation strings intended to make the code easy to understand. In fact, the program contains just 74 lines of non-whitespace, non-comment code. All the code may be found on GitHub here. \"\"\" network.py ~~~~~~~~~~ A module to implement the stochastic gradient descent learning algorithm for a feedforward neural network. Gradients are calculated using backpropagation. Note that I have focused on making the code simple , easily readable , and easily modifiable. It is not optimized , and omits many desirable features. \"\"\" #### Libraries # Standard library import random # Third-party libraries import numpy as np class Network( object ): def __init__(self, sizes): \"\"\"The list ‘‘sizes‘‘ contains the number of neurons in the respective layers of the network. For example , if the list was [2, 3, 1] then it would be a three-layer network , with the first layer containing 2 neurons , the second layer 3 neurons , and the third layer 1 neuron. The biases and weights for the network are initialized randomly , using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won’t set any biases for those neurons , since biases are only ever used in computing the outputs from later layers.\"\"\" self.num_layers = len (sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip (sizes[:-1], sizes[1:])] 1 def feedforward(self, a): \"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\" for b, w in zip (self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data , epochs, mini_batch_size , eta, test_data=None): \"\"\"Train the neural network using mini-batch stochastic gradient descent. The ‘‘training_data ‘‘ is a list of tuples ‘‘(x, y)‘‘ representing the training inputs and the desired outputs",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_40"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". The ‘‘training_data ‘‘ is a list of tuples ‘‘(x, y)‘‘ representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ‘‘test_data ‘‘ is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress , but slows things down substantially.\"\"\" if test_data: n_test = len (test_data) n = len (training_data) for j in xrange (epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange (0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch , eta) if test_data: print \"Epoch {0}: {1} / {2}\" . format ( j, self.evaluate(test_data), n_test) else : print \"Epoch {0} complete\" . format (j) def update_mini_batch(self, mini_batch , eta): \"\"\"Update the network’s weights and biases by applying gradient descent using backpropagation to a single mini batch. The ‘‘mini_batch ‘‘ is a list of tuples ‘‘(x, y)‘‘, and ‘‘eta‘‘ is the learning rate.\"\"\" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b , delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip (nabla_b , delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip (nabla_w , delta_nabla_w)] self.weights = [w-(eta/ len (mini_batch))*nw for w, nw in zip (self.weights , nabla_w)] self.biases = [b-(eta/ len (mini_batch))*nb for b, nb in zip (self.biases, nabla_b)] def backprop(self, x, y): \"\"\"Return a tuple ‘‘(nabla_b , nabla_w)‘‘ representing the gradient for the cost function C_x",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_41"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". ‘‘nabla_b ‘‘ and ‘‘nabla_w ‘‘ are layer-by-layer lists of numpy arrays , similar to ‘‘self.biases‘‘ and ‘‘self.weights ‘‘.\"\"\" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations , layer by layer zs = [] # list to store all the z vectors , layer by layer for b, w in zip (self.biases, self.weights): z = np.dot(w, activation)+b 1 zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons , l = 2 is the # second-last layer, and so on. It’s a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange (2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b , nabla_w) def evaluate(self, test_data): \"\"\"Return the number of test inputs for which the neural network outputs the correct result",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_42"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Note that the neural network’s output is assumed to be the index of whichever neuron in the final layer has the highest activation.\"\"\" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum ( int (x == y) for (x, y) in test_results) def cost_derivative(self, output_activations , y): \"\"\"Return the vector of partial derivatives \\partial C_x / \\partial a for the output activations.\"\"\" return (output_activations -y) #### Miscellaneous functions def sigmoid(z): \"\"\"The sigmoid function.\"\"\" return 1.0/(1.0+np.exp(-z)) def sigmoid_prime(z): \"\"\"Derivative of the sigmoid function.\"\"\" return sigmoid(z)*(1-sigmoid(z)) How well does the program recognize handwritten digits? Well, let’s start by loading in the MNIST data. I’ll do this using a little helper program, mnist_loader.py , to be described below. We execute the following commands in a Python shell, >>> import mnist_loader >>> training_data , validation_data , test_data = mnist_loader.load_data_wrapper() Of course, this could also be done in a separate Python program, but if you’re following along it’s probably easiest to do in a Python shell. After loading the MNIST data, we’ll set up a Network with 30 hidden neurons. We do this after importing the Python program listed above, which is named network, >>> import network >>> net = network.Network([784, 30, 10]) 1 Finally, we’ll use stochastic gradient descent to learn from the MNIST training_data over 30 epochs, with a mini-batch size of 10, and a learning rate of η = 3.0, >>> net.SGD(training_data , 30, 10, 3.0, test_data=test_data) Note that if you’re running the code as you read along, it will take some time to execute – for a typical machine (as of 2015) it will likely take a few minutes to run. I suggest you set things running, continue to read, and periodically check the output from the code. If you’re in a rush you can speed things up by decreasing the number of epochs, by decreasing the number of hidden neurons, or by using only part of the training data",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_43"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". If you’re in a rush you can speed things up by decreasing the number of epochs, by decreasing the number of hidden neurons, or by using only part of the training data. Note that production code would be much, much faster: these Python scripts are intended to help you understand how neural nets work, not to be high-performance code! And, of course, once we’ve trained a network it can be run very quickly indeed, on almost any computing platform. For example, once we’ve learned a good set of weights and biases for a network, it can easily be ported to run in Javascript in a web browser, or as a native app on a mobile device. In any case, here is a partial transcript of the output of one training run of the neural network. The transcript shows the number of test images correctly recognized by the neural network after each epoch of training. As you can see, after just a single epoch this has reached 9,129 out of 10,000, and the number continues to grow, Epoch 0: 9129 / 10000 Epoch 1: 9295 / 10000 Epoch 2: 9348 / 10000 Epoch 27: 9528 / 10000 Epoch 28: 9542 / 10000 Epoch 29: 9534 / 10000 That is, the trained network gives us a classification rate of about 95 percent – 95.42 percent at its peak (“Epoch 28”)! That’s quite encouraging as a first attempt. I should warn you, however, that if you run the code then your results are not necessarily going to be quite the same as mine, since we’ll be initializing our network using (different) random weights and biases. To generate results in this chapter I’ve taken best-of-three runs. Let’s rerun the above experiment, changing the number of hidden neurons to 100. As was the case earlier, if you’re running the code as you read along, you should be warned that it takes quite a while to execute (on my machine this experiment takes tens of seconds for each training epoch), so it’s wise to continue reading in parallel while the code executes",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_44"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". >>> net = network.Network([784, 100, 10]) >>> net.SGD(training_data , 30, 10, 3.0, test_data=test_data) Sure enough, this improves the results to 96.59 percent. At least in this case, using more hidden neurons helps us get better results 8 Of course, to obtain these accuracies I had to make specific choices for the number of epochs of training, the mini-batch size, and the learning rate, η . As I mentioned above, these are known as hyper-parameters for our neural network, in order to distinguish them from the parameters (weights and biases) learnt by our learning algorithm. If we choose our 8 Reader feedback indicates quite some variation in results for this experiment, and some training runs give results quite a bit worse. Using the techniques introduced in chapter 3 will greatly reduce the variation in performance across different training runs for our networks. 1 hyper-parameters poorly, we can get bad results. Suppose, for example, that we’d chosen the learning rate to be η = 0.001, >>> net = network.Network([784, 100, 10]) >>> net.SGD(training_data , 30, 10, 0.001, test_data=test_data) The results are much less encouraging, Epoch 0: 1139 / 10000 Epoch 1: 1136 / 10000 Epoch 2: 1135 / 10000 Epoch 27: 2101 / 10000 Epoch 28: 2123 / 10000 Epoch 29: 2142 / 10000 However, you can see that the performance of the network is getting slowly better over time. That suggests increasing the learning rate, say to η = 0 . 01. If we do that, we get better results, which suggests increasing the learning rate again. (If making a change improves things, try doing more!) If we do that several times over, we’ll end up with a learning rate of something like η = 1 . 0 (and perhaps fine tune to 3.0), which is close to our earlier experiments. So even though we initially made a poor choice of hyper-parameters, we at least got enough information to help us improve our choice of hyper-parameters. In general, debugging a neural network can be challenging",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_45"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". In general, debugging a neural network can be challenging. This is especially true when the initial choice of hyper-parameters produces results no better than random noise. Suppose we try the successful 30 hidden neuron network architecture from earlier, but with the learning rate changed to η = 100.0: >>> net = network.Network([784, 30, 10]) >>> net.SGD(training_data , 30, 10, 100.0, test_data=test_data) At this point we’ve actually gone too far, and the learning rate is too high: Epoch 0: 1009 / 10000 Epoch 1: 1009 / 10000 Epoch 2: 1009 / 10000 Epoch 3: 1009 / 10000 Epoch 27: 982 / 10000 Epoch 28: 982 / 10000 Epoch 29: 982 / 10000 Now imagine that we were coming to this problem for the first time. Of course, we know from our earlier experiments that the right thing to do is to decrease the learning rate. But if we were coming to this problem for the first time then there wouldn’t be much in the output to guide us on what to do. We might worry not only about the learning rate, but about every other aspect of our neural network. We might wonder if we’ve initialized the weights and biases in a way that makes it hard for the network to learn? Or maybe we don’t have enough training data to get meaningful learning? Perhaps we haven’t run for enough epochs? Or maybe it’s impossible for a neural network with this architecture to learn to recognize handwritten digits? Maybe the learning rate is too low? Or, maybe, the learning rate is too high? When you’re coming to a problem for the first time, you’re not always sure. The lesson to take away from this is that debugging a neural network is not trivial, and, just as for ordinary programming, there is an art to it. You need to learn that art of debugging 1 in order to get good results from neural networks. More generally, we need to develop heuristics for choosing good hyper-parameters and a good architecture. We’ll discuss all these at length through the book, including how I chose the hyper-parameters above",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_46"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". We’ll discuss all these at length through the book, including how I chose the hyper-parameters above. Exercise • Try creating a network with just two layers – an input and an output layer, no hidden layer – with 784 and 10 neurons, respectively. Train the network using stochastic gradient descent. What classification accuracy can you achieve? Earlier, I skipped over the details of how the MNIST data is loaded. It’s pretty straightforward. For completeness, here’s the code. The data structures used to store the MNIST data are described in the documentation strings – it’s straightforward stuff, tuples and lists of Numpy ndarray objects (think of them as vectors if you’re not familiar with ndarrays): \"\"\" mnist_loader ~~~~~~~~~~~~ A library to load the MNIST image data. For details of the data structures that are returned , see the doc strings for ‘‘load_data ‘‘ and ‘‘load_data_wrapper ‘‘. In practice , ‘‘load_data_wrapper ‘‘ is the function usually called by our neural network code. \"\"\" #### Libraries # Standard library import cPickle import gzip # Third-party libraries import numpy as np def load_data(): \"\"\"Return the MNIST data as a tuple containing the training data, the validation data, and the test data. The ‘‘training_data ‘‘ is returned as a tuple with two entries. The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image. The second entry in the ‘‘training_data ‘‘ tuple is a numpy ndarray containing 50,000 entries. Those entries are just the digit values (09) for the corresponding images contained in the first entry of the tuple. The ‘‘validation_data ‘‘ and ‘‘test_data ‘‘ are similar , except each contains only 10,000 images. This is a nice data format, but for use in neural networks it’s helpful to modify the format of the ‘‘training_data ‘‘ a little. That’s done in the wrapper function ‘‘load_data_wrapper()‘‘, see below. \"\"\" f = gzip",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_47"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". That’s done in the wrapper function ‘‘load_data_wrapper()‘‘, see below. \"\"\" f = gzip. open ( ’../data/mnist.pkl.gz’ , ’rb’ ) training_data , validation_data , test_data = cPickle.load(f) f.close() return (training_data , validation_data , test_data) 1 def load_data_wrapper(): \"\"\"Return a tuple containing ‘‘(training_data , validation_data , test_data)‘‘. Based on ‘‘load_data ‘‘, but the format is more convenient for use in our implementation of neural networks. In particular , ‘‘training_data ‘‘ is a list containing 50,000 2-tuples ‘‘(x, y)‘‘. ‘‘x‘‘ is a 784-dimensional numpy.ndarray containing the input image. ‘‘y‘‘ is a 10-dimensional numpy.ndarray representing the unit vector corresponding to the correct digit for ‘‘x‘‘. ‘‘validation_data ‘‘ and ‘‘test_data ‘‘ are lists containing 10,000 2-tuples ‘‘(x, y)‘‘. In each case, ‘‘x‘‘ is a 784-dimensional numpy.ndarry containing the input image, and ‘‘y‘‘ is the corresponding classification , i.e., the digit values (integers) corresponding to ‘‘x‘‘. Obviously , this means we’re using slightly different formats for the training data and the validation / test data. These formats turn out to be the most convenient for use in our neural network code.\"\"\" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = zip (training_inputs , training_results) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = zip (validation_inputs , va_d[1]) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = zip (test_inputs , te_d[1]) return (training_data , validation_data , test_data) def vectorized_result(j): \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (09) into a corresponding desired output from the neural network.\"\"\" e = np.zeros((10, 1)) e[j] = 1.0 return e I said above that our program gets pretty good results",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_48"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". This is used to convert a digit (09) into a corresponding desired output from the neural network.\"\"\" e = np.zeros((10, 1)) e[j] = 1.0 return e I said above that our program gets pretty good results. What does that mean? Good compared to what? It’s informative to have some simple (non-neural-network) baseline tests to compare against, to understand what it means to perform well. The simplest baseline of all, of course, is to randomly guess the digit. That’ll be right about ten percent of the time. We’re doing much better than that! What about a less trivial baseline? Let’s try an extremely simple idea: we’ll look at how dark an image is. For instance, an image of a 2 will typically be quite a bit darker than an image of a 1, just because more pixels are blackened out, as the following examples illustrate: This suggests using the training data to compute average darknesses for each digit, 0,1,2, ,9. When presented with a new image, we compute how dark the image is, and then guess that it’s whichever digit has the closest average darkness. This is a simple procedure, and is easy to code up, so I won’t explicitly write out the code – if you’re interested it’s in the GitHub 1 repository. But it’s a big improvement over random guessing, getting 2,225 of the 10,000 test images correct, i.e., 22.25 percent accuracy. It’s not difficult to find other ideas which achieve accuracies in the 20 to 50 percent range. If you work a bit harder you can get up over 50 percent. But to get much higher accuracies it helps to use established machine learning algorithms. Let’s try using one of the best known algorithms, the support vector machine or SVM . If you’re not familiar with SVMs, not to worry, we’re not going to need to understand the details of how SVMs work. Instead, we’ll use a Python library called scikit-learn, which provides a simple Python interface to a fast C-based library for SVMs known as LIBSVM. If we run scikit-learn’s SVM classifier using the default settings, then it gets 9,435 of 10,000 test images correct",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_49"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". If we run scikit-learn’s SVM classifier using the default settings, then it gets 9,435 of 10,000 test images correct. (The code is available here.) That’s a big improvement over our naive approach of classifying an image based on how dark it is. Indeed, it means that the SVM is performing roughly as well as our neural networks, just a little worse. In later chapters we’ll introduce new techniques that enable us to improve our neural networks so that they perform much better than the SVM. That’s not the end of the story, however. The 9,435 of 10,000 result is for scikit-learn’s default settings for SVMs. SVMs have a number of tunable parameters, and it’s possible to search for parameters which improve this out-of-the-box performance. I won’t explicitly do this search, but instead refer you to this blog post by Andreas Müller if you’d like to know more. Mueller shows that with some work optimizing the SVM’s parameters it’s possible to get the performance up above 98.5 percent accuracy. In other words, a well-tuned SVM only makes an error on about one digit in 70. That’s pretty good! Can neural networks do better? In fact, they can. At present, well-designed neural networks outperform every other technique for solving MNIST, including SVMs. The current (2013) record is classifying 9,979 of 10,000 images correctly. This was done by Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. We’ll see most of the techniques they used later in the book. At that level the performance is close to human-equivalent, and is arguably better, since quite a few of the MNIST images are difficult even for humans to recognize with confidence, for example: I trust you’ll agree that those are tough to classify! With images like these in the MNIST data set it’s remarkable that neural networks can accurately classify all but 21 of the 10,000 test images. Usually, when programming we believe that solving a complicated problem like recognizing the MNIST digits requires a sophisticated algorithm",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_50"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Usually, when programming we believe that solving a complicated problem like recognizing the MNIST digits requires a sophisticated algorithm. But even the neural networks in the Wan et al paper just mentioned involve quite simple algorithms, variations on the algorithm we’ve seen in this chapter. All the complexity is learned, automatically, from the training data. In some sense, the moral of both our results and those in more sophisticated papers, is that for some problems: sophisticated algorithm ≤ simple learning algorithm + good training data. 1 Figure 1.1: Credits: 1. Ester Inbar. 2. Unknown. 3. NASA, ESA, G. Illingworth, D. Magee, and P. Oesch (University of California, Santa Cruz), R. Bouwens (Leiden University), and the HUDF09 Team. 1.7 Toward deep learning While our neural network gives impressive performance, that performance is somewhat mysterious. The weights and biases in the network were discovered automatically. And that means we don’t immediately have an explanation of how the network does what it does. Can we find some way to understand the principles by which our network is classifying handwritten digits? And, given such principles, can we do better? To put these questions more starkly, suppose that a few decades hence neural networks lead to artificial intelligence (AI). Will we understand how such intelligent networks work? Perhaps the networks will be opaque to us, with weights and biases we don’t understand, because they’ve been learned automatically. In the early days of AI research people hoped that the effort to build an AI would also help us understand the principles behind intelligence and, maybe, the functioning of the human brain. But perhaps the outcome will be that we end up understanding neither the brain nor how artificial intelligence works! To address these questions, let’s think back to the interpretation of artificial neurons that I gave at the start of the chapter, as a means of weighing evidence",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_51"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Suppose we want to determine whether an image shows a human face or not: We could attack this problem the same way we attacked handwriting recognition – by using the pixels in the image as input to a neural network, with the output from the network a single neuron indicating either “Yes, it’s a face” or “No, it’s not a face”. Let’s suppose we do this, but that we’re not using a learning algorithm. Instead, we’re going to try to design a network by hand, choosing appropriate weights and biases. How might we go about it? Forgetting neural networks entirely for the moment, a heuristic we could use is to decompose the problem into sub-problems: does the image have an eye in the top left? Does it have an eye in the top right? Does it have a nose in the middle? Does it have a mouth in the bottom middle? Is there hair on top? And so on. If the answers to several of these questions are “yes”, or even just “probably yes”, then we’d conclude that the image is likely to be a face. Conversely, if the answers to most of the questions are “no”, then the image probably isn’t a face. Of course, this is just a rough heuristic, and it suffers from many deficiencies. Maybe the person is bald, so they have no hair. Maybe we can only see part of the face, or the face is at an angle, so some of the facial features are obscured. Still, the heuristic suggests that if we can solve the sub-problems using neural networks, then perhaps we can build a neural network for face-detection, by combining the networks for the sub-problems. Here’s 1 a possible architecture, with rectangles denoting the sub-networks. Note that this isn’t intended as a realistic approach to solving the face-detection problem; rather, it’s to help us build intuition about how networks function. Here’s the architecture: It’s also plausible that the sub-networks can be decomposed",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_52"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Here’s the architecture: It’s also plausible that the sub-networks can be decomposed. Suppose we’re considering the question: “Is there an eye in the top left?” This can be decomposed into questions such as: “Is there an eyebrow?”; “Are there eyelashes?”; “Is there an iris?”; and so on. Of course, these questions should really include positional information, as well–“Is the eyebrow in the top left, and above the iris?”, that kind of thing – but let’s keep it simple. The network to answer the question “Is there an eye in the top left?” can now be decomposed: Those questions too can be broken down, further and further through multiple layers. Ultimately, we’ll be working with sub-networks that answer questions so simple they can easily be answered at the level of single pixels. Those questions might, for example, be about the presence or absence of very simple shapes at particular points in the image. Such questions can be answered by single neurons connected to the raw pixels in the image. 1 The end result is a network which breaks down a very complicated question – does this image show a face or not – into very simple questions answerable at the level of single pixels. It does this through a series of many layers, with early layers answering very simple and specific questions about the input image, and later layers building up a hierarchy of ever more complex and abstract concepts. Networks with this kind of many-layer structure – two or more hidden layers – are called deep neural networks . Of course, I haven’t said how to do this recursive decomposition into sub-networks. It certainly isn’t practical to hand-design the weights and biases in the network. Instead, we’d like to use learning algorithms so that the network can automatically learn the weights and biases – and thus, the hierarchy of concepts – from training data. Researchers in the 1980s and 1990s tried using stochastic gradient descent and backpropagation to train deep networks",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_53"
  },
  {
    "document_type": "book",
    "title": "nielsen_neuralnetworksanddeeplearning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\nielsen_neuralnetworksanddeeplearning.pdf",
    "date_published": "2018-09-11",
    "keywords": "",
    "flag": "",
    "chunk_text": ". Researchers in the 1980s and 1990s tried using stochastic gradient descent and backpropagation to train deep networks. Unfortunately, except for a few special architectures, they didn’t have much luck. The networks would learn, but very slowly, and in practice often too slowly to be useful. Since 2006, a set of techniques has been developed that enable learning in deep neural nets. These deep learning techniques are based on stochastic gradient descent and bacpropagation, but also introduce new ideas. These techniques have enabled much deeper (and larger) networks to be trained – people now routinely train networks with 5 to 10 hidden layers. And, it turns out that these perform far better on many problems than shallow neural networks, i.e., networks with just a single hidden layer. The reason, of course, is the ability of deep nets to build up a complex hierarchy of concepts. It’s a bit like the way conventional programming languages use modular design and ideas about abstraction to enable the creation of complex computer programs. Comparing a deep network to a shallow network is a bit like comparing a programming language with the ability to make function calls to a stripped down language with no ability to make such calls. Abstraction takes a different form in neural networks than it does in conventional programming, but it’s just as important. 1 1",
    "chunk_id": "Natural_language_processing_nielsen_neuralnetworksanddeeplearning_page-1-38.json_chunk_54"
  },
  {
    "document_type": "online_article",
    "title": "The Window-Knocking Machine Test",
    "author": "Unknown",
    "source": "https://ines.io/blog/window-knocking-machine-test/",
    "date_published": "Aug 7, 2024",
    "flag": "",
    "chunk_text": "AI is making futurists of us all. With the dizzying speed of new innovations, it’s clear that our lives and work are going to change. So what’s next? How will technology shape our world going forward? And what tools and products should we build? When imagining what the future could look like, it helps to lookbackin time and compare past visions to our reality today. The following paintings were created in the early 1900s and depict the artist’s ideas for what life could look like in the year 2000, assisted by new technology. How people in 1900 imagined the year 2000 (Source:The Washington Post) Now 2000 has long gone and we know what life really looked like. What all of these visions have in common is that they imagine very literal translations of human-shaped tasks into technology. We did indeed solve many of these problems with technology, but we did it quite differently. We invented vacuum cleaners and robotic mops instead of sweeping machines that operate brooms. Other innovations tackled different problems altogether: we didn’t just engineer fire trucks, planes and drones, but also introduced smoke detectors, modern construction techniques and fireproof materials to prevent fires from breaking out in the first place. During the Industrial Revolution and up until the 1950s (!), people in Europe would employ so called“knocker-uppers”, who would walk around the neighbourhood with a long stick and knock on windows to ensure their clients woke up in time. This peculiar profession was of course made entirely obsolete by technology when alarm clocks became reliable enough and affordable for everyone. However, knocking on windows was simply a means to an end – we didn’t build window-knocking machines. We built alarm clocks that solved the same problem in adifferentand much more effective way. Just like how humans knocking on windows was the most effective way before",
    "chunk_id": "Natural_language_processing_the_window-knocking_machine_test_·_ines.io.json_chunk_1"
  },
  {
    "document_type": "online_article",
    "title": "The Window-Knocking Machine Test",
    "author": "Unknown",
    "source": "https://ines.io/blog/window-knocking-machine-test/",
    "date_published": "Aug 7, 2024",
    "flag": "",
    "chunk_text": ". We built alarm clocks that solved the same problem in adifferentand much more effective way. Just like how humans knocking on windows was the most effective way before. “Knocker-uppers” at work (Source:Mashable) When designing new technological solutions, it’s easy to fall into the trap of imagining them as human-shaped replacements because that’s all we know. But the most successful innovations don’t usually work this way. So whenever you’re tasked with developing a system to replace and automate a human task, ask yourself: Am I building a window-knocking machine or an alarm clock? Around 2015 when deep learning was widely adopted and conversational AI became more viable, the industry gotveryexcited about chat bots. Compared to what we can do today, the capabilities were still pretty limited, but optimism and all the new possibilities led to many ambitious product ideas and attempts to make modern workplaces more productive. Some of the most attractive use cases were centered around automating tasks of personal assistants: placing orders, coordinating people or scheduling appointments. The latter is especially painful and time-consuming, as everyone who’s worked in a role with frequent meetings can attest to. So what if we could make a chat bot take over that conversation? Two solutions for scheduling meetings: a chat bot and the Calendly app An alternative approach was introduced byCalendly, a product I love and highly recommend. The premise is simple: Calendly syncs with your calendars and gives you a booking link that lets others schedule appointments in a few clicks, optionally syncing with their calendars, too, and automatically converting time zones. It’s an incredibly effective solution that doesn’t even require AI at its core – it simply reframes the problem and solves it at the root, rather than translating the task performed by a human assistant. You could also say: Calendly is the alarm clock to the window-knocking machine of the chat bot",
    "chunk_id": "Natural_language_processing_the_window-knocking_machine_test_·_ines.io.json_chunk_2"
  },
  {
    "document_type": "online_article",
    "title": "The Window-Knocking Machine Test",
    "author": "Unknown",
    "source": "https://ines.io/blog/window-knocking-machine-test/",
    "date_published": "Aug 7, 2024",
    "flag": "",
    "chunk_text": ". You could also say: Calendly is the alarm clock to the window-knocking machine of the chat bot. Now don’t get me wrong, there are many applications that really benefit from conversational interfaces. But natural language is exactly that:another interface. Whether or not it’s therightinterface is a decision you can’t make without considering the specifics of your application and its intended users. The more recent developments around LLMs and Generative AI have enabled a big leap in natural language understanding technology and also reignited the excitement around chat bots.Retrieval-Augmented Generation(RAG) lets users interact with applications and their specific underlying data, for instance to translate natural language questions into real-time database queries. If you want to know your company’s revenue figures, you can simply ask your bot, instead of looking at a table of numbers. Two solutions for querying revenue and customer data: a chat bot and a graphical user interface The decision between alarm clock and window-knocking machine is much less clear-cut in this example and depends a lot on the intended usage. Natural language questions can be much more intuitive for non-experts, but they probably won’t replace the need for structured data as a source of truth that analysts can work with directly. If this is too inefficient, the answer isn’t to abstract it away behind a conversational interface – it indicates that the existing interface needs to be better. I believe a common misconception about professional tools is that providing a good user experience and making things “simple” requiresabstracting awayas many layers as possible. That’s how we end up with “one click” or “one line of code” solutions that sound great in theory, but have users hit a wall as soon as they want to do things the developers had not considered or intended",
    "chunk_id": "Natural_language_processing_the_window-knocking_machine_test_·_ines.io.json_chunk_3"
  },
  {
    "document_type": "online_article",
    "title": "The Window-Knocking Machine Test",
    "author": "Unknown",
    "source": "https://ines.io/blog/window-knocking-machine-test/",
    "date_published": "Aug 7, 2024",
    "flag": "",
    "chunk_text": ". Just because a user is “non-technical” (whatever this means), it doesn’t mean they’re unable or unwilling to use complex professional products – it’s just that those products need to be designed well and get the job done efficiently. The latest AI technologies have a lot of potential to make humans more productive – but just like with past innovations, the way they’ll translate into products and solutions will likely be very different from our limited imagination of human-shaped tasks. We’re currently seeing a lot of window-knocking machines being built, but only the alarm clocks will remain.",
    "chunk_id": "Natural_language_processing_the_window-knocking_machine_test_·_ines.io.json_chunk_4"
  }
]