{
    "document_type": "research_paper",
    "title": "Breaking NLI Systems with Sentences that Require Simple Lexical Inferences",
    "author": "Max Glockner ; Vered Shwartz ; Yoav Goldberg",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Glockner-2018-Breaking-nli-systems-with-sentences.pdf",
    "date_published": "2018-05-06",
    "keywords": "",
    "flag": "",
    "text": "650 Breaking NLI Systems with Sentences that Require Simple Lexical Inferences Max Glockner 1 , Vered Shwartz 2 and Yoav Goldberg 2 1 Computer Science Department, TU Darmstadt, Germany 2 Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel { maxg216,vered1986,yoav.goldberg } @gmail.com Abstract We create a new NLI test set that shows the deﬁciency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are sim- pler than the SNLI test set, containing sen- tences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is sub- stantially worse across systems trained on SNLI, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences. 1 Introduction Recognizing textual entailment (RTE) ( Dagan et al. , 2013 ), recently framed as natural language inference (NLI) ( Bowman et al. , 2015 ) is a task concerned with identifying whether a premise sen- tence entails, contradicts or is neutral with the hy- pothesis sentence. Following the release of the large-scale SNLI dataset ( Bowman et al. , 2015 ), many end-to-end neural models have been devel- oped for the task, achieving high accuracy on the test set. As opposed to previous-generation meth- ods, which relied heavily on lexical resources, neural models only make use of pre-trained word embeddings. The few efforts to incorporate exter- nal lexical knowledge resulted in negligible per- formance gain ( Chen et al. , 2018 ). This raises the question whether (1) neural methods are inher- ently stronger, obviating the need of external lexi- cal knowledge; (2) large-scale training data allows for implicit learning of previously explicit lexical knowledge; or (3) the NLI datasets are simpler than early RTE datasets, requiring less knowledge. 1 The contradiction example follows the assumption in Bowman et al. ( 2015 ) that the premise contains the most prominent information in the event, hence the premise can’t describe the event of a man holding both instruments. Premise/Hypothesis Label The man is holding a saxophone contradiction 1 The man is holding an electric guitar A little girl is very sad. entailment A little girl is very unhappy. A couple drinking wine neutral A couple drinking champagne Table 1: Examples from the new test set. In this paper we show that state-of-the-art NLI systems are limited in their generalization ability, and fail to capture many simple inferences that re- quire lexical and world knowledge. Inspired by the work of Jia and Liang ( 2017 ) on reading com- prehension, we create a new NLI test set with ex- amples that capture various kinds of lexical knowl- edge (Table 1 ). For example, that champagne is a type of wine (hypernymy), and that saxophone and electric guitar are different musical instru- ments (co-hyponyms). To isolate lexical knowl- edge aspects, our constructed examples contain only words that appear both in the training set and in pre-trained embeddings, and differ by a single word from sentences in the training set. The performance on the new test set is substan- tially worse across systems, demonstrating that the SNLI test set alone is not a sufﬁcient measure of language understanding capabilities. Our results are in line with Gururangan et al. ( 2018 ) and Po- liak et al. ( 2018 ), who showed that the label can be identiﬁed by looking only at the hypothesis and exploiting annotation artifacts such as word choice and sentence length. Further investigation shows that what mostly affects the systems’ ability to correctly predict a test example is the amount of similar exam- ples found in the training set. Given that train- ing data will always be limited, this is a rather inefﬁcient way to learn lexical inferences, stress- ing the need to develop methods that do this more \n651 effectively. Our test set can be used to evalu- ate such models’ ability to recognize lexical infer- ences, and it is available at https://github. com/BIU-NLP/Breaking_NLI . 2 Background NLI Datasets. The SNLI dataset (Stanford Nat- ural Language Inference, Bowman et al. , 2015 ) consists of 570k sentence-pairs manually labeled as entailment, contradiction, and neutral. Premises are image captions from Young et al. ( 2014 ), while hypotheses were generated by crowd-sourced workers who were shown a premise and asked to generate entailing, contradicting, and neutral sen- tences. Workers were instructed to judge the re- lation between sentences given that they describe the same event . Hence, sentences that differ by a single mutually-exclusive term should be consid- ered contradicting, as in “The president visited Al- abama” and “The president visited Mississippi”. This differs from traditional RTE datasets, which do not assume event coreference, and in which such sentence-pairs would be considered neutral. Following criticism on the simplicity of the dataset, stemming mostly from its narrow domain, two additional datasets have been collected. The MultiNLI dataset (Multi-Genre Natural Language Inference, Williams et al. , 2018 ) was collected similarly to SNLI, though covering a wider range of genres, and supporting a cross-genre evaluation. The SciTail dataset ( Khot et al. , 2018 ), created from science exams, is somewhat different from the two datasets, being smaller (27,026 examples), and labeled only as entailment or neutral. The do- main makes this dataset different in nature from the other two datasets, and it consists of more fac- tual sentences rather than scene descriptions. Neural Approaches for NLI. Following the re- lease of SNLI, there has been tremendous inter- est in the task, and many end-to-end neural mod- els were developed, achieving promising results. 2 Methods are divided into two main approaches. Sentence-encoding models (e.g. Bowman et al. , 2015 , 2016 ; Nie and Bansal , 2017 ; Shen et al. , 2018 ) encode the premise and hypothesis individ- ually, while attention-based models align words in the premise with similar words in the hypoth- esis, encoding the two sentences together (e.g. Rockt¨aschel et al. , 2016 ; Chen et al. , 2017 ). 2 See the SNLI leaderboard for a comprehensive list: https://nlp.stanford.edu/projects/snli/ . External Lexical Knowledge. Traditional RTE methods typically relied on resources such as WordNet ( Fellbaum , 1998 ) to identify lexical in- ferences. Conversely, neural methods rely solely on pre-trained word embeddings, yet, they achieve high accuracy on SNLI. The only neural model to date that incorpo- rates external lexical knowledge (from WordNet) is KIM ( Chen et al. , 2018 ), however, gaining only a small addition of 0.6 points in accuracy on the SNLI test set. This raises the question whether the small performance gap is a result of the model not capturing lexical knowledge well, or the SNLI test set not requiring this knowledge in the ﬁrst place. 3 Data Collection We construct a test set with the goal of evaluating the ability of state-of-the-art NLI models to make inferences that require simple lexical knowledge. We automatically generate sentence pairs ( § 3.1 ) which are then manually veriﬁed ( § 3.2 ). 3.1 Generating Adversarial Examples In order to isolate the lexical knowledge aspects, the premises are taken from the SNLI training set. For each premise we generate several hypotheses by replacing a single word within the premise by a different word. We also allow some multi-word noun phrases (“electric guitar”) and adapt deter- miners and prepositions when needed. We focus on generating only entailment and contradiction examples, while neutral examples may be generated as a by-product. Entailment examples are generated by replacing a word with its synonym or hypernym, while contradiction ex- amples are created by replacing words with mu- tually exclusive co-hyponyms and antonyms (see Table 1 ). The generation steps are detailed below. Replacement Words. We collected the replace- ment words using online resources for English learning. 3 The newly introduced words are all present in the SNLI training set: from occur- rence in a single training example (“Portugal”) up to 248,051 examples (“man”), with a mean of 3,663.1 and a median of 149.5. The words are also available in the pre-trained embeddings vo- cabulary. The goal of this constraint is to isolate lexical knowledge aspects, and evaluate the mod- els’ ability to generalize and make new inferences for known words. 3 www.enchantedlearning.com , www.smart-words.org \n652 SNLI Test New Test Instances: contradiction 3,236 7,164 entailment 3,364 982 neutral 3,215 47 Overall 9,815 8,193 Fleiss κ : contradiction 0.77 0.61 entailment 0.69 0.90 Overall 0.67 0.61 Estimated human performance: 87.7% 94.1% Table 2: Statistics of the test sets. 9,815 is the number of samples with majority agreement in the SNLI test set, whose full size is 9,824. Replacement words are divided into topical cat- egories detailed in Table 4 . In several categories we applied additional processing to ensure that ex- amples are indeed mutually-exclusive, topically- similar, and interchangeable in context. We in- cluded WordNet antonyms with the same part-of- speech and with a cosine similarity score above a threshold, using GloVe ( Pennington et al. , 2014 ). In nationalities and countries we focused on coun- tries which are related geographically (Japan, China) or culturally (Argentina, Spain) . Sentence-Pairs. To avoid introducing new in- formation not present in the training data, we sam- pled premises from the SNLI training set that con- tain words from our lists, and generated hypothe- ses by replacing the selected word with its replace- ment. Some of the generated sentences may be un- grammatical or nonsensical, for instance, when re- placing Jordan with Syria in sentences discussing Michael Jordan . We used Wikipedia bigrams 4 to discard sentences in which the replaced word cre- ated a bigram with less than 10 occurrences. 3.2 Manual Veriﬁcation We manually verify the correctness of the au- tomatically constructed examples using crowd- sourced workers in Amazon Mechanical Turk. To ensure the quality of workers, we applied a quali- ﬁcation test and required a 99% approval rate for at least 1,000 prior tasks. We assigned each anno- tation to 3 workers. Following the SNLI guidelines, we instructed the workers to consider the sentences as describing the same event, but we simpliﬁed the annotation process into answering 3 simple yes/no questions: 1. Do the sentences describe the same event? 4 github.com/rmaestre/Wikipedia-Bigram-Open-Datasets 2. Does the new sentence (hypothesis) add new information to the original sentence (premise)? 3. Is the new sentence incorrect/ungrammatical? We then discarded any sentence-pair in which at least one worker answered the third question positively. If the answer to the ﬁrst question was negative, we considered the label as contradiction . Otherwise, we considered the label as entailment if the answer to the second question was negative and neutral if it was positive. We used the major- ity vote to determine the gold label. The annotations yielded substantial agreement, with Fleiss’ Kappa κ = 0 . 61 ( Landis and Koch , 1977 ). We estimate human performance to 94.1%, using the method described in Gong et al. ( 2018 ), showing that the new test set is substantially easier to humans than SNLI. Table 2 provides additional statistics on the test set. 5 4 Evaluation 4.1 Models Without External Knowledge. We chose 3 rep- resentative models in different approaches (sen- tence encoding and/or attention): R ESIDUAL - S TACKED -E NCODER ( Nie and Bansal , 2017 ) is a biLSTM-based single sentence-encoding model without attention. As opposed to traditional multi- layer biLSTMs, the input to each next layer is the concatenation of the word embedding and the summation of outputs from previous lay- ers. E SIM (Enhanced Sequential Inference Model, Chen et al. , 2017 ) is a hybrid TreeLSTM-based and biLSTM-based model. We use the biL- STM model, which uses an inter-sentence atten- tion mechanism to align words across sentences. Finally, D ECOMPOSABLE A TTENTION ( Parikh et al. , 2016 ) performs soft alignment of words from the premise to words in the hypothesis us- ing attention mechanism, and decomposes the task into comparison of aligned words. Lexical-level decisions are merged to produce the ﬁnal classiﬁ- cation. We use the AllenNLP re-implementation, 6 which does not implement the optional intra- sentence attention, and achieves an accuracy of 84.7% on the SNLI test set, comparable to 86.3% by the original system. 5 We note that due to its bias towards contradiction , the new test set can neither be used for training, nor serve as a main evaluation set for NLI. Instead, we suggest to use it in addition to the original test set in order to test a model’s abil- ity to handle lexical inferences. 6 http://allennlp.org/models \n653 Model Train set SNLI test set New test set ∆ Decomposable Attention ( Parikh et al. , 2016 ) SNLI 84.7% 51.9% -32.8 MultiNLI + SNLI 84.9% 65.8% -19.1 SciTail + SNLI 85.0% 49.0% -36.0 ESIM ( Chen et al. , 2017 ) SNLI 87.9% 65.6% -22.3 MultiNLI + SNLI 86.3% 74.9% -11.4 SciTail + SNLI 88.3% 67.7% -20.6 Residual-Stacked-Encoder ( Nie and Bansal , 2017 ) SNLI 86.0% 62.2% -23.8 MultiNLI + SNLI 84.6% 68.2% -16.8 SciTail + SNLI 85.0% 60.1% -24.9 WordNet Baseline - - 85.8% - KIM ( Chen et al. , 2018 ) SNLI 88.6% 83.5% -5.1 Table 3: Accuracy of various models trained on SNLI or a union of SNLI with another dataset (MultiNLI, SciTail), and tested on the original SNLI test set and the new test set. We chose models which are amongst the best performing within their approaches (excluding en- sembles) and have available code. All models are based on pre-trained GloVe embeddings ( Pen- nington et al. , 2014 ), which are either ﬁne-tuned during training (R ESIDUAL -S TACKED -E NCODER and ESIM ) or stay ﬁxed (D ECOMPOSABLE A T - TENTION ). All models predict the label using a concatenation of features derived from the sen- tence representations (e.g. maximum, mean), for example as in Mou et al. ( 2016 ). We use the rec- ommended hyper-parameters for each model, as they appear in the provided code. With External Knowledge. We provide a sim- ple W ORD N ET BASELINE , in which we classify a sentence-pair according to the WordNet relation that holds between the original word w p and the replaced word w h . We predict entailment if w p is a hyponym of w h or if they are synonyms, neutral if w p is a hypernym of w h , and contradiction if w p and w h are antonyms or if they share a common hypernym ancestor (up to 2 edges). Word pairs with no WordNet relations are classiﬁed as other . We also report the performance of KIM (Knowledge-based Inference Model, Chen et al. , 2018 ), an extension of ESIM with external knowl- edge from WordNet, which was kindly provided to us by Qian Chen. K IM improves the attention mechanism by taking into account the existence of WordNet relations between the words. The lex- ical inference component, operating over pairs of aligned words, is enriched with a vector encoding the speciﬁc WordNet relations between the words. 4.2 Experimental Settings We trained each model on 3 different datasets: (1) SNLI train set, (2) a union of the SNLI train set and the MultiNLI train set, and (3) a union of the SNLI train set and the SciTail train set. The mo- tivation is that while SNLI might lack the training data needed to learn the required lexical knowl- edge, it may be available in the other datasets, which are presumably richer. 4.3 Results Table 3 displays the results for all the models on the original SNLI test set and the new test set. De- spite the task being considerably simpler, the drop in performance is substantial, ranging from 11 to 33 points in accuracy. Adding MultiNLI to the training data somewhat mitigates this drop in ac- curacy, thanks to almost doubling the amount of training data. We note that adding SciTail to the training data did not similarly improve the perfor- mance; we conjecture that this stems from the dif- ferences between the datasets. K IM substantially outperforms the other neural models, demonstrating that lexical knowledge is the only requirement for good performance on the new test set, and stressing the inability of the other models to learn it. Both WordNet-informed mod- els leave room for improvement: possibly due to limited WordNet coverage and the implications of applying lexical inferences within context. 5 Analysis We take a deeper look into the predictions of the models that don’t employ external knowledge, fo- cusing on the models trained on SNLI. 5.1 Accuracy by Category Table 4 displays the accuracy of each model per replacement-word category. The neural models tend to perform well on categories which are fre- quent in the training set, such as colors , and badly \n654 Dominant Label Category Instances Example Words Decomposable Attention ESIM Residual Encoders WordNet Baseline KIM Cont. antonyms 1,147 loves - dislikes 41.6% 70.4% 58.2% 95.5% 86.5% cardinals 759 ﬁve - seven 53.5% 75.5% 53.1% 98.6% 93.4% nationalities 755 Greek - Italian 37.5% 35.9% 70.9% 78.5% 73.5% drinks 731 lemonade - beer 52.9% 63.7% 52.0% 94.8% 96.6% antonyms (WN) 706 sitting - standing 55.1% 74.6% 67.9% 94.5% 78.8% colors 699 red - blue 85.0% 96.1% 87.0% 98.7% 98.3% ordinals 663 ﬁfth - 16th 2.1% 21.0% 5.4% 40.7% 56.6% countries 613 Mexico - Peru 15.2% 25.4% 66.2% 100.0% 70.8% rooms 595 kitchen - bathroom 59.2% 69.4% 63.4% 89.9% 77.6% materials 397 stone - glass 65.2% 89.7% 79.9% 75.3% 98.7% vegetables 109 tomato -potato 43.1% 31.2% 37.6% 86.2% 79.8% instruments 65 harmonica - harp 96.9% 90.8% 96.9% 67.7% 96.9% planets 60 Mars - Venus 31.7% 3.3% 21.7% 100.0% 5.0% Ent. synonyms 894 happy - joyful 97.5% 99.7% 86.1% 70.5% 92.1% total 8,193 51.9% 65.6% 62.2% 85.8% 83.5% Table 4: The number of instances and accuracy per category achieved by each model. on categories such as planets , which rarely occur in SNLI. These models perform better than the WordNet baseline on entailment examples ( syn- onyms ), suggesting that they do so due to high lexical overlap between the premise and the hy- pothesis rather than recognizing synonymy. We therefore focus the rest of the discussion on con- tradiction examples. 5.2 Accuracy by Word Similarity The accuracies for ordinals , nationalities and countries are especially low. We conjecture that this stems from the proximity of the contradict- ing words in the embedding space. Indeed, the Decomposable Attention model—which does not update its embeddings during training—seems to suffer the most. Grouping its prediction accuracy by the cosine similarity between the contradicting words reveals a clear trend that the model errs more on contra- dicting pairs with similar pre-trained vectors: 7 Similarity 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1.0 Accuracy 46.2% 42.3% 37.5% 29.7% 20.2% 5.3 Accuracy by Frequency in Training Models that ﬁne-tune the word embeddings may beneﬁt from training examples consisting of test replacement pairs. Namely, for a given replace- ment pair ( w p , w h ), if many training examples la- beled as contradiction contain w p in the premise and w h in the hypothesis, the model may update their embeddings to optimize predicting contradic- tion. Indeed, we show that the ESIM accuracy on test pairs increases with the frequency in which 7 We ignore multi-word replacements in § 5.2 and § 5.3 . their replacement words appear in contradiction examples in the training data: Frequency 0 1-4 5-9 10-49 50-99 100+ Accuracy 40.2% 70.6% 91.4% 92.1% 97.5% 98.5% This demonstrates that the model is capable of learning lexical knowledge when sufﬁcient train- ing data is given, but relying on explicit training examples is a very inefﬁcient way of obtaining simple lexical knowledge. 6 Conclusion We created a new NLI test set with the goal of evaluating systems’ ability to make inferences that require simple lexical knowledge. Although the test set is constructed to be much simpler than SNLI, and does not introduce new vocabulary, the state-of-the-art systems perform poorly on it, sug- gesting that they are limited in their generalization ability. The test set can be used in the future to as- sess the lexical inference abilities of NLI systems and to tease apart the performance of otherwise very similarly-performing systems. Acknowledgments We would like to thank Qian Chen for evaluat- ing KIM on our test set. This work was sup- ported in part by the German Research Founda- tion through the German-Israeli Project Coopera- tion (DIP, grant DA 1600/1-1), an Intel ICRI-CI grant, Theo Hoffenberg, and the Israel Science Foundation grants 1951/17 and 1555/15. Vered is also supported by the Clore Scholars Programme (2017), and the AI2 Key Scientiﬁc Challenges Program (2017). \n655"
}