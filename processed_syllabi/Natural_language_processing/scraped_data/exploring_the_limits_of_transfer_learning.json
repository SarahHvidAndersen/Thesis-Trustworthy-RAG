{
    "document_type": "research_paper",
    "title": "exploring_the_limits_of_transfer_learning",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\exploring_the_limits_of_transfer_learning.pdf",
    "date_published": "2023-09-20",
    "keywords": "",
    "flag": "",
    "text": "Journal of Machine Learning Research 21 (2020) 1- 67 Submitted 1/20; Revised 6/20; Published 6/20 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Colin Raffel ∗ craffel@gmail.com Noam Shazeer ∗ noam@google.com Adam Roberts ∗ adarob@google.com Katherine Lee ∗ katherinelee@google.com Sharan Narang sharannarang@google.com Michael Matena mmatena@google.com Yanqi Zhou yanqiz@google.com Wei Li mweili@google.com Peter J. Liu peterjliu@google.com Google, Mountain View, CA 94043, USA Editor: Ivan Titov Abstract pre-trained models, and code. 1 Keywords: 1. Introduction ∗. Equal contribution. A description of each author’s contribution is available in Appendix A . Correspondence to craffel@gmail.com . 1. https://github.com/google-research/text-to-text-transfer-transformer ©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. arXiv:1910.10683v4  [cs.LG]  19 Sep 2023 \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu to the Internet—for example, the Common Crawl project 2 produces about 20TB of text 2. http://commoncrawl.org \nExploring the Limits of Transfer Learning \"translate English to German: That is good.\" \"cola sentence: The course is jumping well.\" \"summarize: state authorities dispatched emergency crews tuesday to survey the damage after an onslaught of severe weather in mississippi…\" \"stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field.\" T5 \"Das ist gut.\" \"not acceptable\" \"six people hospitalized after a storm in attala county.\" \"3.8\" “T5” refers to our model, which we dub the “ T ext- t o- T ext T ransfer T ransformer”. pre-trained models. 1 \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 2. Setup data. We refer to our model and framework as the “ T ext- t o- T ext T ransfer T ransformer” 2.1 Model the interested reader to the original paper ( Vaswani et al. , 2017 ) or follow-up tutorials 3 , 4 for 3. http://nlp.seas.harvard.edu/2018/04/03/attention.html 4. http://jalammar.github.io/illustrated-transformer/ \nExploring the Limits of Transfer Learning models on “slices” of Cloud TPU Pods. 5 TPU pods are are multi-rack ML supercomputers that contain 1 , 024 TPU v3 chips connected via a high-speed 2D mesh interconnect with 2.2 The Colossal Clean Crawled Corpus 5. https://cloud.google.com/tpu/ \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu • We only retained lines that ended in a terminal punctuation mark (i.e. a period, • We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”. 6 • Some pages had placeholder “lorem ipsum” text; we removed any page where the we used langdetect 7 to filter out any pages that were not classified as English with a 6. https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words 7. https://pypi.org/project/langdetect/ \nExploring the Limits of Transfer Learning comprises reasonably clean and natural English text. We dub this data set the “ C olossal C lean C rawled C orpus” (or C4 for short) and release it as part of TensorFlow Datasets. 8 2.3 Downstream Tasks Datasets. 9 8. https://www.tensorflow.org/datasets/catalog/c4 9. https://www.tensorflow.org/datasets \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu et al. , 2017 ) (i.e. News Commentary v13, Common Crawl, Europarl v7) and newstest2013 data from 2015 and newstest2014 as a validation set ( Bojar et al. , 2015 ). For English to 2.4 Input and Output Format \nExploring the Limits of Transfer Learning in increments of 0 . 2, so we simply rounded any score to the nearest increment of 0 . 2 and value 2 . 57 would be mapped to the string “2.6”). At test time, if the model outputs a \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu training set, but the DPR data set adds about 1 , 000 pronoun resolution examples. Examples 3. Experiments \nExploring the Limits of Transfer Learning 3.1 Baseline 3.1.1 Model size and configuration to a “BERT BASE ” ( Devlin et al. , 2018 ) stack. Specifically, both the block consist of a dense layer with an output dimensionality of d ff = 3072 followed by a mechanisms have an inner dimensionality of d kv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of d model = 768. In total, of parameters of BERT BASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0 . 1 everywhere dropout is applied 3.1.2 Training We pre-train each model for 2 19 = 524 , 288 steps on C4 before fine-tuning. We use a \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu we “pack” multiple sequences into each entry of the batch 10 so that our batches contain roughly 2 16 = 65 , 536 tokens. In total, this batch size and number of steps corresponds to pre-training on 2 35 ≈ 34B tokens. This is considerably less than BERT ( Devlin et al. , 2 . 2T tokens. Using only 2 35 tokens results in a reasonable computational budget while still effect of pre-training for more steps in Sections 3.6 and 3.7 . Note that 2 35 tokens only covers \u000e p max( n, k ) where n is the current training iteration and k is the number of warm-up steps (set to 10 4 in all of our experiments). This sets a constant learning rate of 0 . 01 for the first 10 4 steps, Our models are fine-tuned for 2 18 = 262 , 144 steps on all tasks. This value was chosen 2 16 tokens per batch). We use a constant learning rate of 0 . 001 when fine-tuning. We save a checkpoint every 5 , 000 steps and report results on the model checkpoint corresponding 3.1.3 Vocabulary ( Sennrich et al. , 2015 ; Kudo , 2018 ). For all experiments, we use a vocabulary of 32 , 000 3.1.4 Unsupervised Objective 10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/ index.html#data_generators.generator_utils.pack_examples \nExploring the Limits of Transfer Learning <X> <Y> <X> <Y> <Z> words “for”, “inviting” and “last” (marked with an × ) are randomly chosen for token (shown as <X> and <Y> ) that is unique over the example. Since “for” and “inviting” occur consecutively, they are replaced by a single sentinel <X> . The tokens used to replace them in the input plus a final sentinel token <Z> . 3.1.5 Baseline Performance \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline average 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Baseline standard deviation 0 . 235 0 . 065 0 . 343 0 . 416 0 . 112 0 . 090 0 . 108 No pre-training 66 . 22 17 . 60 50 . 31 53 . 04 25 . 86 39 . 77 24 . 04 different changes. Separately, we also measure the performance of training our model for 2 18 configuration appears, we will mark it with a ⋆ (as in the first row of Table 1 ). We also will boldface any score that is within two standard deviations of the maximum (best) in a models of similar size. For example, BERT BASE achieved an exact match score of 80 . 8 on SQuAD and an accuracy of 84 . 4 on MNLI-matched, whereas we achieve 80 . 88 and 84 . 24, respectively (see Table 16 ). Note that we cannot directly compare our baseline to BERT BASE because ours is an encoder-decoder model and was pre-trained for roughly 1 ⁄ 4 \nExploring the Limits of Transfer Learning For example, on CB our baseline model had an average F1 score of 91 . 22 with a standard deviation of 3 . 237 (see Table 16 ), which may be partly due to the fact that CB’s validation 3.2 Architectures 3.2.1 Model Structures sequence. Specifically, let y i refer to the i th element of the output sequence and x j refer to the j th entry of the input sequence. y i is computed as P j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention shown in Figure 3 . For example, the causal mask (Figure 3 , middle) sets any w i,j to zero if j > i . \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 x 1 x 2 x 3 x 4 x 5 y 5 y 4 y 3 y 2 y 1 of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i . A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j the i th output element from depending on any input elements from “the future”. When producing the i th entry of the output sequence, causal masking prevents the model from attending to the j th entry of the input sequence for j > i . This is used during training \nExploring the Limits of Transfer Learning x 1 x 2 x 3 x 4 y 1 y 2 . Encoder Decoder x 1 x 2 x 3 y 1 y 2 x 2 x 3 y 1 y 2 . Language model x 1 x 2 x 3 y 1 y 2 x 2 x 3 y 1 y 2 . Preﬁx LM as x and y respectively. Left: A standard encoder-decoder architecture uses fully- to-text setting is that causal masking forces the model’s representation of the i th entry of the input sequence to only depend on the entries up until i . To see why this is potentially \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 3.2.2 Comparing Different Model Structures at the same time. To see why, first note an encoder-decoder model with L layers in the encoder and L layers in the decoder has approximately the same number of parameters as a language model with 2 L layers. However, the same L + L encoder-decoder model will have approximately the same computational cost as a language model with only L layers. This is a consequence of the fact that the L layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence \nExploring the Limits of Transfer Learning times for L -layer language models versus L + L -layer encoder-decoder models, suggesting a count, so we make the simplifying assumption that an L + L -layer encoder-decoder model has the same number of parameters as an 2 L -layer language model. BERT BASE -sized layer stack as L and P , respectively. We will use M to refer to the number of FLOPs required for an L + L -layer encoder-decoder model or L -layer decoder-only model • An encoder-decoder model with L layers in the encoder and L layers in the decoder. This model has 2 P parameters and a computation cost of M FLOPs. resulting in P parameters and an M -FLOP computational cost. • An encoder-decoder model with L/ 2 layers each in the encoder and decoder, giving P parameters and an M/ 2-FLOP cost. • A decoder-only language model with L layers and P parameters and a resulting computational cost of M FLOPs. 3.2.3 Objectives 3.2.4 Results This variant has the highest parameter count (2 P ) but the same computational cost as the P -parameter decoder-only models. Surprisingly, we found that sharing parameters across the \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Architecture Objective Params Cost GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Encoder-decoder Denoising 2 P M 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Enc-dec, shared Denoising P M 82 . 81 18 . 78 80 . 63 70 . 73 26 . 72 39 . 03 27 . 46 Enc-dec, 6 layers Denoising P M/ 2 80 . 88 18 . 97 77 . 59 68 . 42 26 . 38 38 . 40 26 . 95 Language model Denoising P M 74 . 70 17 . 93 61 . 14 55 . 02 25 . 09 35 . 28 25 . 86 Prefix LM Denoising P M 81 . 82 18 . 61 78 . 94 68 . 11 26 . 43 37 . 98 27 . 39 Encoder-decoder LM 2 P M 79 . 56 18 . 59 76 . 02 64 . 29 26 . 27 39 . 17 26 . 86 Enc-dec, shared LM P M 79 . 60 18 . 13 76 . 35 63 . 50 26 . 62 39 . 17 27 . 05 Enc-dec, 6 layers LM P M/ 2 78 . 67 18 . 26 75 . 32 64 . 06 26 . 13 38 . 42 26 . 89 Language model LM P M 73 . 78 17 . 54 53 . 81 56 . 51 25 . 23 34 . 31 25 . 38 Prefix LM LM P M 79 . 68 17 . 84 76 . 87 64 . 86 26 . 28 37 . 51 26 . 76 use P to refer to the number of parameters in a 12-layer base Transformer layer stack and M to refer to the FLOPs required to process a sequence using the encoder- 3.3 Unsupervised Objectives \nExploring the Limits of Transfer Learning Objective Inputs Targets Prefix language modeling Thank you for inviting me to your party last week . BERT-style Devlin et al. ( 2018 ) Thank you <M> <M> me to your party apple week . (original text) Deshuffling party me for your to . last fun you inviting week Thank (original text) MASS-style Song et al. ( 2019 ) Thank you <M> <M> me to your party <M> week . (original text) I.i.d. noise, replace spans Thank you <X> me to your party <Y> week . <X> for inviting <Y> last <Z> I.i.d. noise, drop tokens Thank you me to your party week . for inviting last Random spans Thank you <X> to <Y> week . <X> for inviting me <Y> your party last <Z> week .” Note that all of our objectives process tokenized text. For this particular (original text) as a target to denote that the model is tasked with reconstructing the entire input text. <M> denotes a shared mask token and <X> , <Y> , and <Z> denote 3.3.1 Disparate High-Level Approaches \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Objective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo Prefix language modeling 80 . 69 18 . 94 77 . 99 65 . 27 26 . 86 39 . 73 27 . 49 BERT-style ( Devlin et al. , 2018 ) 82 . 96 19 . 17 80 . 65 69 . 85 26 . 78 40 . 03 27 . 41 Deshuffling 73 . 17 18 . 59 67 . 61 58 . 47 26 . 11 39 . 30 25 . 62 Objective GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo BERT-style ( Devlin et al. , 2018 ) 82 . 96 19 . 17 80 . 65 69 . 85 26 . 78 40 . 03 27 . 41 MASS-style ( Song et al. , 2019 ) 82 . 32 19 . 16 80 . 10 69 . 28 26 . 79 39 . 89 27 . 55 ⋆ Replace corrupted spans 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Drop corrupted tokens 84 . 44 19 . 31 80 . 52 68 . 67 27 . 07 39 . 76 27 . 82 3.3.2 Simplifying the BERT Objective in the GLUE score thanks to a significantly higher score on CoLA (60 . 04, compared to our \nExploring the Limits of Transfer Learning Corruption rate GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo 10% 82 . 82 19 . 00 80 . 38 69 . 55 26 . 87 39 . 28 27 . 44 ⋆ 15% 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 25% 83 . 00 19 . 54 80 . 96 70 . 48 27 . 04 39 . 83 27 . 47 50% 81 . 27 19 . 32 79 . 80 70 . 33 27 . 01 39 . 90 27 . 49 baseline average of 53 . 84, see Table 16 ). This may be due to the fact that CoLA involves 3.3.3 Varying the Corruption Rate 3.3.4 Corrupting Spans \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Span length GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline (i.i.d.) 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 2 83 . 54 19 . 39 82 . 09 72 . 20 26 . 76 39 . 99 27 . 63 3 83 . 49 19 . 62 81 . 84 72 . 53 26 . 86 39 . 65 27 . 62 5 83 . 40 19 . 24 82 . 05 72 . 23 26 . 88 39 . 40 27 . 53 10 82 . 85 19 . 33 81 . 84 70 . 44 26 . 79 39 . 49 27 . 69 should be 25 total spans, then the total number of corrupted tokens would be 500 × 0 . 15 = 75 and the average span length would be 75 / 25 = 3. Note that given the original sequence 3.3.5 Discussion 3.4 Pre-training Data set \nExploring the Limits of Transfer Learning of the C4 data set variants we consider as part of TensorFlow Datasets. 11 3.4.1 Unlabeled Data Sets C4 As a baseline, we first consider pre-training on our proposed unlabeled data set as Unfiltered C4 To measure the effect of the heuristic filtering we used in creating C4 an alternate version of C4 that forgoes this filtering. Note that we still use langdetect 11. https://www.tensorflow.org/datasets/catalog/c4 \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu because langdetect sometimes assigns a low probability to non-natural English text. RealNews-like Recent work has used text data extracted from news websites ( Zellers WebText-like Similarly, the WebText data set ( Radford et al. , 2019 ) only uses content appeared in the list prepared by the OpenWebText effort. 12 However, this resulted in Wikipedia The website Wikipedia consists of millions of encyclopedia articles written English Wikipedia text data from TensorFlow Datasets, 13 which omits any markup or Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia 12. https://github.com/jcpeterson/openwebtext 13. https://www.tensorflow.org/datasets/catalog/wikipedia \nExploring the Limits of Transfer Learning Data set Size GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ C4 745GB 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 C4, unfiltered 6.1TB 81 . 46 19 . 14 78 . 78 68 . 04 26 . 55 39 . 34 27 . 21 RealNews-like 35GB 83 . 83 19 . 23 80 . 39 72 . 38 26 . 75 39 . 90 27 . 48 WebText-like 17GB 84 . 03 19 . 31 81 . 42 71 . 40 26 . 80 39 . 74 27 . 59 Wikipedia 16GB 81 . 85 19 . 31 81 . 29 68 . 01 26 . 94 39 . 69 27 . 67 Wikipedia + TBC 20GB 83 . 65 19 . 28 82 . 08 73 . 24 26 . 77 39 . 63 27 . 57 produced a SuperGLUE score of 73 . 24, beating our baseline’s score (using C4) of 71 . 36. This is almost entirely attributable to a boost in performance from 25 . 78 (baseline, C4) to 50 . 93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16 ). MultiRC for pre-training conferred an increase from 68 . 16 to 73 . 72 on the Exact Match score for The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks . This is unsurprising but also unsatisfying if data set that was about 40 × smaller than C4 despite being based on 12 × more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on 2 35 ≈ 34B 3.4.2 Pre-training Data set Size \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Number of tokens Repeats GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Full data set 0 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 2 29 64 82 . 87 19 . 19 80 . 97 72 . 03 26 . 83 39 . 74 27 . 63 2 27 256 82 . 62 19 . 20 79 . 78 69 . 97 27 . 02 39 . 71 27 . 33 2 25 1 , 024 79 . 55 18 . 57 76 . 27 64 . 76 26 . 38 39 . 56 26 . 80 2 23 4 , 096 76 . 34 18 . 33 70 . 92 59 . 29 26 . 37 38 . 84 25 . 81 we only use the first N tokens from C4 (with varying values of N shown in the first column) but still pre-train over 2 35 tokens. This results in the data set being 2 35 ≈ 34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 2 29 , 2 27 , 2 25 and 2 23 tokens. These sizes correspond to repeating the data set 64, 256, 1 , 024, and 4 , 096 times respectively over the course of pre-training. 3.5 Training Strategy \nExploring the Limits of Transfer Learning 0 100 200 300 400 500 Step × 1,000 0.0 0.2 0.4 0.6 0.8 1.0 Training loss Dataset size Full dataset 2 29 2 27 2 25 2 23 sizes considered correspond to repeating the data set between 64 and 4 , 096 times 3.5.1 Fine-tuning Methods hyperparameter of this approach is the inner dimensionality d of the feed-forward network, various values for d . \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Fine-tuning method GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ All parameters 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Adapter layers, d = 32 80 . 52 15 . 08 79 . 32 60 . 40 13 . 84 17 . 88 15 . 54 Adapter layers, d = 128 81 . 51 16 . 62 79 . 47 63 . 03 19 . 83 27 . 50 22 . 63 Adapter layers, d = 512 81 . 54 17 . 78 79 . 18 64 . 30 23 . 45 33 . 98 25 . 81 Adapter layers, d = 2048 81 . 51 16 . 62 79 . 47 63 . 03 19 . 83 27 . 50 22 . 63 Gradual unfreezing 82 . 50 18 . 95 79 . 17 70 . 79 26 . 71 39 . 02 26 . 93 of the model’s parameters. For adapter layers, d refers to the inner dimensionality 2 18 steps. As such, we subdivide the fine-tuning process into 12 episodes of 2 18 / 12 steps each and train from layers 12 − n to 12 in the n th episode. We note that Howard and Ruder strategy of fine-tuning an additional layer after every 2 18 / 12 steps. For adapter layers, we report the performance using an inner dimensionality d of 32, 128, lower-resource tasks like SQuAD work well with a small value of d whereas higher resource enough that it necessitates a large value of d . We found that gradual unfreezing caused 3.5.2 Multi-task Learning \nExploring the Limits of Transfer Learning Examples-proportional mixing A major factor in how quickly a model will overfit to our N task’s data sets is e n , n ∈{ 1 , . . . , N } then we set probability of sampling an example from the m th task during training to r m = min ( e m , K ) / P min ( e n , K ) where K is the artificial data set size limit. Temperature-scaled mixing An alternative way of mitigating the huge disparity between resource languages. 14 To implement temperature scaling with temperature T , we raise 14. https://github.com/google-research/bert/blob/master/multilingual.md \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu each task’s mixing rate r m to the power of 1 ⁄ T and renormalize the rates so that they sum to 1. When T = 1, this approach is equivalent to examples-proportional mixing and as T increases the proportions become closer to equal mixing. We retain the data set size limit K (applied to obtain r m before temperature scaling) but set it to a large value of K = 2 21 . We use a large value of K because increasing the temperature will Equal mixing In this case, we sample examples from each task with equal probability. fine-tune results, we train multi-task models for the same total number of steps: 2 19 + 2 18 = 786 , 432. The results are shown in Table 11 . for most tasks there is a “sweet spot” for K where the model obtains the best performance, and larger or smaller values of K tend to result in worse performance. The exception (for the range of K values we considered) was WMT English to French translation, which is such a from most tasks, with T = 2 performing the best in most cases. The finding that a multi-task 3.5.3 Combining Multi-Task Learning with Fine-Tuning model on an examples-proportional mixture with an artificial data set size limit of K = 2 19 \nExploring the Limits of Transfer Learning Mixing strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline (pre-train/fine-tune) 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Equal 76 . 13 19 . 02 76 . 51 63 . 37 23 . 89 34 . 31 26 . 78 Examples-proportional, K = 2 16 80 . 45 19 . 04 77 . 25 69 . 95 24 . 35 34 . 99 27 . 10 Examples-proportional, K = 2 17 81 . 56 19 . 12 77 . 00 67 . 91 24 . 36 35 . 00 27 . 25 Examples-proportional, K = 2 18 81 . 67 19 . 07 78 . 17 67 . 94 24 . 57 35 . 19 27 . 39 Examples-proportional, K = 2 19 81 . 42 19 . 24 79 . 78 67 . 30 25 . 21 36 . 30 27 . 76 Examples-proportional, K = 2 20 80 . 80 19 . 24 80 . 36 67 . 38 25 . 66 36 . 93 27 . 68 Examples-proportional, K = 2 21 79 . 83 18 . 79 79 . 50 65 . 10 25 . 82 37 . 22 27 . 13 Temperature-scaled, T = 2 81 . 90 19 . 28 79 . 42 69 . 92 25 . 42 36 . 72 27 . 20 Temperature-scaled, T = 4 80 . 56 19 . 22 77 . 99 69 . 54 25 . 04 35 . 82 27 . 45 Temperature-scaled, T = 8 77 . 21 19 . 10 77 . 14 66 . 07 24 . 55 35 . 35 27 . 17 the total size of each data set, with an artificial limit ( K ) on the maximum data set size. Temperature-scaled mixing re-scales the sampling rates by a temperature T . For temperature-scaled mixing, we use an artificial data set size limit of K = 2 21 . examples-proportional mixture (with K = 2 19 ) except that we omit one of the downstream all of the supervised tasks we consider with K = 2 19 . In all of these variants, we follow our standard procedure of pre-training for 2 19 steps before fine-tuning for 2 18 steps. (without fine-tuning) on an examples-proportional mixture with K = 2 19 . We find that \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Training strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Unsupervised pre-training + fine-tuning 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Multi-task training 81 . 42 19 . 24 79 . 78 67 . 30 25 . 21 36 . 30 27 . 76 Multi-task pre-training + fine-tuning 83 . 11 19 . 12 80 . 26 71 . 03 27 . 08 39 . 80 28 . 07 Leave-one-out multi-task training 81 . 98 19 . 05 79 . 97 71 . 68 26 . 93 39 . 79 27 . 87 Supervised multi-task pre-training 79 . 93 18 . 96 77 . 38 65 . 36 26 . 81 40 . 13 28 . 04 3.6 Scaling different approaches by addressing the following premise: “You were just given 4 × more fine-tuned for 2 19 and 2 18 steps respectively. The encoder and decoder are both sized similarly to “BERT BASE ”. To experiment with increased model size, we follow the guidelines of “BERT LARGE ” Devlin et al. ( 2018 ) and use d ff = 4096, d model = 1024, d kv = 64 and the encoder and decoder, producing models with 2 × and 4 × as many parameters as our original model. These two variants also have a roughly 2 × and 4 × the computational cost. Using our baseline and these two larger models, we consider three ways of using 4 × as much computation: Training for 4 × as many steps, training for 2 × as many steps with the 2 × bigger model, and training the 4 × bigger model for the “baseline” number of training steps. the data even when training for 2 23 steps. An alternative way for the model to see 4 × as much data is to increase the batch size by a However, training with a 4 × larger batch size can yield a different outcome than training \nExploring the Limits of Transfer Learning Scaling strategy GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 1 × size, 4 × training steps 85 . 33 19 . 33 82 . 45 74 . 72 27 . 08 40 . 66 27 . 93 1 × size, 4 × batch size 84 . 60 19 . 42 82 . 52 74 . 64 27 . 07 40 . 60 27 . 84 2 × size, 2 × training steps 86 . 18 19 . 66 84 . 18 77 . 18 27 . 52 41 . 03 28 . 19 4 × size, 1 × training steps 85 . 91 19 . 73 83 . 86 78 . 04 27 . 47 40 . 71 28 . 10 4 × ensembled 84 . 77 20 . 10 83 . 09 71 . 74 28 . 05 40 . 53 28 . 57 4 × ensembled, fine-tune only 84 . 05 19 . 57 82 . 36 71 . 55 27 . 55 40 . 22 28 . 09 except ensembling fine-tuned models use 4 × the computation as the baseline. for 4 × as many steps ( Shallue et al. , 2018 ). We include an additional experiment where we train our baseline model with a 4 × larger batch size to compare these two cases. separate fine-tuned versions. While this does not use our entire 4 × computational budget, improves the baseline. There was no clear winner between training for 4 × as many steps or using a 4 × larger batch size, though both were beneficial. In general, increasing the 2 × bigger model for 2 × as long and training a 4 × bigger model on any of the tasks we \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu ensembling N separate models has a similar cost to using a model that has an N × higher 3.7 Putting It All Together Objective We swap out the i.i.d. denoising objective in our baseline for the span-corruption Longer training Our baseline model uses a relatively small amount of pre-training ( 1 ⁄ 4 as much as BERT ( Devlin et al. , 2018 ), 1 ⁄ 16 as much as XLNet ( Yang et al. , 2019 ), 1 ⁄ 64 as steps on a batch size of 2 11 sequences of length 512, corresponding to a total of about 1 trillion pre-training tokens (about 32 × as many as our baseline). In Section 3.4.1 , we Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved • Base. • Small. We consider a smaller model, which scales the baseline down by using d model = 512, d ff = 2 , 048, 8-headed attention, and only 6 layers each in the \nExploring the Limits of Transfer Learning • Large. Since our baseline uses a BERT BASE -sized encoder and decoder, we and structure to BERT LARGE . Specifically, this variant uses d model = 1 , 024, d ff = 4 , 096, d kv = 64, 16-headed attention, and 24 layers each in the encoder and • 3B and 11B. To further explore what kind of performance is possible when d model = 1024, a 24 layer encoder and decoder, and d kv = 128. For the “3B” variant, we use d ff = 16 , 384 with 32-headed attention, which results in around 2 . 8 billion parameters; for “11B” we use d ff = 65 , 536 with 128-headed attention producing a model with about 11 billion parameters. We chose to scale up d ff Multi-task pre-training In Section 3.5.3 , we showed that pre-training on a multi-task standard example-proportional mixing (described in Section 3.5.2 ): 710 , 000 for Small, 2 , 620 , 000 for Base, 8 , 660 , 000 for Large, 33 , 500 , 000 for 3B, and 133 , 000 , 000 for 11B. Fine-tuning on individual GLUE and SuperGLUE tasks So far, when fine-tuning For example, our large batch size of 2 11 length-512 sequences would result in the entire every 1 , 000 steps rather than every 5 , 000 steps to ensure we have access to the model’s \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Beam search All of our previous results were reported using greedy decoding. For tasks of α = 0 . 6 ( Wu et al. , 2016 ) for the WMT translation and CNN/DM summarization Test set Since this is our final set of experiments, we report results on the test set rather with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for English- compute official test set scores. 15 , 16 For SQuAD, evaluating on the test set requires We achieved a state-of-the-art average GLUE score of 90 . 3. Notably, our performance was has historically lagged behind human performance, which is 93 . 6 and 95 . 9 respectively ( Wang 15. http://gluebenchmark.com 16. http://super.gluebenchmark.com \nExploring the Limits of Transfer Learning GLUE CoLA SST-2 MRPC MRPC STS-B STS-B Model Average Matthew’s Accuracy F1 Accuracy Pearson Spearman Previous best 89 . 4 a 69 . 2 b 97 . 1 a 93 . 6 b 91 . 5 b 92 . 7 b 92 . 3 b T5-Small 77 . 4 41 . 0 91 . 8 89 . 7 86 . 6 85 . 6 85 . 0 T5-Base 82 . 7 51 . 1 95 . 2 90 . 7 87 . 5 89 . 4 88 . 6 T5-Large 86 . 4 61 . 2 96 . 3 92 . 4 89 . 9 89 . 9 89 . 2 T5-3B 88 . 5 67 . 1 97 . 4 92 . 5 90 . 0 90 . 6 89 . 8 T5-11B 90 . 3 71 . 6 97 . 5 92 . 8 90 . 4 93 . 1 92 . 8 QQP QQP MNLI-m MNLI-mm QNLI RTE WNLI Model F1 Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy Previous best 74 . 8 c 90 . 7 b 91 . 3 a 91 . 0 a 99 . 2 a 89 . 2 a 91 . 8 a T5-Small 70 . 0 88 . 0 82 . 4 82 . 3 90 . 3 69 . 9 69 . 2 T5-Base 72 . 6 89 . 4 87 . 1 86 . 2 93 . 7 80 . 1 78 . 8 T5-Large 73 . 9 89 . 9 89 . 9 89 . 6 94 . 8 87 . 2 85 . 6 T5-3B 74 . 4 89 . 7 91 . 4 91 . 2 96 . 3 91 . 1 89 . 7 T5-11B 75 . 1 90 . 6 92 . 2 91 . 9 96 . 9 92 . 8 94 . 5 SQuAD SQuAD SuperGLUE BoolQ CB CB COPA Model EM F1 Average Accuracy F1 Accuracy Accuracy Previous best 90 . 1 a 95 . 5 a 84 . 6 d 87 . 1 d 90 . 5 d 95 . 2 d 90 . 6 d T5-Small 79 . 10 87 . 24 63 . 3 76 . 4 56 . 9 81 . 6 46 . 0 T5-Base 85 . 44 92 . 08 76 . 2 81 . 4 86 . 2 94 . 0 71 . 2 T5-Large 86 . 66 93 . 79 82 . 3 85 . 4 91 . 6 94 . 8 83 . 4 T5-3B 88 . 53 94 . 95 86 . 4 89 . 9 90 . 3 94 . 4 92 . 0 T5-11B 91 . 26 96 . 22 88 . 9 91 . 2 93 . 9 96 . 8 94 . 8 MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC Model F1a EM F1 Accuracy Accuracy Accuracy Accuracy Previous best 84 . 4 d 52 . 5 d 90 . 6 d 90 . 0 d 88 . 2 d 69 . 9 d 89 . 0 d T5-Small 69 . 3 26 . 3 56 . 3 55 . 4 73 . 3 66 . 9 70 . 5 T5-Base 79 . 7 43 . 1 75 . 0 74 . 2 81 . 5 68 . 3 80 . 8 T5-Large 83 . 3 50 . 7 86 . 8 85 . 9 87 . 8 69 . 3 86 . 3 T5-3B 86 . 8 58 . 3 91 . 2 90 . 4 90 . 7 72 . 1 90 . 4 T5-11B 88 . 1 63 . 3 94 . 1 93 . 4 92 . 5 76 . 9 93 . 8 WMT EnDe WMT EnFr WMT EnRo CNN/DM CNN/DM CNN/DM Model BLEU BLEU BLEU ROUGE-1 ROUGE-2 ROUGE-L Previous best 33 . 8 e 43 . 8 e 38 . 5 f 43 . 47 g 20 . 30 g 40 . 63 g T5-Small 26 . 7 36 . 0 26 . 8 41 . 12 19 . 56 38 . 35 T5-Base 30 . 9 41 . 2 28 . 0 42 . 05 20 . 34 39 . 40 T5-Large 32 . 0 41 . 5 28 . 1 42 . 50 20 . 68 39 . 75 T5-3B 31 . 8 42 . 6 28 . 2 42 . 72 21 . 02 39 . 94 T5-11B 32 . 1 43 . 4 28 . 1 43 . 52 21 . 55 40 . 69 a ( Lan et al. , 2019 ) b ( Wang et al. , 2019c ) c ( Zhu et al. , 2019 ) d ( Liu et al. , 2019c ) e ( Edunov et al. , 2018 ) f ( Lample and Conneau , 2019 ) g ( Dong et al. , 2019 ) \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu the small SQuAD training set. Human performance on SQuAD is estimated at 82 . 30 and 91 . 22 for the Exact Match and F1 metric respectively ( Rajpurkar et al. , 2016 ), so it is not average score of 84 . 6 ( Liu et al. , 2019c ) to 88 . 9). SuperGLUE was designed to include performance of 89 . 8 ( Wang et al. , 2019b ). Interestingly, on the reading comprehension tasks newstest2014 set use the much larger training set from WMT 2018 ( Edunov et al. , 2018 ), on 2 35 ≈ 34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. \nExploring the Limits of Transfer Learning Model GLUE CNNDM SQuAD SGLUE EnDe EnFr EnRo ⋆ Baseline 83 . 28 19 . 24 80 . 88 71 . 36 26 . 98 39 . 82 27 . 65 Baseline-1T 84 . 80 19 . 62 83 . 01 73 . 90 27 . 46 40 . 30 28 . 34 T5-Base 85 . 97 20 . 90 85 . 44 75 . 64 28 . 37 41 . 37 28 . 98 tokens (the same number used for the T5 model variants) instead of 2 35 ≈ 34B 4. Reflection 4.1 Takeaways Text-to-text Our text-to-text framework provides a simple way to train a single model Architectures While some work on transfer learning for NLP has considered architectural \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Unsupervised objectives Overall, we found that most “denoising” objectives, which train Data sets We introduced the “Colossal Clean Crawled Corpus” (C4), which comprises Training strategies We found that the basic approach of updating all of a pre-trained Scaling We compared various strategies for taking advantage of additional compute, includ- Pushing the limits We combined our above insights and trained substantially larger set, and pre-trained model weights for each T5 variant. 1 \nExploring the Limits of Transfer Learning 4.2 Outlook The inconvenience of large models An unsurprising but important result from our More efficient knowledge extraction Recall that one of the goals of pre-training is Formalizing the similarity between tasks We observed that pre-training on unlabeled also help choose supervised pre-training tasks, which has been shown to be helpful for Language-agnostic models We were disappointed to find that English-only pre-training \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Acknowledgments \nExploring the Limits of Transfer Learning Appendix A. Contributions Appendix B. Converting WNLI to Our Text-to-Text Format Appendix C. Example Predictions on CNN/Daily Mail \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu 1. Ground-truth: leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside - watched by ms moolman . Prediction: leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . 2. Ground-truth: researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us . Prediction: researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us . 3. Ground-truth: executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn’t work . it is ’essential that executions are carried out in a constitutional manner,’ said georgia attorney general sam olens . Prediction: the execution of kelly gissendaner, the only woman on georgia’s death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country . 4. Ground-truth: dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain . Prediction: dani alves was not selected for brazil’s friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter \nExploring the Limits of Transfer Learning of a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news . Appendix D. Preprocessed Examples D.1 CoLA Original input: Sentence: John made Bill master of himself. Processed input: cola sentence: John made Bill master of himself. Original target: 1 Processed target: acceptable D.2 RTE Original input: Sentence 1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). Sentence 2: Slovenia has 3,000 inhabitants. Processed input: rte sentence1: A smaller proportion of Yugoslavia’s Italians were settled in Slovenia (at the 1991 national census, some 3000 inhabitants of Slovenia declared themselves as ethnic Italians). sentence2: Slovenia has 3,000 inhabitants. Original target: 1 Processed target: not_entailment D.3 MNLI Original input: Hypothesis: The St. Louis Cardinals have always won. Premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but Processed input: mnli hypothesis: The St. Louis Cardinals have always won. premise: yeah well losing is i mean i’m i’m originally from Saint Louis and Saint Louis Cardinals when they were there were uh a mostly a losing team but \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: 2 Processed target: contradiction D.4 MRPC Original input: Sentence 1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . Sentence 2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . Processed input: mrpc sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . Original target: 1 Processed target: equivalent D.5 QNLI Original input: Question: Where did Jebe die? Sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Processed input: qnli question: Where did Jebe die? sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0 Processed target: entailment D.6 QQP Original input: Question 1: What attributes would have made you highly desirable in ancient Rome? Question 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? Processed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? \nExploring the Limits of Transfer Learning Original target: 0 Processed target: not_duplicate D.7 SST2 Original input: Sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1 Processed target: positive D.8 STSB Original input: Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25 Processed target: 3.2 D.9 CB Original input: Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1 Processed target: contradiction \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu D.10 COPA Original input: Question: effect Premise: Political violence broke out in the nation. Choice 1: Many citizens relocated to the capitol. Choice 2: Many citizens took refuge in other territories. Processed input: copa choice1: Many citizens relocated to the capitol. choice2: Many citizens took refuge in other territories. premise: Political violence broke out in the nation. question: effect Original target: 1 Processed target: True D.11 MultiRC Original input: Answer: There was only pie to eat, rather than traditional breakfast foods Paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br> Question: Why was Joey surprised the morning he woke up for breakfast? Processed input: multirc question: Why was Joey surprised the morning he woke up for breakfast? answer: There was only pie to eat, rather than traditional breakfast foods paragraph: <b>Sent 1: </b>Once upon a time, there was a squirrel named Joey.<br><b>Sent 2: </b>Joey loved to go outside and play with his cousin Jimmy.<br><b>Sent 3: </b>Joey and Jimmy played silly games together, and were always laughing.<br><b>Sent 4: </b>One day, Joey and Jimmy went swimming together \nExploring the Limits of Transfer Learning at their Aunt Julie’s pond.<br><b>Sent 5: </b>Joey woke up early in the morning to eat some food before they left.<br><b>Sent 6: </b>He couldn’t find anything to eat except for pie!<br><b>Sent 7: </b>Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.<br><b>Sent 8: </b>After he ate, he and Jimmy went to the pond.<br><b>Sent 9: </b>On their way there they saw their friend Jack Rabbit.<br><b>Sent 10: </b>They dove into the water and swam for several hours.<br><b>Sent 11: </b>The sun was out, but the breeze was cold.<br><b>Sent 12: </b>Joey and Jimmy got out of the water and started walking home.<br><b>Sent 13: </b>Their fur was wet, and the breeze chilled them.<br><b>Sent 14: </b>When they got home, they dried off, and Jimmy put on his favorite purple shirt.<br><b>Sent 15: </b>Joey put on a blue shirt with red and green dots.<br><b>Sent 16: </b>The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.<br> Original target: 1 Processed target: True D.12 WiC Original input: POS: N Sentence 1: It was the deliberation of his act that was insulting . Sentence 2: The deliberations of the jury . Word: deliberation Processed input: wic pos: N sentence1: It was the deliberation of his act that was insulting . sentence2: The deliberations of the jury . word: deliberation Original target: 0 Processed target: False D.13 WSC and DPR Original input: Span 2 text: it Span 1 text: stable Span 2 index: 20 Span 1 index: 1 Text: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made it pleasant and airy. Processed input: wsc: The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy. \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: 1 Processed target: stable D.14 CNN/Daily Mail Original input: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game . Processed input: summarize: marouane fellaini and adnan januzaj continue to show the world they are not just teammates but also best mates. the manchester united and belgium duo both posted pictures of themselves out at a restaurant on monday night ahead of their game against newcastle on wednesday . januzaj poses in the middle of fellaini and a friend looking like somebody who failed to receive the memo about it being a jackson 5 themed night. premier league duo adnan januzaj and marouane fellaini pose with a friend on the dance floor . manchester united and belgium duo fellaini and januzaj are good friends both on and off the pitch . manchester united ace fellaini runs over to the bench to celebrate his goal against qpr with friend januzaj . the disco effect in the background adds to the theory, but januzaj doesn’t seem to mind as they later pose on the dance floor with other friends. united haven’t had too many reasons to have a song and dance this season so it seems they may be hitting the discotheques as another form of release. however, victory against newcastle on wednesday would leave manager louis van gaal at least tapping his toes as they continue to fight for a champions league spot this season. januzaj and robin van persie join fellaini in celebrating in front of the manchester united fans at west brom . januzaj receives some words of wisdom \nExploring the Limits of Transfer Learning from manchester united’s dutch manager louis van gaal . januzaj and fellaini are joined by some friends as they take to the dance floor ahead of the newcastle game . Original target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . Processed target: the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . D.15 SQuAD Original input: Question: What does increased oxygen concentrations in the patient’s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient’s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Original target: carbon monoxide Processed target: carbon monoxide D.16 WMT English to German Original input: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote. Processed input: translate English to German: \"Luigi often said to me that he never wanted the brothers to end up in court,\" she wrote. Original target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie. Processed target: \"Luigi sagte oft zu mir, dass er nie wollte, dass die Brüder vor Gericht landen\", schrieb sie. D.17 WMT English to French Original input: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room. Processed input: translate English to French: This image section from an infrared recording by the Spitzer telescope shows a \"family portrait\" of countless generations of stars: the oldest stars are seen as blue dots, while more difficult to identify are the pink-coloured \"new-borns\" in the star delivery room. Original target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers. Processed target: Ce détail d’une photographie infrarouge prise par le télescope Spitzer montre un \"portrait de famille\" des innombrables générations d’étoiles: les plus vieilles étoiles sont en bleu et les points roses, plus difficiles à identifier, sont les \"nouveau-nés\" dans la salle d’accouchement de l’univers. D.18 WMT English to Romanian Original input: Taco Bell said it plans to add 2,000 locations in the US by 2022. Processed input: translate English to Romanian: Taco Bell said it plans to add 2,000 locations in the US by 2022. Original target: Taco Bell a afirmat că, până în 2022, intent , ionează să deschidă 2000 de restaurante în SUA. \nExploring the Limits of Transfer Learning Processed target: Taco Bell a afirmat că, până în 2022, intent , ionează să deschidă 2000 de restaurante în SUA. \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu Appendix E. Scores on Every Task for All Experiments \nGLUE SuperGLUE WMT Score CoLA SST-2 MRPC MRPC STSB STSB QQP QQP MNLI m MNLI mm QNLI RTE CNN/DM SQuAD Score BoolQ CB CB COPA MultiRC MultiRC ReCoRD ReCoRD RTE WiC WSC EnDe EnFr EnRo Table Experiment Average MCC Acc F1 Acc PCC SCC F1 Acc Acc Acc Acc Acc R-1-F R-2-F R-L-F EM F1 Average Acc F1 Acc Acc F1 EM F1 EM Acc Acc Acc BLEU BLEU BLEU 1 ⋆ Baseline average 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 1 Baseline standard deviation 0 . 235 1 . 111 0 . 569 0 . 729 1 . 019 0 . 374 0 . 418 0 . 108 0 . 070 0 . 291 0 . 231 0 . 361 1 . 393 0 . 065 0 . 065 0 . 058 0 . 343 0 . 226 0 . 416 0 . 365 3 . 237 2 . 560 2 . 741 0 . 716 1 . 011 0 . 370 0 . 379 1 . 228 0 . 850 2 . 029 0 . 112 0 . 090 0 . 108 1 No pre-training 66 . 22 12 . 29 80 . 62 81 . 42 73 . 04 72 . 58 72 . 97 81 . 94 86 . 62 68 . 02 67 . 98 75 . 69 58 . 84 39 . 19 17 . 60 36 . 69 50 . 31 61 . 97 53 . 04 65 . 38 71 . 61 76 . 79 62 . 00 59 . 10 0 . 84 20 . 33 17 . 95 54 . 15 54 . 08 65 . 38 25 . 86 39 . 77 24 . 04 2 ⋆ Enc/dec, denoising 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 2 Enc/dec, shared, denoising 82 . 81 55 . 24 91 . 86 91 . 58 88 . 24 87 . 43 87 . 58 88 . 69 91 . 60 83 . 88 84 . 01 90 . 23 73 . 65 41 . 11 18 . 78 38 . 48 80 . 63 88 . 49 70 . 73 77 . 13 95 . 04 96 . 43 65 . 00 66 . 16 22 . 98 68 . 95 68 . 09 70 . 76 68 . 18 75 . 96 26 . 72 39 . 03 27 . 46 2 Enc/dec, 6 layers, denoising 80 . 88 46 . 26 92 . 09 91 . 51 87 . 99 87 . 01 86 . 76 87 . 93 90 . 97 82 . 20 82 . 41 88 . 83 71 . 48 40 . 83 18 . 97 38 . 31 77 . 59 86 . 07 68 . 42 73 . 79 91 . 70 92 . 86 67 . 00 61 . 02 19 . 62 61 . 26 60 . 33 72 . 20 65 . 99 75 . 00 26 . 38 38 . 40 26 . 95 2 Language model, denoising 74 . 70 24 . 50 90 . 60 86 . 08 78 . 92 85 . 22 85 . 42 85 . 40 88 . 99 76 . 72 77 . 05 86 . 02 64 . 62 39 . 49 17 . 93 36 . 91 61 . 14 71 . 37 55 . 02 65 . 47 60 . 08 71 . 43 58 . 00 43 . 03 2 . 94 53 . 35 52 . 31 53 . 07 58 . 62 63 . 46 25 . 09 35 . 28 25 . 86 2 Prefix LM, denoising 81 . 82 49 . 99 92 . 43 91 . 43 88 . 24 87 . 20 86 . 98 88 . 41 91 . 39 82 . 32 82 . 93 88 . 71 74 . 01 40 . 46 18 . 61 37 . 90 78 . 94 87 . 31 68 . 11 75 . 50 93 . 37 91 . 07 60 . 00 63 . 43 21 . 20 65 . 03 64 . 11 71 . 48 65 . 67 73 . 08 26 . 43 37 . 98 27 . 39 2 Enc/dec, LM 79 . 56 42 . 03 91 . 86 91 . 64 88 . 24 87 . 13 87 . 00 88 . 21 91 . 15 81 . 68 81 . 66 88 . 54 65 . 70 40 . 67 18 . 59 38 . 13 76 . 02 84 . 85 64 . 29 72 . 23 85 . 74 89 . 29 57 . 00 60 . 53 16 . 26 59 . 28 58 . 30 65 . 34 64 . 89 70 . 19 26 . 27 39 . 17 26 . 86 2 Enc/dec, shared, LM 79 . 60 44 . 83 92 . 09 90 . 20 85 . 78 86 . 03 85 . 87 87 . 77 91 . 02 81 . 74 82 . 29 89 . 16 65 . 34 40 . 16 18 . 13 37 . 59 76 . 35 84 . 86 63 . 50 70 . 49 91 . 41 87 . 50 55 . 00 60 . 21 16 . 89 57 . 83 56 . 73 63 . 54 63 . 48 70 . 19 26 . 62 39 . 17 27 . 05 2 Enc/dec, 6 layers, LM 78 . 67 38 . 72 91 . 40 90 . 40 86 . 52 86 . 82 86 . 49 87 . 87 91 . 03 80 . 99 80 . 92 88 . 05 65 . 70 40 . 29 18 . 26 37 . 70 75 . 32 84 . 06 64 . 06 71 . 38 85 . 25 89 . 29 60 . 00 57 . 56 16 . 79 55 . 22 54 . 30 66 . 79 63 . 95 71 . 15 26 . 13 38 . 42 26 . 89 2 Language model, LM 73 . 78 28 . 53 89 . 79 85 . 23 78 . 68 84 . 22 84 . 00 84 . 88 88 . 70 74 . 94 75 . 77 84 . 84 58 . 84 38 . 97 17 . 54 36 . 37 53 . 81 64 . 55 56 . 51 64 . 22 59 . 92 71 . 43 64 . 00 53 . 04 1 . 05 46 . 81 45 . 78 58 . 84 56 . 74 69 . 23 25 . 23 34 . 31 25 . 38 2 Prefix LM, LM 79 . 68 41 . 26 92 . 09 90 . 11 86 . 27 86 . 82 86 . 32 88 . 35 91 . 35 81 . 71 82 . 02 89 . 04 68 . 59 39 . 66 17 . 84 37 . 13 76 . 87 85 . 39 64 . 86 71 . 47 93 . 37 91 . 07 57 . 00 58 . 67 16 . 89 59 . 25 58 . 16 64 . 26 66 . 30 71 . 15 26 . 28 37 . 51 26 . 76 4 Language modeling with prefix 80 . 69 44 . 22 93 . 00 91 . 68 88 . 48 87 . 20 87 . 18 88 . 39 91 . 41 82 . 66 83 . 09 89 . 29 68 . 95 40 . 71 18 . 94 38 . 15 77 . 99 86 . 43 65 . 27 73 . 55 83 . 95 87 . 50 55 . 00 59 . 65 18 . 89 61 . 76 60 . 76 68 . 59 65 . 67 73 . 08 26 . 86 39 . 73 27 . 49 4 BERT-style ( Devlin et al. , 2018 ) 82 . 96 52 . 49 92 . 55 92 . 79 89 . 95 87 . 68 87 . 66 88 . 47 91 . 44 83 . 60 84 . 05 90 . 33 75 . 45 41 . 27 19 . 17 38 . 72 80 . 65 88 . 24 69 . 85 76 . 48 94 . 37 94 . 64 61 . 00 63 . 29 25 . 08 66 . 76 65 . 85 72 . 20 69 . 12 75 . 00 26 . 78 40 . 03 27 . 41 4 Deshuffling 73 . 17 22 . 82 87 . 16 86 . 88 81 . 13 84 . 03 83 . 82 86 . 38 89 . 90 76 . 30 76 . 34 84 . 18 58 . 84 40 . 75 18 . 59 38 . 10 67 . 61 76 . 76 58 . 47 69 . 17 63 . 70 78 . 57 56 . 00 59 . 85 12 . 70 45 . 52 44 . 36 57 . 04 64 . 89 68 . 27 26 . 11 39 . 30 25 . 62 5 BERT-style ( Devlin et al. , 2018 ) 82 . 96 52 . 49 92 . 55 92 . 79 89 . 95 87 . 68 87 . 66 88 . 47 91 . 44 83 . 60 84 . 05 90 . 33 75 . 45 41 . 27 19 . 17 38 . 72 80 . 65 88 . 24 69 . 85 76 . 48 94 . 37 94 . 64 61 . 00 63 . 29 25 . 08 66 . 76 65 . 85 72 . 20 69 . 12 75 . 00 26 . 78 40 . 03 27 . 41 5 MASS-style ( Song et al. , 2019 ) 82 . 32 47 . 01 91 . 63 92 . 53 89 . 71 88 . 21 88 . 18 88 . 58 91 . 44 82 . 96 83 . 67 90 . 02 77 . 26 41 . 16 19 . 16 38 . 55 80 . 10 88 . 07 69 . 28 75 . 08 84 . 98 89 . 29 63 . 00 64 . 46 23 . 50 66 . 71 65 . 91 72 . 20 67 . 71 78 . 85 26 . 79 39 . 89 27 . 55 5 ⋆ Replace corrupted spans 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 5 Drop corrupted tokens 84 . 44 60 . 04 92 . 89 92 . 79 89 . 95 87 . 28 86 . 85 88 . 56 91 . 54 83 . 94 83 . 92 90 . 74 79 . 42 41 . 27 19 . 31 38 . 70 80 . 52 88 . 28 68 . 67 75 . 90 96 . 02 94 . 64 56 . 00 65 . 06 23 . 92 65 . 54 64 . 60 71 . 12 67 . 40 74 . 04 27 . 07 39 . 76 27 . 82 6 Corruption rate = 10% 82 . 82 52 . 71 92 . 09 91 . 55 88 . 24 88 . 19 88 . 15 88 . 47 91 . 40 83 . 50 84 . 51 90 . 33 75 . 45 41 . 05 19 . 00 38 . 53 80 . 38 88 . 36 69 . 55 74 . 98 92 . 37 92 . 86 62 . 00 66 . 04 24 . 66 67 . 93 67 . 09 70 . 76 67 . 24 75 . 96 26 . 87 39 . 28 27 . 44 6 ⋆ Corruption rate = 15% 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 6 Corruption rate = 25% 83 . 00 53 . 47 93 . 00 92 . 44 89 . 46 87 . 36 87 . 36 88 . 68 91 . 53 84 . 44 84 . 15 90 . 77 74 . 01 41 . 69 19 . 54 39 . 14 80 . 96 88 . 61 70 . 48 76 . 39 93 . 02 92 . 86 68 . 00 65 . 46 24 . 66 68 . 20 67 . 39 73 . 65 67 . 87 72 . 12 27 . 04 39 . 83 27 . 47 6 Corruption rate = 50% 81 . 27 46 . 26 91 . 63 91 . 11 87 . 99 87 . 87 87 . 64 88 . 70 91 . 57 83 . 64 84 . 10 90 . 24 70 . 76 41 . 51 19 . 32 38 . 89 79 . 80 87 . 76 70 . 33 75 . 02 93 . 05 92 . 86 68 . 00 62 . 97 24 . 13 64 . 94 64 . 13 72 . 20 68 . 50 77 . 88 27 . 01 39 . 90 27 . 49 7 ⋆ Baseline (i.i.d.) 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 7 Average span length = 2 83 . 54 53 . 82 92 . 20 93 . 05 90 . 44 87 . 85 87 . 71 88 . 42 91 . 40 84 . 28 84 . 46 90 . 88 77 . 62 41 . 23 19 . 39 38 . 69 82 . 09 89 . 69 72 . 20 77 . 06 90 . 43 91 . 07 70 . 00 66 . 28 26 . 13 71 . 34 70 . 61 75 . 45 68 . 34 78 . 85 26 . 76 39 . 99 27 . 63 7 Average span length = 3 83 . 49 53 . 90 92 . 43 92 . 25 89 . 46 87 . 49 87 . 53 88 . 72 91 . 51 84 . 85 84 . 84 90 . 99 77 . 26 41 . 50 19 . 62 38 . 94 81 . 84 89 . 66 72 . 53 76 . 85 94 . 37 94 . 64 70 . 00 67 . 64 28 . 75 70 . 84 69 . 90 74 . 73 67 . 71 77 . 88 26 . 86 39 . 65 27 . 62 7 Average span length = 5 83 . 40 52 . 12 93 . 12 92 . 63 89 . 71 88 . 70 88 . 47 88 . 84 91 . 64 84 . 32 84 . 29 90 . 79 76 . 90 41 . 39 19 . 24 38 . 82 82 . 05 89 . 79 72 . 23 77 . 06 83 . 06 89 . 29 69 . 00 68 . 16 30 . 12 71 . 36 70 . 53 75 . 81 69 . 91 79 . 81 26 . 88 39 . 40 27 . 53 7 Average span length = 10 82 . 85 50 . 11 92 . 09 91 . 95 88 . 97 88 . 45 88 . 22 88 . 86 91 . 63 84 . 34 84 . 28 91 . 07 76 . 17 41 . 38 19 . 33 38 . 80 81 . 84 89 . 39 70 . 44 76 . 45 87 . 40 89 . 29 65 . 00 66 . 87 29 . 59 69 . 82 68 . 94 72 . 56 67 . 55 75 . 96 26 . 79 39 . 49 27 . 69 8 ⋆ C4 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 8 C4, unfiltered 81 . 46 48 . 01 91 . 63 92 . 72 89 . 95 87 . 79 87 . 60 88 . 31 91 . 27 82 . 30 82 . 34 88 . 71 72 . 20 41 . 09 19 . 14 38 . 54 78 . 78 87 . 04 68 . 04 75 . 75 89 . 17 91 . 07 62 . 00 65 . 52 25 . 60 62 . 42 61 . 58 69 . 68 67 . 08 72 . 12 26 . 55 39 . 34 27 . 21 8 RealNews-like 83 . 83 56 . 55 92 . 66 92 . 06 88 . 97 87 . 71 87 . 37 88 . 51 91 . 49 84 . 35 84 . 46 90 . 61 78 . 34 41 . 38 19 . 23 38 . 84 80 . 39 88 . 50 72 . 38 77 . 00 93 . 09 94 . 64 66 . 00 65 . 92 23 . 82 74 . 56 73 . 72 75 . 81 66 . 61 80 . 77 26 . 75 39 . 90 27 . 48 8 WebText-like 84 . 03 56 . 38 93 . 12 92 . 31 89 . 22 88 . 69 88 . 68 88 . 65 91 . 56 84 . 70 84 . 84 90 . 83 77 . 62 41 . 23 19 . 31 38 . 70 81 . 42 89 . 15 71 . 40 76 . 88 83 . 08 89 . 29 66 . 00 64 . 10 24 . 24 72 . 24 71 . 36 75 . 45 68 . 03 82 . 69 26 . 80 39 . 74 27 . 59 8 Wikipedia 81 . 85 45 . 53 92 . 32 91 . 67 88 . 24 85 . 62 86 . 40 88 . 37 91 . 34 82 . 61 83 . 25 90 . 96 77 . 26 41 . 39 19 . 31 38 . 81 81 . 29 89 . 18 68 . 01 76 . 12 56 . 03 80 . 36 67 . 00 65 . 01 25 . 92 69 . 03 68 . 06 74 . 73 67 . 08 76 . 92 26 . 94 39 . 69 27 . 67 8 Wikipedia + TBC 83 . 65 55 . 53 92 . 78 92 . 41 89 . 22 86 . 67 86 . 27 89 . 47 92 . 29 84 . 38 83 . 45 91 . 94 76 . 90 41 . 22 19 . 28 38 . 67 82 . 08 89 . 70 73 . 24 76 . 22 95 . 40 92 . 86 69 . 00 51 . 59 50 . 93 69 . 53 68 . 51 77 . 62 66 . 93 81 . 73 26 . 77 39 . 63 27 . 57 9 ⋆ Full data set 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 9 2 29 (64 repeats) 82 . 87 53 . 82 92 . 78 91 . 79 88 . 73 87 . 56 87 . 58 88 . 73 91 . 54 84 . 07 84 . 21 90 . 59 73 . 65 41 . 18 19 . 19 38 . 67 80 . 97 88 . 90 72 . 03 76 . 76 92 . 96 92 . 86 66 . 00 65 . 11 26 . 76 69 . 35 68 . 49 75 . 81 67 . 24 82 . 69 26 . 83 39 . 74 27 . 63 9 2 27 (256 repeats) 82 . 62 50 . 60 92 . 32 92 . 07 88 . 73 87 . 83 87 . 60 88 . 65 91 . 54 83 . 43 84 . 37 90 . 12 75 . 81 41 . 24 19 . 20 38 . 70 79 . 78 87 . 63 69 . 97 75 . 29 93 . 42 91 . 07 63 . 00 61 . 82 23 . 61 66 . 27 65 . 39 73 . 65 66 . 30 80 . 77 27 . 02 39 . 71 27 . 33 9 2 25 (1 , 024 repeats) 79 . 55 43 . 84 91 . 28 89 . 32 85 . 05 85 . 92 85 . 74 88 . 05 91 . 09 81 . 29 81 . 72 87 . 90 69 . 31 40 . 66 18 . 57 38 . 13 76 . 27 84 . 58 64 . 76 72 . 63 83 . 97 82 . 14 64 . 00 59 . 39 17 . 94 56 . 94 56 . 04 64 . 98 65 . 20 73 . 08 26 . 38 39 . 56 26 . 80 9 2 23 (4 , 096 repeats) 76 . 34 32 . 68 89 . 45 89 . 84 86 . 03 83 . 49 83 . 42 87 . 18 90 . 61 77 . 80 78 . 69 85 . 47 64 . 62 40 . 16 18 . 33 37 . 66 70 . 92 80 . 20 59 . 29 69 . 85 73 . 48 73 . 21 56 . 00 57 . 66 14 . 38 46 . 69 45 . 79 59 . 57 65 . 05 68 . 27 26 . 37 38 . 84 25 . 81 10 ⋆ All parameters 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 10 Adapter layers, d = 32 80 . 52 45 . 33 91 . 63 90 . 59 86 . 76 88 . 38 88 . 06 86 . 99 90 . 26 83 . 63 83 . 94 90 . 72 67 . 15 34 . 50 15 . 08 32 . 15 79 . 32 87 . 70 60 . 40 65 . 32 50 . 87 73 . 21 52 . 00 58 . 61 19 . 41 65 . 50 64 . 58 62 . 09 64 . 58 73 . 08 13 . 84 17 . 88 15 . 54 10 Adapter layers, d = 128 81 . 51 45 . 35 92 . 89 91 . 49 88 . 24 87 . 73 87 . 65 87 . 73 90 . 93 83 . 64 84 . 09 90 . 52 72 . 56 36 . 71 16 . 62 34 . 37 79 . 47 87 . 61 63 . 03 69 . 20 52 . 21 75 . 00 56 . 00 61 . 08 18 . 05 67 . 94 66 . 97 68 . 59 66 . 77 73 . 08 19 . 83 27 . 50 22 . 63 10 Adapter layers, d = 512 81 . 54 44 . 25 93 . 35 91 . 00 87 . 25 88 . 74 88 . 44 88 . 02 91 . 15 83 . 08 83 . 80 89 . 62 74 . 37 38 . 63 17 . 78 36 . 25 79 . 18 87 . 32 64 . 30 73 . 18 59 . 86 71 . 43 56 . 00 62 . 94 18 . 57 66 . 56 65 . 74 70 . 76 67 . 87 74 . 04 23 . 45 33 . 98 25 . 81 10 Adapter layers, d = 2048 82 . 62 49 . 86 92 . 55 91 . 30 87 . 99 88 . 46 88 . 35 88 . 36 91 . 40 83 . 63 83 . 18 90 . 66 76 . 53 39 . 44 18 . 30 37 . 06 79 . 40 87 . 36 68 . 61 74 . 53 88 . 00 91 . 07 58 . 00 61 . 10 18 . 89 66 . 73 66 . 06 73 . 29 71 . 16 75 . 96 25 . 64 36 . 92 26 . 93 10 Gradual Unfreezing 82 . 50 51 . 74 91 . 97 92 . 61 89 . 71 87 . 27 86 . 90 88 . 26 91 . 35 83 . 42 83 . 49 89 . 71 75 . 09 40 . 88 18 . 95 38 . 40 79 . 17 87 . 30 70 . 79 75 . 51 93 . 09 94 . 64 70 . 00 62 . 03 21 . 51 65 . 69 64 . 79 72 . 92 69 . 12 77 . 89 26 . 71 39 . 02 26 . 93 11 ⋆ Baseline (pre-train/fine-tune) 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 11 Equal 76 . 13 39 . 47 90 . 94 82 . 90 75 . 74 78 . 83 78 . 44 86 . 45 89 . 71 82 . 08 82 . 92 90 . 13 59 . 93 40 . 95 19 . 02 38 . 39 76 . 51 85 . 61 63 . 37 73 . 06 82 . 37 83 . 93 65 . 00 60 . 89 17 . 52 60 . 51 59 . 70 61 . 01 60 . 03 65 . 38 23 . 89 34 . 31 26 . 78 11 Examples-proportional, K = 2 16 80 . 45 42 . 07 91 . 97 90 . 97 87 . 50 85 . 41 85 . 04 86 . 89 90 . 10 83 . 01 83 . 66 90 . 74 72 . 56 41 . 16 19 . 04 38 . 59 77 . 25 85 . 72 69 . 95 76 . 67 86 . 38 89 . 29 70 . 00 65 . 93 27 . 91 62 . 78 61 . 95 76 . 90 65 . 83 73 . 08 24 . 35 34 . 99 27 . 10 11 Examples-proportional, K = 2 17 81 . 56 47 . 35 91 . 40 91 . 55 88 . 24 86 . 15 85 . 93 86 . 94 90 . 06 82 . 76 84 . 12 90 . 79 75 . 09 41 . 06 19 . 12 38 . 47 77 . 00 85 . 87 67 . 91 77 . 89 77 . 54 85 . 71 57 . 00 67 . 78 27 . 07 61 . 51 60 . 54 79 . 06 65 . 20 74 . 04 24 . 36 35 . 00 27 . 25 11 Examples-proportional, K = 2 18 81 . 67 46 . 85 91 . 63 91 . 99 88 . 73 87 . 68 87 . 20 86 . 93 90 . 35 83 . 30 84 . 01 91 . 47 73 . 29 40 . 96 19 . 07 38 . 43 78 . 17 86 . 74 67 . 94 76 . 57 78 . 88 87 . 50 62 . 00 67 . 70 30 . 85 63 . 43 62 . 54 76 . 53 65 . 67 67 . 31 24 . 57 35 . 19 27 . 39 11 Examples-proportional, K = 2 19 81 . 42 45 . 94 91 . 63 92 . 20 89 . 22 88 . 44 88 . 32 86 . 84 90 . 10 83 . 73 84 . 29 91 . 84 70 . 40 41 . 26 19 . 24 38 . 71 79 . 78 88 . 15 67 . 30 75 . 66 75 . 59 87 . 50 59 . 00 68 . 22 30 . 64 65 . 32 64 . 29 73 . 65 65 . 05 69 . 23 25 . 21 36 . 30 27 . 76 11 Examples-proportional, K = 2 20 80 . 80 42 . 55 92 . 78 91 . 27 87 . 99 88 . 36 88 . 10 86 . 10 89 . 62 84 . 15 84 . 26 92 . 20 68 . 95 41 . 05 19 . 24 38 . 46 80 . 36 88 . 27 67 . 38 73 . 21 76 . 18 83 . 93 62 . 00 67 . 57 26 . 86 66 . 12 65 . 22 76 . 90 64 . 73 69 . 23 25 . 66 36 . 93 27 . 68 11 Examples-proportional, K = 2 21 79 . 83 44 . 45 91 . 28 89 . 00 84 . 31 87 . 54 87 . 40 84 . 93 88 . 53 82 . 54 84 . 16 90 . 85 67 . 87 40 . 51 18 . 79 37 . 92 79 . 50 87 . 48 65 . 10 71 . 16 68 . 88 85 . 71 57 . 00 62 . 75 23 . 40 64 . 50 63 . 65 72 . 92 64 . 11 71 . 15 25 . 82 37 . 22 27 . 13 11 Temperature-scaled, T = 2 81 . 90 54 . 00 91 . 74 90 . 56 86 . 76 85 . 11 84 . 60 86 . 40 89 . 74 83 . 47 84 . 15 91 . 51 72 . 56 41 . 09 19 . 28 38 . 54 79 . 42 87 . 77 69 . 92 76 . 73 92 . 37 92 . 86 57 . 00 69 . 80 31 . 90 66 . 65 65 . 74 72 . 92 67 . 08 75 . 96 25 . 42 36 . 72 27 . 20 11 Temperature-scaled, T = 4 80 . 56 45 . 38 91 . 97 89 . 68 85 . 78 83 . 13 82 . 76 86 . 39 90 . 00 82 . 78 84 . 19 91 . 16 73 . 65 41 . 09 19 . 22 38 . 51 77 . 99 86 . 81 69 . 54 76 . 76 97 . 36 96 . 43 59 . 00 68 . 10 31 . 48 64 . 26 63 . 27 74 . 73 64 . 26 71 . 15 25 . 04 35 . 82 27 . 45 11 Temperature-scaled, T = 8 77 . 21 40 . 07 91 . 06 88 . 11 83 . 33 79 . 20 79 . 06 86 . 60 89 . 90 83 . 05 83 . 56 90 . 21 59 . 93 41 . 01 19 . 10 38 . 40 77 . 14 85 . 99 66 . 07 73 . 94 93 . 70 94 . 64 60 . 00 66 . 36 26 . 86 63 . 46 62 . 60 62 . 09 63 . 32 65 . 38 24 . 55 35 . 35 27 . 17 12 ⋆ Unsupervised pre-training + fine-tuning 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 12 Multi-task training 81 . 42 45 . 94 91 . 63 92 . 20 89 . 22 88 . 44 88 . 32 86 . 84 90 . 10 83 . 73 84 . 29 91 . 84 70 . 40 41 . 26 19 . 24 38 . 71 79 . 78 88 . 15 67 . 30 75 . 66 75 . 59 87 . 50 59 . 00 68 . 22 30 . 64 65 . 32 64 . 29 73 . 65 65 . 05 69 . 23 25 . 21 36 . 30 27 . 76 12 Multi-task pre-training + fine-tuning 83 . 11 51 . 42 92 . 66 91 . 73 88 . 73 88 . 06 87 . 70 88 . 61 91 . 61 84 . 09 84 . 31 91 . 85 76 . 53 41 . 15 19 . 12 38 . 59 80 . 26 88 . 50 71 . 03 79 . 54 81 . 69 87 . 50 65 . 00 70 . 72 31 . 48 65 . 94 65 . 03 81 . 23 68 . 18 73 . 08 27 . 08 39 . 80 28 . 07 12 Leave-one-out multi-task training 81 . 98 48 . 00 93 . 23 91 . 72 88 . 24 87 . 76 87 . 32 88 . 61 91 . 44 84 . 00 84 . 11 90 . 79 72 . 20 41 . 34 19 . 05 38 . 77 79 . 97 88 . 10 71 . 68 78 . 35 86 . 76 89 . 29 66 . 00 68 . 09 29 . 49 66 . 23 65 . 27 79 . 06 68 . 65 78 . 85 26 . 93 39 . 79 27 . 87 12 Supervised multi-task pre-training 79 . 93 36 . 60 92 . 43 91 . 58 88 . 24 87 . 03 86 . 78 88 . 15 91 . 20 82 . 87 83 . 16 90 . 13 70 . 76 41 . 12 18 . 96 38 . 49 77 . 38 85 . 65 65 . 36 75 . 66 68 . 87 83 . 93 58 . 00 64 . 81 21 . 93 55 . 37 54 . 61 71 . 12 67 . 40 75 . 96 26 . 81 40 . 13 28 . 04 13 ⋆ Baseline 83 . 28 53 . 84 92 . 68 92 . 07 88 . 92 88 . 02 87 . 94 88 . 67 91 . 56 84 . 24 84 . 57 90 . 48 76 . 28 41 . 33 19 . 24 38 . 77 80 . 88 88 . 81 71 . 36 76 . 62 91 . 22 91 . 96 66 . 20 66 . 13 25 . 78 69 . 05 68 . 16 75 . 34 68 . 04 78 . 56 26 . 98 39 . 82 27 . 65 13 1 × size, 4 × training steps 85 . 33 60 . 29 93 . 81 94 . 06 91 . 67 89 . 42 89 . 25 89 . 15 91 . 87 86 . 01 85 . 70 91 . 63 78 . 34 41 . 52 19 . 33 38 . 96 82 . 45 90 . 19 74 . 72 79 . 17 94 . 75 92 . 86 71 . 00 67 . 34 29 . 70 72 . 63 71 . 59 78 . 34 72 . 10 82 . 69 27 . 08 40 . 66 27 . 93 13 1 × size, 4 × batch size 84 . 60 56 . 08 93 . 12 92 . 31 89 . 22 88 . 85 88 . 84 89 . 35 92 . 07 85 . 98 86 . 13 91 . 07 80 . 14 41 . 70 19 . 42 39 . 08 82 . 52 90 . 21 74 . 64 78 . 78 93 . 69 94 . 64 72 . 00 68 . 09 30 . 95 74 . 73 73 . 90 76 . 53 70 . 06 81 . 73 27 . 07 40 . 60 27 . 84 13 2 × size, 2 × training steps 86 . 18 62 . 04 93 . 69 93 . 36 90 . 69 89 . 18 89 . 23 89 . 35 92 . 05 87 . 23 87 . 05 92 . 68 81 . 95 41 . 74 19 . 66 39 . 14 84 . 18 91 . 29 77 . 18 80 . 98 97 . 36 96 . 43 74 . 00 71 . 34 35 . 68 77 . 11 76 . 34 80 . 51 69 . 28 85 . 58 27 . 52 41 . 03 28 . 19 13 4 × size, 1 × training steps 85 . 91 57 . 58 94 . 38 92 . 67 89 . 95 89 . 60 89 . 60 89 . 44 92 . 14 87 . 05 87 . 12 93 . 12 83 . 39 41 . 60 19 . 73 39 . 08 83 . 86 91 . 32 78 . 04 81 . 38 89 . 09 94 . 64 73 . 00 73 . 74 40 . 40 78 . 25 77 . 40 81 . 59 70 . 22 91 . 35 27 . 47 40 . 71 28 . 10 13 4 × ensembled 84 . 77 56 . 14 93 . 46 93 . 31 90 . 67 89 . 71 89 . 60 89 . 62 92 . 24 86 . 22 86 . 53 91 . 60 77 . 98 42 . 10 20 . 10 39 . 56 83 . 09 90 . 40 71 . 74 77 . 58 89 . 85 91 . 07 66 . 00 69 . 32 29 . 49 72 . 67 71 . 94 76 . 90 69 . 12 72 . 12 28 . 05 40 . 53 28 . 09 13 4 × ensembled, fine-tune only 84 . 05 54 . 78 92 . 78 93 . 15 90 . 44 88 . 34 88 . 12 89 . 27 91 . 97 85 . 33 85 . 88 90 . 98 77 . 62 41 . 66 19 . 57 39 . 12 82 . 36 89 . 86 71 . 56 77 . 43 90 . 07 92 . 86 69 . 00 67 . 31 26 . 34 70 . 47 69 . 64 75 . 45 68 . 18 74 . 04 27 . 55 40 . 22 28 . 09 Table 16: Score achieved on every task we consider for all of the experiments in this paper. In the first column, we list the table where the condensed results were presented for a given experiment. As in the main text, a row marked with ⋆ denotes our baseline model (described in Section 3.1 ). \nRaffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li and Liu"
}