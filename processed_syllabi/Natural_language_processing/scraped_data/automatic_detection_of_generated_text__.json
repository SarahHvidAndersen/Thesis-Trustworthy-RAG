{
    "document_type": "research_paper",
    "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
    "author": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, Douglas Eck",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\automatic_detection_of_generated_text__.pdf",
    "date_published": "2020-05-11",
    "keywords": "",
    "flag": "",
    "text": "Automatic Detection of Generated Text is Easiest when Humans are Fooled Daphne Ippolito †‡ ∗ daphnei@seas.upenn.edu Daniel Duckworth ‡ * duckworthd@google.com Chris Callison-Burch †‡ ccb@seas.upenn.edu Douglas Eck ‡ deck@google.com Abstract Recent advancements in neural language mod- elling make it possible to rapidly generate vast amounts of human-sounding text. The ca- pabilities of humans and automatic discrimi- nators to detect machine-generated text have been a large source of research interest, but hu- mans and machines rely on different cues to make their decisions. Here, we perform care- ful benchmarking and analysis of three popu- lar sampling-based decoding strategies—top- k , nucleus sampling, and untruncated random sampling—and show that improvements in de- coding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer ex- cerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our ﬁndings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems. 1 Introduction State-of-the-art generative language models are now capable of producing multi-paragraph ex- cerpts that at a surface level are virtually indis- tinguishable from human-written content ( Zellers et al. , 2019 ; Radford et al. , 2019 ; Adelani et al. , 2020 ). Often, only subtle logical fallacies or id- iosyncrasies of language give away the text as machine-generated, errors that require a close reading and/or domain knowledge for humans to detect. Deceptive text, whether human- or machine- generated, has entered the sphere of public con- cern ( Cooke , 2018 ). It propogates quickly ( Vosoughi et al. , 2018 ), sets political agendas ∗ Equal contribution, ‡ Google, † University of Pennsylva- nia ( Vargo et al. , 2018 ), inﬂuences elections ( Allcott and Gentzkow , 2017 ), and undermines user trust ( Wang et al. , 2012 ; Song et al. , 2015 ). Recently, Adelani et al. ( 2020 ) have shown that automati- cally generated reviews are perceived to be as ﬂu- ent as human-written ones. As generative tech- nology matures, authors, well-meaning or other- wise, will increasingly employ it to augment and accelerate their own writing. It is more impera- tive now than ever for both humans and automated systems to be able to detect and identify machine- generated texts in the wild. However, there has thus been little inquiry into the textual proper- ties that cause humans to give generated text high human-like ratings compared to those that cause automatic systems to rate it highly. To speak of texts produced by language mod- els, we must ﬁrst consider how these texts are generated. A neural language model encodes a probability distribution over the next word in a sequence given the previous words. 1 A decod- ing strategy is an algorithm that generates se- quences from a language model by determining how words should get selected from this distribu- tion. The ﬁeld has largely moved toward prob- abilistic decoding strategies that randomly sam- ple from the output distribution token-by-token. However, when many low-likelihood words cu- mulatively contain quite a bit of probability mass, choosing one of these words can lead to odd or contradictory phrases and semantic errors. Hu- mans are quick to notice these types of errors. For this reason, it has become common to mod- ify the language model’s output probability dis- tribution to increase the chance of sampling to- kens with high likelihood according to the lan- guage model. Top- k random sampling, where low-likelihood words are restricted from being 1 Often these ‘words” are actually subword character se- quences such as BPE tokens ( Sennrich et al. , 2016 ). arXiv:1911.00650v2  [cs.CL]  7 May 2020 \ngenerated, is one such method. A language model that is only permitted to produce high-likelihood words is less likely to make a poor choice and cre- ate the type of mistakes that are easy for humans to detect. Since humans are not proﬁcient at identi- fying when a model subtly favors some utterances more often than a human author would, they don’t notice the over-representation of high-likelihood words in the generated text. In contrast, automatic systems excel at identifying statistical anomalies and struggle to build deeper semantic understand- ing. Top- k in particular creates text that is easy for machines to detect but very hard for humans. Thus, we observe the general trend: as the num- ber of unlikely words available to be chosen is in- creased, humans get better at detecting fakes while automatic systems get worse . In this work, we study three popular random decoding strategies—top- k , nucleus, and temper- ature sampling—applied to GPT-2 ( Radford et al. , 2019 ). We draw a large number of excerpts gener- ated by each strategy and train a family of BERT- based ( Devlin et al. , 2019 ) binary classiﬁers to label text excerpts as human-written or machine- generated. We ﬁnd large differences in human rater and classiﬁer accuracy depending on the de- coding strategy employed and length of the gen- erated sequences. Regardless of strategy, we ﬁnd human raters achieve signiﬁcantly lower accuracy than the automatic discriminators. We also show that when a decoding strategy severely modiﬁes the unigram token distribution, as top- k does, hu- mans have trouble detecting the resultant gener- ated text, but automatic classiﬁers ﬁnd it the eas- iest to discriminate. Worryingly, we further ﬁnd that classiﬁers are brittle; they generalize poorly when trained to discriminate samples from one strategy and then evaluated on samples from an- other. In summary, our contributions are: • A comprehensive study of generated text de- tection systems’ sensitivity to model struc- ture, decoding strategy, and excerpt length. • An analysis of human raters’ ability to iden- tify machine-generated content, and how hu- man raters differ from automatic detectors. 2 Related Work Generative Language Models With a sufﬁ- ciently large training set and number of trainable parameters, neural language models based on the Transformer architecture ( Vaswani et al. , 2017 ) are capable of generating convincing, human-like excerpts up to several paragraphs in length. GPT- 2 ( Radford et al. , 2019 ), G ROVER ( Zellers et al. , 2019 ), and Transformer-DMCA ( Liu et al. , 2018 ) are a few examples of large, publicly available models with this ability. G ROVER , in particular, has been shown to generate fake news that is more trustworthy than human-written fake news accord- ing to human raters. Human Detection The task of trying to guess whether text is coming from a robot or a fellow human was made famous by the Turing Test ( Tur- ing , 1950 ). It continues to be used is chatbot eval- uation ( Lowe et al. , 2017 ). The related (but not identical) task of asking human raters to judge the quality of machine-generated excerpts remains the gold-standard for evaluating open-domain genera- tion systems ( van der Lee et al. , 2019 ). Kreps et al. ( 2020 ), Gehrmann et al. ( 2019 ), and others have stressed the importance of humans being able to identify fake content on the web. Automatic Detection The rise of machine- generated content has led to the development of automated systems to identify it. G ROVER was designed to not only generate convincing news ex- cerpts but to also identify them using a ﬁne-tuned version of the generative model itself ( Zellers et al. , 2019 ). GLTR, expecting attackers to use sampling methods that favor high-likelihood to- kens, aims to make machine-generated text de- tectable by computing histograms over per-token log likelihoods ( Gehrmann et al. , 2019 ). Bakhtin et al. ( 2019 ) frame human-text detection as a rank- ing task and evaluate their models’ cross-domain and cross-model generalization, ﬁnding signiﬁ- cant loss in quality when training on one do- main and evaluating on another. Schuster et al. ( 2019 ) argue that the language distributional fea- tures implicitly or explicitly employed by these detectors are insufﬁcient; instead, one should look to explicit fact-veriﬁcation models. Finally, dis- criminators for whether text is machine-generated are a promising research direction in adversarial training ( Lin et al. , 2017 ; Li et al. , 2017 ) and in automatic evaluation of generative model quality ( Novikova et al. , 2017 ; Kannan and Vinyals , 2017 ; Lowe et al. , 2017 ). Natural Language Understanding Automatic detection of machine-generated text beneﬁts from a semantic understanding of the text. Contradic- \ntions, falsehoods, and topic drift can all indicate that an excerpt was machine-generated. Encoder- only Transformer models such as BERT ( Devlin et al. , 2019 ) have been shown to do very well at tasks requiring this understanding. While we ﬁne- tune BERT for the task of classifying whether text was machine-generated, others have used the con- textual word embeddings from a pre-trained BERT model without ﬁne-tuning to compute a quality score for generated text ( Zhang et al. , 2020 ). It is worth noting that recent work has raised ques- tions as to whether BERT truly builds a semantic understanding to make its predictions, or whether it merely takes advantage of spurious statistical differences between the text of different classes ( Niven and Kao , 2019 ). 3 Task Deﬁnition We frame the detection problem as a binary clas- siﬁcation task: given an excerpt of text, label it as either human-written or machine-generated. In particular, we are interested in how variables such as excerpt length and decoding strategy impact performance on this classiﬁcation task. We thus create several datasets. Each is approximately balanced between positive examples of machine- generated text and negative examples of human- written text. While they all share the same human- written examples, each dataset contains a different set of machine-generated examples sampled using one particular decoding strategy. We also build ad- ditional datasets by truncating all of the examples to a particular sequence length, By training a separate classiﬁer on each dataset, we are able to answer questions about which de- coding strategy results in text that is the easiest to automatically disambiguate from human-written text. We are also able to answer questions about how the length of the examples in the training set impacts our ability to automatically classify ex- cerpts of that same length as either human-written or machine-generated. 4 Dataset Methodology All of our generated text samples are drawn from GPT-2, a state-of-the-art Transformer-based gen- erative language model that was trained on text from popular web pages ( Radford et al. , 2019 ). While we use the GPT-2 L ARGE model with 774M parameters, we found that similar trends to those reported here hold in experiments with smaller language models. Given an autoregressive language model that deﬁnes a probability distribution over the next to- ken given the previous tokens in a sequence, a decoding strategy generates text by deciding how to output a token at each step based on the pre- dicted distributions. Perhaps the most straightfor- ward decoding strategy is to randomly choose a to- ken with probability proportional to its likelihood. A challenge with the random sampling approach is that these probability distributions often contain a long tail of vocabulary items that are individu- ally low-probability but cumulatively comprise a substantial amount of probability mass. Holtzman et al. ( 2020 ) observe that choosing tokens from this tail often leads to incoherent generations. Top- k sampling, nucleus sampling, and (in the extreme) beam search have all been proposed to heuristically promote samples with higher per- token likelihoods. Top- k and nucleus sampling both do so by setting the likelihood of tokens in the tail of the distribution to zero. Top- k restricts the distribution to all but the k most likely tokens, where k is a constant ( Fan et al. , 2018 ). Nucleus sampling, also called top- p , truncates the distribu- tion at each decoding step t to the k t -most-likely next tokens such that the cumulative likelihood of these tokens is no greater than a constant p ( Holtz- man et al. , 2020 ). We thus consider three different decoding strat- egy settings: • Sample from the untruncated distribution • Top- k , choosing k =40 ( Radford et al. , 2019 ). • Nucleus sampling (aka top- p ), choosing p =0.96 ( Zellers et al. , 2019 ). In addition, we form “negative” examples of human-written text by taking excerpts of web text that come from the same distribution as GPT-2’s training data. 2 By picking text that resembles GPT-2’s train set, we ensure that our classiﬁers can’t simply take advantage of stylistic differences between the human-written text corpus and the kind of text GPT-2 was trained to generate. For each decoding method, we construct a train- ing dataset by pairing 250,000 generated samples with 250,000 excerpts of web text. 5,000 addi- tional paired samples are kept aside for validation and test datasets. Lastly, we ﬁlter out excerpts with fewer than 192 WordPiece tokens ( Wu et al. , 2 https://github.com/openai/ gpt-2-output-dataset \n2016 ) (excerpts might be quite short if the model produces an end-of-text token early on). See Ap- pendix 1 for ﬁnal dataset sizes. A crucial question when generating text with a language model is whether or not to provide a priming sequence which the language model should continue. Unconditioned samples, where no priming text is provided, in conjunction with top- k sampling, lead to pathological behavior for discriminators as the ﬁrst token of the generated text will always be one of k possible options. On the other hand, if long sequences of human text are used as priming, the space of possible gener- ated sequences is larger, but the detection problem shifts from one of “how human-like is the gener- ated text?” to “how well does the generated text follow the priming sequence?”. Since in this study we are interested in the former simpler question, we create two datasets, one with no priming, and one with the minimum amount of priming possible: a single token of web text. This means that for every excerpt of web text in the training set, there is an excerpt of machine- generated text that starts with the same token. We ﬁnd that even with limited priming, the ability of automatic detectors can be strongly impacted. To study the effect of excerpt length, we con- struct variations of the above datasets by truncat- ing all excerpts to ten possible lengths ranging from 2 to 192 WordPiece tokens ( Wu et al. , 2016 ). In total, we obtain sixty dataset variations: one per sampling method, truncation length, and choice of priming or no priming. 5 Automatic Detection Method The primary discriminator we employ is a ﬁne- tuned BERT classiﬁer ( Devlin et al. , 2019 ). We ﬁne-tune one instance of BERT per dataset vari- ation described above. For the longest sequence length, n =192, we compare BERT’s performance with several simple baselines that have been pro- posed in other work. Fine-tuned BERT We ﬁne-tune BERT-L ARGE (cased) on the task of labeling a sentence as human- or machine- generated. The models are trained for 15 epochs, with checkpoints saved ev- ery 1000 steps, and a batch size of 256. All results are reported on the test set using the checkpoint for which validation accuracy was highest. Bag-of-Words For each sequence, we compute a bag-of-words embedding where each dimension corresponds to a token in GPT-2’s 50,000 token BPE vocabulary ( Sennrich et al. , 2016 ), and we count how many times that token appears in the text sequence. We then train a logistic regression binary classiﬁer to predict human- or machine- written given this 50,000-dimensional embedding. We experimented with truncating embedding size by removing entries for infrequent vocabulary words, but this did not improve performance. Histogram-of-Likelihood Ranks Following GLTR ( Gehrmann et al. , 2019 ), we compute the probability distribution of the next word given the previous words in a text sequence according to a trained language model (in our case the same GPT-2 model that was used for generation). At each sequence position, we rerank the vocabulary words by likelihood, and record the rank of the ground-truth next word within this list. These ranks are then binned. GLTR uses four bins, counting (1) the number of times the top 1 word is seen, (2) the number of times words ranked 2 through 5 are seen, (3) words ranked 6-100, and (4) words ranked > 100. However, we observe higher accuracy when 50 bins are spread uniformly over the possible rankings. This means that since there are 50,000 vocabulary words, the ﬁrst bin counts the number of times the actual next word was within the 1,000 mostly likely next words, the second bin counts the 1,001-2,000th, and so on. We then train logistic regression binary classiﬁers to predict human- or machine-written given either the 4-dimensional histograms or 50-dimensional histograms as input. Total Probability Solaiman et al. ( 2019 ) pro- pose a very simple baseline consisting of a thresh- old on the total probability of the text sequence. An excerpt is predicted as machine-generated if its likelihood according to GPT-2 is closer to the mean likelihood over all machine-generated se- quences than to the mean of human-written ones. 6 Human Detection Method The human evaluation task is framed similarly to the automatic one. We ask the raters to decide whether a passage of text was written by a human or by a computer algorithm. (Full instructions are in the Appendix.) Raters are allowed to choose between four options: “deﬁnitely” or “possibly” machine-generated and “deﬁnitely” or “possibly” human-written. They are ﬁrst shown an excerpt of length 16 WordPiece tokens. After they make \nBERT BagOfWords HistGLTRBuckets Hist50Buckets TotalProb Human Method acc AUC acc AUC acc AUC acc AUC acc acc k40-1wordcond 0.88 0.99 0.79 0.87 0.52 0.52 0.69 0.76 0.61 0.64 p0.96-1wordcond 0.81 0.89 0.60 0.65 0.53 0.56 0.54 0.56 0.63 0.77 p1.0-1wordcond 0.79 0.92 0.59 0.62 0.53 0.55 0 . 5 4 0 . 5 5 0 . 6 5 0. 7 1 Table 1: Performance (accuracy and AUC) of the ﬁne-tuned BERT classiﬁer and several simple baselines on detect- ing length-192 sequences generated with one word of priming (1worccond). Note that p1.0 refers to untruncated random sampling, where we sample from 100% of the probability mass. The last column shows human perfor- mance on the same task where accuracy with a 50% baseline is computed by randomly pairing samples from each decoding strategy with a human-written sample. a guess, the length of the excerpt is doubled, and they are asked the same question again. This con- tinues until the entire passage of length 192 tokens is shown. Passages are equally likely to be human- written or machine-generated, with the machine- generated excerpts being evenly split between the three sampling strategies considered in this paper. Initially, Amazon Mechanical Turk (AMT) raters were employed for this task, but rater accu- racy was poor with over 70% of the “deﬁnitely” votes cast for “human” despite the classes be- ing balanced. Accuracy, even for the longest se- quences, hovered around 50%. The same study was then performed with university students who were ﬁrst walked through ten examples (see Ap- pendix Table 4) as a group. Afterward, they were asked to complete the same tasks that had been sent to the AMT workers. No additional guid- ance or direction was given to them after the ini- tial walk-through. We will refer to this group as the “expert raters.” Among them, 52.1% of “def- initely” votes were cast for human, and accuracy on the longest excerpt length was over 70%. The human evaluation dataset consisted of 150 excerpts of web text and 50 excerpts each from the three decoding strategies. Each question was shown to at most three raters, leading to 900 total annotations from the untrained workers and 475 from the expert raters. A more detailed breakdown can be found in the Appendix. 7 Automatic Detection Results Simple Baselines Table 1 shows the perfor- mance of the baseline discriminators on length- 192 sequences, as compared with ﬁne-tuned BERT. Reassuringly, BERT far surpasses all sim- ple baselines, indicating that it is not fully possi- ble to solve the detection problem without com- plex sequence-based understanding. The simplest baseline, TotalProb, which makes a decision based on the likelihood of the sequence, performs sur- prisingly well (over 60% accuracy for all sampling methods) relative to the methods which involve training logistic regression models. Logistic regression on bag-of-words is the best of the baselines, beating out the histogram-based methods. While Gehrmann et al. ( 2019 ) report an AUC of 0.87 on classifying text as real or gener- ated using logistic regression on the four buckets of the GLTR system, we report AUC between 0.52 and 0.56 for this task. The discrepancy is likely due to the fact that the human-written text in our discriminator training set comes from the same distribution as the text used to train the language model, while in GLTR the human text comes from children’s books, scientiﬁc abstracts, and news- paper articles. The selection of training data for learned detection systems is crucial. In real-world applications, the choice ought to reﬂect the genres that builders of text-generation systems are trying to impersonate. Fine-tuned BERT In Figure 1a , we begin by ob- serving discriminator accuracy as a function of ex- cerpt length and sampling method. As can be in- tuitively expected, as sequence length increases, so too does accuracy. For unconditioned text de- coded with nucleus (p0.96) and untruncated (p1.0) random sampling, we ﬁnd discriminator accuracy increases from 55%, near random, to about 81% for the longest sequences tested. In contrast, dis- criminators trained and evaluated on top- k achieve over 80% accuracy even on 16-token excerpts. Why are top- k ’s samples so easy to detect? In Figure 2b , we see the percentage of probability mass concentrated in the k most common token types for each sampling method. While random sampling and nucleus sampling are very similar to human-written texts, we see top-k concentrating up to 80% of its mass in the ﬁrst 500 most com- mon tokens. The other sampling methods as well as human-written texts require at least 1,100 token types for the same. It is clear that top- k ’s distribu- \n50% 55% 60% 65% 70% 75% 80% 85% 90% 95% 100% 0 32 64 96 128 160 192 Accuracy Sequence length in tokens Accuracy of BERT Fine-tuned Discriminator k40-1wordcond k40-nowordcond p0.96-1wordcond p0.96-nowordcond p1.0-1wordcond p1.0-nowordcond (a) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 4 8 16 32 64 96 128 160 192 Sequence length in tokens Fraction of BERT Discriminator Errors that are Machine-generated Labeled as Human-written k40-1wordcond p0.96-1wordcond p1.0-1wordcond (b) Figure 1: In (a) , accuracy increases as the length of the sequences used to train the discriminator is increased. In (b) , we see that the BERT ﬁne-tuned discriminator predicts about the same number of false-positives as false- negatives when trained with samples generated using top- p sampling. However, for top- k , it more often mistakes machine-generated text to be human-written, while for untruncated random sampling the opposite is the case. tion over unigrams strongly diverges from human- written texts–an easy feature for discriminators to exploit. In fact, See et al. ( 2019 ) note that it takes setting k to 1000 to achieve about the same amount of rare word usage and fraction of non-stopword text as as human writing. 3 This makes it very easy for the model to pick out machine-generated text based on these distributional differences. One way to help resolve this problem is to add priming text. Doing so causes more rare words to be incorporated into the top- k of the unigram distribution. Adding even a single human word of priming signiﬁcantly reduces the performance of detectors trained with top- k random sampling. Without priming, a discriminator trained on se- quences of length 2 can classify with ∼ 90% ac- curacy the provenance of the text (Figure 1a ). By adding one priming token, accuracy drops to ∼ 65%. Even on the longest 192-length sequences, top- k discriminator accuracy is 6% lower on the primed dataset than the unprimed one. When generating with nucleus or untruncated random sampling, adding a priming token is not as impactful, as these methods are already sam- pling from a large fraction (or all) of the probabil- ity distribution. This is seen in Figure 2a where at the very ﬁrst step of unprimed generation, nu- cleus sampling selects from 3075 possible vocab- ulary words, and at later positions selects from on 3 when decoding from the GPT-2 small model with 117M parameters. average more than 500. Untruncated random sam- pling always selects from the entire 50,000 word vocabulary, whereas top- k only selects from k . Transferability In Table 2 , we show how dis- criminators trained with samples from one decod- ing strategy can transfer at test time to detect- ing samples generated using a different decoding strategy. Unsurprisingly a discriminator trained on top- k generalizes poorly to other sampling meth- ods: accuracy drops to as low as 42.5%, worse than chance . Conversely, training the discrimi- nator with sequences sampled from the untrun- cated distribution leads to little transferability to detecting top- k samples. Only the discriminator trained with nucleus sampling (a compromise be- tween unmodiﬁed sampling and top- k ) was able to detect sequences from the other sampling strate- gies without too much of a hit to accuracy. As ex- pected, a discriminator trained on an equal portion of data from each decoding method does reason- ably at detecting all three. Perhaps this lack of transferability is related to each discriminator’s calibration. Indeed, the de- gree to which a discriminator’s average predic- tion deviates from 50% is a direct indicator of its accuracy. In Table 3 , we observe that of the three BERT discriminators, only that trained on top- p samples predicts ‘machine-generated’ on ap- proximately 50% of in-domain examples as ex- pected. This same discriminator’s behavior holds on datasets generated by other sampling strategies \n0 50 100 150 200 Position in sequence 500 1000 1500 2000 2500 3000 3500 4000 4500 k Mean k Chosen at each Position during Generation with Nucleus Sampling p0.96-nowordcond p0.96-1wordcond (a) 0 500 1000 1500 2000 2500 k most common unique tokens 0% 20% 40% 60% 80% 100% % all tokens p1.0-1wordcond k40-1wordcond p0.96-1wordcond webtext (b) Figure 2: In (a) , the average (over sequences in the test set) k chosen at each step during generating with nucleus sampling is plotted. Adding a single word of priming strongly impacts the k s chosen for the ﬁrst few positions, but this difference quickly dissipates. In (b) , we consider the ﬁrst token generated in each sequence by top- k , and plot what fraction of these are captured by the k most common unique tokens from the vocabulary. Overall, at its ﬁrst step, top- k concentrates 80% of its probability mass in the 500 most common tokens from the vocabulary. ! !\"# !\"$ !\"% !\"& !\"' !\"( !\") !\"* !\"+ # #( %$ (& #$* #+$ ,-./-01-23-04562702589-0: ;<=1578028>2?=5-<2@<<8<:256=52=<-2A=1670-B 4-0-<=5-C2D=E-3-C2=:2F/G=0BH<755-0 9&!B#H8<C180C I!\"+(B#H8<C180C I#\"!B#H8<C180C (a) (b) (c) Figure 3: (a) and (b) show human rater accuracy of correctly identifying an excerpt as human-written or machine- written, shown with 80% conﬁdence internals, in (a) , broken up by decoding strategy and in (b) , overall. Accuracy increases as raters observe more tokens. (c) shows that for short excerpts, most rater mistakes are them incorrectly thinking machine-generated text is human written. The two errors types become more balanced at longer lengths. Eval top- k nucleus random Train top-k 90.1 57.1 43.8 nucleus 79.1 81.3 78.4 random 47.8 63.7 81.7 mixed 88.7 74.2 72.2 Table 2: Accuracy of BERT ﬁne-tuned discriminator when trained on samples from one strategy (rows) and evaluated on another (columns). Trained on samples with 192 tokens. The ‘mixed’ dataset is one containing an equal portion of samples from each strategy. as well. In contrast, we observe that discrimi- nators trained on top-k and untruncated random samples severely underestimate the percentage of machine-generated excerpts in out-of-domain datasets. Even within domain (Figure 1b ), we ﬁnd both discriminators heavily favor a single class, in- Eval top- k nucleus random Train top-k 60.9 27.9 14.5 nucleus 49.2 51.7 48.9 random 7.3 22.6 38.3 Table 3: Average probability of ‘machine-generated’ according to each length-192 discriminator. The ex- pected in-domain probability is 0.5. One token of con- ditioning. creasingly so as the number of tokens increases. Human Evaluation Overall human performance across all sampling methods is shown in Figure 3 b. Even with the multi-paragraph 192-length ex- cerpts, human performance is only at 71.4%, in- dicating that even trained humans struggle to cor- rectly identify machine-generated text over a quar- \nTruth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 H M H H M H H M M M EDIT:OKAY!, I guess that’ll work for now. > http://www.teamfortress.com/ and then go buy the game and experience some of the best online gaming I have ever played. ˆ ˆBoth girls had a really fun time and I had a GREAT time making both of these costumes. Everything was altered even a little bit(dying the pants a darker grey and painting the boots and shirts) But my piece de resistance would have to be my eldest’s Medi-Gun.If you have any questions about the costumes, I would be happy to assist you!Oh and here’s a video of my daughter before the costume was completed.Thanks! Image copyright Getty Images Image caption Women mourn over the cofﬁn of one of the vic- tim’s of Sunday’s bombing in Ankara ¶ Who’d be in Turkey’s shoes right now? ¶ Since July last year, hundreds of soldiers and civilians have been killed in terrorist attacks. Suicide bombs have torn into crowds of demonstrators and tourists. Military convoys have been targeted in the heart of the capital. ¶ A long-running Kurdish insurgency, once thought to be close to resolution after years of painstaking efforts to build bridges, has erupted once more. ¶ The country is awash with Syrian and other refugees. The government has been under pressure to stop them moving on into Europe and prevent would-be jihadis travelling the other way. ¶ How dangerous is Turkey’s unrest? ¶ Tears and destruction amid PKK crackdown ¶ Turkey v Islamic State v the Kurds Truth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 M M H - - M M - - H First off, this thread has done a pretty good job of describing in detail yet another broken touchscreen. That’s the difference between a smartphone and a PC with no prying eyes having to snap shots for the police to ﬁnd. ¶ What I would like to address is the mindset that generally surrounds Chrome OS users. To me this is analogous to saying that Apple does“hate their Windows”, or that HP does“hate their Macs” as if http://twitter.com/) (and that quote is from two years ago), that anyone who covers smartphones and tablets from a “PC” perspective is just jealous. ¶ Chrome OS is for browsing the web, PC processors can do stronger things in that regard, Windows is a juggernaut on those fronts. This is how I see it. Yes, it can be slow. And yes, you need a fast CPU FOR ALABAMA, GOOD WEEKS ¶ AND A TOUR OF CAIRO ¶ THE ALABAMA COM- MITTEE ON THE STUDY OF THE AMERICAN SECURITY AGENDA, ¶ America’s fu- ture has been mapped out in carved stone. Metro Atlanta’s last US congressman, Bill Posey, was a inextricable integral element of the Citadel project as it became another metaphor for Atlanta’s transformation from an industry backwater into the ﬁnance and information hub of the nation’s capital. Meanwhile, Cobb County – Atlanta’s geode of change – is home to some of the largest industrial parks in the South, a regional cultural center, a 100-year-old manufac- turing town and a potent symbol of the former city’s cherished Georgian past. The gentry still live there, the defunct industrial landscapes carry the names of Truth Raters p1.0 k40 p0.96 Truth Raters p1.0 k40 p0.96 M H - - M M H - M - Exidentia at Eurnari, is an upcoming Cryptopia event which is currently still in devel- opment. Be a part of the ﬁrst live stream of this year’s event on 15-16 January 2016! ¶ Since the release of v1.22, Exidentia has received a fair amount of user feedback. This event takes place in the underwater Cryptopia they have built. During this event, you will learn about the ocean and areas around it, and be reached by a treasure hunter that helps you explore the different areas. ¶ There will be six different levels in this event that you will become acquainted with: thought Polar Lava, Ocean Seared Cones and Celestine Floors, Sea Damaged Aerie Bricks, coast Puddle (congipit stopping at red water), Shaikh Swamp and Bugmite. At rotating points, you will learn how to access various types of creatures Ever since the opening of the North American College of Art Education in 1990, the demand for art education in America has grown steadily, and in recent years we have seen the rise of students that pursue art education not in the classroom but at art academies. This year saw another 50 percent increase in the number of art academies in the United States offering courses – with an additional 10 percent of students in 2017 taking art. ¶ Some major changes have occurred in recent years with regard to the art curriculum and the way students learn, and we will explore each of these in coming months as we look at the various forms of art education. There is no one-size-ﬁts-all approach for this or any other ﬁeld of study, and students who begin a course in art education may change their plans based on what they see that course, including what lessons they have completed and the resources available, to create meaningful experiences of artistic creation. ¶ One important area Table 4: Some 192-token examples where at least two expert raters agreed with each other, but were not in agree- ment with the automatic discriminators. The ﬁrst row shows examples where the ground-truth was human-written, the second shows machine-generated examples where the corresponding discriminator guessed incorrectly, and the third shows machine-generated examples where the discriminator was correct, but raters got it wrong. ter a time. However, it is worth noting that our best raters achieved accuracy of 85% or higher, sug- gesting that it is possible for humans to do very well at this task. Further investigation is needed into how educational background, comfort with English, participation in more extensive training, and other factors can impact rater performance. To break up the accuracies by sampling method in a way that is comparable to the results shown for the automatic discriminators, we pair each machine-generated example with a randomly se- lected one of webtext to create a balanced dataset for each sampling strategy. Performance is shown in Figure 3 a. Top- k produces the text that is hard- est for raters to correctly distinguish, but as shown in Section 7 , it is the easiest for our automatic de- tection systems. Samples from untruncated ran- dom sampling and nucleus sampling with p =0.96 are equivalently difﬁcult for raters to classify as machine-generated. Our human evaluation results suggest that much lower p -values than the 0.92 to 0.98 range proposed in Zellers et al. ( 2019 ) might be necessary in order to generate text that is con- sidered signiﬁcantly more human-like to human raters than the text produced by using the untrun- cated distribution. Table 4 gives several examples where human raters and our BERT-based discriminators dis- agreed. When raters incorrectly labeled human- written text as machine-generated, often the ex- cerpts contained formatting failures introduced when the HTML was stripped out. In the mid- dle two examples, topic drift and falsehoods such as Atlanta being the “information hub of the na- tion’s capital” allowed humans to correctly detect the generated content. However, in the bottom two examples, the high level of ﬂuency left human raters fooled. Overall we ﬁnd that human raters—even “ex- pert” trained ones—have consistently worse ac- curacy than automatic discriminators for all de- coding methods and excerpt lengths. In our ex- periments, randomly-selected pairs of raters agree with each other on a mere 59% of excerpts on average. (In comparison, raters and discrimina- tors agree on 61% to 70% of excerpts depending on the discriminator considered). We surmise that the gap between human and machine performance will only grow as researchers inevitably train big- ger, better detection models on larger amounts of \ntraining data. While improved detection models are inevitible, it is unclear how to go about im- proving human performance. GLTR proposes pro- viding visual aids to humans to improve their per- formance at detecting generated-text, but it is un- likely that their histogram-based color-coding will continue to be effective as generative methods get better at producing high-quality text that lacks sta- tistical anomalies. 8 Conclusion In this work, we study the behavior of auto- mated discriminators and their ability to iden- tify machine-generated and human-written texts. We train these discriminators on balanced bi- nary classiﬁcation datasets where all machine- generated excerpts are drawn from the same gener- ative model but with different decoding strategies. We ﬁnd that, in general, discriminators transfer poorly between decoding strategies, but that train- ing on a mix of data from methods can help. We also show the rate at which discriminator accuracy increases as excerpts are lengthened. We further study the ability of expert human raters to perform the same task. We ﬁnd that rater accuracy varies wildly, but has a median of 74%, which is less than the accuracy of our best- performing discriminator. Most interestingly, we ﬁnd that human raters and discriminators make de- cisions based on different qualities, with humans more easily noticing semantic errors and discrimi- nators picking up on statistical artifacts. In our ex- periments, these artifacts are most prominent with top- k sampling. However, any strategy that over- samples high-likelihood words is susceptible. As the p in nucleus sampling is set increasingly lower to achieve more ﬂuent text (some systems are al- ready using p as low as 0.5 ( Miculicich et al. , 2019 )), the distributional deviations that plague top- k text will surface in nucleus sampling as well. Holtzman et al. ( 2020 ) explain how a unique at- tribute of human language is that it dips in and out of low probability zones. This variance in likeli- hood is what makes human-written text interest- ing and exciting to read. Today’s generation sys- tems have not yet solved the problem of mimick- ing the human cadence without introducing poor word choices that are easy for humans to detect. Generation systems often optimize for fooling hu- mans without acknowledging the trade-off that ex- ists between human perception of quality and ease of automatic detection. We therefore suggest three prongs for future research: 1. Identifying ways to improve the language models and decoding strategies we use in or- der to generate text that is both exciting (ie. unlikely) and semantically plausible. 2. Building better world understanding into au- tomatic discriminators so that they are more capable of detecting the types of errors that humans notice. 3. Developing tools and educational materi- als to improve humans’ ability to detect machine-generated text. These may include automatic detectors with components that ex- plain their predictions. Finally, we would like to note that all of our ex- periments were performed with English language models, and it remains an open question how the trade-off between ease of human detection and ease of automatic detection might differ for lan- guages that are very different from English. Acknowledgements This research is based upon work supported in part by U.S. DARPA KAIROS Program No. FA8750- 19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies, either expressed or implied, of DARPA or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copy- right annotation therein. We also thank Noah Fiedel, Peter Liu, Sharan Narang, Joao Sedoc, Yun William Yu, and Hugh Zhang for their valuable feedback."
}