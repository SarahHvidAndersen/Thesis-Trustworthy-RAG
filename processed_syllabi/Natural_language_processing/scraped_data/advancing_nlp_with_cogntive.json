{
    "document_type": "research_paper",
    "title": "advancing_nlp_with_cogntive",
    "author": "",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\advancing_nlp_with_cogntive.pdf",
    "date_published": "2019-04-05",
    "keywords": "",
    "flag": "",
    "text": "Advancing NLP with Cognitive Language Processing Signals Nora Hollenstein ETH Zurich noraho@ethz.ch Maria Barrett University of Copenhagen mjb@di.ku.dk Marius Troendle University of Zurich m.troendle@uzh.ch Francesco Bigiolli ETH Zurich fbigiol@ethz.ch Nicolas Langer University of Zurich n.langer@uzh.ch Ce Zhang ETH Zurich ce.zhang@ethz.ch Abstract When we read, our brain processes language and generates cognitive processing data such as gaze patterns and brain activity. These sig- nals can be recorded while reading. Cognitive language processing data such as eye-tracking features have shown improvements on single NLP tasks. We analyze whether using such human features can show consistent improve- ment across tasks and data sources. We present an extensive investigation of the beneﬁts and limitations of using cognitive processing data for NLP. Speciﬁcally, we use gaze and EEG features to augment models of named entity recognition, relation classiﬁcation, and senti- ment analysis. These methods signiﬁcantly outperform the baselines and show the poten- tial and current limitations of employing hu- man language processing data for NLP. 1 Introduction When reading, humans process language “auto- matically” without reﬂecting on each step — Hu- mans string words together into sentences, under- stand the meaning of spoken and written ideas, and process language without thinking too much about how the underlying cognitive process hap- pens. This process generates cognitive signals that could potentially facilitate natural language pro- cessing tasks. In recent years, collecting these signals has be- come increasingly easy and less expensive ( Pa- poutsaki et al. , 2016 ); as a result, using cognitive features to improve NLP tasks has become more popular. For example, researchers have proposed a range of work that uses eye-tracking or gaze signals to improve part-of-speech tagging ( Bar- rett et al. , 2016 ), sentiment analysis ( Mishra et al. , 2017 ), named entity recognition ( Hollenstein and Zhang , 2019 ), among other tasks. Moreover, these signals have been used successfully to regular- ize attention in neural networks for NLP ( Barrett et al. , 2018 ). However, most previous work leverages only eye-tracking data, presumably because it is the most accessible form of cognitive language pro- cessing signal. In addition, most state-of-the-art work focused on improving a single task with a single type of cognitive signal. But can cognitive processing signal bring consistent improvements across modality (e.g., eye-tracking and/or EEG) and across various NLP tasks? And if so, does the combination of different sources of cognitive signals bring incremental improvements? In this paper, we aim at shedding light on these questions. We present, to the best of our knowl- edge, the ﬁrst comprehensive study to analyze the beneﬁts and limitations of using cognitive lan- guage processing signals to improve NLP across multiple tasks and modalities (types of signals). Speciﬁcally, we go beyond state-of-the-art in two ways: (Multiple Signals) We consider both eye-tracking and electroencephalography (EEG) data as exam- ples of cognitive language processing data. Eye- tracking records the readers gaze positions on the screen and serves as an indirect measure of the cognitive reading process. EEG records electrical brain activity along the scalp and is a more direct measure of physiological processes, including lan- guage processing. This is also the ﬁrst application leveraging EEG data to improve NLP tasks. (Multiple Tasks) We then construct named entity recognition, relation classiﬁcation, and sentiment analysis models with gaze and EEG features. We analyze three methods of adding these cognitive signals to machine learning architectures for NLP. First, we simply add the features to existing sys- tems (Section 4 ). Second, we show how these fea- arXiv:1904.02682v1  [cs.CL]  4 Apr 2019 \ntures can be generalized so that recorded data is not required at test data (Section 5.1 ). And third, in a multi-task setting we learn gaze and EEG fea- tures as auxiliary tasks to aid the main NLP task (Section 6 ). In summary, the most important insights gained from this work include: 1. Using cognitive features shows consistent im- provements over a range of NLP tasks even with- out large amounts of recorded cognitive signals. 2. While integrating gaze or EEG signals sepa- rately signiﬁcantly outperforms the baselines, the combination of both does not further improve the results. 3. We identify multiple directions of future re- search: How can cognitive signals, such as EEG data, be preprocessed and de-noised more efﬁ- ciently for NLP tasks? How can cognitive features of different sources be combined more effectively for natural language processing? All experiments presented in this paper are avail- able 1 to provide a foundation for future work to better understand these questions. 2 Related Work 2.1 Eye-tracking The beneﬁts of eye movement data have been assessed in various domains, including NLP and computer vision. Eye-trackers provide millisecond-accurate records about where humans look when they are reading. Although it is mostly still being recorded in controlled environments, re- cent approaches have shown substantial improve- ments in recording gaze data by using cameras of mobile devices ( G´omez-Poveda and Gaudioso , 2016 ; Papoutsaki et al. , 2016 ). Hence, gaze data will become more accessible and available in much larger volumes in the next few years ( San Agustin et al. , 2009 ; Sewell and Komogort- sev , 2010 ), which will facilitate the creation of siz- able datasets enormously. The beneﬁt of eye-tracking in human language processing is supported by intensive study in psy- cholinguistics during the 20th century and on- wards. For example, when humans read a text, they do not focus on every single word. The num- ber of ﬁxations and the ﬁxation duration on a word depends on a number of linguistic factors ( Clifton 1 https://github.com/DS3Lab/zuco-nlp/ et al. , 2007 ; Demberg and Keller , 2008 ). Differ- ent features even allow us to study early and late cognitive processing separately. First, word length, frequency and predictability from context affect ﬁxation duration and counts. The frequency effect was ﬁrst noted by Rayner ( 1977 ) and has been consistently reported in vari- ous studies since, e.g. Just and Carpenter ( 1980 ); Rayner and Duffy ( 1986 ); Cop et al. ( 2017 ). Sec- ond, readers are more likely to ﬁxate on open-class words ( Carpenter and Just , 1983 ). It even appears that eye movements are reliable indicators of syn- tactical categories ( Barrett and Søgaard , 2015 ). Word familiarity also inﬂuences how long read- ers look at a word. Although two words may have the same frequency value, they may differ in fa- miliarity and predictability from context. Effects of word familiarity on ﬁxation time have also been demonstrated in a number of studies ( Juhasz and Rayner , 2003 ; Williams and Morris , 2004 ) as have word predictability effects, e.g. McDonald and Shillcock ( 2003 ). A range of work of using eye-tracking signals to improve natural language processing tasks has been proposed and shows promising results. Gaze data has been used to improve tasks such as part- of-speech tagging ( Barrett et al. , 2016 ), sentiment analysis ( Mishra et al. , 2017 ), prediction of mul- tiword expressions ( Rohanian et al. , 2017 ), sen- tence compression ( Klerke et al. , 2016 ), and word embedding evaluation ( Søgaard , 2016 ). Further- more, gaze data has been used to regularize atten- tion in neural architectures on NLP classiﬁcation tasks ( Barrett et al. , 2018 ). 2.2 EEG To the best of our knowledge, there are no applica- tions leveraging EEG data to improve NLP tasks. There are, however, good reasons to try to com- bine the two sources. EEG could provide the miss- ing information in the eye movements to disam- biguate different cognitive processes. An extended ﬁxation duration only tells us that extended cogni- tive processing occurs, but not which process . EEG and eye-tracking use the same temporal resolution with non-invasive technologies ( Sereno and Rayner , 2003 ). Dambacher and Kliegl ( 2007 ) found that longer ﬁxation duration correlates with larger N400 amplitude effects. N400 is part of the normal brain response to words and other mean- ingful stimuli ( Kutas and Federmeier , 2000 ). Ef- fects of word predictability on eye movements and \nEEG co-registration have also been studied in se- rialized word representation and in natural reading ( Dimigen et al. , 2011 ). Other aspects relevant for linguistic processing can be observed in the EEG signal itself. For in- stance, term relevance can be associated with brain activity with signiﬁcant changes in certain brain areas ( Eugster et al. , 2014 ), differences in process- ing verbs and noun, concrete nouns and abstract nouns, as well as common nouns and proper nouns are also observed ( Weiss and Mueller , 2003 ). Fur- thermore, there is a correspondence between com- putational grammar models and certain EEG ef- fects ( Hale et al. , 2018 ). Collecting EEG data is more expensive and time-consuming than collecting eye-tracking data, which is why brain activity data is commonly less accessible. Moreover, collecting EEG data from subjects in a naturalistic reading environment is even more challenging. Hence, related work in this area is very limited. Subsequently, while we rely on standard practices when leveraging gaze data, our experiments using EEG data are more experimental. 3 Data The Zurich Cognitive Language Processing Cor- pus (ZuCo; Hollenstein et al. ( 2018 )) is the main data source of this work. It is the ﬁrst freely available dataset 2 of simultaneous eye-tracking and EEG recordings of natural sentence reading. This corpus includes recordings of 12 adult, na- tive speakers reading approximately 1100 English sentences. The corpus contains both natural reading and task-solving reading paradigms. For this work, we make use of the ﬁrst two reading paradigms of ZuCo, during which the subjects read natu- rally at their own speed and without any spe- ciﬁc task other than answering some control ques- tions testing their reading comprehension. The ﬁrst paradigm includes 300 sentences (7737 tokes) from Wikipedia articles ( Culotta et al. , 2006 ) that contained semantic relations such as employer , award and job title . The second paradigm con- tains 400 positive, negative and neutral sentences (8138 tokens) from the Stanford Sentiment Tree- bank ( Socher et al. , 2013 ), to analyze the elicita- tion of emotions and opinions during reading. The 2 The data is available here: https://osf.io/ q3zws/ same sentences were read by all 12 subjects. 3.1 Gaze features ZuCo readily provides 5 eye-tracking features: number of ﬁxations (NFIX), the number of all ﬁx- ations landing on a word; ﬁrst ﬁxation duration (FFD), the duration of the ﬁrst ﬁxation on the cur- rent word; total reading time (TRT), the sum of all ﬁxation durations on the current word; gaze dura- tion (GD), the sum of all ﬁxations on the current word in the ﬁrst-pass reading before the eye moves out of the word; and go-past time (GPT), the sum of all ﬁxations prior to progressing to the right of the current word, including regressions to previous words that originated from the current word. Fix- ations shorter than 100 ms were excluded, since these are unlikely to reﬂect language processing ( Sereno and Rayner , 2003 ). To increase the ro- bustness of the signal, the eye-tracking features are averaged over all subjects. 3.2 EEG features Since eye-tracking and EEG were recorded simul- taneously, we were able to extract word-level EEG features. During the preprocessing of ZuCo 23 electrodes in the outermost circumference (chin and neck) were used to detect muscular artifacts and were removed for subsequent analyses. Thus, each EEG feature, corresponding to the duration of a speciﬁc ﬁxation, contains 105 electrode val- ues. The EEG signal is split into 8 frequency bands, which are ﬁxed ranges of wave frequen- cies and amplitudes over a time scale: theta1 (4-6 Hz), theta2 (6.5-8 Hz), alpha1 (8.5-10 Hz), alpha2 (10.5-13 Hz), beta1 , (13.5-18 Hz) beta2 (18.5-30 Hz), gamma1 (30.5-40 Hz) and gamma2 (40-49.5 Hz). These frequency ranges are known to corre- late with certain cognitive functions. For instance, theta activity reﬂects cognitive control and work- ing memory ( Williams et al. , 2019 ), alpha activity has been related to attentiveness ( Klimesch , 2012 ), gamma-band activity has been used to detect emo- tions ( Li and Lu , 2009 ) and beta frequencies af- fect decisions regarding relevance ( Eugster et al. , 2014 ). Even though the variability between sub- jects is much higher in the EEG signal, we also average all features over all subjects. 4 Tasks To thoroughly evaluate the potential of gaze and brain activity data, we perform experiments on the three information extraction tasks described in this \nsection. Current state-of-the-art systems are used for all tasks and different combinations of cogni- tive features are evaluated. 4.1 Named Entity Recognition The performance of named entity recognition (NER) systems can successfully be improved with eye-tracking features ( Hollenstein and Zhang , 2019 ). However, this has not been explored for EEG signals. We use the state-of-the-art neural architecture for NER by Lample et al. ( 2016 ) 3 . Their model successfully combines word-level and character-level embeddings, which we augment with embedding layers for gaze and/or EEG features. Word length and fre- quency are known to correlate and interact with gaze features (e.g. Just and Carpenter ( 1980 ); Rayner ( 1977 )), which is why we selected a base model that allows us to combine the cognitive features with word-level and character-level in- formation. We use the named entity annota- tions from https://github.com/DS3Lab/ ner-at-first-sight . Features For this task, we used the 17 gaze fea- tures proposed by Hollenstein and Zhang ( 2019 ) for NER. These features include relevant informa- tion from early and late word processing as well as context features from the surrounding words. We extracted 8 word-level EEG features, one for each frequency band (The neural architecture of this system does not allow for raw normalized EEG and gaze features as is the case for relation classiﬁcation and sentiment analysis.). The fea- ture values were averaged over the 105 electrode values. These features are mapped to the dura- tion of the gaze features. Thus, in the experiments we tested EEG features during total reading time of the words and EEG features merely during the ﬁrst ﬁxations. The latter yielded better results. The gaze and EEG features values (originally in milliseconds (for gaze) and microvolts (for EEG)) were normalized and concatenated to the character and word embeddings as one-hot vectors. Experiments All models were trained on both ZuCo paradigms described above (15875 tokens) with 10-fold cross validation (80% training, 10% development, 10% test) and early stopping was performed after 20 epochs of no improvement on the development set to reduce training time. For 3 https://github.com/glample/tagger the experiments, the default values for all parame- ters were maintained. The word embeddings were initialized with the pre-trained GloVe vectors of 100 dimensions ( Pennington et al. , 2014 ) and the character-based embeddings were trained on the corpus at hand (25 dimensions). 4.2 Relation Classiﬁcation The second information extraction task we ana- lyze is classifying semantic relations in sentences. As a state-of-the art relation classiﬁcation method we use the winning system from SemEval 2018 ( Rotsztejn et al. , 2018 ), which combines convo- lutional and recurrent neural networks to lever- age the best architecture for different sentence lengths. We consider the following 11 relation types: award, employer, education, founder, vis- ited, wife, political-afﬁliation, nationality, job- title, birth-place and death-place . We use the an- notations provided by Culotta et al. ( 2006 ). Features For this task, we employed the 5 gaze features on word-level provided in the ZuCo data: number of ﬁxations, ﬁrst ﬁxation duration, total reading time, gaze duration and go-past time. The eye-tracking feature values were normalized over all occurrences in the corpus. The EEG features were extracted by averaging the 105 electrode val- ues over all ﬁxations for each word and then nor- malized. All word features in a sentence were con- catenated and ﬁnally padded to the maximum sen- tence length. The eye-tracking and/or EEG feature vectors were appended to the word embeddings. Experiments We performed 5-fold cross valida- tion over 566 samples (sentences can include more than one relation type). We split the data into 80% training data and 20% test data. Due to the small size of the dataset, we used the same preprocess- ing steps and parameters as proposed by the Se- mEval 2018 system. The word embeddings were initialized with the pre-trained GloVe vectors of 300 dimensions. 4.3 Sentiment Analysis The third NLP task we choose for this work is sentiment analysis. Based on the analysis by Barnes et al. ( 2017 ), we implemented a bidirec- tional LSTM with an attention layer for the classi- ﬁcation of sentence-level sentiment labels. Features Analogous to the relation classiﬁca- tion, the 5 word-level eye-tracking features were \nNER RelClass Sentiment (2) Sentiment (3) P R F1 P R F1 P R F1 P R F1 baseline 84.5 81.7 82.9 62.6 56.6 57.7 82.5 82.5 82.5 57.1 57.6 57.2 gaze 86.2 84.3 85.1 ** 65.1 61.9 62.0** 84.7 84.6 84.6 ** 61.4 61.7 61.5 ** EEG 86.7 81.5 83.9* 68.3 64.8 65.1 ** 83.6 83.6 83.6** 60.5 60.2 60.3** gaze+EEG 85.1 83.2 84.0** 66.3 59.3 60.8** 84.3 84.3 84.3** 59.8 60.0 59.8** Table 1: Precision (P), recall (R) and F1-score (F1) for the four tasks augmented with gaze features, EEG features, and both. Signiﬁcance is indicated with the asterisks: * = p < 0.01, ** = p < 0.0008 (Bonferroni method). normalized and concatenated before being ap- pended to the sentence embeddings. The raw EEG data (105 electrode values per word) were aver- aged and normalized. Experiments 10-fold cross validation was per- formed over the 400 sentences with available sen- timent labels from ZuCo (123 neutral, 137 nega- tive and 140 positive sentences). We test ternary classiﬁcation as well as binary classiﬁcation. For the latter, we remove all neutral sentences from the training data. Word embeddings were initial- ized with pre-trained vectors of 300 dimensions ( Mikolov et al. , 2013 ). All models are trained for 10 epochs with batch sizes of 32. The initial learning rate is set to 0.001. It was halved every 3 passes or every 10 passes, for binary classiﬁcation and ternary classiﬁcation respectively (due to the larger training set). 5 Evaluation For each information extraction task described in the previous section we trained baseline models, models augmented with gaze features, with EEG features, and with both. All the baseline mod- els were trained solely on textual information (i.e. word embeddings without any gaze or EEG fea- tures). We trained single-subject models and mod- els in which the features values are averaged over all subjects. The results of the averaged models are shown in Table 1 . We observe consistent improvements over the baselines for all tasks when augmented with cognitive features. The models with gaze fea- tures, EEG features and the combination thereof all outperform the baseline. Notably, while the combination of gaze and EEG features also out- perform the baseline, they do not improve over us- ing gaze or EEG individually. We perform statistical signiﬁcance testing using permutation (as described in Dror et al. ( 2018 )) over all tasks. In addition, we apply the conser- vative Bonferroni correction for multiple hypothe- ses, where the global null hypothesis is rejected if p < α/N , where N is the number of hypothe- ses ( Dror et al. , 2017 ). In our setting, α = 0 . 01 and N = 12 , accounting for the combination of the 4 tasks and 3 conﬁgurations (EEG, gaze, EEG+gaze). The improvements in 11 conﬁgura- tions out of 12 are also statistically signiﬁcant un- der the Bonferroni correction. Despite the limited amount of data, this result suggests that augment- ing NLP systems with cognitive features is a gen- eralizable approach. Subject analysis In an additional analysis we also evaluate the single-subject models to test the robustness of averaging the feature values over all readers. By the example of binary and ternary sentiment analysis, Figure 1 depicts the variabil- ity of the results between the subjects. In contrast to the averaged models, the best subject for bi- nary sentiment classiﬁcation reaches an F1-score of 85% with the combination of gaze and EEG data. Moreover, it shows how the averaged models perform almost as good as the best subject. Note that the best-performing subject for gaze is not necessarily the same subject as for the best EEG model. We also trained models that only take into account the feature values of the ﬁve best subjects. However, when averaging over all subjects, the signal-to-noise ratio is higher and provides better results than training on the best ﬁve subjects only. While previous research had shown the same ef- fect for using eye-tracking data from multiple sub- jects in NLP, this had no yet been shown for EEG data. 5.1 No real-time recorded data required While adding these cognitive features to a system show the potential of this type of data, it is not very practical if real-time recordings of EEG and/or eye-tracking are required at prediction time. Fol- lowing Barrett et al. ( 2016 ), we evaluate feature aggregation on word-type level. This means that all cognitive features are averaged over the word occurrences. As a result a lexicon of lower-cased word types with their averaged gaze and EEG fea- \nFigure 1: Comparison of single-subject models and features averaged over all subject for both binary sentiment classiﬁcation (left) and ternary sentiment classiﬁcation (right). Each dot represents a single subject model, each line an averaged feature model. Note that the best-performing subject for gaze is not necessarily the same subject as for the best EEG model. ture values was compiled. Words in the training data as well as in the test set are assigned these features if the words occurs in the type-aggregated lexicon or receives unknown features values oth- erwise. Thus, recorded human data is not required at test time. We evaluate the concept of type aggregation on the tasks described above. We choose 3 bench- mark datasets and add the aggregated EEG and/or eye-tracking features to words occurring in ZuCo. For NER we use the CoNLL-2003 corpus ( Sang and De Meulder , 2003 ), for relation classiﬁcation we use the full Wikipedia dataset provided by ( Cu- lotta et al. , 2006 ) and for sentiment analysis we use the Stanford Sentiment Treebank (SST). The same experiment settings as above were applied here. To avoid overﬁtting we did not use the ofﬁ- cial train/test splits but performed cross validation. Table 2 shows the details about these datasets and the results. We can observe a consistent im- provement using type-aggregated gaze features. However, the effect of type-aggregated EEG fea- tures is mixed. Type aggregation shows not only that recorded gaze or EEG data is not necessary at test time, but also that improvements can be achieved with human data without requiring large quantities of recorded data. 6 Multi-task learning We further investigate multi-task learning (MTL) as an additional machine learning strategy to ben- eﬁt from cognitive features. The intuition behind MTL is that training signals of one task, the aux- iliary task, improves the performance of the main task, by sharing information throughout the train- ing process. In our case, we learn gaze and EEG features as auxiliary tasks to improve the main NLP task. In previous work, it has been shown that MTL can be used successfully for sequence labelling tasks ( Bingel and Søgaard , 2017 ) due to some compelling beneﬁts, including its potential to ef- ﬁciently regularize models and to reduce the need for labeled data. Moreover, gaze duration has been predicted as an auxiliary task to improve sentence compression ( Klerke et al. , 2016 ), and to better predict the readability of texts ( Gonz´alez-Garduno and Søgaard , 2018 ). To the best of our knowledge, EEG features have not been used in MTL to im- prove NLP tasks. In multi-task learning it is important that the tasks that are learned simultaneously are related to a certain extent ( Caruana , 1997 ; Collobert et al. , 2011 ). Assuming that the cognitive processes in the human brain during reading are related, there should be a gain from training on gaze and EEG data when learning to extract information from text. Thus, we assess the hypothesis that MTL might also be useful in our scenario. Experiments We utilized the Sluice networks ( Ruder et al. , 2017 ), where the network learns to which extent the layers are shared between the tasks. Thus, we re-formulated the sentiment anal- ysis as sequence labelling tasks on phrase level. For binary sentiment analysis, the classes NEU- TRAL and NOT-NEUTRAL were predicted. We did not have to modify the named entity recog- nition task and the relation classiﬁcation was not tested since only sentence level labels are avail- able. \nNER RelClass Sentiment (2) Sentiment (3) corpus CoNLL-2003 Wikipedia SST SST tokens 302811 32953 165165 202125 sentences 22137 1794 9612 11853 unknown tokens 41.09% 30.31% 26.02% 25.96% baseline 94.02 76.94 82.01 57.13 gaze 94.41** 77.85 ** 81.64 57.48 ** EEG 94.58** 76.40 80.07 54.27 gaze + EEG 94.63 ** 77.01 79.74 54.80 Table 2: The top part shows the size of the datasets used for the type-aggregation experiments, including the percentage of unknown tokens, i.e. tokens not in the lexicon of aggregated type features. The bottom part shows F1-scores of type aggregation on external benchmark corpora. Signiﬁcance is indicated with the asterisks: * = p < 0.01, ** = p < 0.0008 (Bonferroni method). main task aux task(s) accuracy - 87.34 freq 91.29 NER FFD 87.34 FFD freq 91.87 EEG a 87.31 EEG a freq 91.79 - 60.99 freq 61.15 Sentiment TRT 61.31 binary TRT freq 61.13 EEG b 61.01 EEG b freq 61.56 - 61.03 freq 61.02 Sentiment FFD 61.05 ternary FFD freq 61.10 EEG t 61.05 EEG t freq 61.17 Table 3: Results of the multi-task learning experiments on NER, binary and ternary sentiment analysis. We ran 5-fold cross validation for all experi- ments over the same data as described in Section 3 . As our baselines we used single-task learn- ing and learning word frequency as an auxiliary task to an NLP task. Word frequencies were ex- tracted from the British National Corpus ( Kilgar- riff , 1995 ). The experiments ran with the default settings recommended by ( Ruder et al. , 2017 ). In accordance to their results, the Sluice networks yielded consistently higher results than hard pa- rameter sharing. As a main task the network learned to predict NER, binary or ternary sentiment labels. As aux- iliary tasks the network learned a single gaze or EEG feature. We used ﬁve eye-tracking features: number of ﬁxations (NFIX), mean ﬁxation du- ration (MFD), ﬁrst ﬁxation duration (FFD), to- tal reading time (TRT), and ﬁxation probability (FIXP). Additionally, we tested four EEG features, one for each combined frequency band: EEG t (i.e. the average values of theta1 and theta2 ), EEG a , EEG b , EEG g . The features were discretized and binned. Results Table 3 shows the results of these ex- periments. Note that only the best feature combi- nations are included in the table. Learning word frequency as an auxiliary task is a strong base- line. Learning gaze and EEG features as auxiliary tasks does not improve the performance over the single-task baseline for NER and only minimally for sentiment analysis. Learning two auxiliary tasks, a gaze of EEG feature and word frequency in parallel yields modest improvements over the frequency baseline. Adding further auxiliary tasks with additional gaze or EEG features did not yield better results. Moreover, the combination of learning gaze and brain activity features did also not bring further improvements. As we know that gaze and frequency band EEG features represent different cognitive processes in- volved in reading, our main and auxiliary tasks should in fact be related. However, it seems like the noise-to-signal ratio in the EEG features is too high to achieve signiﬁcant results. As stated by Gonz´alez-Garduno and Søgaard ( 2018 ), it is im- portant to establish whether the same feature rep- resentation can yield good results for all tasks in- dependently. To gain further insights into these results, we analyze how well these human features can be learned. 6.1 Learning cognitive features Using the same experiment setting as for the above described MTL experiments, we ﬁrst trained single-task baselines for each of the gaze and EEG features. Then, we trained each gaze feature in 3 MTL settings: (1) word frequency as an auxiliary task, (2) the remaining gaze features as parallel auxiliary tasks and (3) the EEG features as parallel \ngaze features EEG features NFIX MFD FFD TRT FIXP EEG t EEG a EEG b EEG g - 64.14 84.60 55.21 65.04 46.66 40.67 36.14 39.50 30.48 freq 71.01 84.64 63.68 71.99 56.34 53.36 49.75 52.79 41.34 gaze 71.34 84.78 63.60 72.20 55.77 53.53 49.38 52.58 40.95 EEG 71.15 84.64 62.10 72.03 55.63 53.47 46.77 52.54 37.27 Table 4: Learning cognitive features in an MTL setting. Columns = main tasks, rows = auxiliary tasks. auxiliary tasks. The same applies to EEG features as main tasks. The results in Table 4 show that gaze features have far higher baselines than EEG features. Presumably EEG is harder to learn be- cause it has larger variance in the data. Moreover, while the eye-tracking data is limited to the visual component of the cognitive processes, EEG data additionally contains a motor component and a se- mantic component during the reading process. Learning word frequency as an auxiliary task considerably helps all gaze and EEG features. The known correlation between eye-tracking and word frequency ( Rayner and Duffy , 1986 ) is clearly beneﬁcial for learning gaze features. Moreover, a frequency effect can also be found in early EEG signals, i.e. during the ﬁrst 200ms of reading a word ( Hauk and Pulverm¨uller , 2004 ). 7 Discussion In accordance with previous work (e.g. Barrett et al. ( 2016 ); Mishra and Bhattacharyya ( 2018 )), we showed consistent improvements when using gaze data in a range of information extraction tasks, with recorded token-level features and with type-aggregated features on benchmark corpora. The patterns in the results are less consistent when enhancing NLP methods with EEG signals. While we can still show signiﬁcant improvements over the baseline models, in general the models lever- aging EEG features yield lower performance than the ones with gaze features. A plausible explana- tion for this is that the combination of gaze and EEG features decreases the signal-to-noise ratio even more than for only one type of cognitive data. Another interpretation is that the eye-tracking and EEG signal contain information that is (too) simi- lar. Thus, the combination does not improve yield better results. Consequently, there are some open questions: How can EEG signals be preprocessed and de- noised more efﬁciently for NLP tasks? How can EEG and eye-tracking (and other cognitive pro- cessing signals or fortuitous data ( Plank , 2016 )) be combined more effectively to improve NLP ap- plications? The models leveraging type-aggregated cog- nitive features show that improvements can be achieved without requiring large amounts of recorded data and provide evidence that this type of data can be generalized on word type level. Al- though these results indicate that huge amounts of recorded data are not necessary for perfor- mance gains, one of the limitations of this work is the effort of collecting cognitive processing sig- nals from humans. However, webcam-based eye- trackers (e.g. Papoutsaki et al. ( 2016 )) and com- mercially available EEG devices (e.g Stytsenko et al. ( 2011 )) are becoming more accurate and user-friendly. Finally, the multi-task learning experiments provide insights into the correlation of learning NLP tasks together with word frequency and cog- nitive features. While the results are not as promis- ing as our main experiments, it reveals qualities of the individual gaze and EEG features. For fu- ture work, a possible approach to combine the po- tential of exceptionally good single-subject mod- els and multi-task learning, would be to learn gaze and/or EEG features from multiple subjects at the same time. This has been shown to improve accu- racy on brain-computer interface tasks and helps to further reduce the variability between subjects ( Panagopoulos , 2017 ). One of the challenges of NLP is to learn as much as possible from limited resources. Using cognitive language processing data may allow us take a step towards meta-reasoning, the process of discovering the cognitive processes that are used to tackle a task in the human brain ( Grifﬁths et al. , 2019 ), and in turn be able to improve NLP. 8 Conclusion We presented an extensive study of improv- ing NLP tasks with eye-tracking and electroen- cephalography data as instances of cognitive pro- cessing signals. We showed how adding gaze and/or EEG features to a range of information extraction tasks, namely named entity recogni- \ntion, relation classiﬁcation and sentiment analy- sis, yields signiﬁcant improvements over the base- lines. Moreover, we showed how these features can be generalized at word type-level so that no recorded data is required during prediction time. Finally, we explored a multi-task learning setting to simultaneously learn NLP tasks and cognitive features. In conclusion, the gaze and EEG signals of hu- mans reading text, even though noisy and avail- able in limited amounts, show great potential in improving NLP tasks and facilitate insights into language processing which can be applied to NLP, but need to be investigated in more depth."
}