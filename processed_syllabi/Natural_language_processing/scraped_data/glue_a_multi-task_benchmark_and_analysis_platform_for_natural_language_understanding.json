{
    "document_type": "research_paper",
    "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    "author": "Alex Wang ; Amanpreet Singh ; Julian Michael ; Felix Hill ; Omer Levy ; Samuel Bowman",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\Wang-2018-Glue-a-multi-task-benchmark-and-ana.pdf",
    "date_published": "2018-08-30",
    "keywords": "",
    "flag": "",
    "text": "353 GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding Alex Wang 1 , Amanpreet Singh 1 , Julian Michael 2 , Felix Hill 3 , Omer Levy 2 , and Samuel R. Bowman 1 1 New York University, New York, NY 2 Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA 3 DeepMind, London, UK { alexwang,amanpreet,bowman } @nyu.edu { julianjm,omerlevy } @cs.washington.edu felixhill@google.com Human ability to understand language is gen- eral , ﬂexible , and robust . In contrast, most NLU models above the word level are designed for a speciﬁc task and struggle with out-of-domain data. If we aspire to develop models with understand- ing beyond the detection of superﬁcial correspon- dences between inputs and outputs, then it is crit- ical to develop a uniﬁed model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com ): a benchmark of nine diverse NLU tasks, an auxil- iary dataset for probing models for understand- ing of speciﬁc linguistic phenomena, and an on- line platform for evaluating and comparing mod- els. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efﬁcient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo ( Peters et al. , 2018 ), a powerful transfer learning tech- nique, as well as state-of-the-art sentence repre- sentation models. The best models still achieve fairly low absolute scores. Analysis with our diag- nostic dataset yields similarly weak performance over all phenomena tested, with some exceptions. The GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difﬁ- culties. As the goal of GLUE is to spur develop- ment of generalizable NLU systems, we design the benchmark such that good performance should re- Corpus | Train | Task Domain Single-Sentence Tasks CoLA 8.5k acceptability misc. SST-2 67k sentiment movie reviews Similarity and Paraphrase Tasks MRPC 3.7k paraphrase news STS-B 7k textual sim. misc. QQP 364k paraphrase online QA Inference Tasks MNLI 393k NLI misc. QNLI 108k QA/NLI Wikipedia RTE 2.5k NLI misc. WNLI 634 coref./NLI ﬁction books Table 1 : Task descriptions and statistics. Bold de- notes tasks for which there is privately-held test data. All tasks are binary classiﬁcation, except STS-B (regression) and MNLI (three classes). quire models to share substantial knowledge (e.g., trained parameters) across tasks, while maintain- ing some task-speciﬁc components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we ex- pect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with single- sentence inputs: Corpus of Linguistic Acceptabil- ity (CoLA; Warstadt et al. 2018 ) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013 ) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, ( Dolan and Brockett , 2005 )), Quora Question Pairs 1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017 ). The remain- ing four tasks are formatted as natural language in- ference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018 ) and Recog- 1 data.quora.com/First-Quora-Dataset- Release-Question-Pairs \n354 Single Sentence Similarity and Paraphrase Natural Language Inference Model Avg CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI Single-task 64.8 35.0 90.2 68.8/80.2 86.5 / 66.1 55.5/52.5 76.9 / 76.7 61.1 50.4 65.1 Multi-task 69.0 18.9 91.6 77.3 / 83.5 85.3/63.3 72.8/71.1 75.6/75.9 81.7 61.2 65.1 CBoW 58.9 0.0 80.0 73.4/81.5 79.1/51.4 61.2/58.7 56.0/56.4 75.1 54.1 62.3 Skip-Thought 61.5 0.0 81.8 71.7/80.8 82.2/56.4 71.8/69.7 62.9/62.8 74.7 53.1 65.1 InferSent 64.7 4.5 85.1 74.1/81.2 81.7/59.1 75.9/75.3 66.1/65.7 79.8 58.0 65.1 DisSent 62.1 4.9 83.7 74.1/81.7 82.6/59.5 66.1/64.8 58.7/59.1 75.2 56.4 65.1 GenSen 66.6 7.7 83.1 76.6/83.0 82.9/59.8 79.3 / 79.2 71.4/71.3 82.3 59.2 65.1 Table 2 : Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation ( Matthews , 1975 ). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006 , Bar Haim et al. 2006 , Giampic- colo et al. 2007 , Bentivogli et al. 2009 ), as well as versions of SQuAD ( Rajpurkar et al. , 2016 ) and Winograd Schema Challenge ( Levesque et al. , 2011 ) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the bench- mark is measured per task as well as in aggregate, averaging performance across tasks. Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also in- cludes a dataset of hand-crafted examples for probing trained models. This dataset is designed to highlight common phenomena, such as the use of world knowledge, logical operators, and lexi- cal entailments, that models must grasp if they are to robustly solve the tasks. Each of the 550 ex- amples is an NLI sentence pair tagged with the phenomena demonstrated. We ensure that the data is reasonably diverse by producing examples for a wide variety of linguistic phenomena, and bas- ing our examples on naturally-occurring sentences from several domains. We validate our data by us- ing the hypothesis-only baseline from Gururangan et al. ( 2018 ) and having six NLP researchers man- ually validate a random sample of the data. Baselines To demonstrate the benchmark in use, we apply multi-task learning on the training data of the GLUE tasks, via a model that shares a BiL- STM between task-speciﬁc classiﬁers. We also train models that use the same architecture but are trained on a single benchmark task. Finally, we evaluate the following pretrained models: average bag-of-words using GloVe embeddings (CBoW), Skip-Thought ( Kiros et al. , 2015 ), InferSent ( Con- neau et al. , 2017 ), DisSent ( Nie et al. , 2017 ), and GenSen ( Subramanian et al. , 2018 ). Tags Sentence Pair Quantiﬁers Double Negation I have never seen a hummingbird not ﬂying. I have never seen a hummingbird. Active/Passive Cape sparrows eat seeds, along with soft plant parts and insects. Cape sparrows are eaten. Named Entities World Knowledge Musk decided to offer up his per- sonal Tesla roadster. Musk decided to offer up his per- sonal car. Table 3 : Diagnostic set examples. Systems must predict the relationship between the sentences, ei- ther entailment , neutral , or contradiction when one sentence is the premise and the other is the hypothesis, and vice versa. Examples are tagged with the phenomena demonstrated. We group each phenomena into one of four broad categories. We ﬁnd that our models trained directly on the GLUE tasks generally outperform those that do not, though all models obtain fairy low absolute scores. Probing the baselines with the diagnos- tic data, we ﬁnd that performance on the bench- mark correlates with performance on the diag- nostic data, and that the best baselines similarly achieve low absolute performance on the linguis- tic phenomena included in the diagnostic data. Conclusion We present the GLUE benchmark, consisting of: (i) a suite of nine NLU tasks, built on established annotated datasets and covering a diverse range of text genres, dataset sizes, and difﬁculties; (ii) an online evaluation platform and leaderboard, based primarily on private test data; (iii) an expert-constructed analysis dataset. Exper- iments indicate that solving GLUE is beyond the capability of current transfer learning methods. \n355"
}