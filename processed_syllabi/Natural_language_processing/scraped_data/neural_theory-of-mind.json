{
    "document_type": "research_paper",
    "title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs",
    "author": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\neural_theory-of-mind.pdf",
    "date_published": "2023-04-04",
    "keywords": "",
    "flag": "",
    "text": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs Maarten Sap ♠♦ Ronan Le Bras ♠ Daniel Fried ♦ Yejin Choi ♠♥ ♠ Allen Institute for AI, Seattle, WA, USA ♦ Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA ♥ Paul G. Allen School of Computer Science, University of Washington, Seattle, WA, USA maartensap@cmu.edu Abstract Social intelligence and Theory of Mind (T O M), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allow humans to effec- tively navigate and understand everyday social interactions. As NLP systems are used in in- creasingly complex social situations, their abil- ity to grasp social dynamics becomes crucial. In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theory-based perspective. We show that one of today’s largest language models (GPT-3; Brown et al. , 2020 ) lacks this kind of social intelligence out-of-the box, using two tasks: S OCIAL IQ A ( Sap et al. , 2019b ), which mea- sures models’ ability to understand intents and reactions of participants of social interactions, and T O M I ( Le et al. , 2019 ), which measures whether models can infer mental states and re- alities of participants of situations. Our results show that models struggle sub- stantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on S OCIAL IQ A and T O M I , respectively. To conclude, we draw on theories from pragmat- ics to contextualize this shortcoming of large language models, by examining the limita- tions stemming from their data, neural archi- tecture, and training paradigms. Challeng- ing the prevalent narrative that only scale is needed, we posit that person-centric NLP ap- proaches might be more effective towards neu- ral Theory of Mind. 1 Introduction With the growing prevalence of AI and NLP sys- tems in everyday social interactions, the need for AI systems with social intelligence and Theory of Mind (T O M), i.e., the ability to infer and reason about the intents, feelings, and mental states of oth- ers, becomes increasingly evident ( Pereira et al. , Reasoning about mental states and realities Social commonsense and emotional intelligence Although Taylor was older and stronger, they lost to Alex in the wrestling match. James and Abby are in the bedroom. Abby put the pen in the desk drawer. Abby leaves the bedroom. James moves the pen into the bag. How would Alex feel as a result? Where does James think Abby will look for the pen? ashamed boastful drawer bag Measuring Neural Theory of Mind Figure 1: Theory of Mind is the ability for humans to reason about the intents, reactions, and mental states of others. We asses these abilities in LLMs through two question-answering tasks that measure social common- sense and emotional intelligence (S OCIAL IQ A ; top) and reasoning about people’s mental states and reali- ties (T O M I ; bottom); ﬁnding that GPT-3 ( ) struggles on both tasks. We discuss why that may be, drawing from theories of the pragmatics of language. 2016 ; Langley et al. , 2022 ). For humans, Theory of Mind is a crucial component that enables us to in- teract and communicate effectively with each other ( Premack and Woodruff , 1978 ; Apperly , 2010 ). It allows us, for example, to infer that someone likely feels boastful instead of ashamed after winning a wrestling match (Fig. 1 ; top). In addition, T O M also enables us to reason about people’s mental re- alities, e.g., if someone was out of the room while a pen was moved, she will likely search for the pen arXiv:2210.13312v2  [cs.CL]  3 Apr 2023 \nwhere she last saw it instead of where it was moved to (Fig. 1 ; bottom). While humans develop it naturally, T O M and social intelligence remain elusive goals for modern AI systems ( Choi , 2022 ), including large neural language models (LLMs). With advances in scal- ing the sizes of models and datasets, these LLMs have proven very impressive at generating human- like language for conversational, summarization, or sentence continuation settings, often with zero to few examples to learn from ( Brown et al. , 2020 ; Clark et al. , 2021 ; Chowdhery et al. , 2022 ). How- ever, increasing scrutiny has shed light on the short- comings of these LLMs, showing that they often fall prey to spurious correlational patterns instead of displaying higher-order reasoning ( Elkins and Chun , 2020 ; Dale , 2021 ; Marcus , 2022 ). In line with EMNLP 2022’s theme, we examine the open research question of whether and how much LLMs—which are the backbone of most modern NLP systems—exhibit social intelligence and T O M abilities. Using some of the largest En- glish models in existence (GPT-3; Brown et al. , 2020 ), we demonstrate that out-of-the-box LLMs struggle at two types of reasoning abilities that requisites for Theory of Mind (shown in Fig. 1 ). We argue that these reasoning abilities are neces- sary but not sufﬁcient for Theory of Mind, and that larger models will likely provide upper bounds on what equivalent-but-smaller models are capable of. We ﬁrst assess whether LLMs can reason about social commonsense and emotional intelligence with respect to social interactions (§ 3 ), using the S OCIAL IQ A benchmark ( Sap et al. , 2019b ) illus- trated in Fig. 1 (top). Results show our best per- forming few-shot GPT-3 setup achieving only 55% accuracy, lagging > 30% behind human perfor- mance. Furthermore, social reasoning about the protagonists of situations is easier for GPT-3 (5- 15% absolute difference) compared to reasoning about other secondary participants. Second, we measure LLMs’ ability to under- stand other people’s mental states and realities in short stories (§ 4 ). We use the T O M I QA bench- mark (illustrated in Fig. 1 ; bottom; Le et al. , 2019 ), which was inspired by the classic Sally-Ann False Belief Theory of Mind test ( Baron-Cohen et al. , 1985 ). Here, our results show that GPT-3 models peak at 60% accuracy on questions about partic- ipants’ mental states, compared to 90–100% on factual questions. Our novel insights show that reasoning about social situations and false beliefs still presents a signiﬁcant challenge for large language models, de- spite their seemingly impressive performance on tasks that could require social intelligence (e.g., story generation, dialogues). In § 5 , we ﬁrst ex- amine these shortcomings; drawing on theories of the pragmatics of language, we speculate that the type of texts in LLMs’ training datasets could sub- stantially limit learning social intelligence. Then, we outline some possible future directions towards socially aware LLMs, reﬂecting on the feasibil- ity of interactional data selection, person-centric inductive biases, and interaction-based language learning. Our ﬁndings suggest that only increasing the scale of LLMs is likely not the most effective way to create socially aware AI systems, challeng- ing a prevalent narrative in AI research ( Narang and Chowdhery , 2022 ). 2 Theory of Mind & Large LMs Why do LLMs need Theory of Mind? Social intelligence, Theory of Mind, and commonsense reasoning have been a longstanding but elusive goal of artiﬁcial intelligence for decades ( Gun- ning , 2018 ; Choi , 2022 ). These reasoning abil- ities are becoming increasingly necessary as AI assistants are used in situations that require social intelligence and Theory of Mind in order to op- erate effectively ( Wang et al. , 2007 ; Dhelim et al. , 2021 ; Langley et al. , 2022 ). For example, new tech- nologies are emerging where AI is used to interact and adapt to users ( Bickmore and Picard , 2005 ; Jaques , 2019 ), e.g., voice assistants, and tutoring systems; or where AI helps enhance communica- tion between multiple users, e.g., email autocom- plete ( Chen et al. , 2019 ), AI-assisted counseling ( Kearns et al. , 2020 ; Allen , 2020 ; Sharma et al. , 2021 ), or facilitated discussion ( Rosé et al. , 2014 ). As we move beyond just asking single-turn ques- tions to social and interactive AI assistants, higher- order reasoning becomes necessary ( McDonald and Pearson , 2019 ). For example, AI systems should be capable of more nuanced understand- ing, such as ensuring an alarm is on if someone has a job interview the next morning ( Dhelim et al. , 2021 ), knowing to call for help when an elderly person falls ( Pollack , 2005 ), inferring personality and intentions in dialogues ( Mairesse et al. , 2007 ; Wang et al. , 2019 ), reasoning about public com- mitments ( Asher and Lascarides , 2013 ), predicting \n0 20 40 60 80 100 0 10 20 30 Social  IQa accuracy k (number of examples ) random ada curie davinci human Figure 2: Accuracy on the S OCIAL IQ A dev. set, bro- ken down by LLM model type and size, as well as num- ber of few-shot examples ( k ). emotional and affective states ( Litman and Forbes- Riley , 2004 ; Jaques et al. , 2020 ), and incorporating empathy, interlocutor perspective, and social intel- ligence ( Kearns et al. , 2020 ; Sharma et al. , 2021 ). What is Theory of Mind? Theory of Mind (T O M) describes the ability that we, as humans, have to ascribe and infer the mental states of others, and to predict which likely actions they are going to take ( Apperly , 2010 ). 1 This ability is closely re- lated to (interpersonal) social intelligence ( Ganaie and Mudasir , 2015 ), which allows us to navigate and understand social situations ranging from sim- ple everyday interactions to complex negotiations ( Gardner et al. , 1995 ). Interestingly, the development of Theory of Mind and language seem to happen around sim- ilar ages in children ( Sperber and Wilson , 1986 ; Wellman , 1992 ; Miller , 2006 ; Tauzin and Gergely , 2018 ). 2 Theories of the pragmatics of language and communication can frame our understanding of this link ( Rubio-Fernandez , 2021 ), positing that one needs to reason about an interlocutor’s mental state (T O M) to effectively communicate and un- derstand language ( Grice , 1975 ; Fernández , 2013 ; Goodman and Frank , 2016 ; Enrici et al. , 2019 ). 3 1 While Theory of Mind is well developed in most adults ( Ganaie and Mudasir , 2015 ), reasoning and inference capa- bilities can be inﬂuenced by age, culture, neurodiversity, or developmental disorders ( Korkmaz , 2011 ). 2 The direction of the T O M-language association is still debated ( de Villiers , 2007 ). Some researchers believe lan- guage development enables T O M-like abilities ( Pyers and Senghas , 2009 ; Rubio-Fernandez , 2021 ). On the other hand, some argue that language develops after T O M since preverbal infants already could possess some level of T O M-like abilities ( Onishi and Baillargeon , 2005 ; Southgate and Vernetti , 2014 ; Poulin-Dubois and Yott , 2018 ). 3 Most cognitive studies on this subject focus on the English language, which is not representative of the wide variation of 0 20 40 60 80 100 Effect React Want Social IQa accuracy Reasoning dimension Agent Other Figure 3: Comparing the accuracy of GPT-3-D A V INCI (35-shot) on S OCIAL IQ A when the reasoning is about the main agent of the situation versus others. 3 S OCIAL IQ A : Do LLMs have Social Intelligence and Social Commonsense? A crucial component of Theory-of-Mind is the abil- ity to reason about the intents and reactions of par- ticipants of social interactions. To measure this, we use the dev. set of the S OCIAL IQ A QA benchmark ( Sap et al. , 2019b ), which was designed to probe so- cial and emotional intelligence in various everyday situations. This benchmark covers questions about nine social reasoning dimensions, drawn from the A TOMIC knowledge graph ( Sap et al. , 2019a ). S OCIAL IQ A instances consist of a context, ques- tion, and three answer choices, written in English. Each question relates to a speciﬁc reasoning dimen- sion from A TOMIC : six dimensions focus on the pre- and post-conditions of the agent or protago- nist of the situation (e.g., needs, intents, reactions, next actions), and three dimensions focus on the post-conditions of other participants involved in the situation (reaction, next action, effect). In to- tal, there are 1954 three-way QA tuples; see Tab. 1 for examples, and Tab. 3 in Appendix A for per- dimension counts. 3.1 Probing LLMs with S OCIAL IQ A To probe our language models, we use a k -shot lan- guage probing setup, following Brown et al. ( 2020 ). We select the answer that has the highest likelihood under the language model conditioned on the con- text and question, as described in Appendix C . To test the limits of what the models can do, we select k examples that have the same A TOMIC rea- soning dimension as the question at hand, varying k language structures, and thus limits the cognitive conclusions one can draw about the link between language and Theory of Mind ( Blasi et al. , 2022 ). \nSituation Answers Focus (a) Remy was working late in his ofﬁce trying to catch up. He had a big stack of papers. What does Remy need to do before this? Needed to be behind Agent Be more efﬁcient Finish his work (b) Casey wrapped Sasha’s hands around him because they are in a romantic relationship. How would you describe Casey? Very loving towards Sasha Agent Wanted Being kept warm by Sasha (c) Tracy held a baby for 9 months and then gave birth to addison. What will happen to Tracy? Throw her baby at the wall Agent Cry Take care of her baby (d) Kai gave Ash some bread so they could make a sandwich. How would Kai feel afterwards? Glad they helped Agent Good they get something to eat Appreciative (e) Aubrey was making extra money by babysitting Tracey’s kids for the summer. What will Tracy want to do next? Save up for a vacation Others Let Aubrey know that they are appreciated Pay off her college tuition (f) The people bullied Sasha all her life. But Sasha got revenge on the people. What will the people want to do next? Do whatever Sasha says Others Get even Flee from Sasha (g) After everyone ﬁnished their food they were going to go to a party so Kai decided to ﬁnish his food ﬁrst. What will others want to do next? Eat their food quickly Others Throw their food away Go back for a second serving (h) Aubrey fed Tracy’s kids lunch today when Tracy had to go to work. What will happen to Aubrey? Be grateful Agent Get paid by Tracy Get yelled at by Tracy (i) Sasha was the most popular girl in school when she accepted Jordan’s invitation to go on a date. What will Jordan want to do next? Plan a best friends outing with Sasha Others Plan a romantic evening with Sasha Go on a date with Valerie Table 1: Examples of S OCIAL IQ A questions, which person the questions focus on ( Agent , Others ), and the human gold answers ( ) and GPT-3-D A V INCI predictions ( ). from 0 to 35 in increments of 5. We use three GPT- 3 model sizes: GPT-3-A DA (smallest), and GPT- 3-C URIE and GPT-3-D A V INCI (two largest). 3.2 S OCIAL IQ A Results Shown in Fig. 2 , GPT-3 models perform sub- stantially worse than humans (>30% less) on S O - CIAL IQ A , 4 and also worse than models ﬁnetuned on the S OCIAL IQ A training set (>20%; Lourie et al. , 2021 ). 5 Although it is not surprising that GPT-3-D A V INCI reaches higher accuracies than GPT-3-A DA and GPT-3-C URIE , the gains are small, which suggests that increasing model size might not be enough to reach human-level accuracy. These ﬁndings are in line with recent BIG-Bench results on S OCIAL IQ A with the BIG-G (128B pa- rameters; Srivastava et al. , 2022 ) and PaLM (353B parameters; Chowdhery et al. , 2022 ) LLMs, which 4 We ﬁnd similar results when using I NSTRUCT GPT ( Ouyang et al. , 2022 ) instead of GPT-3-D A V INCI . 5 Lourie et al. ( 2021 ) achieves 83% on the test set, as shown on the AI2 S OCIAL IQ A leaderboard . lag behind humans with 45% and 73% accuracy, respectively (see Fig. 7 in Appendix A.2 ). Focusing on GPT-3-D A V INCI , while increasing the number of examples k improves performance, the differences are marginal after k =10 examples (only 1% increase from 10 to 35 examples). This suggest that performance either plateaus or follows a logarithmic relationship with increasing number of conditioning examples. Finally, we examine the differences in GPT- 3-D A V INCI with respect to which participant is the focus. Shown in Fig. 3 , we ﬁnd that GPT-3- D A V INCI performs consistently better on agent- centric questions, compared to other-oriented ques- tions. Shown in the example predictions in Tab. 1 , GPT-3-D A V INCI often confuses which participant is being asked about. In example (e), after Aubrey babysat for Tracy, GPT-3-D A V INCI fails to pre- dict that Tracy will likely want to “ let Aubrey know they are appreciated ,” and instead mistakenly pre- dicts that Tracy will want to “ save up for vacation ,” which is what Aubrey would likely do. GPT-3- \n0 20 40 60 80 100 0 5 10 15 20 25 ToMi accuracy (Mind only) k (number of examples) random ada curie davinci Figure 4: Accuracy on the T O M I dev. set M IND ques- tions of varying sizes of GPT-3, and with varying num- ber of examples ( k ). D A V INCI displays a similar participant confusion in example (f) in Tab. 1 . 4 T O M I : Can LLMs Reason about Mental States and Realities? Another key component of Theory of Mind is the ability to reason about mental states and realities of others, recognizing that they may be different than our own mental states. As a measure of this ability in humans, psychologists developed the Sally Ann false-belief test ( Wimmer and Perner , 1983 ), in which two people (Sally and Ann) are together in a room with a ball, a basket, and a box, and while Sally is away, Ann moves the ball from the basket to the box. When asked where Sally will look for her ball, Theory of Mind allows us to infer that Sally will look in the basket (where she left the ball), instead of in the box (where the ball is, unbeknownst to Sally). To measure the false-belief abilities of LLMs, we use the T O M I QA dataset of English Sally-Ann- like stories and questions ( Le et al. , 2019 ). 6 T O M I stories were created using a stochastic rule-based algorithm that samples two participants, an object of interest, and a set of locations or containers, and weaves together a story that involves an object being moved (see Tab. 2 ). All questions have two possible answers: the original object location, and the ﬁnal object location. We investigate how LLMs answer the T O M I story-question pairs, distinguishing between ques- tions about factual object locations (F ACT ) and questions about where participants think objects 6 T O M I is a more challenging version of the rule-based datasets by Nematzadeh et al. ( 2018 ) and Grant et al. ( 2017 ), as it contains randomly inserted distractor actions that prevent trivial reverse engineering. 0 20 40 60 80 100 0 2 4 8 16 24 ToMi accuracy k (number of examples) Fact Mind Mind-TB Mind-FB Figure 5: Accuracy of GPT-3-D A V INCI by number of examples ( k ), by reasoning type (F ACT vs. M IND ; M IND -T B vs. M IND -F B ). are located (i.e., their mental states; M IND ). The F ACT questions either ask about the object’s origi- nal (F ACT -M EM ) or ﬁnal (F ACT -R EAL ) location. The M IND questions cover ﬁrst-order (e.g., “ where will Abby look for the object? ”; M IND -1st) and second-order beliefs (e.g., “ where does James think that Abby will look for the object? ”; M IND -2nd). We further distinguish the M IND questions between true belief (T B ) and false belief (F B ), i.e., stories where a participant was present or absent when an object was moved, respectively. Importantly, answering the M IND questions re- quires Theory of Mind and reasoning about reali- ties and mental states of participants—regardless of the true- or false-belief setting—whereas F ACT questions do not require such T O M. There are a total of 1861 two-way QA pairs in our T O M I probe set, with 519 F ACT and 1342 M IND questions (see Tab. 4 in Appendix B for more detailed counts). 4.1 Probing LLMs with T O M I We use the k -shot probing setup to test this T O M component in LLMs, with k ∈{ 2 , 4 , 8 , 16 , 24 } . We select k examples of the same reasoning type (i.e., F ACT -M EM , M IND -1st, etc.), ensuring a 50- 50 split between true- and false-belief examples for the M IND questions. As before, we test GPT-3- A DA , GPT-3-C URIE , and GPT-3-D A V INCI . 4.2 T O M I Results Shown in Fig. 4 , our results indicate that GPT-3 models struggle substantially with the T O M I ques- tions related to mental states (M IND ), reaching 60% accuracy in the best setup. As expected, the best performance is reached with GPT-3-D A V INCI compared to smaller models which do not surpass \nType Story Question Answers (a) F ACT Sophia entered the study. Noah entered the study. The dress is in the treasure chest. Noah exited the study. Hannah entered the garden. Sophia moved the dress to the box. Where is the dress really? box treasure chest (b) M-1-F B Noah entered the garden. Nathan entered the garden. Evelyn likes the pumpkin. The banana is in the basket. Nathan exited the garden. Noah moved the banana to the suitcase. Where will Nathan look for the banana? basket suitcase (c) M-2-T B Lily entered the patio. Aiden is in the patio. Mila entered the patio. Mila hates the radish. The coat is in the box. Aiden moved the coat to the crate. Mila exited the patio. Where does Aiden think that Mila searches for the coat? crate box (d) M-1-T B Elizabeth entered the cellar. Carter entered the cellar. The slippers is in the crate. Elizabeth moved the slippers to the container. Carter exited the cellar. Where will Carter look for the slippers? container crate (e) M-1-F B Evelyn entered the living room. Jackson entered the playroom. James entered the playroom. The beans are in the treasure chest. James exited the playroom. Jackson moved the beans to the pantry. Jackson exited the playroom. James entered the living room. Where will James look for the beans? treasure chest pantry (f) M-2-F B Isla likes the potato. Ella entered the laundry. Oliver entered the laundry. The slippers are in the box. Ella exited the laundry. Oliver moved the slippers to the basket. Isla entered the ofﬁce. Where does Ella think that Oliver searches for the slippers? basket box Table 2: Example stories in the T O M I dev. dataset, with GPT-3-D A V INCI predictions (with k =16 examples) and gold answers. “Type” denotes reasoning type, M-1 and M-2 denote M IND -1st and M IND -2nd, resp. 55% accuracy; however, as before, the gains from scaling up GPT-3 are very small. Similarly, in- creasing the number of few-shot examples beyond k = 4 does not substantially improve performance, corroborating ﬁndings on S OCIAL IQ A . Further examining GPT-3-D A V INCI with re- spect to question types, we show that the model struggles substantially more with questions about mental states (55–60% for k > 0 ) compared to factual questions (90–100% for k > 0 ; Fig. 5 ; columns). Furthermore, the difference between per- formance on M IND -T B and M IND -F B questions shows an interesting pattern when conditioning on an increasing number of examples k (Fig. 5 ; lines): GPT-3-D A V INCI ’s M IND -T B accuracy ﬁrst in- creases, peaks at k = 4 , then decreases. This peak seems to be due to the model defaulting to the most recent object location (i.e., the correct M IND -T B answer), as illustrated in example (e) in Tab. 2 . Apparent in Fig. 10 in Appendix B , this recency bias is a phenomenon that has been previously doc- umented in LLMs ( O’Connor and Andreas , 2021 ). In general, GPT-3-D A V INCI ’s comparably poor performance for M IND -T B and M IND -F B ques- tions at k > 8 suggests that it cannot properly answer questions about participants’ mental states and realities. 5 Discussion: Towards NLP with Neural Theory of Mind Most humans develop social intelligence and The- ory of Mind naturally. However, in this work, we showed that these abilities do not emerge automati- cally in large-pretrained language models. These shortcomings contrast with the wealth of successes of LLMs at a variety of tasks, including tasks that potentially require social intelligence. For exam- ple, GPT-3 has been shown to generate stories with emotional arcs that are virtually indistinguishable from human-written stories ( Clark et al. , 2021 ). Ad- ditionally, recent work has used GPT-3 to generate social commonsense knowledge related to protago- nists of situations ( West et al. , 2022 ). While those ﬁndings suggest some level of social and emotional intelligence in LLMs, our explorations highlight the limits of these abilities, and raise the open ques- tion: how can we create NLP systems with true social intelligence and Theory of Mind? To begin answering this question, we ﬁrst dis- cuss the current LLMs training paradigm (§ 5.1 ), drawing from theories of pragmatics to examine why these models are not learning social intelli- gence efﬁciently. Then, we outline some possible future directions to bias models towards Theory of Mind (§ 5.2 ), through person-centric neural archi- \ntectures, data selection, and training objectives. 5.1 The Pragmatics of “Static” Text To understand why LLMs are still struggling with social intelligence, we examine LLMs’ training paradigm through the lens of pragmatics . As dis- cussed in § 2 , pragmatics provides a connection be- tween language development and Theory of Mind ( Sperber and Wilson , 1986 ; Miller , 2006 ; Tauzin and Gergely , 2018 ): learning to communicate effec- tively with language requires reasoning about what our interlocutor knows or does not know ( Grice , 1975 ; Fernández , 2013 ; Goodman and Frank , 2016 ; Enrici et al. , 2019 ). 7 One major use of language by people is to com- municate about relationships and personal experi- ences ( Clark and Schaefer , 1989 ; Dunbar , 1993 ). This is fundamentally different from the training data of LLMs, which consists of language found in what we call static texts: documents that are written for a general audience and are relatively self-contained and topically focused (e.g., news ar- ticles, books, Wikipedia articles; Gao et al. , 2020 ; Dodge et al. , 2021 ). Such static text is typically written such that readers only require the language itself as input, which they then combine with their world knowledge and commonsense to understand its meaning ( Graesser et al. , 1994 ). If AI systems are to learn social intelligence and Theory of Mind, we posit that static text has certain limitations, from a pragmatics lens, outlined below. Reporting bias. Following Grice’s maxim of quantity ( Grice , 1975 ), static text often avoids re- dundancy by omitting content that is known by both the author and the reader ( Clark and Brennan , 1991 ). Also known as reporting bias ( Gordon and Van Durme , 2013 ; Lucy and Gauthier , 2017 ), this phenomenon likely limits LLMs’ ability to learn social commonsense knowledge from static text. Lack of communicative intent and alternatives. A corollary to reporting bias, static text does not provide any direct access to communicative intent (why words were used) or to alternatives (which words were not used, and why). This reasoning about intents, alternatives, and their implications is highly predictive of the pragmatic inferences 7 Note here that, in contrast to other work ( Bender and Koller , 2020 ; Bisk et al. , 2020 ), we do not focus on whether LLMs “understand” language, instead we examine whether LLMs can answer questions about the emotions and mental states of participants of situations. people draw about their interlocutors ( Goodman and Frank , 2016 ) — for example, when someone answers Where does Taylor live? with Somewhere in the U.S. , it implies that they likely do not know or do not want to share the exact location, since, if they did, they would have been more speciﬁc. This poses a likely limitation that LLMs only learn what words are used, but not which words were not used, and why. Lack of communicative effects. Language is primarily learned ( Wells and Bridges , 1981 ; Tomasello et al. , 2005 ) and used ( Clark , 1996 ) in collaborative and interactive settings ( Clark and Schaefer , 1989 ), which allow interlocutors to give immediate feedback to each other on whether their language was understood ( Clark and Krych , 2004 ) or should be adjusted ( Krauss and Weinheimer , 1966 ), and observe the perlocutionary effects that their language has on their partners ( Austin , 1975 ). Since static text has no such feedback, LLMs learn from all texts, as if they were all equally under- standable by readers. Centering theory. At any given time, most text focuses on describing one protagonist and their re- lation to their surroundings, according to Centering Theory ( Grosz et al. , 1995 ). As such, main char- acters and their mental states are more likely to be described , whereas other participants might only be mentioned . Additionally, main characters or protagonists are more likely to be referred to with pronouns, whereas secondary characters with their names. Thus, a model trained purely on static text might not learn to reason about social intelligence or men- tal states and realities of different characters of situations; they might not even inherently learn to resolve coreference for multiple characters ( Sak- aguchi et al. , 2020 ). In fact, challenges of corefer- ence resolution could explain why GPT-3 models struggle on S OCIAL IQ A which contains questions with pronouns, and centering theory and main char- acter biases in static text could explain why models ﬁnd non-protagonist questions more challenging. On the other hand, T O M I does not contain any pro- nouns, and thus requires social intelligence beyond coreference resolution. 5.2 Future directions towards LLMs with Theory of Mind While there is no one best path towards LLMs with social intelligence and Theory of Mind, it seems \nlikely that progress will require challenging the standard paradigm of training on static text with the language modeling objective. Based on our ﬁndings and the limitations we discussed, we re- ﬂect on some possible directions forward. Beyond static text as training data? Perhaps the key is in the data: the knowledge contained in static text might be too limited for models to learn social intelligence, for reasons described in § 5.1 Socially grounded text (containing elaborations of communicative intents, character mental states, speaker identities, etc.) could enable more efﬁcient learning of Theory of Mind abilities ( Bender and Koller , 2020 ; Bisk et al. , 2020 ; Hovy and Yang , 2021 ), similar to how visual groundings can help with learning physical knowledge ( Zhang et al. , 2022a ). Examples of such datasets include “Social Stories,” which are devised to help individuals with autism improve their interpersonal skills ( Gray , 1995 ), or the Story Commonsense ( Rashkin et al. , 2018 ) and GLUCOSE ( Mostafazadeh et al. , 2020 ) commonsense-annotated story datasets. Alterna- tively, perhaps interactional texts, such as dialogues and other datasets that were explicitly created to require reasoning about mental states, could help with neural Theory of Mind ( Bara et al. , 2021 ). Nevertheless, the scale of training datasets seems to be crucial for LLMs ( Kaplan et al. , 2020 ; Chowd- hery et al. , 2022 ), which poses a challenge: text datasets rich in social intelligence and interactions are not easily found naturally due to reporting bi- ases, and they are costly to create ( Rashkin et al. , 2018 ; Mostafazadeh et al. , 2020 ). Promising re- sults on commonsense reasoning suggest a possi- ble hybrid approach: LLMs could be jointly or sequentially trained on static text and common- sense knowledge bases or socially grounded or interactional text ( Bosselut et al. , 2019 ; Hwang et al. , 2021 ), ﬁrst trained on static text and then enhanced for commonsense knowledge via rein- forcement learning ( Zhou et al. , 2021 ). Person-centric neural inductive biases? While more socially grounded training data could help, LLMs might also learn social intelligence better if they are designed with person-centric inductive biases and training objectives. Hinting at this, prior work has shown that training entity-centric neural architectures on text with entity coreference infor- mation yields more entity-aware LLMs, both in recurrent ( Henaff et al. , 2017 ; Ji et al. , 2017 ; Yang et al. , 2017 ; Liu et al. , 2019 ) and Transformer- based models ( Févry et al. , 2020 ; De Cao et al. , 2020 ; Rosset et al. , 2020 ; Zhang et al. , 2022c ). However, Theory of Mind and social intelligence require much richer social grounding than corefer- ence chains, which is challenging to obtain for su- pervised settings, especially at the scale that LLMs require. Thus, unsupervised approaches to adding inductive biases to models could be a promising so- lution. Future work could look to cognitive science and neuroscience research for possible directions ( Langley et al. , 2022 ), such as exploring LLMs’ equivalents of human concept cells (i.e., sets of neurons that activate for important people or con- cepts; Bowers , 2017 ; Calvo Tapia et al. , 2020 ). Alternatively, examining the internal or latent representations of LLMs could point to future di- rections towards inductive biases for neural Theory of Mind. As an example, recent work has found ev- idence of latent representations of grounded seman- tics in models trained only on static text ( Li et al. , 2021 ), which can be tied to real-world grounding with a small amount of additional supervised train- ing ( Patel and Pavlick , 2022 ). Future work might similarly analyze deep learning models for repre- sentations of Theory of Mind, toward augmenting the models with structure or objectives that surface and strengthen these representations. Interactive and experiential grounding? It is possible, nevertheless, that socially grounded data and person-centric inductive biases will not sufﬁce. Some researchers have argued that language un- derstanding could only emerge from interactions and experiences ( Bender and Koller , 2020 ; Bisk et al. , 2020 ). Likely, this applies to Theory of Mind and social intelligence as well, due to lack of com- municative intents and alternatives in static text. Future work could explore approaches grounded more explicitly in interaction, intents, and alterna- tives, e.g., by explicitly predicting possible next steps and learning why predictions were wrong. In fact, promising research has shown that using an interactive learning or multi-agent communication paradigm can enable some Theory of Mind capa- bilities of models ( Hawkins et al. , 2019 ; Lazaridou et al. , 2020 ; Zhu et al. , 2021 ; Wang et al. , 2022 ). However, there are limits to the types of Theory of Mind that can be learned from interactive simula- tions, which are often task-speciﬁc (e.g., describing objects in an image; Lazaridou et al. , 2020 ; Steinert- Threlkeld et al. , 2022 ). Furthermore, models that \nwere trained in interactive simulation settings of- ten struggle to generalize beyond the simulation environment ( Ludwin-Peery et al. , 2021 ; Mu and Goodman , 2021 ). Based on promising results by Lazaridou et al. ( 2020 ); Zhu et al. ( 2021 ), future work might create generalizable LLMs with neural Theory of Mind through hybrid approaches that combine pretraining with interactive learning: up- dating models trained on static text using super- vision either from humans ( Stiennon et al. , 2020 ; Ouyang et al. , 2022 ; Scheurer et al. , 2022 ) or from proxies for human behavior or social environments ( Ammanabrolu et al. , 2022a , b ) based on broad cov- erage LLMs ( Perez et al. , 2022 ). Probing and evaluating T O M While neural Theory of Mind and social intelligence may re- main an elusive goal for some time, developing measures of those abilities in systems can be done in tandem. We encourage further research in devel- oping benchmarks that measure speciﬁc social abil- ities in LLMs (e.g., Sap et al. , 2019b ; Zadeh et al. , 2019 ), especially those that minimize annotation artifacts and spurious correlations ( Schwartz et al. , 2017 ; Gururangan et al. , 2018 ; Le et al. , 2019 ). Additionally, we encourage further investigations into probing the latent knowledge within LLMs ( Tenney et al. , 2019 ; Li et al. , 2021 ) or examining how LLMs handle entities and people ( Onoe et al. , 2022 ; Schuster and Linzen , 2022 ), which could shed light onto better data choices and inductive biases towards neural Theory of Mind and social intelligence. 6 Conclusion We explore the open question of whether and how much modern large-scale language models (LLMs) can reason about social intelligence and Theory of Mind. Our results show that out-of-the-box LLMs struggle substantially with these abilities, which we argue are necessary but not sufﬁcient aspects of Theory of Mind. Speciﬁcally, GPT-3’s social intelligence as measured by S OCIAL IQ A lags be- hind humans (>30%), and the model struggles to answer T O M I questions about mental states (55- 60%) compared to factual questions (90–100%). In light of these shortcomings, we critically exam- ine the large language model pretraining paradigm from a pragmatics-based perspective, and discuss possible directions towards enabling true social in- telligence in NLP systems. We make our preprocessed datasets available at http://maartensap.com/neuralToM . 7 Limitations Our work focuses on investigating the Theory of Mind abilities in large pretrained language models, but we focus on accessing GPT-3 ( Brown et al. , 2020 ) through an API, since we do not have ac- cess to some of the larger models out there (PaLM; Chowdhery et al. , 2022 ) nor do we have the com- putational resources to run an open-source version of GPT-3 (OPT; Zhang et al. , 2022b ). We hypoth- esize that results would not be drastically differ- ent with such models, based on the low accuracy displayed on S OCIAL IQ A in the recently released BIG-Bench experiments ( Srivastava et al. , 2022 ). Nevertheless, we hope developers of larger LLMs will investigate these T O M abilities to conﬁrm or refute our ﬁndings. We measure the ability to answer questions about people’s mental states using T O M I , which is an au- tomatically constructed corpus of stories involving people, objects, and locations. The automatic na- ture of the creation process could induce biases and artifacts, such as objects being in locations that are plausible but not typical (e.g., bananas in a closet), which could inﬂuence model’s ability to answer questions properly. Based on the near-perfect ac- curacy on the factual questions, however, this may not be a signiﬁcant issue. Future work should in- vestigate more naturalistic settings to probe this ability in LLMs. A potential limitation of our work is that mod- els could latch onto surface patterns and spurious correlations in our two datasets. For example, the- oretically, a model prompted with many T O M I examples may be able to reverse-engineer the data creation algorithm to ﬁnd the solution to each ques- tion. However, this would be a bigger limitation if our claims were that LLMs do have social intelli- gence and Theory of Mind; instead, given that our results show low performance on these tasks even though they are potentially easier due to correla- tional patterns, this would indicate that LLMs have potentially even less reasoning abilities. Additionally, while we operationalize our mea- sure of social intelligence and Theory of Mind through two speciﬁc tasks, S OCIAL IQ A and T O M I , these abilities are much broader. As noted earlier, we view these benchmarks as necessary but not suf- ﬁcient conditions for LLMs to have T O M; solving \nthe benchmarks does not imply that LLMs have T O M, but LLMs with T O M should be able to solve them. We hope that future research will further investigate other aspects of Theory of Mind abil- ities in large pretrained LMs, drawing on social science research. For example, future work could make use of the “unexpected content” task ( Gopnik and Astington , 1988 ) or the “George Washington University Social Intelligence Test” ( Hunt , 1928 ) to measure the social intelligence of LLMs. Finally, the focus on English language LLMs and benchmarks for Theory of Mind is another limita- tion of our work. Echoing recent cognitive science work that argues the need for non-English cognitive science investigations ( Blasi et al. , 2022 ). Speciﬁ- cally, false-belief abilities are greatly inﬂuenced by language structure and grammar ( Boeg Thomsen et al. , 2021 ; Zhang and Zhou , 2022 ). Broader Sociotechnical Implications AI systems are part of a broader sociotechnical sys- tem that also involves individual motivations and societal norms ( Johnson and Verdicchio , 2017 ). As such, per a contextualist view of AI (instead of utopian or dystopian; Barbour , 1992 ), we envision AI systems with social intelligence and Theory of Mind being used in ways that enhance human’s lives, autonomy, and agency ( Chan , 2022 ). In par- allel, we strongly support the development and re- search of policy and regulation, to prevent misuses of AI with social intelligence ( Wischmeyer and Rademacher , 2020 ; Crawford , 2021 ; Reich et al. , 2021 ). Acknowledgements We would like to thank Jack Hessel, Rowan Zellers, Jena D. Hwang, Prithviraj Ammanabrolu for their feedback on preliminary versions of this work, and Anna Jafarpour and Noah Goodman for fruitful cognitive science discussions about the research. We also thank the anonymous reviewers for their thoughtful comments. This research was supported by the Allen Institute for AI and the DARPA MCS program through NIWC Paciﬁc (N66001-19-2- 4031)."
}