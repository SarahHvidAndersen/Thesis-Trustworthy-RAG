{
    "document_type": "research_paper",
    "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
    "author": "Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\7581_Chatbot_Arena_An_Open_Platform_for.pdf",
    "date_published": "2024-06-03",
    "keywords": "Machine Learning, ICML",
    "flag": "",
    "text": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference Wei-Lin Chiang * 1 Lianmin Zheng * 1 Ying Sheng 2 Anastasios N. Angelopoulos 1 Tianle Li 1 Dacheng Li 1 Banghua Zhu 1 Hao Zhang 3 Michael I. Jordan 1 Joseph E. Gonzalez 1 Ion Stoica 1 Abstract Large Language Models (LLMs) have unlocked new capabilities and applications; however, evalu- ating the alignment with human preferences still poses significant challenges. To address this is- sue, we introduce Chatbot Arena, an open plat- form for evaluating LLMs based on human pref- erences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowd- sourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced LLM leaderboards, widely cited by leading LLM developers and companies. The platform is publicly available at https://chat.lmsys.org . 1. Introduction Recent advancements in large language models (LLMs) have significantly expanded their capabilities beyond tradi- tional natural language processing boundaries, addressing a broad array of general tasks ( OpenAI , 2023 ; Gemini et al. , 2023 ; Touvron et al. , 2023 ). These developments underscore the potential of LLMs but also have raised concerns with re- spect to performance evaluation. Current benchmarks often * Equal contribution 1 UC Berkeley 2 Stanford 3 UCSD. Corre- spondence to: Wei-Lin Chiang <weichiang@berkeley.edu>. Proceedings of the 41 st International Conference on Machine Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). Live Static Codeforces Weekly Contests MMLU, HellaSwag, GSM-8K Ground Truth Chatbot Arena MT-Bench, AlpacaEval Human Preference Question Source Evaluation Metric Figure 1. Classification of LLM benchmarks: We categorize along two dimensions: whether the questions are from a static dataset or a live, fresh source, and whether the evaluation met- ric relies on ground truth or (approximated) human preferences. MMLU ( Hendrycks et al. , 2020 ), HellaSwag ( Zellers et al. , 2019 ), GSM-8K ( Cobbe et al. , 2021 ), MT-Bench ( Zheng et al. , 2023b ), and AlpacaEval ( Li et al. , 2023 ) are common examples of static benchmarks. Chatbot Arena is the one introduced in this paper. fail to capture the nuanced and diverse aspects of these mod- els, particularly in assessing their alignment with human preferences in real-world, open-ended tasks. To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of ques- tions (either static or live) and the evaluation metric (either ground truth or human preference). According to these fac- tors, benchmarks can be classified into four categories, as shown in Figure 1 . While a range of benchmarks is benefi- cial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly be- cause such evaluations are inexpensive and reproducible. However, these static, ground-truth-based benchmarks ex- hibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings ( Zheng et al. , 2023b ). Secondly, the test sets in these benchmarks are static, meaning they can become con- taminated over time, which undermines the reliability of the evaluation results ( Yang et al. , 2023 ). Furthermore, for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable. Conse- quently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage. Creating such a benchmark platform entails significant chal- lenges. It requires the collection of live, fresh, and diverse 1 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference user questions to accurately represent real-world scenarios. Additionally, developing scalable, incremental, and efficient ranking systems is essential for evaluating a large number of models. Moreover, ensuring the quality of human evalua- tions is crucial given the noisy nature of human preferences. To this end, we introduce Chatbot Arena, a benchmarking platform for LLMs that features anonymous, randomized battles in a crowdsourced setting. Chatbot Arena is a free website open to all users. 1 On this website, a user can ask a question and get answers from two anonymous LLMs. Af- terward, the user casts a vote for the model that delivers the preferred response, with the models’ identities revealed only after voting. This crowdsourced method effectively gathers a diverse array of fresh user prompts, accurately reflecting real-world LLM applications. Armed with this data, we employ a suite of powerful statistical techniques, ranging from the statistical model of Bradley & Terry ( 1952 ) to the E-values of Vovk & Wang ( 2021 ), to estimate the ranking over models as reliably and sample-efficiently as possible. With these tools in hand, we have designed efficient sam- pling algorithms specifically to select model pairs in a way that accelerates the convergence of rankings while retaining statistical validity. We conduct a thorough analysis of the collected data to en- sure the credibility of our platform. We demonstrate that the user-generated questions are sufficiently diverse to en- compass a wide range of LLM use cases and are sufficiently challenging to differentiate between models. Furthermore, we confirm that the crowd-sourced votes are highly consis- tent with expert evaluations. We have been running our system since Apr 2023 and have received over 240K votes from about 90K users in over 100 different languages as of Jan 2024. To encourage user engagement, we have made over 50 state-of-the-art models available for free. We also collaborate with leading model developers such as OpenAI, Google, Anthropic, Mistral, Hugging Face, and various universities, incorporating their latest models into our platform. We keep the community engaged by routinely updating the leaderboard, publishing analytical blogs, releasing datasets, and sharing information via tweets. Because of its unique and significant value, our leaderboard has emerged as one of the most referenced in the LLM field and has become a benchmark for the industry. We commit to making our data and code available, ensuring that this platform is open-source and open-accessible. We make the following contributions: • We build the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit. 2 1 https://chat.lmsys.org 2 The number was estimated by Google Analytics as of March • We conduct an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback. • We will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena. • We design an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree. 2. Related Work LLM Benchmarks. We briefly review the common LLM benchmarks, following the classification presented in Fig- ure 1 . The most prevalent benchmarks are static, ground- truth-based ones, typically in the form of multiple-choice questions or question-answering tasks with predefined an- swers and test cases. These benchmarks encompass a range of topics including language understanding, mathematics, coding, and logical reasoning. Prominent examples in this category are MMLU ( Hendrycks et al. , 2020 ), Hel- laSwag ( Zellers et al. , 2019 ), GSM-8K ( Cobbe et al. , 2021 ), BigBench ( Srivastava et al. , 2023 ), AGIEval ( Zhong et al. , 2023 ), and HumanEval ( Chen et al. , 2021 ). Benchmarks focusing on safety, such as ToxicChat ( Lin et al. , 2023 ), and comprehensive suites like HELM ( Liang et al. , 2022 ), also exist. In addition to closed-ended questions, bench- marks can include open-ended questions that are evaluated by human judgment, which can be rated by experts or crowd workers such as Amazon Mechanical Turk ( Karpinska et al. , 2021 ; Geng et al. , 2023 ; Wang et al. , 2023 ). The recent trend includes utilizing GPT-4 for approximating human judg- ment ( Chiang & Lee , 2023 ), with notable instances being MT-Bench ( Zheng et al. , 2023b ) and AlpacaEval ( Li et al. , 2023 ). In addition to static benchmarks, live benchmarks that include fresh questions are also available. These ques- tions can be obtained from annual exams or weekly online contests such as Codeforces ( Li et al. , 2022 ; Huang et al. , 2023 ). They can also be sourced from human interaction. Some studies have explored using live human interaction for reinforcement learning from human preference ( Bai et al. , 2022 ; Ouyang et al. , 2022 ; Touvron et al. , 2023 ). However, these studies are typically limited to specific organizations. In this paper, we introduce Chatbot Arena, the first open, large-scale, and crowdsourced benchmark platform that uti- lizes live human interaction. Risks of Static Benchmarks. Static benchmarks have cer- tain issues, including contamination, saturation, overfitting, and a lack of human alignment ( Yang et al. , 2023 ; Oren et al. , 2023 ). DynaBench ( Kiela et al. , 2021 ) identifies these challenges and recommends the use of a live benchmark 2024. Note that user visit may not convert to votes as our website also offers “direct chat” mode. 2 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference that incorporates a human-in-the-loop approach for classical NLP benchmarks. Our system adopts a similar spirit. How- ever, our focus is on chatting with LLMs, and we implement this on a significantly larger user scale. Ranking System. Ranking systems have been a well- studied topic in statistics. Related topics include probability models ( Hunter , 2004 ; Rao & Kupper , 1967 ), rank elicita- tion ( Szörényi et al. , 2015 ; Busa-Fekete et al. , 2014a ; b ), and online experiment design ( Chernoff , 1992 ; Karimi et al. , 2021 ). The Elo rating system has also been used for LLMs ( Bai et al. , 2022 ; Boubdir et al. , 2023 ). Contributing to this literature, we introduce techniques for accelerating ranking convergence and detecting abnormalities, specifi- cally applied to large-scale, real-world settings of LLMs. Human Preference Dataset. Owing to the significance of human preferences, several datasets and analyses exist that incorporate human preferences. These include Ope- nAssistant ( Köpf et al. , 2023 ), HH-RLHF ( Bai et al. , 2022 ), LMSYS-Chat-1M ( Zheng et al. , 2023a ), and synthetic ap- proximations of human preferences like UltraFeedback ( Cui et al. , 2023 ) and Nectar ( Zhu et al. , 2023 ). Our prior data release, LMSYS-Chat-1M ( Zheng et al. , 2023a ), is similarly collected via crowdsourcing. However, LMSYS-Chat-1M comprises solely conversations and lacks human preference data, rendering it unsuitable for direct use in ranking studies. This paper focuses on the analysis of preference data for ranking purposes. 3. Human Preference Data Collection In this section, we discuss our interface design to collect human preferences and present summary statistics. 3.1. Interface Chatbot Arena crowd-sources feedback from users for model evaluation. Our goal is to design an ease-of-use in- terface to reduce friction for users to contribute data. Since we collect feedback from many users, it is difficult to set a consistent grading rubric across different people. Hence, we adopt a pairwise comparison mechanism where users only need to compare two model responses and vote for the better one, instead of requiring users to provide an absolute score. In each battle, two anonymous models are sampled. To encourage data diversity, we do not preset any input prompt on the website. Users are free to input any prompt to the two models. We believe this creates incentives for user en- gagement, particularly given that we offer a free service. It also helps us collect a diverse set of inputs representing real-world usage. After models provide their answers, user compare them side-by-side and vote for the preferred an- swer. If a user cannot choose in the first turn, the user can continue chatting until identifying a winner. For those who are unsure, we also present two buttons, “tie” or “both are bad.” Figure 8 shows a screenshot of our interface. Before using our service, users are required to accept terms of use, which gives us their consent to release the data publicly. 3.2. Data Statistics We began collecting data in April 2023. As of Jan 2024, we have received around 240K votes from over 90K users. Our data involves more than 50 models, including both pro- prietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These con- versations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers. We summarize statistics in Table 1 along with other existing human preference datasets. Figure 10 in the Appendix shows the vote count per model. On average, 8K votes are collected for each model. In Fig- ure 2 , we select a set of representative models and present their win rate and the number of battles. Note that we em- ploy non-uniform sampling to concentrate votes on model pairs that have similar performance due to higher uncer- tainty. This helps us reduce the number of votes required to reach stable results. We later develop an adaptive sampling method and demonstrate its effectiveness against random sampling. See Section 5 for further analysis. To ensure anonymity, we use keywords to filter out con- versations containing model identity such as model name (e.g., GPT, Claude) or companies (e.g., OpenAI, Anthropic). To avoid misuse, we adopt OpenAI moderation API to flag conversations that contain unsafe content. The flagged user requests account for 3% of the total requests. Figure 9 in the Appendix shows the number of valid user votes over time, where we get 1-2K votes per day in recent months and spikes as we introduce new models or leaderboard updates. 4. From Pairwise Comparisons to Rankings Our data consists of pairwise comparisons—but how can we use these comparisons to recover a ranking over all M mod- els? This is a well-studied topic in the literature on learning to rank ( Liu et al. , 2009 ), and we present our perspective here. We let A = { ( m, m ′ ) : m < m ′ and m, m ′ ∈ [ M ] } denote our comparative data set. We consider a sequential setting, where at time t ∈ N , we serve the human a pair of models A t ∈A (which we pick), and in turn we observe the human’s response H t ∈ [0 , 1] . As an example, we might have that A t = (1 , 2) and H t = 1 , 3 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference Table 1. Statistics of human preference datasets, including Anthropic HH ( Bai et al. , 2022 ), OpenAssistant Conversations ( Köpf et al. , 20 23 ), and Chatbot Arena (as of 2024/1/21). The tokens are counted by Llam a 2 ’s t o ke n i z e r. “ Co n v ” = Co n ve rsa t i o n . “ La n g ” = Langua ge. Dataset # Convs # Models # Users # Langs Avg. # Turns Avg. # Tokens Avg. # Tokens per Sample per Prompt per Response Anthropic HH 338,704 - 143 1 2.3 18.9 78.9 OpenAssistant 66,497 - 13,500 35 - 36.9 214.2 Chatbot Arena (20240121) 243,329 50 90,051 149 1.3 94.9 269.0 0.00 0.68 0.69 0.75 0.71 0.76 0.77 0.75 0.76 0.79 0.86 0.90 0.32 0.00 0.50 0.59 0.61 0.59 0.59 0.59 0.62 0.72 0.70 0.80 0.31 0.50 0.00 0.54 0.56 0.50 0.57 0.52 0.69 0.73 0.60 0.87 0.25 0.41 0.46 0.00 0.54 0.48 0.51 0.56 0.58 0.53 0.73 0.84 0.29 0.39 0.44 0.46 0.00 0.42 0.54 0.55 0.58 0.63 0.67 0.76 0.24 0.41 0.50 0.52 0.58 0.00 0.49 0.55 0.58 0.61 0.64 0.73 0.23 0.41 0.43 0.49 0.46 0.51 0.00 0.56 0.58 0.63 0.72 0.71 0.25 0.41 0.48 0.44 0.45 0.45 0.44 0.00 0.54 0.68 0.65 0.62 0.24 0.38 0.31 0.42 0.42 0.42 0.42 0.46 0.00 0.61 0.58 0.61 0.21 0.28 0.27 0.47 0.37 0.39 0.37 0.32 0.39 0.00 0.53 0.57 0.14 0.30 0.40 0.27 0.33 0.36 0.28 0.35 0.42 0.47 0.00 0.52 0.10 0.20 0.13 0.16 0.24 0.27 0.29 0.38 0.39 0.43 0.48 0.00 gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1 llama-2-70b-chat llama-2-13b-chat llama-2-7b-chat mistral-7b-instruct mistral-7b-instruct llama-2-7b-chat llama-2-13b-chat llama-2-70b-chat claude-instant-1 gpt-3.5-turbo-0613 claude-2.1 gemini-pro-dev-api mixtral-8x7b-instruct-v0.1 mistral-medium gpt-4-0613 gpt-4-turbo 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Model B Model A 0 2564 1189 1192 858 3053 1991 270 141 157 106 144 2564 0 566 263 756 2227 1025 355 414 409 264 197 1189 566 0 775 371 382 773 103 45 51 40 52 1192 263 775 0 71 744 869 136 862 66 45 61 858 756 371 71 0 74 564 53 31 30 30 37 3053 2227 382 744 74 0 351 650 113 117 75 114 1991 1025 773 869 564 351 0 842 572 388 283 155 270 355 103 136 53 650 842 0 459 241 202 101 141 414 45 862 31 113 572 459 0 383 134 369 157 409 51 66 30 117 388 241 383 0 251 621 106 264 40 45 30 75 283 202 134 251 0 521 144 197 52 61 37 114 155 101 369 621 521 0 gpt-4-turbo gpt-4-0613 mistral-medium mixtral-8x7b-instruct-v0.1 gemini-pro-dev-api claude-2.1 gpt-3.5-turbo-0613 claude-instant-1 llama-2-70b-chat llama-2-13b-chat llama-2-7b-chat mistral-7b-instruct mistral-7b-instruct llama-2-7b-chat llama-2-13b-chat llama-2-70b-chat claude-instant-1 gpt-3.5-turbo-0613 claude-2.1 gemini-pro-dev-api mixtral-8x7b-instruct-v0.1 mistral-medium gpt-4-0613 gpt-4-turbo 0 500 1000 1500 2000 2500 3000 Model B Model A Figure 2. Win-rate (left) and battle count (right) between a subset of models in Chatbot Arena. indicating that the human prefers model 2 over model 1. In the ensuing text, we will primarily focus on the binary case— where H t ∈{ 0 , 1 } —but our approach will generalize to any form of feedback, including the possibility of allowing the human to express different degrees of preference or to say the models are tied. One critical goal is to estimate the win matrix : θ ∗ ( a ) = E [ H t | A t = a ] , for all a ∈A ; see the left panel of Figure 2 for an illustration of the (empirical) win matrix. In the binary case, the a entry in the win matrix corresponds to the probability the human prefers model a 2 to a 1 when shown the pair a . Finding the win matrix is a relatively straightforward mean-estimation problem; we will provide details in Section 5 . Formally, consider a score s ( P ) ∈ R M , where P is a joint distribution over A and H (by default, we will target a uni- form distribution over A ). Each model has a true score s ( P ) m , and better models will have higher scores. In partic- ular, we have the rank of model m : rank( P ) m = 1 + X m ′ ∈ [ M ] 1 { s ( P ) m ′ > s ( P ) m } . (1) The best model has rank 1 . If there is another model tied for best, they will both get assigned rank 1 . Picking a score. A standard score function in this setting is the vector of Bradley-Terry (BT) coefficients ( Bradley & Terry , 1952 ). In the Bradley-Terry model, H t ∈{ 0 , 1 } , and the probability model m beats model m ′ is modeled via a logistic relationship: P ( H t = 1) = 1 1 + e ξ m ′ − ξ m , (2) where ξ is an M -length vector of so-called BT coefficients. Without loss of generality, we take ξ 1 = 0 (since the model is invariant to addition in ξ ). Our goal is to estimate the pop- ulation Bradley-Terry coefficients, i.e., those that minimize the binary cross-entropy: s ( P ) = argmin ξ E ( A,H ) ∼ P \u0014 ℓ \u0012 H, 1 1 + e ξ A 2 − ξ A 1 \u0013\u0015 , (3) where ℓ is the binary cross-entropy loss, ℓ ( h, p ) = − ( h log( p ) + (1 − h ) log(1 − p )) . Although the BT model technically assumes a parametric form for the model win rates, the seminal results of Huber et al. ( 1967 ); White ( 1982 ) show that maximum likelihood estimators are still asymptotically normal even when these assumptions do not hold, so long as the so-called “sandwich” covariance matrix is used; see Section 5 for details, and see Appendix B for a nonparametric extension of the Bradley- Terry model. Finally, we remark that previous evolutions of 4 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference our online interface have reported different ranking scores, such as the Elo score ( Elo , 1967 ) instead of the BT coeffi- cients. We made this change because the BT coefficients are better for the purpose of statistical estimation. 5. Efficient Approximate Ranking In Section 4 we described how to calculate the win matrix, score, and rank. Now we describe our estimation proce- dures. Win matrix estimation. Estimation of the win ma- trix is relatively straightforward. Define X t ( a ) = 1 P t ( a ) H t 1 { A t = a } , where P t ( a ) is the probability of sam- pling pair a at time t , and X t as the according vector. Then the estimator is ˆ θ T = 1 T T X t =1 X t . (4) Note that E [ X t ( a )] = θ ∗ ( a ) for all t , and thus ˆ θ T is an unbiased estimator of θ ∗ . We will furthermore estimate the covariance matrix as b Σ T = 1 T T X t =1 ( X t − ˆ θ T )( X t − ˆ θ T ) ⊤ . (5) Under the appropriate regularity conditions, we have that √ T b Σ − 1 / 2 ( ˆ θ − θ ∗ ) →N (0 , I d ) , (6) and we construct confidence intervals accordingly. For an understanding of the appropriate regularity conditions, see Durrett ( 2019 ), Theorem 8.2.8, where condition (ii) is trivially satisfied so long as P t ( a ) > ϵ > 0 , and condition (i) is implied by the almost-sure convergence of P t ( a ) to a limiting distribution P ( a ) . Estimating the BT scores. To estimate the BT coefficients, mirroring ( 3 ) , we perform (reweighted) maximum likeli- hood estimation on our data points: s ( ˆ P ) = argmin ξ T X t =1 1 P ( A t ) ℓ \u0012 H t , 1 1 + e ξ At, 2 − ξ At, 1 \u0013 , (7) where A t ∼ P . We perform the inverse weighting by P ( A t ) because this allows us to target a score with a uniform distribution over A . To compute confidence intervals on the BT coefficients, we employ two strategies: (1) the pivot bootstrap ( DiCiccio & Efron , 1996 ), and (2) the “sandwich” robust standard errors outlined in Huber et al. ( 1967 ) (see also Freedman ( 2006 ) for an outline of the necessary technical assumptions). Ulti- mately, based on the results of a simulation study described in Appendix A , we choose to deploy the sandwich intervals due to their smaller size in large samples. Approximate rankings. Finally, we report an approximate ranking for each model that accounts for the uncertainty in the estimation of the score. Given an M -dimensional confidence set C satisfying P ( s ( P ) ∈C ) ≥ 1 − α, (8) we extract an approximate ranking R m = 1 + P m ′ ∈ [ M ] 1 { inf C m ′ > sup C m } . The uniform validity of C directly implies that P ( ∃ m : R m > rank( P ) m ) ≤ α — i.e., with high probability, no model’s performance is un- derstated. A guarantee on the other side—that no model’s performance is overstated—is possible by interchanging the inf and sup . To get the uniform confidence set, we construct the chi-squared interval implied by the central limit theo- rem using the sandwich estimate of the variance. In other words, we construct the interval { ξ : T ˆ V − 1 / 2 (ˆ ξ − ξ ) ≤ χ 2 1 − α,M − 1 , where ˆ ξ is our MLE of the BT coefficients and ˆ V ξ is the sandwich variance of the logistic regression. Active sampling rule. Our sampling rule was to choose the model pair a ∈A proportionally to the reduction in confidence interval size by sampling that pair: P t ( a ) ∝ s ˆ Σ t,a,a |{ t : A t = a }| − s ˆ Σ t,a,a |{ t : A t = a }| + 1 . (9) 5.1. Detecting Anomalous Users On a different note, we take a first step towards identify- ing anomalous IP addresses in our dataset. In a dataset of U unique IPs, we let IP = { 1 , . . . , U } be the set of all IP addresses. Consider a “test” user, outside this database, who gives ratings H ′ 1 , . . . , H ′ n when presented actions A ′ 1 , . . . , A ′ n . The idea of our procedure is to com- pare the distribution of ratings for the new user to the his- torical distribution of ratings for a given action. We let H a = { H t : A t = a } and every time a user submits a vote, we calculate the following number: p i = 1 |H A ′ i | + 1    1 + X h ∈H A ′ i 1 { h ≥ H ′ i }    . (10) Under the null hypothesis that H A ′ i is exchangeable with H ′ i , p i is a valid p-value (see Appendix C for a proof). Fur- thermore, the dependence of these p-values asymptotically is negligible. With this p-value in hand, we can test against this null hypothesis sequentially by using Fisher’s combination test ( Fisher , 1928 ) along with a variant of the Bonferroni correction. In particular, for each user, after their j th vote, we compute M j = − 2 j P i =1 log( p i ) . At 5 randomly cho- sen values of j between 1 and 100, we identify a user as 5 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference anomalous if M j ≥ χ 2 2 j, 1 − α/ 5 . (The times are randomly chosen, as to avoid anomalous users strategizing to hack this p-value.) Despite the heuristic application of this procedure, it seems to work well in our small-scale tests reported in Table 5 . 6. Data Analysis To examine whether Arena’s crowdsourced data reflects real-world use cases, we conduct topic modeling on the user prompts. We show how effective are these prompts in distinguishing models. Lastly, we validate the vote quality by relabeling data with experts. 6.1. Topic Modeling on User Prompts To study the prompt diversity, we build a topic modeling pipeline with BERTopic 3 ( Grootendorst , 2022 ). We start with transforming user prompts into representation vectors using OpenAI’s text embedding model (text-embedding-3- small). To mitigate the curse of dimensionality for data clustering, we employ UMAP (Uniform Manifold Approx- imation and Projection) ( McInnes et al. , 2020 ) to reduce the embedding dimension from 1,536 to 5. We then use the hierarchical density-based clustering algorithm, HDB- SCAN, to identify topic clusters with minimum cluster size 32. Finally, to obtain topic labels, we sample 10 prompts from each topic cluster and feed into GPT-4-Turbo for topic summarization. The pipeline identifies 600 clusters covering a wide range of topics including poetry writing, coding, math, and medical queries. We present the top-16 topic clusters in Figure 3 . We observe that the largest cluster only accounts for 1% of the entire set and the rest quickly drop to <0.5%, and the similarity between clusters is small, showing a long-tail and diverse distribution. Due to space limit, we present the similarity matrix and cluster hierarchy of top-64 clusters in Figure 11 and 12 in Appendix. 6.2. Can Arena Prompts Distinguish Models? Next, we study how effective are these topic clusters in distinguishing models strengths. Constructing challenging prompts has become increasingly difficult due to LLMs’ fast growing capabilities. For example, open models such as Llama-2-70b-chat can likely answer inquiries about movie or travel recommendation as good as GPT-4, but not in other domains such as reasoning or coding. To demon- strate, we sample 30 prompts from seven topic clusters and compare the performance of Llama-2-70b-chat and GPT-4. To control variables, we factor out user votes and consider LLM-as-judge ( Zheng et al. , 2023b ) to evaluate model re- 3 https://github.com/MaartenGr/BERTopic Medical Queries and Information ( 0.4% ) Role-Playing Games ( 0.4% ) Movie Reviews and Discussions ( 0.5% ) SQL Database Table Queries ( 0.5% ) Web Development Essentials ( 0.7% ) Animal Behavior and Pet Care Queries ( 0.4% ) Cooking and Recipes ( 0.7% ) Email and Letter Writing Assistance ( 0.8% ) Operations & Fleet Management ( 0.8% ) Sports and Athletics Queries ( 0.8% ) Advanced Mathematical Concepts ( 0.4% ) Philosophical Texts & Concepts ( 0.4% ) AI Impact and Applications ( 0.9% ) Original Joke Requests ( 0.5% ) Poetry Writing & Styles ( 0.8% ) Word Play and Phonetics ( 1.0% ) 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Similarity Figure 3. Similarity matrix of top-16 topic clusters. The number followed by the topic label represents the cluster size in percentage. Note that similarity is computed by cluster’s centroid embeddings, hence diagonals are always one. Table 2. GPT-4-0613’s win-rate against Llama-2-70b-chat on 30 sample prompts from various topic clusters. We use GPT-4-turbo as judge to evaluate model responses in pairwise comparison. Topic Cluster Win-rate Size Python Game Programming Challenge 96.7% 0.2% C/C++ Process Multi-Threading 86.7% 0.3% SQL Query Database Assistance 73.3% 0.2% Poetry Writing Prompts 66.7% 1.1% Python Coding Basics 65.0% 0.2% Linguistic Analysis & Wordplay 58.3% 0.7% Travel Itinerary Planning 58.3% 0.4% Movie Recommendations & Ratings 53.3% 0.2% sponse. Results are shown in Table 2 , where we see GPT-4 has significantly higher win-rate (up to 97%) in clusters that require coding and reasoning skills. On the other hand, for clusters with less problem-solving tasks, GPT-4 win-rate drops to below 60%. We show examples in Appendix D.1 . This result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models. Building Challenging Benchmark. To further demonstrate the prompt quality, we show it is possible to construct a chal- lenging benchmark with crowd-sourced user prompts. To ensure both topic coverage and quality, we first run the topic modeling pipeline and follow a similar procedure in Zheng et al. ( 2023a ) to select challenging questions sampled from each topic cluster. Examples prompts and evaluation proce- dures can be found in the Appendix D.2 and Appendix D.3 , respectively. We observe the selected prompts are highly effective in differentiating models. In Figure 4 , we compare Arena bench against a widely used LLM benchmark, MT- Bench ( Zheng et al. , 2023b ). We can see that Arena Bench effectively reveals a significant gap in performance between proprietary and the strongest open models. 6.3. Validating Vote Quality To assess the quality of crowdsourced votes, we randomly selected 160 battles between GPT-4-Turbo and Llama-2- 13B, as well as GPT-4-Turbo and GPT-3.5-Turbo-0613. We 6 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference 0 2 4 6 8 Llama-2-7B-Chat Llama-2-70B-Chat Vicuna-33b-v1.3 OpenChat-3.5 Starling-LM-7B-alpha Mixtral-8x7B-Instruct Claude-2.1 GPT-3.5-Turbo-0613 GPT-4-0613 GPT-4-0314 GPT-4-Turbo Arena Bench MT Bench Score Model Figure 4. Model’s performance between Arena Bench and MT- Bench, showing an increased gap between open and proprietary models. Both uses GPT-4 as judge. then asked experts 4 to label their preference per comparison. The experts were given the prompts and answers blindly, and asked to carefully fact-check model’s answer with ex- ternal resources like search engine. Manually labeling each data point took on average 3-5 minutes. For reference, we also use GPT-4 as a judge for pairwise comparisons. The agreement rate between crowd-users, experts, and GPT-4- judge are presented in Table 3 . The corresponsing win-rate are shown in Table 4 . To summarize, we observe high agreement rates (72% to 83%) between Arena crowd-user and experts in both setup. Note that agreement rates between two experts are around similar levels (79.4% and 89.8%). As for the 10%-20% disagreement between experts, it is mostly due to some user prompts don’t have a ground truth answer. Depending on the preference of the evaluator, sometimes both answers can be argued as being better than the other one, such as the examples in Appendix D.4 . The gap between crowd- vs-expert agreement rate and expert-vs-expert agreement rate (5%-10%) is mostly attributed to crowd user making mistakes or overlooking factual errors in model’s response. Overall, the agreement rates presented in Table 3 validate the decent quality of crowd-sourced votes in Chatbot Arena. 7. Experiments 7.1. Ranking system Computing the rank on real data. In this section, we report results from our experiments on approximate ranking. For this experiment, we ran a replay of T = 213 , 576 his- torical votes from our online platform and calculate the BT coefficients using our earlier-described estimation algorithm 4 The laborers are graduate students at UC Berkeley. Table 3. Pairwise agreement rate between crowd-user, gpt-4 judge, and experts on pairwise battles. The top part of the table is between GPT-4-Turbo and Llama-2-13b-chat. The bottom is between GPT- 4-Tur bo and GPT-3.5-Turbo-0613. Llama-2-13b Expert 1 Expert 2 GPT-4 Crowd 72.8% 77.8% 75.6% Expert 1 - 89.8% 81.0% Expert 2 - - 78.5% GPT-3.5-Turbo Expert 1 Expert 2 GPT-4 Crowd 73.8% 83.1% 75.6% Expert 1 - 79.4% 76.3% Expert 2 - - 79.3% Table 4. GPT-4-Turbo’s win-rate across crowd-user, gpt-4 judge, and experts on pairwise battles against Llama-2-13b and GPT-3.5- Turbo-0613. Baseline Arena User Expert 1 Expert 2 GPT-4 Llama-2-13b 81.2% 89.4% 86.9% 78.8% GPT-3.5-Turbo 76.3% 82.5% 89.4% 79.4% with confidence intervals; see Figure 5 for these intervals (with and without multiplicity correction; the formal notion of approximate ranking technically requires multiplicity correction, but it makes the intervals looser). Evaluating the coverage of the intervals. A natural follow- up question is whether or not the intervals are doing their job correctly: whether they cover the true BT coefficients with probability at least (and almost exactly) 1 − α . Of course, this cannot be evaluated on real data, so we run a simulation. A vector of BT coefficients is drawn, with each coordinate sampled i.i.d. from a distribution beta (1 /γ, 1 /γ ) ; we take γ = 2 in Figure 6 (and we vary γ in Appendix A ). Given these coefficients, a dataset is synthesized, and the coverage and average width are computed for each of 20 trials. The results can be seen in Figure 6 for the uncorrected intervals The coverage of the intervals behaves as expected, centering around 1 − α , regardless of the number of models. Mean- while, the more models are included, the larger the intervals become. Evaluating the active sampling rule. Next, we discuss the evaluation of our active sampling rule as Equation ( 9 ) for win matrix estimation. We evaluate this sampling rule by taking the best fit BT coefficients to our 213,576 point sized holdout set, and then sampling from that distribution using our active sampling algorithm. The results are displayed in Figure 7 . It is hard to tell by looking at plots, but the improvement is substantial: To estimate θ ∗ to a precision of 0.2, random needs 6,800 samples and adaptive needs 4,400 samples; meanwhile to estimate the score to a precision of 0.3, random needs 17,200 samples and adaptive needs 16,400 samples. Thus, the random baseline requires 54% and 5% more data to achieve the same level of precision, 7 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference 0.0 0.5 1.0 1.5 2.0 2.5 zephyr-7b-beta (#19-37) wizardlm-13b (#19-37) llama2-70b-steerlm-chat (#17-37) solar-10.7b-instruct-v1.0 (#18-36) dolphin-2.2.1-mistral-7b (#17-37) pplx-70b-online (#17-31) gpt-3.5-turbo-1106 (#15-29) openchat-3.5 (#14-29) llama-2-70b-chat (#14-28) openhermes-2.5-mistral-7b (#12-28) vicuna-33b (#7-25) starling-lm-7b-alpha (#7-26) gpt-3.5-turbo-0314 (#7-22) wizardlm-70b (#7-22) tulu-2-dpo-70b (#6-21) yi-34b-chat (#6-19) claude-instant-1 (#6-19) gemini-pro (#6-18) gpt-3.5-turbo-0613 (#6-18) claude-2.1 (#6-18) mixtral-8x7b-instruct-v0.1 (#4-18) gemini-pro-dev-api (#3-18) claude-2.0 (#3-14) claude-1 (#3-8) mistral-medium (#3-8) gpt-4-0613 (#3-7) gpt-4-0314 (#2) gpt-4-turbo (#1) corrected uncorrected Figure 5. Intervals for the BT coefficients with and without mul- tiplicity correction. The multiplicity correction, in this case a chi-square CLT interval, is technically required for the purpose of calculating the ranking, because it ensures all scores are simulta- neously contained in their intervals (and the ranking is a function of all the scores). However, it induces extra conservatism, so we report both intervals. 0 20000 40000 60000 80000 100000 n 0.8 0.9 Coverage Coverage 0 20000 40000 60000 80000 100000 n 0.0 0.5 1.0 1.5 Average Interval Width Average Interval Width M 4 7 10 15 20 Figure 6. Intervals for the BT coefficients as a function of the number of samples and the number of models M . 0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24 average width ( ) 0 10000 20000 30000 n random pairwise adaptive 0.25 0.30 0.35 0.40 0.45 0.50 average width ( s ) 0 10000 20000 30000 n Figure 7. Interval widths on the win matrix (upper figure) and on the BT coefficients (lower figure) as a function of the number of samples, for random sampling and also adaptive sampling. Im- provements from adaptive sampling can be seen in both cases, although they are more subtle on the scale of the score. Table 5. Confusion matrix of different α . “Pred.” means predicted. P ositive means anomalous and negative means normal. α = 0 . 1 Pred. Positive Pred. Negative Actual Positive 13/14 12/36 Actual Negative 1/14 24/36 α = 0 . 3 Pred. Positive Pred. Negative Actual Positive 21/29 4/21 Actual Negative 8/29 17/21 respectively. One can see from the plots in Figure 7 that these results are not cherry-picked: the sample-efficiency of our method is better at all values on the horizontal axis. 7.2. Anomalous Users Detection We evaluate the outlier detection method in Section 5.1 . We construct the evaluation set by manually identifying 25 anomalous users whose inputs are highly repetitive or meaningless (e.g., asking “hi” for 100 times or inputting garbled texts). We randomly sample 25 normal users with at least 50 votes, and inspect their input prompts to ensure no abnormal behaviors. As mentioned in Section 5.1 , per user we compute five M j and identify the user as anomalous if M j ≥ χ 2 2 j, 1 − α/ 5 . We present results of two different α (i.e., the significance leval) in Table 5 . We find the detec- tion method effective (e.g., reaching 90% true positive and 60-70% true negative rate). We inspect the false negative errors and find those are from users do not always behave abnormally, making them harder to detect. 8. Discussion Limitations. Although our user base is extensive, we an- ticipate that it will primarily consist of LLM hobbyists and 8 \nChatbot Arena: An Open Platform for Evaluating LLMs by Human Preference researchers who are eager to experiment with and evaluate the latest LLMs. This inclination may result in a biased distribution of users. Additionally, despite the wide array of topics encompassed by the prompts discussed in previous sections, the data predominantly comes from our online chat interface. This source might not accurately reflect the real-world usage of LLMs in production environments or specialized domains, potentially leading to a skewed prompt distribution. Moreover, our study concentrates on assessing the helpfulness of LLMs but overlooks their safety aspects. We recognize the possibility and necessity of a parallel mechanism to evaluate the safety of these models. Future Directions. In our future work, we plan to develop comprehensive topic leaderboards and establish a dedicated section for multimodal and agent-based LLMs in more dy- namic, gamified settings, catering to more complex tasks. We also believe our approach to detecting harmful users could be improved and made more formally rigorous by using the theory of nonnegative supermartingales and E- values ( Howard et al. , 2020 ; Waudby-Smith & Ramdas , 2020 ; Vovk & Wang , 2021 ; Ramdas et al. , 2023 ); this would deal with the dependence, but the variants we tried did not perform well in terms of power. 9. Conclusion In this paper, we present Chatbot Arena, an open platform for evaluating LLMs through crowdsourced, pairwise hu- man preferences. We conduct an in-depth analysis of the crowdsourced user prompts and preference votes to validate the diversity and quality. We develop an efficient model sampling and ranking algorithm. Our dataset including 100K pairwise preference votes will be released for future research. Acknowledgments This project is partly supported by sponsorship from Kaggle, MBZUAI, a16z, Together AI, Anyscale, and HuggingFace. This project is also partly supported by Accenture, AMD, Google, IBM, Intel, Microsoft, Samsung SDS, SAP, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship. The authors would like to thank Siyuan Zhuang for insightful discussion and Tijana Zrni´c for helpful feed- back on the manuscript. Impact Statement This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."
}