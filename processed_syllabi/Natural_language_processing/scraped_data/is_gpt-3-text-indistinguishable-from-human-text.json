{
    "document_type": "research_paper",
    "title": "Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text",
    "author": "Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, Yejin Cho",
    "source": "raw_syllabi\\master_courses\\Natural_language_processing\\pdf_material\\is_gpt-3-text-indistinguishable-from-human-text.pdf",
    "date_published": "2022-03-09",
    "keywords": "",
    "flag": "",
    "text": "Is GPT-3 Text Indistinguishable from Human Text? S CARECROW : A Framework for Scrutinizing Machine Text Yao Dou ∗ † Maxwell Forbes ∗† Rik Koncel-Kedziorski † Noah A. Smith †‡ Yejin Choi †‡ † Paul G. Allen School of Computer Science & Engineering, University of Washington ‡ Allen Institute for AI {douy,mbforbes,nasmith,yejin}@cs.washington.edu kedzior@uw.edu Abstract Modern neural language models can produce remarkably ﬂuent and grammatical text. So much, in fact, that recent work by Clark et al. ( 2021 ) has reported that conventional crowd- sourcing can no longer reliably distinguish be- tween machine-authored (GPT-3) and human- authored writing. As errors in machine gener- ations become ever subtler and harder to spot, it poses a new challenge to the research com- munity for robust machine text evaluation. We propose a new framework called S CARE - CROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identiﬁed by laypeo- ple, the ten error categories of S CARECROW — such as r e du n dancy , co m mo n sense e r rors , and inc o he r ence —are identiﬁed through sev- eral rounds of crowd annotation experiments without a predeﬁned ontology. We then use S CARECROW to collect over 41k error spans in human-written and machine- generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time conﬁgura- tions. Our approach successfully quantiﬁes measurable gaps between human authored text and generations from models of several sizes, including fourteen conﬁgurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make re- markable differences on the perceived quality of machine text. We release our training mate- rial, annotation toolkit and dataset at https: //yao-dou.github.io/scarecrow/ . 1 Introduction Clark et al. ( 2021 ) demonstrated the challenges of human evaluation in the era of GPT-3 ( Brown ∗ Equal contribution Off-Prompt The long-rumored Apple car might ﬁnally become a reality. Prompt (human-authored) According to the Financial Times, Apple's been talking to \"a small group of contract manufacturers to explore making an electric vehicle,\" which would ostensibly be an autonomous car. All this does sound like the loose ends of Apple's CarPlay rollout: hiring 1,200 engineers for the iOS team, building the CarPlay-speciﬁc testing track, developing a Lincoln Navigator, then poaching Burberry’s head of product design to lead the integration of software and hardware. WWDC 2015 We know what you're thinking: Another Monday? Continuation written by GPT-3 DaVinci The most likely meaning of “track” in this context is a driving area, which doesn’t make sense for CarPlay. Apple would develop their own car, not make a Lincoln Navigator, which already exists. Burberry’s head of product design wouldn't have the technical expertise needed for this particular job. While Apple CarPlay is also about cars, this isn’t actually relevant. This is a change of subject and doesn’t follow the narrative. Grammar / Usage It would be weird to hire 1,200 engineers during a “rollout” (a product launch). Neither the speculation, nor the rollout described next, really make sense to call “loose ends.” 1 1 2 2 4 5 3 6 7 3 4 5 6 7 Commonsense Figure 1: After a model (here, GPT-3 DaVinci) has read the prompt (top sentence) and generated a continuation (next paragraph), the S CARECROW annotation frame- work provides a systematic way for humans to mark issues throughout the text and explain what is wrong. Our own annotations are pictured here. et al. , 2020 ), as crowd workers are no longer able to reliably distinguish GPT-3’s generations from human-written text. Or are they? In this paper, we propose a new framework for systematically scrutinizing machine text so that even crowd workers, despite the known challenges reported by recent literature, can suc- cessfully critique seemingly ﬂuent generations. We not only quantify a measurable gap between ma- 1 arXiv:2107.01294v3  [cs.CL]  7 Mar 2022 \nERROR TYPE DEFINITION EXAMPLE Language Errors Gra m mar and U s age Missing, extra, incorrect, or out of order words . . . explaining how cats feel emot i cons . . . Off - Prompt Generation is unrelated to or contradicts prompt PROMPT : Dogs are the new kids. GENERA - TION : Visiting the de n tist can be scary R e du n dant Lexical, semantic, or execessive topical repe- tition Merchants worry about poor se r vice or se r vice that is bad . . . Self - Co n tr a di c tion Generation contradicts itself Amtrak plans to la y off man y e m plo y ees, though it has no plans cut e m ployee hours. Inc o he r ent Confusing, but not any error type above Mary gave her kids cheese toast but drew a map of it on her toast. Factual Errors Bad Math Math or conversion mistakes . . . it costs over £1,000 ($18,868) . . . Enc y clop e dic Facts that annotator knows are wrong Japanese Prime Mini s ter Justin Trudeau said Monday . . . Co m mo n sense Violates basic understanding of the world The dress was made at the spa. Reader Issues Needs Google Search needed to verify claim Jose Celana, an artist based in Pe n sacola, FL , . . . Techn i cal Ja r gon Text requires expertise to understand . . . an 800-megawatt phot o voltaic plant was built . . . Table 1: Error types in the S CARECROW framework, grouped into three categories. The categories are explained further in § 4.4 , and detailed deﬁnitions and examples for each error type is provided in Appendix A . chine text and human text, but reveal the distribu- tions of speciﬁc categories of issues, and pinpoint their occurrences in text written by several sizes of language models as well as humans. To achieve this, we develop S CARECROW , a methodology for eliciting categorical judgements of errors in machine-generated text from crowd workers. One goal in natural language generation (NLG) is to produce ﬂuent outputs which can be read by laypeople. As such, we propose that im- portant errors to address are those which are rec- ognized by readers without NLP expertise. Our framework allows crowd workers to annotate prob- lems in model outputs at the span level. A single such annotation is shown in Figure 1 . To make this possible, we establish a categoriza- tion of shortcomings commonly found in machine generated text (Table 1 ). This error schema covers a broad scope of problems as identiﬁed by experts, but has been honed according to what is salient to non-expert readers through several pilot rounds of crowd annotation without a ﬁxed label set. The result is a framework that is usable by everyday people with minimal training, but covers the error phenomena found in real machine-generated text. Labeling spans of text using speciﬁc error types cre- ates a picture of contemporary model generations with an unprecedented level of detail. In contrast to judging text holistically ( Celikyilmaz et al. , 2021 ), insights from this method are speciﬁc and practical, as it measures exactly how and where problems arise. We conduct a large-scale analysis of human- written and machine-generated text using S CARE - CROW , collecting 13k annotations of 1.3k para- graphs, amassing 41k spans labeled with error type, severity, and an explanation. Through this, we characterize in which ways GPT-3’s generations are better than those of previous models, and which aspects do not improve with increased data and pa- rameters. We also provide a rigorous error analysis of text generated by several other contemporary language models, examining the impact of model size, training data, and decoding strategy. We provide our detailed annotator training sys- tem and task interface so that future researchers may employ and reﬁne them for error analyses of machine-generated text. We hope this will con- tribute to the standardization of NLG human evalu- ation ( Howcroft et al. , 2020 ). 2 Key Findings We perform a large-scale annotation of errors in English news text generated by ﬁve sources (four 2 \nGPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.002 0.004 0.006 0.008 Span coverage Encyclopedic GPT-2 S GPT-2 XL Grover GPT-3 Human 0.00 0.05 0.10 0.15 0.20 Span coverage Incoherent GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.002 0.004 0.006 Span coverage Bad Math GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Self-Contradiction GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.025 0.050 0.075 0.100 0.125 0.150 Span coverage Needs Google GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Commonsense GPT-2 S GPT-2 XL Grover GPT-3 Human 0.0 0.1 0.2 0.3 Span coverage Off-Prompt GPT-2 S GPT-2 XL Grover GPT-3 Human 0.00 0.01 0.02 0.03 Span coverage Grammar / Usage GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 0.020 Span coverage Redundant GPT-2 S GPT-2 XL Grover GPT-3 Human 0.000 0.005 0.010 0.015 Span coverage Technical Jargon Figure 2: Average portion of tokens annotated with each error type ( y -axis) across models ( x -axis), with 95% conﬁdence intervals. We group the trends into several broad categories. Decreasing: ﬁne-tuning and increasing model size improves performance. Model plateau: increasing model size to GPT-3 does not correlate with further improvements. Rising and falling: errors become more prevalent with some models, then improve. Humans highest: these spans are labeled most on human-authored text; both are reader issues (distinct from errors ; see Table 1 ). Details: all models, including GPT-3, use the same “apples-to-apples” decoding hyperparam- eters: top- p =0.96, temperature=1, and no frequency penalty. models and ground truth articles). We present Fig- ures 2 , 3 , and 4 as summaries of our main results. As a reminder to readers, Grover ( Zellers et al. , 2019 ) is the same model size and architecture as GPT-2 XL ( Radford et al. , 2019 ), but trained in- domain (on news text). As such, our results cover three increasing model sizes (GPT-2 Small, XL, and GPT-3 ( Brown et al. , 2020 )), one change in domain (Grover), and ground-truth text (Human). For GPT-3, we also study a variety of decoding conﬁgurations (Figure 4 ). The main quantity we measure (on y -axes) is span coverage , which is the average portion of tokens that ends up covered by annotations of a particular error type. Since it is possible that multi- ple spans nest or overlap, there is no upper bound for this quantity. (See Figure 12 for a comparison of span coverage with other measurement alterna- tives.) Figure 2 measures span coverage for each type of span separately, Figure 3 stacks them, and Figure 4 removes non-error spans (reader issues) before adding them (as in Figure 3 , but without showing the individual types). The following are our key ﬁndings. 1. Scaling pays off to improve Enc y clop e dic , Co m mo n sense , and Inc o he r ent errors (Fig. 2 ). These error categories decrease with in-domain training (Grover) and larger model size (GPT-3). Human text still shows the fewest of these kinds of errors. 2. Scaling beneﬁts plateau for Off - Prompt , Bad Math , and Gra m mar and U s age errors (Fig. 2 ). These three error categories see a model plateau in error reduction when scaling to GPT-3. Of these error types, humans still commit fewer Off - Prompt (more: § E.1 ) and Gra m mar and U s age errors, but Bad Math ap- pears saturated for our domain. 3. Self - Co n tr a di c tion and R e du n dant errors exhibit more complex scaling behavior (Fig. 2 ). We roughly categorize these trends as rising and falling : increasing for medium or large-scale models, but dropping for human-authored text. Text generated by GPT-2 Small is so often incoherent that there is little possibility for Self- Co n tr a di c tion (more: § E.2 ), and the increase in R e du n dant errors varies based on how errors are counted (more: § E.3 ). 4. Human-authored text produces the most reader issues (Figs. 2 and 3 ). The Needs Google and Techn i cal Ja r gon span categories both have a humans highest trend, and both fall under reader issues : problems that are not necessar- ily errors , but that still prevent full comprehension 3 \nGPT-2 S GPT-2 XL Grover-Mega GPT-3 Human 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Span coverage Average Span Coverage Across Models Bad_Math Commonsense Encyclopedic Grammar_Usage Incoherent Needs_Google Off-prompt Redundant Self-contradiction Technical_Jargon Figure 3: Average portion of tokens covered by span annotations, broken down by error type. All models, including GPT-3, use the same apples-to-apples decod- ing hyperparameters: top- p =0.96, temperature=1, and no frequency penalty. We scale each span by its to- ken length, normalize by generation token lengths, and remove severity-1 Gra m mar and U s age errors (see § C ). GPT-2 S argmax t=0.4, p=0.96 GPT-2 XL t=1.0, p=0.4 t=0.7, p=0.96 Grover-Mega t=1.0, p=0.9 t=1.0, p=0.96 t=1.0, p=0.9 t=1.0, p=0.7 t=1.0, p=0.7 t=1.0, p=0.96 t=1.0, p=0.4 t=0.4, p=0.96 t=0.7, p=0.96 argmax Human Model (both green hues: GPT-3 w/ decoding config in legend) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Span coverage \"Apples-to-apples\" decoding setup (used when comparing models elsewhere) Errors Across All Decoding Configurations GPT-3, with Frequency Penalty: 0 (none) 1 (full) Figure 4: Taking the average span coverage (Figure 3 ) and removing reader issues ( Techn i cal Ja r gon and Needs Google ), we plot values and 95% conﬁdence intervals for all models, including all decoding hyper- parameters we tested for GPT-3. We ﬁnd a surprisingly large change in annotated errors depending on the de- coding setting used. or factual veriﬁcation of the text (more: § E.4 ). Furthermore, human-authored text is not free from error annotations (Figure 3 ). This can serve either as a control for baseline error rates (more: § E.6 ), or as a mechanism for critiquing human writing. 5. Decoding hyperparameters have a huge im- pact (Figure 4 ). For the previous ﬁndings, we ﬁx the sampling conﬁguration for all models to an apples-to-apples setup for fair comparison: top- p = 0.96, (softmax) temperature = 1, and no frequency penalty (i.e., word repetition penalty; deﬁned pre- cisely in § 5.2 , Equation 1 ). To study the effects of these decoding settings, we annotate text generated by GPT-3 using a variety of values for top- p and temperature, both with and without a frequency penalty. To our surprise, the decoding hyperparameters considerably affected error rates (more: § E.5 ). As seen in Figure 4 , the worst sampling procedure for GPT-3 (argmax sampling with no frequency penalty) performed even worse than GPT-2 XL. But the best sampling procedure (surprisingly, also argmax sampling, but with a frequency penalty) produced text with as few apparent S CARECROW error spans as those authored by humans (more: § E.6 ). All of these ﬁndings are discussed in more detail in Appendix E . 3 Evaluation of Natural Language Generation We make our study in the area of open-ended natu- ral language generation, a loose term for generat- ing longer texts with an increased level of creative freedom. The common factor in all open-ended generation tasks such as story, blog, and dialog generation is the wide and diverse nature of target outputs. Lexically and even semantically dissimi- lar responses to the same prompt could be equally valid. For example, a model prompted with the blog title “Recipes for success this Holiday season” could describe how to roast a turkey or strategies for dealing with the stresses of holiday travel. This allowable variation poses a particular dif- ﬁculty for the evaluation of generation systems. Traditionally, text generation quality for tasks like machine translation or graph-to-text generation has been measured by word overlap with human- authored references ( Papineni et al. , 2002 ; Lin , 2004 ). Though measures like BLEU allow for mul- tiple references, they break down when the space of allowable outputs is large, as in open-ended gener- ation. Recently introduced metrics seek to remedy this problem ( Hashimoto et al. , 2019 ; Pillutla et al. , 2021 ), but the gold standard for evaluating gener- ated text is still human judgment. However, current approaches to eliciting human 4 \njudgement of generated text often do not provide detailed insight into where models are making progress, where they are failing, and the scope of these failures. A/B-style testing allows for di- rectly comparing one system against others ( Clark and Smith , 2021 ), but can only express relative im- provements. Simple Likert scale judgements can assess text quality, but do not explain why a gener- ated text receives a given rating, or which segment of the text is problematic. Insights into model fail- ures often come instead from a small scale expert analysis of outputs. However, these “error analy- ses,” once a staple of NLP research, have become less common in recent years, perhaps due to their small size and high variance. A hypothesis of the current work is that a well de- signed error analysis annotation framework could be used by crowdworkers to annotate large amounts of text, thereby providing detailed information about model progress and failures as well as ac- tionable directions for future research. Such a framework would be easy to learn, reusable, and independent of particular models or experimental conditions. In what follows, we outline the details of such a method. 4 S CARECROW Annotation Methodology This section describes the high-level annotation methodology for S CARECROW . 4.1 Prompt and Generation Our annotations consider two segments of text: a one-sentence prompt, and a one-paragraph gener- ation. The prompt is human-written. It provides both starting tokens for model generation, as well as context for humans to evaluate whether a model is able to stay on-prompt—both topically and fac- tually. Annotators know that the prompt is written by a human. The generation is either text sampled from a language model, or the human-authored continua- tion to the prompt. Annotators, who do not know whether the generation came from a model or hu- mans, assess this text. A paragraph length (80–145 tokens) is chosen to balance expressiveness with scope. For expressiveness, models must be given a sufﬁcient number of tokens to express their ca- pabilities lexically, syntactically, and semantically. One paragraph allows for signiﬁcantly more vari- ation than a single sentence. On the other hand, assessing multiple paragraphs is challenging, both Inconsistent about how many moons Mars has. 1 2 3 Self- Contradiction Inconsistent about how many moons Mars has. Needs Google Bad Math Reader Issues Factual Language Figure 5: S CARECROW interface for annotating a sin- gle span: (1) highlighting a span (and later, an an- tecedent); (2) completing the annotation, with the error type, explanation, and severity; (3) the error annotation is saved—interactive controls allow detailed viewing and editing of spans (not shown). as a crowdsourcing task itself, and because it broad- ens the kinds of errors to include larger narrative scope. We leave extensions of S CARECROW to longer narrative lengths for future work. 4.2 Span Labeling Annotators select spans that contain problems in the generation. The spans are automatically snapped to word boundaries. We choose spans to balance speciﬁcity (i.e., vs. simply comment- ing on the text as a whole) with ease of use (vs. imposing a more structured annotation schema). 4.3 Span Selection We instruct workers to select the smallest span— minimally a single word—that contains an issue. Sometimes this involves an entire phrase, sentence, 5 \nor multiple sentences. We aim for speciﬁcity be- cause during aggregation, it is possible to “back off” annotations to larger spans, but not the inverse. Once they select a span, workers (1) label the er- ror type, (2) choose a severity level, and (3) explain their reasoning behind the error. Workers use the annotation interface shown in Figure 5 to mark a span with these three steps. We describe each step in greater detail in the next three sections. 4.4 Error Types Each selected span is labeled with exactly one error type. Multiple errors may be marked with partially or fully overlapping spans in the case that one text segment contains multiple problems. We chose ten error types to balance three crite- ria: linguistic analysis, observed errors in gener- ated text, and capabilities of everyday people with one to two hours of training. 1 We developed the schema by starting with the ﬁrst two criteria (lin- guistic analysis and observed errors), and reﬁning it over several pilot annotation studies, with 30 crowd workers performing 750 total annotations of 60 paragraphs before beginning data collection. We broadly group the errors into three categories: language errors, factual errors, and reade r issues . Language errors are issues with internal and ex- ternal structure of text: which ideas are expressed, and whether they are expressed coherently and con- sistently. Factual errors denote that the information presented is known to be incorrect. Reader issues, on the other hand, are cases where the text is too technical or obscure to assess its factuality. Hence, reader issues are not errors, per se, but regions where a reader would need assistance outside of the text itself for comprehension. We present the ten error types in Table 1 (several pages back). Appendix A provides more details, examples, and explanations for all error types. 4.5 Severity Errors naturally vary in how jarring they are to a reader. We deﬁne three error severity levels, and ask annotators to pick one for each error. The severity levels are as follows. (1) Almost no impact on quality; just a small problem. (2) Understandable, but difﬁcult; what’s written is still comprehensible, but there’s clearly an issue. (3) Very difﬁcult to understand; the error almost com- pletely ruins the text. 1 The complete training material is available for download. We provide examples of each severity in Ap- pendix B.1 . In this paper, we omit an analysis of the severity labels (except for an illustration in Fig- ure 12 ), but include it in our data release for future work to explore. 4.6 Explanation Finally, we ask annotators to explain their reason- ing behind each error in natural language. We pro- vide example explanations during training, but do not impose strict guidelines. This paper primarily focuses on quantitative error analysis, but we an- ticipate the error explanations may warrant future investigation. 4.7 Annotation Process We use Amazon Mechanical Turk (AMT) for all data collection. Training We ﬁrst pay each worker $40 to take an extensive qualiﬁcation task, which both trains them in the span categorization scheme and quizzes their understanding. We pass workers if they score ≥ 90 points out of 100 points (details in Appendix B.2 ). Annotation Workers annotate each paragraph us- ing a custom annotation interface (shown partially in Figure 5 ), for which we pay $3.50. We calculated $3.50 per annotation by aiming to pay workers at least $15/hour. After several annotation rounds, we observed considerable variation in time per annota- tion, 2 so this cost should not be necessarily seen as a requirement for S CARECROW annotations. 5 Data Collection We collect 13k human annotations of 1.3k para- graphs using S CARECROW , resulting in over 41k spans. 5.1 Models We consider four model conﬁgurations to test re- cent state-of-the-art transformer-based ( Vaswani et al. , 2017 ) models. GPT-2 Small ( Radford et al. , 2019 ) The 117M parameter variant of GPT-2, which is pretrained on WebText, without additional ﬁne-tuning. GPT-2 XL ( Radford et al. , 2019 ) The 1.5B pa- rameter variant of GPT-2, (WebText, no ﬁne- tuning). 2 Median: 212s, mean: 265s, std. dev.: 199s. 6 \nGrover-Mega ( Zellers et al. , 2019 ) The 1.5B pa- rameter variant of Grover, a model with the same architecture and parameter count of GPT-2, trained on news articles and their metadata. GPT-3 DaVinci ( Brown et al. , 2020 ) The 175B parameter variant of GPT-3, which is trained on a version of the Common Crawl web scrape with additional ﬁltering and deduplicating. In addition, we also use the actual human-written text from the data sources we draw from, which we denote as Human . 5.2 Decoding strategies We consider three main hyperparameters when sam- pling from models: p for top-p or nucleus sampling ( Holtzman et al. , 2020 ), an alternative to top-k ; 3 t for the softmax temperature ; and f.p. for frequency penalty . The frequency penalty scales a token’s likelihood based on how many times it was already generated by applying the following modiﬁcation to the model’s output: ℓ i ( t ) ← ℓ i ( t ) − c <i ( t ) · α f (1) where ℓ i ( t ) is the model’s output for token t at the i -th position, 4 c <i ( t ) is the count of token t ’s sam- pled occurrences prior to the i -th position, and α f is the frequency penalty. We omit studying presence penalty , another hyperparameter offered for GPT-3, simply due to annotation budget constraints. To compare models as consistently as possible, we set identical decoding strategies for our primary data collection. We refer to this as the “apples-to- apples” decoding setup throughout the paper: p = 0 . 96 t = 1 . 0 f.p. = 0 However, we also wish to study the effects of these decoding strategies. We annotate generations from the strongest available model (currently, GPT- 3) varying the following parameters: 3 We omit separate studies of top-k , due to results presented by Holtzman et al. ( 2020 ), and OpenAI’s removal of top-k from the GPT-3 API. 4 While ℓ i ( t ) is deﬁned to be “logits (un-normalized log- probabilities),” because it is un-normalized, we anticipate that it is simply the model’s output before the log( softmax ( · )) is applied. See OpenAI’s description of frequency and pres- ence penalties: https://beta.openai.com/docs/ api-reference/parameter-details p ∈{ 0 . 4 , 0 . 7 , 0 . 9 , 0 . 96 } t ∈{ 0 . 0 (argmax) , 0 . 4 , 0 . 7 , 1 . 0 } f.p. ∈{ 0 (none) , 1 (full) } For budget reasons, we only vary p and t independently—i.e., we set p = 0 . 96 when varying t , and t = 1 . 0 when varying p . 5.3 Prompt Selection We use news articles as the sources of prompts for models to condition on for generation. Speciﬁcally, we use news articles found in the Common Crawl. We select the ﬁrst sentence as the prompt. Our use of news text is constrained by two fac- tors. First GPT-3 is trained on the Common Crawl, from 2016 through 2019. We wish to avoid testing GPT-3 by generating from articles it saw during training, due to the possibility of copying ( Carlini et al. , 2021 ). Second, news articles began heav- ily covering the COVID-19 pandemic beginning around February 2020. Though testing models’ ca- pabilities to generate text about unseen events is a valuable line of study, the distribution shift caused by COVID-19 in news writing about all aspects of life is difﬁcult to overstate. As such, to make the comparison more amenable to models’ training data, we consider news articles from January 2020. We select articles where there is a known topic—such as Food or Sports —from the Common Crawl metadata, to allow for studying any effect of coarse-grained subject. 5.4 Generation We generate between 80 and 145 tokens 5 from each model as a continuation to the ﬁrst sentence of the news article. We stop generating when we heuristically detect the ﬁrst sentence boundary after 80 tokens. If the model does not end a sentence between 80 and 145 tokens, we sample again. For the Human setting, we use the remainder of the article, similarly stopping after the ﬁrst sentence boundary after 80 tokens. 5.5 Annotation Crowdsourcing Workers ﬁrst complete training and qualiﬁcation tasks. We provide more details in 4.7 . From pilot studies, we discovered that each er- ror, depending on its severity and clarity, has only a 5 Counted by Stanza tokenization ( Qi et al. , 2020 ), not byte-pair encoding (BPE) or whitespace-separated tokens. 7 \nlow to moderate chance of being identiﬁed by each worker. However, most worker-identiﬁed errors were truly problems. In other words, annotators labeled issues with high precision and low recall. To account for this, we have 10 workers annotate each paragraph. We examine the agreement and variability of annotations in Appendix C . Dataset statistics We provide detailed dataset statistics in Appendix D . 6 Error Prediction A natural question is: using this data, can machines learn to detect and classify errors in machine gen- erated text? Task We frame this problem as a span classiﬁ- cation task. Given a span from a generated text, the goal is to classify its error type or output “No Error” if there is none. Positive examples for each error class are taken from our data. We sample random spans that were not labeled with any error type as negative examples. To ensure a breadth of span lengths, we sample 3 negative spans for every length of error span in the generated text. We split the generated texts into train, development, and test sets using 1063 texts (28029 error spans), 100 texts (2538 spans) and 100 texts (2677 spans) respectively. Model We use a standard span classiﬁcation model inspired by Wadden et al. ( 2019 ). This model encodes every generated text using a pre- trained language model (RoBERTa-large). Spans are represented with the ﬁnal layer of this encod- ing. Following previous work, we concatenate the start and end tokens with a task-speciﬁc learned length embedding. The resulting vector is passed through a feedforward network which reduces its dimensionally to the number of error categories plus a “No Error” option. The resulting model has 357M trainable parameters. The model is trained to minimize the cross entropy of the correct span cate- gory. We train for 15 epochs using AdamW with a learning rate of 10 − 6 . We validate after each epoch and use the checkpoint with the lowest validation loss (epoch 8). Evaluation To evaluate the error prediction model, we use per-token precision, recall, and F 1 score per error category. We classify every span up to length 30 in a generated text. We take as gold labels the aggregated human error spans collected Error Model Human P R F 1 P R F 1 Bad Math – 0 – 0.72 0.14 0.24 Co m mo n sense 0.77 0.06 0.10 0.17 0.02 0.04 Enc y clop e dic – 0 – 0.22 0.03 0.05 Gra m mar and U s age 0.29 0.23 0.26 0.30 0.04 0.08 Inc o he r ent 0.59 0.34 0.43 0.69 0.15 0.24 Off - Prompt 0.67 0.29 0.41 0.88 0.31 0.46 R e du n dant 0.23 0.82 0.36 0.88 0.35 0.50 Self - Co n tr a di c tion 0.08 0.23 0.12 0.51 0.09 0.16 Techn i cal Ja r gon 0.18 0.74 0.29 0.61 0.12 0.20 Needs Google 0.59 0.96 0.73 0.78 0.20 0.32 Table 2: Model prediction results against combined spans of 10 annotators, compared with humans scored as one-vs-rest (i.e., 1-vs-9). Bold F 1 scores denote the higher average; values marked “–” cannot be computed due to division by zero. Takeaway: Humans have higher precision in every error type except Co m mon- sense , but relatively sparse annotations lead to lower computed recall. This allows the model to achieve higher F 1 scores for half of the span categories. in our data. In other words, models predict the com- bined spans of all 10 annotators. For comparison, we also report as Human the average metrics of one annotator versus the others (i.e., 1-vs-9). 6 Results Table 2 shows the error prediction capa- bility of this model in terms of precision and recall. As we noted earlier, a single human annotator can be thought of as a high precision, low recall judge. These results bear out this claim. For all but one cat- egory, humans have higher precision annotations. However, the models trained on the aggregation of human labels can achieve considerably higher recall. For half of the error categories, this leads to higher model F 1 scores than the human annotators. We see that the model is successful at identifying information that human’s would have to manually verify ( Needs Google ), achieving nearly perfect recall with precision close to 0.6. The model can also identify Gra m mar and U s age , Inc o he r ent , and R e du n dant errors with higher recall than an individual human annotator, though at the cost of precision (sometimes in the .20s). 7 Related Work Automated evaluation metrics such as BLEU ( Pa- pineni et al. , 2002 ), ROUGE ( Lin , 2004 ), ME- TEOR ( Banerjee and Lavie , 2005 ), and BERTScore 6 The difference in available references (10 for models, 9 for humans) mean this setup makes it easier for models to score higher in precision, and for humans to score higher in recall. Despite this, humans still achieve higher precision, and models still achieve higher recall. 8 \n( Zhang et al. , 2019 ) compute a generation’s score based on a (set of) reference(s). Their use is well- established in tasks like machine translation and summarization, but they are less helpful in open- ended text generation, where there is a vast diver- sity of possible high-quality continuations. Recent studies propose automated metrics for open-ended text generation evaluation such as: Per- ception Score ( Gu et al. , 2021 ), which diffuses eval- uation onto a multidimensional space and assigns a single score; UNION ( Guan and Huang , 2020 ), which learns to distinguish human-written stories from negative samples by generating perturbations of human-written stories; and MAUVE ( Pillutla et al. , 2021 ), which compares the distribution of machine-generated text to that of human language. An alternate recent approach to assessing open- ended text generation was presented in TuringAd- vice ( Zellers et al. , 2021 ), where crowd workers assess machine-generated advice in response to Reddit posts. In their error analysis, Zellers et al. connect problems in generated text to core NLP tasks, such as Self - Co n tr a di c tion errors as in- stances of failed natural language inference ( Monz and de Rijke , 2001 ), or Off - Prompt errors as cases of failed reading comprehension ( Richardson et al. , 2013 ). While past work has attempted to guide text generation using discriminative models trained for such tasks ( Holtzman et al. , 2018 ), it remains an open challenge. Comparative human evaluations of natural lan- guage generations ask annotators to rank system outputs relative to each other. Text is typically eval- uated using a few global criteria, such as ﬂuency and relevance, using discrete (e.g., 5-point) ( Sai et al. , 2020 ) or continuous scales ( Novikova et al. , 2018 ). Recent work even automates this approach, running a human evaluation alongside automatic metrics on leaderboard submissions ( Khashabi et al. , 2021 ). In the RoFT system ( Dugan et al. , 2020 ), annotators attempt to detect the boundary be- tween human- and machine-written text as a proxy for assessing quality. Table 3 summarizes the dif- ferences between these schemes and S CARECROW . See Celikyilmaz et al. ( 2021 ) for a recent survey of text generation evaluation techniques across both human and automatic metrics. While these approaches may be helpful— sometimes ( Card et al. , 2020 )—at ranking systems, they do not give us insight into exactly which parts of a generation fall short, and why . One approach Method GC SET DE RR EE RS SA Likert-Scale ✓ ✓ ✓ RankME ✓ ✓ ✓ RoFT ✓ ✓ ✓ S CARECROW ✓ ✓ ✓ ✓ Table 3: Comparison of different natural language gen- eration human evaluations. Here, GC : General Crite- ria, SET : Speciﬁc Error Type, DE : Direct Evaluation, RR : Relative Ranking, EE : Error Explanation, RS : Rating Scale, SA : Span Annotation. related to or annotation method is pursued by Wood et al. ( 2018 ), who develop a collaborative mobile app where users draw “grafﬁti” commentary on news articles. S CARECROW aims to assess model generations the way we would critique human- written text: by locating, coarsely categorizing, and explaining problems. 8 Conclusion We present S CARECROW , a method for identifying and explaining issues in generated text. Along with the annotation framework, we present an analysis of the S CARECROW method applied to several large neural language models in an open-ended news gen- eration task. We release our data and methodology to the community. Acknowledgments The authors thank members of xlab for their feed- back on this work. This research is supported in part by NSF (IIS-1714566), DARPA MCS pro- gram through NIWC Paciﬁc (N66001-19-2-4031), DARPA SemaFor program, and Allen Institute for AI."
}