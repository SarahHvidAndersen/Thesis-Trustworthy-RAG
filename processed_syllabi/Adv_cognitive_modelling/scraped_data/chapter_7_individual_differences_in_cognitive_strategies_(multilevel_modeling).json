{
    "document_type": "online_article",
    "title": "Chapter 7 Individual Differences in Cognitive Strategies (Multilevel modeling)",
    "author": "Riccardo Fusaroli",
    "source": "https://fusaroli.github.io/AdvancedCognitiveModeling2023/individual-differences-in-cognitive-strategies-multilevel-modeling.html",
    "date_published": "Unknown",
    "flag": "",
    "text": "Our exploration of decision-making models has so far focused on single agents or averaged behavior across many agents. However, cognitive science consistentlyreveals that individuals differ systematically in how they approach tasks and process information. Some people may be more risk-averse, have better memory, learn faster, or employ entirely different strategies than others. This chapter introduces multilevel modeling as a powerful framework for capturing these individual differences while still identifying population-level patterns. Multilevel modeling (also called hierarchical modeling) provides a powerful framework for addressing this challenge. It allows us to simultaneously: Consider our matching pennies game: different players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to capture these variations while still understanding what patterns hold across the population. Consider our matching pennies game: players might vary in their strategic sophistication, memory capacity, or learning rates. Some may show strong biases toward particular choices while others adapt more flexibly to their opponents. Multilevel modeling allows us to quantify these variations while still understanding what patterns hold across the population. After completing this chapter, you will be able to: Understand how multilevel modeling balances individual and group-level information Distinguish between complete pooling, no pooling, and partial pooling approaches to modeling group and individual variation Use different parameterizations to improve model efficiency Evaluate model quality through systematic parameter recovery studies Apply multilevel modeling techniques to cognitive science questions Traditional approaches to handling individual differences often force a choice between two extremes: Treats all participants as identical by averaging or combining their data Estimates a single set of parameters for the entire group Ignores individual differences entirely Example: Fitting a single model to all participants’ data combined Analyzes each participant completely separately Estimates separate parameters for each individual Fails to leverage information shared across participants and can lead to unstable estimates Example: Fitting separate models to each participant’s data Multilevel modeling offers a middle ground through partial pooling. Individual estimates are informed by both individual-level data and the overall population distribution. Individual parameters are treated as coming from a group-level distribution Estimates are informed by both individual data and the population distribution Creates a balance between individual and group information Example: Hierarchical Bayesian model with parameters at both individual and group levels This partial pooling approach is particularly valuable when: Data per individual is limited (e.g., few trials per participant) Individual differences are meaningful but not completely independent We want to make predictions about new individuals from the same population Before diving into code, let’s understand the structure of our multilevel models using graphical model notation. These diagrams help visualize how parameters relate to each other and to the observed data. In this model, each agent has an individual bias parameter (θ) that determines their probability of choosing “right” (1) versus “left” (0).\nWe are now conceptualizing our agents as being part of (sampled from) a more general population. This general population is characterized by a population level average parameter value (e.g. a general bias of 0.8 as we all like right hands more) and a certain variation in the population (e.g. a standard deviation of 0.1, as we are all a bit different from each other). Each biased agent’s bias is then sampled from that distribution. The key elements are: Population parameters: μ_θ (mean bias) and σ_θ (standard deviation of bias) Individual parameters: θ_i (bias for agent i) Observed data: y_it (choice for agent i on trial t) This model is more complex, with each agent having two parameters: a baseline bias (α) and a memory sensitivity parameter (β). The key elements are: Population parameters: μ_α, σ_α, μ_β, σ_β (means and standard deviations) Individual parameters: α_i (bias for agent i), β_i (memory sensitivity for agent i) Transformed variables: m_it (memory state for agent i on trial t) Observed data: y_it (choice for agent i on trial t) These graphical models help us understand how information flows in our models and guide our implementation in Stan. Again, it’s practical to work in log odds. Why? Well, it’s not unconceivable that an agent would be 3 sd from the mean. So a biased agent could have a rate of 0.8 + 3 * 0.1, which gives a rate of 1.1. It’s kinda impossible to choose 110% of the time the right hand. We want an easy way to avoid these situations without too carefully tweaking our parameters, or including exception statements (e.g. if rate > 1, then rate = 1). Conversion to log odds is again a wonderful way to work in a boundless space, and in the last step shrinking everything back to 0-1 probability space. N.B. we model all agents with some added noise as we assume it cannot be eliminated from empirical studies. [MISSING: PARALLELIZE] Note that as the n of trials increases, the memory model matches the random model better and better Remember that the simulated parameters are:\n* biasM <- 0\n* biasSD <- 0.1\n* betaM <- 1.5\n* betaSD <- 0.3 Prep the data Our first multilevel model focuses on the biased random agent. For each agent, we’ll estimate an individual bias parameter (theta) that determines their probability of choosing “right” versus “left”. These individual parameters will be modeled as coming from a population distribution with meanthetaMand standard deviationthetaSD. This approach balances two sources of information:\n1. The agent’s individual choice patterns\n2. The overall population distribution of bias parameters The model implements the following hierarchical structure: Population level: θᵐ ~ Normal(0, 1), θˢᵈ ~ Normal⁺(0, 0.3) Individual level: θᵢ ~ Normal(θᵐ, θˢᵈ) Data level: yᵢₜ ~ Bernoulli(logit⁻¹(θᵢ)) Let’s implement this in Stan: Besides the usual prior predictive checks, prior posterior update checks, posterior predictive checks, based on the population level estimates; we also want to plot at least a few of the single agents to assess how well the model is doing for them. [MISSING: PLOT MODEL ESTIMATES AGAINST N OF HEADS BY PARTICIPANT] Now we’ll implement a more complex model for the memory agent. This model has two parameters per agent: bias: baseline tendency to choose “right” (log-odds scale) beta: sensitivity to the memory of opponent’s past choices Like the random agent model, we’ll use a multilevel structure where individual parameters come from population distributions. However, this model presents some additional challenges: The hierarchical structure for this model is: Population level: μ_bias ~ Normal(0, 1), σ_bias ~ Normal⁺(0, 0.3) μ_beta ~ Normal(0, 0.3), σ_beta ~ Normal⁺(0, 0.3) Individual level: bias_i ~ Normal(μ_bias, σ_bias) beta_i ~ Normal(μ_beta, σ_beta) Transformed variables: Data level: Let’s implement this model. [MISSING: DAGS] Code, compile and fit the model When implementing multilevel models, we sometimes encounter sampling efficiency issues, especially when group-level variance parameters are small or data is limited. This creates a “funnel” in the posterior distribution that’s difficult for the sampler to navigate efficiently. Non-centered parameterization addresses this by reparameterizing individual parameters as standardized deviations from the group mean: Instead of: θᵢ ~ Normal(μ, σ)\nWe use: θᵢ = μ + σ · zᵢ, where zᵢ ~ Normal(0, 1) This is conceptually similar to when we z-score variables in regression models. This approach separates the sampling of the standardized individual parameters (zᵢ) from the group-level parameters (μ and σ), improving sampling efficiency. The transformation between these parameterizations is invertible, so the models are equivalent, but the non-centered version often performs better computationally. In our code, we implement this by: Sampling standardized individual parameters (biasID_z, betaID_z) Multiplying by group SD and adding group mean to get individual parameters To better understand the trade-offs between different modeling approaches, let’s implement and compare three ways of handling individual differences: Each approach has advantages and disadvantages: Let’s compare how these approaches perform with our memory agent data [MISSING: PARAMETER RECOVERY IN A MULTILEVEL FRAMEWORK (IND VS POP)] In this chapter, we’ve explored how multilevel modeling provides a principled approach to analyzing data with hierarchical structure. By implementing models for both biased agents and memory agents, we’ve seen how to: Multilevel modeling offers several key advantages for cognitive modeling (only some of which have been exemplified here):\n-Improved parameter estimationfor individuals with limited data\n-Detection of population-level patternswhile respecting individual differences\n-More efficient use of datathrough partial pooling of information\n-Capacity to model correlationsbetween different cognitive parameters The practical implementation challenges we’ve encountered—such as sampling difficulties with correlated parameters and the need for non-centered parameterization—are common in cognitive modeling applications. Developing familiarity with these techniques prepares you for implementing more complex models in your own research."
}