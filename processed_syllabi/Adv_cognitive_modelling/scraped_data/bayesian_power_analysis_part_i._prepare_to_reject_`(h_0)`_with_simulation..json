{
    "document_type": "online_article",
    "title": "Bayesian power analysis: Part I. Prepare to reject `\\(H_0\\)` with simulation.",
    "author": "Unknown",
    "source": "https://solomonkurz.netlify.app/blog/bayesian-power-analysis-part-i/",
    "date_published": "Unknown",
    "flag": "",
    "text": "By A. Solomon Kurz July 18, 2019 Edited on April 21, 2021, to remove thebroom::tidy()portion of the workflow. If you’d like to learn how to do Bayesian power calculations usingbrms, stick around for this multi-part blog series. Here with part I, we’ll set the foundation. Many journals, funding agencies, and dissertation committees require power calculations for your primary analyses. Frequentists have a variety of tools available to perform these calculations (e.g.,here). Bayesians, however, have a more difficult time of it. Most of our research questions and data issues are sufficiently complicated that we cannot solve the problems by hand. We need Markov chain Monte Carlo methods to iteratively sample from the posterior to summarize the parameters from our models. Same deal for power. If you’d like to compute the power for a given combination of\\(N\\), likelihood\\(p(\\text{data} | \\theta)\\), and set of priors\\(p (\\theta)\\), you’ll need to simulate. It’s been one of my recent career goals to learn how to do this. You know how they say:The best way to learn is to teach. This series of blog posts is the evidence of me learning by teaching. It will be an exploration of what a Bayesian power simulation workflow might look like. The overall statistical framework will be withinR(R Core Team, 2022), with an emphasis on code style based on thetidyverse(Wickham et al., 2019;Wickham, 2022). We’ll be fitting our Bayesian models with Bürkner’sbrmspackage(Bürkner, 2017,2018,2022). For this series, I’m presuming you are familiar with linear regression, familiar with the basic differences between frequentist and Bayesian approaches to statistics, and have a basic sense of what we mean by statistical power. Here are some resources if you’d like to shore up. Let’s load our primary packages. Thetidyversehelps organize data and we model withbrms. Consider a case where you have some dependent variable\\(Y\\)that you’d like to compare between two groups, which we’ll call treatment and control. Here we presume\\(Y\\)is continuous and, for the sake of simplicity, is in a standardized metric for the control condition. Letting\\(c\\)stand for control and\\(i\\)index the data row for a given case, we might write that as\\(y_{i, c} \\sim \\operatorname{Normal} (0, 1)\\). The mean for our treatment condition is 0.5, with the standard deviation still in the standardized metric. In the social sciences a standardized mean difference of 0.5 would typically be considered a medium effect size. Here’s what that’d look like. Sure, those distributions have a lot of overlap. But their means are clearly different and we’d like to make sure we plan on collecting enough data to do a good job showing that. A power analysis will help. Within the conventional frequentist paradigm, power is the probability of rejecting the null hypothesis\\(H_0\\)in favor of the alternative hypothesis\\(H_1\\), given the alternative hypothesis is “true.” In this case, the typical null hypothesis is $$H_0\\text{: } \\mu_c = \\mu_t,$$ or put differently, $$\nH_0\\text{: } \\mu_t - \\mu_c = 0.\n$$ And the alternative hypothesis is often just $$H_1\\text{: } \\mu_c \\neq \\mu_t,$$ or otherwise put, $$\nH_1\\text{: } \\mu_t - \\mu_c \\neq 0.\n$$ Within the regression framework, we’ll be comparing\\(\\mu\\)s using the formula $$\\begin{align*} y_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i & = \\beta_0 + \\beta_1 \\text{treatment}_i, \\end{align*}$$ where\\(\\text{treatment}\\)is a dummy variable coded 0 = control 1 = treatment and varies across cases indexed by\\(i\\). In this setup,\\(\\beta_0\\)is the estimate for\\(\\mu_c\\)and\\(\\beta_1\\)is the estimate of the difference between condition means,\\(\\mu_t - \\mu_c\\). Thus our focal parameter, the one we care about the most in our power analysis, will be\\(\\beta_1\\). Within the frequentist paradigm, we typically compare these hypotheses using a\\(p\\)-value for\\(H_0\\)with the critical value,\\(\\alpha\\), set to .05. Thus, power is the probability we’ll have\\(p < .05\\)when it is indeed the case that\\(\\mu_c \\neq \\mu_t\\). We won’t be computing\\(p\\)-values in this project, but we will use 95% intervals. Recall that the result of a Bayesian analysis, the posterior distribution, is the probability of the parameters, given the data\\(p (\\theta | \\text{data})\\). With our 95% Bayesian credible intervals, we’ll be able to describe the parameter space over which our estimate of\\(\\mu_t - \\mu_c\\)is 95% probable. That is, for our power analysis, we’re interested in the probability our 95% credible intervals for\\(\\beta_1\\)contain zero within their bounds when we know a priori\\(\\mu_c \\neq \\mu_t\\). The reason we know\\(\\mu_c \\neq \\mu_t\\)is because we’ll be simulating the data that way. What our power analysis will help us determine is how many cases we’ll need to achieve a predetermined level of power. The conventional threshold is .8. To make this all concrete, let’s start with a simple example. We’ll simulate a single set of data, fit a Bayesian regression model, and examine the results for the critical parameter\\(\\beta_1\\). For the sake of simplicity, let’s keep our two groups, treatment and control, the same size. We’ll start with\\(n = 50\\)for each. We already decided above that $$\\begin{align*} y_{i, c} & \\sim \\operatorname{Normal}(0, 1) \\text{ and} \\\\ y_{i, t} & \\sim \\operatorname{Normal}(0.5, 1). \\end{align*}$$ Here’s how we might simulate data along those lines. In case it wasn’t clear, the two variablesgroupandtreatmentare redundant. Whereas the former is composed of names, the latter is the dummy-variable equivalent (i.e., control = 0, treatment = 1). The main event was how we used thernorm()function to simulate the normally-distributed values fory. Before we fit our model, we need to decide on priors. To give us ideas, here are thebrmsdefaults for our model and data. A few things: Notice that here we’re using the0 + Interceptsyntax. This is becausebrmshandles the priors for the default intercept under the presumption you’ve mean-centered all your predictor variables. However, since ourtreatmentvariable is a dummy, that assumption won’t fly. The0 + Interceptallows us to treat the model intercept as just another\\(\\beta\\)parameter, which makes no assumptions about centering. Along those lines, you’ll noticebrmscurrently defaults to flat priors for the\\(\\beta\\)parameters (i.e., those for whichclass = b). And finally, the default prior on\\(\\sigma\\)is moderately widestudent_t(3, 0, 2.5). By default,brmsalso sets the left bounds for\\(\\sigma\\)parameters at zero, making that a folded-$t$ distribution. If you’re confused by these details, spend some time with thebrmsreference manual(Bürkner, 2020), particularly thebrmandbrmsformulasections. In this project, we’ll be primarily using two kinds of priors: default flat priors and weakly-regularizing priors. Hopefully flat priors are self-explanatory. They let the likelihood (data) dominate the posterior and tend to produce results similar to those from frequentist estimators. As for weakly-regularizing priors, McElreath covered them in his text. They’re mentioned a bit in theStanteam’sPrior Choice Recommendationswiki, and you can learn even more from Gelman, Simpson, and Betancourt’s (2017)The prior can only be understood in the context of the likelihood. These priors aren’t strongly informative and aren’t really representative of our research hypotheses. But they’re not as absurd as flat priors, either. Rather, with just a little bit of knowledge about the data, these priors are set to keep the MCMC chains on target. Since ouryvariable has a mean near zero and a standard deviation near one and since our sole predictor,treatmentis a dummy, setting\\(\\operatorname{Normal}(0, 2)\\)as the prior for both\\(\\beta\\)parameters might be a good place to start. The prior is permissive enough that it will let likelihood dominate the posterior, but it also rules out ridiculous parts of the parameter space (e.g., a standardized mean difference of 20, an intercept of -93). And since we know the data are on the unit scale, we might just center our folded-Student-$t$ prior on one and add a gentle scale setting of one. Feel free to disagree and use your own priors. The great thing about priors is that they can be proposed, defended, criticized and improved. The point is to settle on the priors you can defend with written reasons. Select ones you’d feel comfortable defending to a skeptical reviewer. Here’s how we might fit the model. Before we look at the summary, we might check the chains in a trace plot. We’re looking for “stuck” chains that don’t appear to come from a normal distribution (the chains are a profile-like view rather than histogram, allowing for inspection of dependence between samples). Yep, the chains all look good. Here’s the parameter summary. The 95% credible intervals for our\\(\\beta_1\\)parameter, termedtreatmentin the output, are well above zero. Another way to look at the parameter summary is with thebrms::fixef()function. Especially with simple models like this, a lot of the time we spend waiting forbrms::brm()to return the model is wrapped up in compilation. This is becausebrmsis a collection of user-friendly functions designed to fit models withStan(Stan Development Team, 2020,2021a,2021b). With each new model,brm()translates your model intoStancode, which then gets translated to C++ and is compiled afterwards (seehereorhere). However, we can use theupdate()function to update a previously-compiled fit object with new data. This cuts out the compilation time and allows us to get directly to sampling. Here’s how to do it. Behold thefixef()parameter summary for our updated model. Well how about that? In this case, our 95% credible intervals for the\\(\\beta_1\\)treatmentcoefficient did include zero within their bounds. Though the posterior mean, 0.30, is still well away from zero, here we’d fail to reject\\(H_0\\)at the conventional level. This is why we simulate. To recap, we’ve We’re more than half way there! It’s time to do our first power simulation. In this post, we’ll play with three ways to do a Bayesian power simulation. They’ll all be similar, but hopefully you’ll learn a bit as we transition from one to the next. Though if you’re impatient and all this seems remedial, you could probably just skip down to the final example,Version 3. For our power analysis, we’ll need to simulate a large number of data sets, each of which we’ll fit a model to. Here we’ll make a custom function,sim_d(), that will simulate new data sets just like before. Our function will have two parameters: we’ll set our seeds withseedand determine how many cases we’d like per group withn. Here’s a quick example of how our function works. Now we’re ready to get down to business. We’re going to be saving our simulation results in a nested data frame,s. Initially,swill have one column ofseedvalues. These will serve a dual function. First, they are the values we’ll be feeding into theseedargument of our custom data-generating function,sim_d(). Second, since theseedvalues serially increase, they also stand in as iteration indexes. For our second step, we add the data simulations and save them in a nested column,d. In the first argument of thepurrr::map()function, we indicate we want to iterate over the values inseed. In the second argument, we indicate we want to serially plug thoseseedvalues into the first argument within thesim_d()function. That argument, recall, is the well-namedseedargument. With the final argument inmap(),n = 50, we hard code 50 into thenargument ofsim_d(). For the third step, we expand ourpurrr::map()skills from above topurrr::map2(), which allows us to iteratively insert two arguments into a function. Within this paradigm, the two arguments are generically termed.xand.y. Thus our approach will be.x = d, .y = seed. For our function, we specify~update(fit, newdata = .x, seed = .y). Thus we’ll be iteratively inserting our simulatedddata into thenewdataargument and will be simultaneously inserting ourseedvalues into theseedargument. Also notice that the number of iterations we’ll be working with is determined by the number of rows in theseedcolumn. We are defining that number asn_sim. Since this is just a blog post, I’m going to take it easy and use 100. But if this was a real power analysis for one of your projects, something like 1,000 would be better. Finally, you don’t have to do this, but I’m timing my simulation by savingSys.time()values at the beginning and end of the simulation. The entire simulation took just about a minute on mynew laptop. Your mileage may vary. Let’s take a look at what we’ve done. In our 100-row nested tibble, we have all our simulated data sets in thedcolumn and all of ourbrmsfit objects nested in thefitcolumn. Next we’ll usefixef()and a little wrangling to extract the parameter of interest,treatment(i.e.,\\(\\beta_1\\)), from each simulation. We’ll save the results asparameters. As an aside, I know I’m moving kinda fast with all this wackypurrr::map()/purrr::map2()stuff. If you’re new to using thetidyversefor iterating and saving the results in nested data structures, I recommend fixing an adult beverage and cozying up with Hadley Wickham’s presentation,Managing many models. And if you really hate it, both Kruschke and McElreath texts contain many examples of how to iterate in a more baseRsort of way. Anyway, here’s what those 100\\(\\beta_1\\)summaries look like in bulk. The horizontal lines show the idealized effect size (0.5) and the null hypothesis (0). Already, it’s apparent that most of our intervals indicate there’s more than a 95% probability the null hypothesis is not credible. Several do. Here’s how to quantify that. With the secondmutate()line, we used a logical statement withinifelse()to code all instances where the lower limit of the 95% interval (Q2.5) was greater than 0 as a 1, with the rest as 0. That left us with a vector of 1’s and 0’s, which we saved ascheck. In thesummarise()line, we took the mean of that column, which returned our Bayesian power estimate. That is, in 66 of our 100 simulations, an\\(n = 50\\)per group was enough to produce a 95% Bayesian credible interval that did not straddle 0. I should probably point out that a 95% interval for whichQ97.5 < 0would have also been consistent with the alternative hypothesis of\\(\\mu_c \\neq \\mu_t\\). However, I didn’t bother to work that option into the definition of ourcheckvariable because I knew from the outset that that would be a highly unlikely result. But if you’d like to work more rigor into your checks, by all means do. And if you’ve gotten this far and have been following along with code of your own, congratulations! You did it! You’ve estimated the power of a Bayesian model with a given\\(n\\). Now let’s refine our approach. I really like it that oursobject contains all ourbrm()fits. It makes it really handy to do global diagnostics like making sure our\\(\\widehat R\\)values are all within a respectable range. Man those\\(\\widehat R\\)values look sweet. It’s great to have a workflow that lets you check them. But holding on to all those fits can take up a lot of memory. If the only thing you’re interested in are the parameter summaries, a better approach might be to do the model refitting and parameter extraction in one step. That way you only save the parameter summaries. Here’s how you might do that. Like before, this only about a minute. As a point of comparison, here are the sizes of the results from our first approach to those from the second. That’s a big difference. Hopefully you get the idea. With more complicated models and 10+ times the number of simulations, size will eventually matter. Anyway, here are the results. Same parameter summaries, lower memory burden. So far, both of our simulation attempts resulted in our saving the simulated data sets. It’s a really nice option if you ever want to go back and take a look at those simulated data. For example, you might want to inspect a random subset of the data simulations with box plots. In this case, it’s no big deal if we keep the data around or not. The data sets are fairly small and we’re only simulating 100 of them. But in cases where the data are larger and you’re doing thousands of simulations, keeping the data could become a memory drain. If you’re willing to forgo the luxury of inspecting your data simulations, it might make sense to run our power analysis in a way that avoids saving them. One way to do so would be to just wrap the data simulation and model fitting all in one function. We’ll call itsim_d_and_fit(). Now iterate 100 times once more. That was pretty quick. Here’s what it returned. By wrapping our data simulation, model fitting, and parameter extraction steps all in one function, we simplified the output such that we’re no longer holding on to the data simulations or thebrmsfit objects. We just have the parameter summaries and theseed, making the product even smaller. But the primary results are the same. We still get the same power estimate, too. But my goal was to figure out what\\(n\\)will get me power of .8 or more!, you say. Fair enough. Try increasingnto 65 or something. If that seems unsatisfying, welcome to the world of simulation. Since our Bayesian models are complicated, we don’t have the luxury of plugging a few values into some quick power formula. Just as simulation is an iterative process, determining on the right values to simulate over might well be an iterative process, too. Anyway, that’s the essence of thebrms/tidyverseworkflow for Bayesian power analysis. You follow these steps: In addition, we played with a few approaches based on logistical concerns like memory. In the next post,part II, we’ll see how the precision-oriented approach to sample-size planning is a viable alternative to power focused on rejecting null hypotheses. Special thanks to Christopher Peters (@statwonk) for the helpful edits and suggestions. Bürkner, P.-C. (2017).brms: An R package for Bayesian multilevel models using Stan.Journal of Statistical Software,80(1), 1–28.https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms.The R Journal,10(1), 395–411.https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2020).brmsreference manual, Version 2.14.4.https://CRAN.R-project.org/package=brms/brms.pdf Bürkner, P.-C. (2022).brms: Bayesian regression models using ’Stan’.https://CRAN.R-project.org/package=brms Cohen, J. (1988).Statistical power analysis for the behavioral sciences. L. Erlbaum Associates.https://www.worldcat.org/title/statistical-power-analysis-for-the-behavioral-sciences/oclc/17877467 Gelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood.Entropy,19(10, 10), 555.https://doi.org/10.3390/e19100555 Grolemund, G., & Wickham, H. (2017).R for data science. O’Reilly.https://r4ds.had.co.nz Kruschke, J. K. (2015).Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press.https://sites.google.com/site/doingbayesiandataanalysis/ Kurz, A. S. (2020).Doing Bayesian data analysis in brms and the tidyverse(version 0.3.0).https://bookdown.org/content/3686/ Kurz, A. S. (2020).Statistical rethinking with brms, Ggplot2, and the tidyverse: Second edition(version 0.1.1).https://bookdown.org/content/4857/ Kurz, A. S. (2020).Statistical rethinking with brms,ggplot2, and the tidyverse(version 1.2.0).https://doi.org/10.5281/zenodo.3693202 Maxwell, S. E., Kelley, K., & Rausch, J. R. (2008). Sample size planning for statistical power and accuracy in parameter estimation.Annual Review of Psychology,59(1), 537–563.https://doi.org/10.1146/annurev.psych.59.103006.093735 McElreath, R. (2020).Statistical rethinking: A Bayesian course with examples in R and Stan(Second Edition). CRC Press.https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015).Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press.https://xcelab.net/rm/statistical-rethinking/ Peng, R. D. (2019).R programming for data science.https://bookdown.org/rdpeng/rprogdatascience/ R Core Team. (2022).R: A language and environment for statistical computing. R Foundation for Statistical Computing.https://www.R-project.org/ Stan Development Team. (2020, February 10).RStan: The R interface to Stan.https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html Stan Development Team. (2021a).Stan reference manual, Version 2.27.https://mc-stan.org/docs/2_27/reference-manual/ Stan Development Team. (2021b).Stan user’s guide, Version 2.26.https://mc-stan.org/docs/2_26/stan-users-guide/index.html Wickham, H. (2020).The tidyverse style guide.https://style.tidyverse.org/ Wickham, H. (2022).tidyverse: Easily install and load the ’tidyverse’.https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse.Journal of Open Source Software,4(43), 1686.https://doi.org/10.21105/joss.01686 © A. Solomon Kurz (2022)Made withHugo Apéro.Based onBlogophonicbyFormspree."
}