{
    "document_type": "online_article",
    "title": "Chapter 9 Mixture models",
    "author": "Riccardo Fusaroli",
    "source": "https://fusaroli.github.io/AdvancedCognitiveModeling2023/mixture-models.html",
    "date_published": "Unknown",
    "flag": "",
    "text": "When we model human cognition and behavior, we often find ourselves facing a puzzling reality: people don’t always follow a single, consistent strategy. Consider a person playing our matching pennies game - sometimes they might carefully track their opponent’s patterns, other times they might rely on a simple bias toward choosing “heads,” and occasionally they might respond completely randomly when their attention lapses. Traditional cognitive models that assume a single process are ill-equipped to capture this complexity. Mixture models provide an elegant solution to this challenge. Rather than assuming behavior reflects just one cognitive process, mixture models allow us to combine multiple different processes within a unified modeling framework. In the previous chapter, we explored model comparison techniques that help us select between competing cognitive models. Mixture models approach in a sense reframe that very problem - instead of asking “which model is correct?”, they allow us to ask “how much does each model contribute?” This shift acknowledges the possibility that multiple cognitive processes might coexist, either within a single individual or across a population. After completing this chapter, you will be able to: Understand how mixture models combine multiple cognitive processes Implement mixture models for different types of behavioral data using Stan Estimate mixture weights and component-specific parameters Evaluate mixture models through posterior predictive checks Compare mixture models to single-process alternatives In this chapter, we will: Examine reaction time data to visualize how mixture distributions appear in practice Develop a theoretical foundation for mixture modeling Implement a simple mixture model combining biased and random choice processes (thus going back to our matching pennies example) Evaluate and validate this model through posterior checks Extend to multilevel mixture models that capture individual differences Compare mixture models with traditional single-process approaches Before diving into the mathematics and implementation of mixture models, let’s start with a concrete example that visually demonstrates why we need them. Reaction time (RT) data provides a particularly clear window into the mixture of cognitive processes. In many cognitive tasks, participants are asked to respond as quickly as possible while still being accurate. However, attention fluctuates over time. Let’s simulate a scenario where participants sometimes engage in deliberate thinking (producing a log-normal distribution of RTs) and sometimes experience attentional lapses (producing more or less random responses with a uniform distribution of RTs). Now, let’s visualize the reaction time distribution: In this example, we can clearly see how the overall reaction time distribution (black line) is a combination of two distinct processes: Deliberate Thinking (Blue): A log-normal distribution centered around 500ms, representing focused cognitive processing of the task. Attentional Lapses (Red Line): A uniform distribution spanning from very quick to very slow responses, representing trials where attention has drifted, leading to either impulsive responses or delayed responses due to mind-wandering. The mixture of these processes creates a complex distribution with a prominent peak (from the deliberate process) and extended tails (from the attentional lapses). A standard single-process model assuming only a log-normal distribution would fail to capture these extended tails, leading to poor fit and potentially misleading conclusions about the cognitive processes involved. This is exactly the situation where mixture models excel. They allow us to represent observed data as coming from a weighted combination of different underlying processes. Next, we’ll formalize this intuition and extend it to decision-making models. At their core, mixture models represent data as coming from a weighted combination of different “component” distributions or processes. Mathematically, a mixture model can be expressed as: p(y) = π_1p_1(y) + π_2p_2(y) + … + π_kp_k(y) Where: p(y) is the overall probability of observing data point y p_j(y) is the probability of y according to component model j π_j is the weight or mixing proportion of component j (with all π_j summing to 1) Each component distribution p_j(y) can have its own parameters, and the mixing proportions π_j determine how much each component contributes to the overall model. An alternative and often useful way to think about mixture models is through latent (unobserved) variables. We can introduce a latent categorical variable z that indicates which component generated each observation. For example, in a two-component mixture: We can then formulate the mixture model as: p(y_i, z_i) = p(z_i)p(y_i∣z_i) Where p(z_i=j)=π_j is the prior probability of component j, and p(y_i∣z_i = j) is the likelihood of y_i under component j. This latent variable perspective is particularly useful for implementing mixture models in Bayesian frameworks like Stan. In cognitive modeling, mixture models can represent several important phenomena: Attentional fluctuations: As in our reaction time example, performance may reflect both focused engagement and attentional lapses Strategy switching: Individuals might switch between different strategies over time Dual-process theories: Behavior might arise from multiple cognitive systems (e.g., automatic vs. controlled) Individual differences: Different individuals might use different strategies Exploration vs. exploitation: Some decisions might reflect exploring new options while others exploit known rewards Now that we understand the theoretical foundation, let’s implement a simple mixture model for choice data. We’ll model behavior as a mixture of two processes: This could represent, for example, a person who sometimes carefully performs a task but occasionally responds randomly due to attentional lapses. In previous chapters we generated data to use for fitting models. Let’s use that same data but focus on a particular agent who might be mixing strategies: Let’s visualize this data to see if we can detect patterns suggestive of a mixture: For binary choice data, it’s harder to visually detect a mixture compared to reaction times. The overall proportion of right choices falls between what we’d expect from random choice (0.5) and fully biased choice with rate 0.8, which is consistent with a mixture of these processes. Now let’s implement a Stan model that represents choices as coming from a mixture of a biased process and a random process: Now let’s fit the model and evaluate its performance: Let’s check the model diagnostics to ensure our inference is reliable: To verify that our model can accurately recover parameters, let’s examine how close our inferred parameters are to the true values used in the simulation: The model has recovered the true parameters reasonably well, providing confidence in our approach. In a real project, we’d want to run this across a broad range of parameter values! Our model appears to capture both the overall proportion of right choices and the distribution of run lengths in the observed data. This suggests the mixture model is adequately representing the data-generating process. The mixture model provides valuable insights into the cognitive processes underlying the observed behavior: Bias Parameter (θ): The bias parameter (estimated as approximately r round(mean(draws$bias_p), 2)) represents the probability of choosing the right option when the participant is following the biased process. Noise Parameter (π): The noise parameter (estimated as approximately r round(mean(draws$noise_p), 2)) represents the proportion of choices that come from the random process rather than the biased process. This can be interpreted as the frequency of attentional lapses or exploratory behavior. The mixture model thus decomposes behavior into two distinct processes, providing a more nuanced understanding than a single-process model could offer. This has important cognitive implications: for instance, besides the usual focus on “deliberate” reaction times, individual variations attentional lapses might be affected by the experimental condition, or an underlying diagnosis, thus providing richer information. Now let’s extend our approach to a multilevel (hierarchical) mixture model that can accommodate individual differences across multiple participants. This allows us to estimate both population-level parameters and individual-specific variations. Now we’ll prepare data from multiple agents for the multilevel model: The multilevel mixture model allows us to examine individual differences in both bias and mixture weights: An important advantage of multilevel mixture models is their ability to reveal correlations between parameters across individuals: The correlation between bias and noise parameters can provide important insights into cognitive processes. For example, a negative correlation might suggest that individuals with stronger biases tend to have fewer random lapses, while a positive correlation could indicate that strong biases are associated with more exploratory behavior.\nHowever, in the simulation process we did not include any correlation between the bias and noise parameters, so the correlation we observe here is correctly estimated as centered at 0. Finally, let’s compare our mixture model with a single-process alternative to determine which better captures the observed behavior. First, let’s implement a simple single-process model that assumes all choices come from a biased process: Now let’s compare the single-process model with our mixture model using leave-one-out cross-validation (LOO-CV): Based on the model comparison, the single model appears to better capture the data-generating process than the single-process alternative. This might seem counterintuitive, but it highlights that predictive performance is not necessarily the best way of choosing your model.\nWhat happens here is that a combination of two binomials can be mathematically reduced to a single binomial (in this case with a lower rate that the biased component). This is why the single-process model performs better in terms of LOO-CV, even though the mixture model is more realistic and provides a richer interpretation of the data. Theory should guide you in this case. Mixture models have found numerous applications in cognitive science. Here are a few examples that highlight their versatility: Attention and Vigilance: Modeling attentional lapses during sustained attention tasks as a mixture of focused and random responses. Memory: Representing recognition memory as a mixture of more implicit familiarity and more explicit recollection processes, each with distinct characteristics. *Decision Making: Modeling economic choices as combinations of heuristic and deliberative processes, with the proportion varying based on task demands. Learning: Capturing the transition from rule-based to automatic processing during skill acquisition, with the mixture weights shifting over time. Individual Differences: Identifying subgroups of participants who employ qualitatively different strategies to solve the same task. Mixture models represent a crucial step forward in our cognitive modeling toolkit, allowing us to capture the complexity and variability inherent in human behavior. Through this chapter, we’ve seen how combining multiple cognitive strategies within a single model can provide richer and more realistic accounts of decision-making processes. Several key insights emerge from our exploration of mixture models: Beyond Single-Process Simplifications: Mixture models allow us to move beyond the false choice between oversimplified single-strategy models and intractably complex specifications. By combining a small number of interpretable components, we can capture substantial behavioral complexity while maintaining mathematical and computational tractability. Bayesian Implementation: The Bayesian implementation of mixture models in Stan provides powerful tools for inference. We can estimate not only the parameters of different cognitive strategies but also their relative contributions to behavior and how they might vary across individuals. Model Validation: Mixture models require careful attention to identifiability and validation. Through parameter recovery studies and posterior predictive checks, we’ve seen how to verify that our specifications can reliably recover true parameter values and generate realistic behavioral patterns."
}