{
    "document_type": "book",
    "title": "The The generalized context model: an exemplar model of classification",
    "author": "Robert M. Nosofsky",
    "source": "raw_syllabi\\master_courses\\Adv_cognitive_modelling\\pdf_material\\The_generalized_context_model_an_exempla.pdf",
    "date_published": "2023-05-11",
    "keywords": "",
    "flag": "",
    "text": "18 Model description Conceptual overview According to the generalized context model (GCM) (Nosofsky,  1986 ), people represent categories by storing individual exemplars (or examples) in memory, and classify objects based on their similarity to these stored exemplars. For example, the model assumes that people represent the category of ‘birds’ by storing in memory the vast collection of different sparrows, robins, eagles, ostriches (and so forth) that they have experi- enced. If an object is sufficiently similar to some of these bird exemplars, then the person would tend to classify the object as a ‘bird’. This exem- plar view of categorization contrasts dramatically with major alternative approaches that assume that people form abstract summary representa- tions of categories, such as rules or idealized prototypes. The standard version of the GCM adopts a multidimensional scaling (MDS)   approach to modelling similarity relations among exemplars (Shepard,  1958 ,  1987 ). In this approach, exemplars are represented as points in a multidimensional psychological space  . Similarity between exemplars is a decreasing function of their distance in the space. In many applications, a first step in the modelling is to conduct similarity-scaling studies to derive MDS solutions for the exemplars and to discover their locations in the multidimensional similarity space (Nosofsky,  1992b ). A crucial assumption in the modelling, however, is that similarity is not an invariant relation, but a highly context-dependent one. To take an example from Medin  and Schaffer  ( 1978 ), humans and mannequins may be judged as highly similar in a context that emphasizes struc- tural appearance, but would be judged as highly dissimilar in a context that emphasizes vitality. In the GCM, the context-dependent nature of 2 The generalized context model: an exemplar model of classification Robert M.   Nosofsky The writing of this chapter was supported by grants FA9550–08–1–0486 from the Air Force Office of Scientific Research and MH48494 from the National Institute of Mental Health. \nGCM 19 similarity is modelled in terms of a set of selective-attention  weights that systematically modify the structure of the psychological space in which the exemplars are embedded (Carroll & Wish,  1974 ). As will be illus- trated below, the weights serve to ‘stretch’ the psychological space along highly attended, relevant dimensions, and to ‘shrink’ the space along unattended irrelevant dimensions. This stretching and shrinking can have profound influences on similarity relations among exemplars and on the resulting classification predictions from the model. Finally, the model assumes that the individual exemplars may be stored in memory with differing memory strengths  . The memory strength of an exemplar may be influenced by factors such as frequency of presentation, recency of presentation, different forms of feedback provided during learning, and so forth. When a test item is presented so as to be classified, the exemplars that are most likely to be retrieved (and therefore to influence most strongly the classification decision) are those that are highly similar to the test item and that have high memory strengths. However, because exemplar retrieval is a probabilis- tic process, all exemplars stored in memory may influence classification decision making. The conceptual ideas summarized above are illustrated schematic- ally in  Figure 2.1 . Consider first the top panel. We suppose that there are two categories, A and B, with five exemplars in each category. The exemplars are composed of two dimensions. Exemplars A2 and B4 are close together in the space, so are highly similar to one another; whereas exemplars A5 and B2 are far apart, so are highly dissimilar. (The symbols used to illustrate the exemplars vary in their size in order to illustrate that the exemplars may be stored in memory with differing strengths.) Imagine that the observer is asked to classify test stimulus i into one of the two categories. According to the model, the observer sums the simi- larity of test item i to all of the exemplars of Category A and to all of the exemplars of Category B (weighted by the memory strengths of the examplars), and the classification decision is based on the relative mag- nitude of these summed similarities. In the top panel of  Figure 2.1 , test item i is roughly equally similar to the exemplars of the two categories, so the observer would classify the test item into the two categories with roughly equal probability. Notice, however, that the horizontal dimension is far more relevant than is the vertical dimension for discriminating between the members of the two categories. (That is, all exemplars of Category B tend to have large values along the horizontal dimension, whereas all exemplars of Category A tend to have small values along the horizontal dimension.) Presumably, an experienced observer would learn this aspect of the \nRobert M. Nosofsky 20 category structure, so would learn to give greater attention to the more relevant horizontal dimension than to the less relevant vertical dimen- sion. The bottom panel of  Figure 2.1  illustrates the same category struc- ture, but now assuming the above-described selective-attention strategy. The space is ‘stretched’ along the more-attended horizontal dimension and is ‘shrunk’ along the less-attended vertical dimension. In effect, by implementing this selective-attention strategy, the observer is attempting to optimize similarity relations for the given classification task (Nosofsky, 1984 ,  1986 ). There is now greater separation between the exemplars of contrasting categories (lowered between-category similarity), yet less separation among the exemplars within each category (heightened with- in-category similarity). Furthermore, note that this selective-attention strategy has a profound influence on the classification predictions from the model. In the top panel, item i was roughly equally similar to the exemplars of the two categories; however, following selective attention to the relevant dimension (bottom panel), the test item is now far more similar to the exemplars of Category A. Thus, in the latter situation, the exemplar model predicts that the test item would be classified into Category A with high probability. A5 A5 A3 A1 A3 A2 A4 B4 B1 B5 B2 B3 A1 A2 B5 B3 B2 B4 B1 i i A4 Figure 2.1    Schematic illustration of a category structure to explain the workings of the GCM. The top panel illustrates the category structure with equal attention to both dimensions. The bottom panel illustrates the structure following selective attention to the horizontal dimension. \nGCM 21 Computational assumptions In this section I present a brief description of how the GCM is formal- ized. The description assumes that there is an initial training phase in which observers are presented with n unique training exemplars. The training phase is followed by a test phase in which both training items and new transfer items might be presented. On each trial during the test phase, the observer classifies the test item into one of K N categories. According to the model, the probability with which item i is classified into Category J during the test phase is given by: P i V b V J jJ j n ij K kK ik k n K K s s N J b | ( ) =                   = = = ∑ ∑ 1 1 1 γ γ ∑ , (1) where s ij denotes the similarity between item i and exemplar j ; V jJ denotes the memory strength of exemplar j with respect to Category J ; γ is a freely estimated response-scaling parameter (0 < γ ); and b J (0 < b J ) denotes the response-bias for Category J . Thus, according to the model, the observer sums the similarity  of item i to all exemplars j belonging to Category J , weighted by their Category J memory strengths (and by any differential response bias). This summed similarity constitutes the ‘evidence’ in favour of Category J . This evidence is then divided by the summed evidence for all of the categories to predict the Category- J clas- sification probability. The parameter γ in Equation  1  is a response-scaling parameter that influences the degree of determinism in classification responding (Ashby & Maddox,  1993 ; Nosofsky & Zaki,  2002 ). When γ = 1, the observer responds probabilistically by ‘probability matching’  to the rela- tive summed similarities of each category; whereas when γ grows greater than 1, observers respond more deterministically with the category that yields the largest summed similarity. The memory-strength values ( V jJ ) in Equation  1  are typically not free parameters but rather are given by the nature of the experimental design. Usually, they are set equal to the relative frequency with which each exemplar j is provided with Category J feedback during the classification training phase. (In the most usual classification learning paradigms, the exemplars are presented with equal frequency and each exemplar is assigned to only a single category. In that simple case, all exemplars j that are assigned to Category J receive \nRobert M. Nosofsky 22 memory strengths equal to 1; whereas the memory strength of an exem- plar with respect to all its unassigned categories is equal to 0.) To apply Equation  1 , one needs to compute the similarity between item i and each exemplar j , s ij . In the standard version of the GCM, each exemplar j is represented as a single point in an M -dimensional psycho- logical space. Let x jm denote the value of exemplar j on Dimension m . The distance between item i and exemplar j is given by the weighted Minkowski power model  , ij m im jm r m M r w x x d = −           = ∑ | | , / 1 1 (2) where the value r determines the form of the distance metric. In situ- ations involving highly separable-dimension stimuli (Garner,  1974 ; Shepard,  1964 ), the value r is typically set equal to 1, which yields a city-block distance metric  . By contrast, in situations involving integral-dimension stimuli, the value r is set equal to 2, which yields a Euclidean distance metric  . The w m values in Equation  2  are freely estimated attention-weight parameters (with 0 ≤ w m ≤ 1, and ∑ w m = 1), reflecting the degree of attention that observers give to each dimension m in making their classification judgments. 1 A geometric interpret- ation for the operation of the attention weights is that of stretching and shrinking the psychological space along its component dimen- sions (see  Figure 2.1 ). Finally, the similarity between item i and exemplar j is given by ij cd s ij p e = − , (3) where c is a freely estimated sensitivity parameter that reflects the rate at which similarity declines with distance. When c is large, the similar- ity gradient is steep; that is, similarity falls off rapidly with increasing distance in the space. In this situation, the GCM acts very much like a nearest-neighbour classifier  (i.e., one in which classification decisions are based primarily on the category membership of a test item’s near- est neighbour). When c is small, the similarity gradient is shallow, and 1 More precisely, the weights measure the span of each dimension relative to some prior, ‘neutral’ scaling or physical specification of the stimuli. To the extent that a prior scaling was obtained under conditions in which each dimension was equally relevant, then the weights estimated in a classification task can be interpreted in terms of the amount of ‘attention’ devoted to each dimension for purposes of the classification. \nGCM 23 numerous exemplars may make major contributions to the classification decision. The value p in Equation  3  determines the shape of the function relating similarity to distance. In most cases, p is set equal to one, which yields an exponential relation between similarity and psychological dis- tance (Shepard,  1987 ). In situations involving highly confusable stim- uli, however, p is sometimes set equal to 2, yielding a Gaussian relation between similarity and distance (Nosofsky,  1985 ; for a theoretical inter- pretation, see Ennis,  1988 ). In sum, the free parameters used for fitting the GCM to classifica- tion data are the sensitivity parameter c in Equation  3 , the set of atten- tion weights w m in Equation  2 ; and the response-bias parameters and response-scaling parameter γ in Equation  1 . (Because they can be constrained to sum to one, there are only M − 1 freely varying atten- tion weights and K N − 1 freely varying response-bias parameters.) All other quantities, including the x im coordinate values (Equation  2 ), the V kK memory strengths (Equation  1 ), the distance metric (value of r in Equation  2 ), and the similarity gradient (value of p in Equation  3 ) are fixed by the nature of the experimental design or are derived from inde- pendent sources. Motivation The GCM is a generalization of the original context model proposed by Medin  and Schaffer  ( 1978 ). Nosofsky  ( 1984 ,  1986 ) integrated the Medin–Schaffer context model  with classic theories developed in the areas of choice and similarity. This integration provided a firm theoretical foundation for the context model and allowed the original version to be extended to more widely varying classification paradigms. For example, the original context model was applied in highly simplified domains involving stimuli that varied along only binary-valued, separable dimen- sions. By comparison, most applications of the GCM are in continuous- dimension domains. In addition, the GCM is readily applied to predict classification performance involving integral-dimension stimuli  as well as separable-dimension stimuli  (Nosofsky,  1987 ). Moreover, when the GCM is used in combination with MDS approaches (in order to locate exemplars in psychological similarity spaces), it can be applied to pre- dict classification performance in rich and complex domains in which the underlying stimulus dimensions may not be known (e.g., Shin & Nosofsky,  1992 ). In the original version of the context model, Medin   and Schaffer proposed what is essentially Equation  1  as a choice rule for classifica- tion. (Their equation did not include the memory-strength terms, bias \nRobert M. Nosofsky 24 parameters, or the γ response-scaling parameter.) However, they did not provide a strong justification for the use of the choice rule beyond the fact that it seemed to work. Nosofsky  ( 1984 ,  1986 ) provided a deeper foundation for the context-model response rule by noting a strong rela- tion between it and   the classic similarity-choice model (SCM) for predict- ing confusions in identification paradigms (Luce,  1963 ; Shepard,  1957 ). I briefly review this motivating relation here. In an identification paradigm, there are n distinct stimuli, and each stimulus is assigned a unique response. The data are summarized in an n x n identification confusion matrix (see left panel of  Figure 2.2 ), where each cell ( i , j ) of the matrix gives the conditional probability with which stimulus i is identified as stimulus j . The SCM is one of the leading descriptive models for predicting confusion probabilities in identification paradigms. According to the SCM, the probability with which stimulus i is identified as stimulus j is given by P i b s b s j ij k ik k n ( | ) , j = = ∑ 1 (4) where b j (0 < b j ) is the bias for response j , and where s ij (0 < s ij , s ij = s ij ) is the similarity between stimuli i and j . In a classification paradigm, the n stimuli are assigned to one of K N categories ( K N < n ). The data are summarized in an n × K N confusion matrix, where cell ( i , J ) of the matrix gives the conditional probability with which stimulus i is classified in Category J . An illustration with K N = 2 is provided in the right panel of  Figure 2.2 . An intuitively compelling idea for predicting classification confusions from identification confusions was first proposed by Shepard  , Hovland  , and Jenkins  ( 1961 ).   The idea was that, in a classification paradigm, any time a stimulus is confused with another member of its own category, it would result in a correct classification response. Only between-class stimulus confusions would result in classification errors. Coined the mapping hypothesis 2 by Nosofsky ( 1986 ), the idea is illustrated schemat- ically in  Figure 2.2 . In the figure, we imagine that stimuli 1–4 belong to Category A, whereas stimuli 5–8 belong to Category B. As illustrated in  Figure 2.2 , according to the mapping hypothesis, stimulus 3 would 2 Identification paradigms involve a one-to-one mapping of stimuli onto responses. Classification paradigms involve a many-to-one mapping of stimuli onto responses. The mapping hypothesis allows one to use data from one-to-one mapping paradigms to  predict performance in many-to-one mapping paradigms. \nGCM 25 receive the correct response in the classification paradigm whenever it is confused with stimuli 1–4 in the identification paradigm. More generally, if the mapping hypothesis is correct, then the prob- ability that stimulus i is classified into Category J would be found by summing over the probabilities that it is confused with any member of Category J in the identification paradigm. It is straightforward to show that, if the SCM (Equation  4 ) describes these identification confusion probabilities, then the predicted classification probabilities are given by what is essentially Equation  1 , i.e., the context-model response rule. The main difference is that because the response set has changed, the individual-item response bias parameters are replaced with category- level response-bias parameters. 3 Figure 2.2    Left panel: an 8 × 8 stimulus-response (S-R) confusion matrix for an identification experiment. Right panel: an 8 × 2 S-R confusion matrix for a classification experiment (the same stimuli are used as in the identification task). Stimuli 1–4 are assigned to Category A, and stimuli 5–8 are assigned to Category B. According to the mapping hypothesis, one predicts the probability that stimulus 3 is classified in Category A by summing over the probabilities that stimulus 3 is identified as either stimulus 1, 2, 3, or 4 in the identification task. (From Nosofsky ( 1986 ), published by APA. Reprinted with permission.) 3 A further requirement for the mapping hypothesis to hold is that the γ response-scaling par- ameter be equal to one, which was true in the original formulation of the context model. \nRobert M. Nosofsky 26 Although intuitively compelling, Shepard et al . ( 1961 ) observed sys- tematic failures in using the mapping hypothesis to predict classification confusions on the basis of identification confusions. Although a full dis- cussion goes beyond the scope of this chapter, Nosofsky  ( 1984 ,  1986 ) proposed that the mapping hypothesis not be abandoned completely. The key idea was that the similarity parameters that operate in Equations  1 and  4  may change systematically across the identification and classi- fication paradigms, because of the operation of the selective-attention processes depicted in  Figure 2.1 . Thus, Nosofsky ( 1986 ) proposed that rather than there being a direct mapping relation, a more abstract indir- ect mapping relation may connect identification and classification per- formance. According to this proposal, performance in both paradigms is governed by similarity relations among distinct exemplars, as formalized in Equations  1  and  4 . However, these inter-exemplar similarity relations change systematically across the paradigms because of the operation of selective attention. Nosofsky ( 1984 ,  1986 ,  1987 ) provided strong sup- port for this idea in theoretical and empirical work that investigated rela- tions between identification and classification performance. The motivation for the similarity equations used in the GCM (Equations  2  and  3 ) grows directly from decades of research and theoretical development in the area of similarity. MDS   approaches to modelling similarity have long been among the most general and suc- cessful approaches in the field. Indeed, so successful have been those approaches that some of the discovered regularities have been proposed as candidates for universal laws of psychological generalization (Shepard, 1987 ). In particular, there is a great deal of support for the idea that psychological distance relations among integral-dimension stimuli are well described by embedding the stimuli in multidimensional Euclidean spaces, whereas psychological distance relations among separable- dimension stimuli are better approximated by a city-block distance metric  . Furthermore, Shepard ( 1987 ) reviewed decades of research that point to an approximately exponential relation (Equation  3  with p = 1) between similarity, measured in terms of probability of stimulus gener- alization, and distance in these psychological spaces. In their original formulation of the context model, Medin and Schaffer  ( 1978 ) proposed an inter-dimensional multiplicative rule for computing stimulus similarity. The similarity between stimuli i and j was given by ij m i j m M s s m = = ∏ δ ( , ) , 1 (5) \nGCM 27 where s m (0 ≤ s m ≤ 1) is a freely estimated dimension- m similarity param- eter; δ is an indicator variable set equal to 1 if stimuli i and j mismatch on dimension m , and set equal to zero if they match on dimension m . Thus, the overall similarity between stimuli i and j is simply the product of their similarities along each of their mismatching dimensions. This inter- dimensional multiplicative rule  is a special case of the MDS approach to modelling similarity that is embodied in Equations  2  and  3  of the GCM. In particular, an inter-dimensional multiplicative rule arises whenever p = r in Equations  2  and  3  (see Nosofsky,  1986 , p. 42, for further discus- sion). The particular highly constrained rule used by Medin and Schaffer (Equation  5 ) arises when, in addition, the stimuli vary along a set of independent, binary-valued dimensions. However, as explained above, use of the MDS approach in the GCM allows for a far more general application of the model to diverse classification domains. Because of its combination of assumptions involving an exemplar- based category representation and a non-linear similarity rule  , the GCM is highly sensitive to effects of specific exemplars and to within- class correlational structure in categories (for extensive discussion, see, for example, Medin and Schaffer,  1978 ; Nosofsky,  1992a ). A variety of experiments have been reported for illustrating the importance of such sensitivity. For example, in some studies, category structures have been designed in which an individual stimulus i is more similar than is stimu- lus j to the prototype  (central tendency) of their category; yet, stimulus j has high similarity to specific exemplars of the category, whereas stimu- lus i does not. In such studies, it is often found that subjects classify more accurately the stimulus with high exemplar-specific similarity than the stimulus with high similarity to the prototype (for reviews and examples, see Nosofsky,  1992a ,  2000 ; Nosofsky & Zaki,  2002 ), Exemplar models with non-linear similarity rules (such as the GCM) account naturally for such effects, whereas major alternative models, such as prototype models, do not. Implementation recommendations In the present section I present some practical advice on implement- ing the GCM. The key free parameters that are almost always estimated are the sensitivity parameter ( c ) in Equation  3  and the attention-weight parameters (the w m ) in Equation  2 . An interesting working hypothesis is that, with learning, the participant may adopt a set of ideal-observer weights, for example, a set of weights that would allow the participant to maximize his or her percentage of correct classifications. Thus, it is interesting to compare the best-fitting attention-weight parameters to \nRobert M. Nosofsky 28 the ideal-observer weights. In many situations, the best-fitting weights turn out to approximate the ideal-observer weights (e.g., Nosofsky,  1984 , 1986 ,  1991b ). Unless there are strong experimental manipulations involving dif- ferential category payoffs or frequencies, the response-bias parameters tend not to contribute substantially to the model fits, and can generally be set equal to one. The γ response-scaling parameter (Equation  1 ) is important when modelling performance at the individual-observer level, to allow the model to capture the deterministic response strategies that individual observers sometimes use (e.g., McKinley & Nosofsky,  1995 ; Nosofsky & Zaki,  2002 ). Practical experience suggests, however, that in fitting averaged data, γ can be held fixed at one without much loss in predictive accuracy. As discussed in the introduction, in situations involving fairly discrim- inable stimuli, an exponential decay function for relating similarity to distance ( p = 1 in Equation  3 ) is always assumed. For integral-dimension stimuli, a Euclidean metric ( r = 2) is assumed in Equation  2 ; whereas for highly separable-dimension stimuli, a city-block metric ( r = 1) is assumed. By contrast, in situations involving highly confusable percep- tual stimuli, the values p = 2 and r = 2 tend to provide much better fits. (These parameter settings are probably reflecting extensive perceptual noise in the stimulus representations that exists in high-confusability situations – see Ennis ( 1988 ) for further discussion.) The memory-strength  values (Equation  1 ) are generally not treated as free parameters. Instead, they are usually set proportional to the relative frequency with which each stimulus is presented in combination with associated category feedback during the training phase of the experi- ment. In cases in which one models trial-by-trial performance data, a memory-strength value is often attached to each individual exemplar presented on each trial; in this case, the memory strengths are assumed to decay exponentially with lag of presentation of the exemplars (e.g., McKinley & Nosofsky,  1995 ). In general, the GCM is fitted to classification choice-probability data by using a maximum-likelihood criterion, under the assumption that the  distribution of responses into categories is multinomial in form (see Nosofsky,  1989 , for examples). Hierarchically nested versions of the model, in which some parameters are held fixed at default values, can be tested against the full version of the model by using standard likelihood- ratio techniques (see Nosofsky,  1989 , for examples) or alternative meth- ods such as AIC or BIC that penalize models for their number of free parameters. Because analytic solutions for the maximum-likelihood parameters are generally not available except in exceedingly simple cases, \nGCM 29 computer search is used for locating the best-fitting parameters. As is the case for fitting any highly nonlinear model to data, multiple starting configurations should be used in the parameter searches to guard against local minima. The GCM has been a highly successful and widely applied model. In addition, its free parameters are easily interpretable and provide measure- ments of psychological processes of fundamental interest. For example, researchers may be interested in the extent to which different populations of subjects adopt alternative patterns of selective attention  to the dimen- sions that compose a set of stimuli (e.g., Viken et al .,  2002 ). The derived attention-weight parameters from the model provide such information. For these reasons, an important recent development is the availability of a general-purpose computer package for conducting Bayesian analyses of the GCM (Vanpaemel,  2009 ; see also Lee,  2008 ). The package pro- vides estimates of the posterior distributions of the model’s parameters to allow for easy inference and interpretation of the effects of different experimental manipulations on psychological processing. Application to an example An example application of the GCM to a previously published data set (Nosofsky  ,  1989 ) is briefly reviewed in  Figure 2.3  and  Table 2.1 . Because the fits are to averaged data, they should be considered as merely illustra- tive. The stimuli were a set of semicircles with an embedded radial line. The semicircles varied orthogonally in their size (four levels) and in the angle or orientation of the radial line (four levels) to create a 16-member stimulus set. Subjects were tested in four different categorization condi- tions, which are illustrated schematically in  Figure 2.3 . In the figure, the columns of each 4 × 4 grid correspond to the different levels of angle, and the rows correspond to the different levels of size. Cells that are marked in their centre with a boldface 1 represent training exemplars of Category 1, whereas cells marked with a 2 were training exemplars of Category 2. Unmarked cells were unassigned transfer stimuli. During an initial training phase, subjects were presented with only the train- ing exemplars and were provided with trial-by-trial corrective feedback. During a subsequent test phase, subjects were presented with both train- ing and transfer stimuli. Feedback continued to be provided on trials in which training exemplars were presented, but was withheld on trials in which unassigned transfer stimuli were presented. As can be seen in the figure, to provide evidence of generality, a var- iety of different category structures were tested. The Size and Angle cat- egorizations provide examples of ‘unidimensional’ category structures  , in \nRobert M. Nosofsky 30 Table 2.1 Maximum-likelihood parameters and summary fi ts for full and restricted versions of the exemplar-similarity categorization model (reprinted from Nosofsky,  1989 ) Parameters Fits Condition Model c w 1 b 1 SSE %Var –In L Size Unconstrained 1.60 0.10 0.50 0.015 99.4 40.8 Equal attention 2.38 (0.50) 0.49 0.077 97.0 72.0 Equal bias 160 0.10 (0.50) 0.015 99.4 40.8 Angle Unconstrained 3.20 0.98 0.43 0.010 99.6 44.3 Equal attention 3.57 (0.50) 0.45 0.305 86.4 164.3 Equal bias 3.09 1.00 (0.50) 0.029 98.7 56.8 Criss-cross Unconstrained 1.62 0.80 0.45 0.025 95.2 47.7 Equal attention 1.23 (0.50) 0.45 0.087 83.1 64.6 Equal bias 3.00 0.93 (0.50) 0.046 91.1 56.7 Diagonal Unconstrained 2.42 0.81 0.49 0.023 98.4 483 Equal attention 1.81 (0.50) 0.48 0.217 85.0 109.4 Equal bias 2.42 81 (0.50) 0.021 98.6 49.1 Values in parentheses were constrained a priori. The parameter w 1 gives the attention weight for Angle, and 1 – w 1 the attention weight for Size. SSE is the sum of squared deviations between predicted and observed Category 1 probabilities; %Var is the percentage of variance accounted for; ln L is the log likelihood. which one dimension is far more relevant than is the other for purposes of classification. These structures were tested to provide clear evidence of the role of selective attention in classification. Consider, for example, the Angle categorization. Note that stimulus 14 is an unassigned trans- fer stimulus. In terms of ‘overall similarity’  , stimulus 14 is more similar to the exemplars of Category 2 than to the exemplars of Category 1 (it lies extremely close to exemplar 15 of Category 2). However, if subjects attend selectively to the relevant angle dimension, then the space will be stretched along the horizontal angle dimension, and shrunk along the vertical size dimension, rendering stimulus 14 more similar to the exemplars of Category 1. An analogous situation holds for stimulus 9 of the Size categorization. The Criss-Cross categorization  provides an example of a continuous-dimension biconditional structure. Note that the structure is non-linearly separable  , i.e., the members of the contrast- ing categories cannot be separated by drawing a straight line through the space. Whereas various categorization models are applicable only in cases involving linearly separable categories, the GCM applies generally regardless of the form of the category structure. Finally, the Diagonal \nGCM 31 categorization   provides an example of a fairly ‘natural’ two-dimensional category structure in which the exemplars of contrasting categories can be separated by drawing a diagonal line through the space. In a preliminary similarity-scaling study involving the collection of identification confusion data (see Nosofsky,  1989 , for details), a Figure 2.3    Schematic illustration of the four category structures tested by Nosofsky ( 1989 ). Cells marked with a 1 (2) depict training exemplars of Category 1 (2). Unmarked cells depict unassigned transfer stimuli. Top-left value in each cell is the predicted Category-1 response probability from the GCM. Top-right value in each cell is the observed Category-1 response probability. (Reprinted from Nosofsky,  1989 .) \nRobert M. Nosofsky 32 two-dimensional scaling solution was derived for the set of 16 stimuli. Not surprisingly, the derived two-dimensional solution closely reflected the 4 × 4 grid structure of the stimulus set (see Nosofsky,  1989 ,  Figure 2.3 ). This two-dimensional scaling solution was then used in combin- ation with the GCM to predict the classification data obtained in each of the four categorization conditions. The free parameters in the ‘full’ version of the model were the sensitivity parameter c , the dimension-1 attention weight w 1 (with w 2 = 1 − w 1 ), and the Category-1 response- bias parameter b 1 (with b 2 = 1 − b 1 ). All other parameters were set at the default values described previously in this chapter. (Because the stimuli were highly confusable, the Gaussian-Euclidean version of the model was used, with p = 2 and r = 2.) Parameters were estimated for each individual categorization condition that provided a maximum-likelihood fit to the data. In addition, special cases of the model were also fit to the data. In one special case, the attention weights were held fixed at w 1 = w 2 = 0.5; and in a second special case, the bias parameters were held fixed at b 1 = b 2 = 0.5. By comparing the fits of the special-case versions to the full version of the GCM, one gains evidence regarding the importance of the various free parameters. The results of the model fitting are reported in  Figures 2.3  and  2.4 and in  Table 2.1 . In  Figure 2.3 , the top-right value in each cell gives the observed probability with which subjects classified the stimulus into Category 1, whereas the top-left value gives the predicted probability from the GCM. A summary of all of these observed-against-predicted probabilities for all four categorization conditions is provided in the scatterplot in  Figure 2.4 . It can be seen from inspection that the model provides extremely accurate predictions of the data in all four condi- tions. The quantitative summary fits from the model are also reported in  Table 2.1 . Although the criterion of fit was to maximize likelihood (or, equivalently, to minimize the negative log-likelihood), the table also reports the sum-of-squared deviations between the predicted and observed classification probabilities, as well as the percentage of vari- ance accounted for in each condition. Finally, the table reports the fit results for the special-case models described above. Of greatest interest are the results for the special-case version that assumes equal attention to the two stimulus dimensions. In all conditions, this special-case model fits significantly worse than does the full version (see Nosofsky,  1989 , for detailed reports of likelihood- ratio tests that compare the full version to the special cases). These results provide clear evidence of the need to incorporate assumptions about selective attention into the modelling. They can be understood most easily by considering the results for the critical transfer stimuli \nGCM 33 described above. For example, in the Angle categorization  , the equal- attention model predicted that transfer stimulus 14 would be classi- fied into Category 2 with high probability, because it has greater overall similarity to the exemplars of Category 2 than to Category 1. However, the full version of the model, which allows for selective attention to the relevant angle dimension, predicts correctly that transfer stimulus 14 is classified with somewhat higher probability into Category 1. Indeed, the maximum-likelihood estimate for the attention-weight parameter in the Angle categorization is w 1 = 0.98 (see  Table 2.1 ), providing clear evi- dence for the operation of selective attention in this condition. Whereas the role of the selective-attention parameters in the GCM has been emphasized and studied systematically in past research, much less 1.0 .8 DIAGONAL ANGLE SIZE CRISS-CROSS .6 .4 .2 .0 .0 .2 .4 PREDICTED PROBABILITY OBSERVED PROBABILITY .6 .8 1.0 Figure 2.4    Scatterplot of observed against GCM-predicted Category-1 response probabilities for each stimulus in all four categorization conditions. (Reprinted from Nosofsky,  1989 .) \nRobert M. Nosofsky 34 emphasis has been placed on the role of the response-bias parameters. However, in the same way that certain patterns of selective attention may yield better classification performance than others (depending on the category structure), so may certain patterns of response bias yield better performance than others (for examples in situations involving categories with differing variabilities, see Cohen, Nosofsky, & Zaki,  2001 ). Thus, to the extent that human observers are adaptive and adjust parameter settings to achieve task goals, it would not be surprising to find evidence of systematic shifts in patterns of category response bias as a function of experimental conditions. Relations and extensions Although a detailed discussion goes beyond the scope of this chapter, it is important to remark that the GCM is closely related to a variety of rational, Bayesian  , and powerful statistical/machine-learning  models of categorization (e.g., Anderson,  1990 ; Ashby & Alfonso-Reese,  1995 ; Jakel, Scholkopf, & Wichman,  2008 ; Nosofsky,  1990 ,  1991a ; Shepard, 1987 ). Thus, it is interesting to speculate that exemplar-based classifi- cation schemes evolved because they provide highly adaptive and flex- ible solutions to the goals of classification given the structure of natural categories and limits on the computational complexity of the mind (Anderson,  1990 ). Furthermore, perhaps because the GCM builds upon classic past models in the fields of similarity and classification, and it has itself been a highly successful and widely applied model, there have also been numerous extensions of the GCM. In this final section I briefly review some of these extensions. One extension is Kruschke  ’s ( 1992 ) highly influential ALCOVE (attention-learning covering-map) model. In essence, ALCOVE adopts the GCM’s exemplar-based representational assumptions, its MDS- based similarity assumptions, and its assumptions about selective attention, and embeds them within the framework of a connectionist learning network. A key potential advantage is that, whereas the selec- tive-attention weights are free parameters in the GCM, ALCOVE pro- vides a mechanism that learns the attention weights on a trial-by-trial basis. In addition, in standard applications of the GCM, the memory strengths associated with individual exemplars are assumed simply to be proportional to the frequency with which each exemplar is presented in combination with given category feedback. By contrast, in ALCOVE, learning is error driven, and association weights develop between exem- plars and categories that are more intricate and potentially more adap- tive than those allowed by the GCM. \nGCM 35 Whereas the GCM is limited to predicting choice-probability data in classification, other major extensions of the model enable it also to pre- dict classification response times (RTs). Nosofsky  and Palmeri  ’s ( 1997 ) exemplar-based random-walk (EBRW)  model adopts the same representa- tional assumptions as does the GCM. However, classification decision- making is governed by a random-walk process. The random walk is driven by exemplars that are retrieved from memory. Importantly, in a two- category experiment, the GCM response-rule (Equation  1 ) emerges as a special case of the EBRW model, so the EBRW provides a process-level interpretation for the GCM. It also predicts successfully a wide variety of fundamental effects involving classification RT data, including effects of distance-from-boundary, familiarity, practice, and probabilistic feed- back (Nosofsky & Palmeri,  1997 ; Nosofsky & Stanton,  2005 ). Another major extension is Lamberts  ’ ( 2000 ) extended generalized context model for response times (EGCM-RT).  This extension also adopts the GCM’s exemplar-based representational and similarity assumptions. However, it assumes that categorization involves the gradual construction of a per- ceptual representation  through a process of information accumulation. In particular, the perceptual representation of a stimulus is gradually built through a process of stochastic, parallel sampling of the stimulus’s individual dimensions. Classification RTs are determined by the length of time with which the sampling process operates, which in turn is closely related to outputs from the GCM response rule. A long-standing debate in the classification literature has involved a contrast between exemplar and prototype models. Whereas exemplar models   assume that categories are represented in terms of individually stored exemplars, prototype models  assume instead a single summary representation that is the central tendency of the individual exemplars. In  Chapter 3  of this volume, Minda  and Smith  consider prototype mod- els  . They argue that prototype models provide better accounts of data than do exemplar models in a variety of experiments involving visual categories. However, this conclusion has been disputed in numerous studies (see e.g., Busemeyer, Dewey, & Medin,  1984 ; Nosofsky,  2000 ; Nosofsky & Zaki,  2002 ; Palmeri & Flanery,  2002 ; Rehder & Hoffman, 2005 ; Stanton, Nosofsky, & Zaki,  2002 ; Zaki & Nosofsky,  2004 ,  2007 ; Zaki et al .,  2003 ). An intermediate view is that multiple prototypes may be formed to represent a category, where a varying number of exem- plars may be averaged together to form each individual prototype. In the varying abstraction model (VAM)  of Vanpaemel  and Storms  ( 2008 ), all possible multiple-prototype representations are considered, and the version that fits best is taken as the underlying category representation. Although the VAM is far more complex than the GCM, results from \nRobert M. Nosofsky 36 Vanpaemel and Storms suggest that the added complexity is justified in terms one’s ability to accurately predict human classification perform- ance. A closely related alternative idea is the Rex Leopold I model  of De Schryver  , Vandist  , and Rosseel  ( 2009 ). This model posits that categories are not represented in terms of the complete set of experienced exem- plars, but in terms of reduced subsets of the exemplars. Analogous to the VAM, all possible reduced subsets are considered, and the best-fitting subset is taken as the category representation. Another important extension of the GCM is Stewart  and Brown  ’s ( 2005 ) similarity-dissimilarity exemplar model . This model posits that the evidence in favour of a category is not based solely on the similarity of a test item to the exemplars of that category, but on its dissimilarity to the exemplars of contrast categories. This extended model accounts success- fully for the use of difference information and category-contrast effects in classification. A final example is Pothos  and Bailey  ’s ( 2009 ) exten- sion of the GCM to judgments of category intuitiveness and unsuper- vised categorization. This unsupervised GCM operates by computing, for any given partitioning of exemplars into categories, the overall pre- diction error  associated with that partitioning. The category assignment that results in the smallest prediction error is considered to be the most intuitive. Finally, although I have focused on the issue of categorization in this chapter, perhaps the most important achievement of the GCM is that essentially the same model has been used to account for varieties of other fundamental cognitive processes, including individual-item identifica- tion, the development of automaticity, and old-new recognition perform- ance (e.g., Nosofsky,  1986 ,  1987 ,  1988 ,  1991b ; Nosofsky & Zaki,  1998 ; Nosofsky & Stanton,  2006 ; Palmeri,  1997 ). The successful applications of the GCM in these domains, and the use of the model to explain rela- tions between categorization and performance in other fundamental tasks, suggests the possibility of developing unified, exemplar-based the- ories of cognitive representation and processing."
}