{
    "document_type": "online_article",
    "title": "Chapter 8 Model Comparison in Cognitive Science",
    "author": "Riccardo Fusaroli",
    "source": "https://fusaroli.github.io/AdvancedCognitiveModeling2023/model-comparison-in-cognitive-science.html",
    "date_published": "Unknown",
    "flag": "",
    "text": "Cognitive science aims to understand the processes that give rise to human thought and behavior. To do this effectively, we often create formal models that represent our hypotheses about these underlying processes. However, human cognition is complex, and multiple theoretical accounts might plausibly explain the same observed behaviors. This is where model comparison becomes essential.\nModel comparison is the principled evaluation of competing models to determine which best explains observed data. Model comparison techniques as described here balance a model’s ability to fit existing data against its ability to generalize to new observations, helping us avoid the trap of overfitting. But remember, model comparison is not a fail-safe procedure to determine which model embodies the truth, as always we need to be careful, tentative and open about the probabilistic and fallible nature of our inference. After completing this chapter, you will be able to: Implement cross-validation techniques for comparing cognitive models using Stan Calculate and interpret expected log predictive density (ELPD) scores Assess model predictions through posterior and prior predictive checks Understand the strengths and limitations of different model comparison approaches Apply these techniques to compare competing cognitive models using real data Model comparison serves multiple purposes in cognitive science: Theory Testing: Different models often represent competing theoretical accounts of cognitive processes. Comparing their fit to data helps evaluate these theories. Parsimony: When multiple models can explain the data, more complex models should only be preferred if they are justified by better predictive performance. Generalization: By assessing how well different models predict new data, we can evaluate their ability to capture general patterns rather than just fitting to specific samples. Individual Differences: Model comparison can reveal whether different individuals or groups are better described by different cognitive strategies. This chapter demonstrates these principles using our matching pennies models as concrete examples. We’ll compare simple random choice models against more sophisticated memory-based approaches, showing how to rigorously evaluate which better explains observed behavior. Imagine having several models of what might be going on and wanting to know which is the best explanation of the data. For example: Model comparison defines a broad range of practices aimed at identifying the best model for a given dataset. What “best” means is, however, a non-trivial question. Ideally, “best” would mean the model describing the mechanism that actually generated the data. However, knowing the truth is a tricky proposition and we need to use proxies. There are many of such proxies in the literature, for instance Bayes Factors (see Nicenboim et al 2023,https://vasishth.github.io/bayescogsci/book/ch-comparison.html). In this course, we rely on predictive performance - this helps combat overfitting, but has limitations we’ll discuss at the end. In other words, this chapter will assess models in terms of their (estimated) ability to predict new (test) data. Remember that predictive performance is a very useful tool, but not a magical solution. It allows us to combat overfitting to the training sample (your model snuggling to your data so much that it fits both signal and noise), but it has key limitations, which we will discuss at the end of the chapter. To learn how to make model comparison, in this chapter, we rely on our usual simulation based approach to ensure that the method is doing what we want. We simulate the behavior of biased agents playing against the memory agents. This provides us with data generated according to two different mechanisms: biased agents and memory agents. We can fit both models separately on each of the two sets of agents, so we can compare the relative performance of the two models: can we identify the true model generating the data (in a setup where truth is known)? This is what is usually called “model recovery” and complements nicely “parameter recovery”. In model recovery we assess whether we can identify the correct model, in parameter recovery we assess whether - once we know the correct model - we can identify the correct parameter values. Let’s get going. In this example, we have: Model 1 (Biased Agent): Makes choices with a consistent 80% bias toward the right option Model 2 (Memory Agent): Adjusts choices based on memory of previous patterns The critical insight is that both models can produce similar-looking data, making it difficult to determine which cognitive process generated the observed behavior by simple visual inspection. Formal model comparison techniques give us principled ways to evaluate which model better explains the data while accounting for model complexity and generalization ability. Cross-validation is a fundamental technique for comparing models based on their predictive performance. The core idea is simple: a good model should not only fit the observed data but also generalize well to new, unseen data. When we fit a model to data, there’s always a risk of overfitting - capturing noise or idiosyncrasies in the particular sample rather than the underlying pattern we care about. [MISSING: A QUICK ILLUSTRATION OF AN EXAMPLE] Cross-validation helps us find the optimal balance between fitting the training data and generalizing to new data. When the datasets are small, as it is often the case in cognitive science, keeping a substantial portion of the data out - substantial enough to be representative of a more general population - is problematic as it risks starving the model of data: there might not be enough data for reliable estimation of the parameter values. This is where the notion of cross-validation comes in: we can split the dataset in k folds, let’s say k = 10. Then each fold is in turn kept aside as validation set, the model is fitted on the other folds, and its predictive performance tested on the validation set. Repeat this operation of each of the folds. This operation ensures that all the data can be used for training as well as for validation, and is in its own terms quite genial. However, this does not mean it is free of shortcomings. First, small validation folds might not be representative of the diversity of true out-of-sample populations - and there is a tendency to set k equal to the number of datapoints (leave-one-out cross validation). Second, there are many ways in which information could leak or contaminate across folds if the pipeline is not very careful (e.g. via data preprocessing scaling the full dataset, or hyper-parameter estimation). Third, and crucial for our case here, cross validation implies refitting the model k times, which for Bayesian models might be very cumbersome (I once had a model that took 6 weeks to run). The basic idea of cross-validation is to: There are several variations of cross-validation: In k-fold cross-validation, we:\n1. Divide the data into k equally sized subsets (folds)\n2. Use k-1 folds for training and the remaining fold for testing\n3. Repeat k times, each time using a different fold as the test set\n4. Average the k test performance metrics This visualization shows how 5-fold cross-validation works: Leave-one-out is a special case of k-fold cross-validation where k equals the number of data points. In each iteration, we:\n1. Hold out a single observation for testing\n2. Train on all other observations\n3. Repeat for every observation\n4. Average the performance metrics This approach can be very computationally intensive for large datasets or complex models. Cross-validation is especially important in Bayesian modeling for several reasons: However, cross-validation for Bayesian models presents two key challenges: Computational cost: Bayesian models fitted with MCMC can take hours or days to run, making it impractical to refit them k times for cross-validation Proper scoring: We need appropriate metrics for evaluating predictive performance in a Bayesian framework Next, we’ll see how these challenges are addressed. When comparing Bayesian models, we use the expected log predictive density (ELPD) as our metric. This measures how well the model predicts new data points on the log scale. For a single observation, the log predictive density is: \\[\\log p(y_i | y_{-i})\\] where\\(y_i\\)is the observation we’re trying to predict, and\\(y_{-i}\\)represents all other observations that were used for training. For the entire dataset, we sum across all observations: \\[\\text{ELPD} = \\sum_{i=1}^{n} \\log p(y_i | y_{-i})\\] The ELPD has several desirable properties: Computing the ELPD exactly requires fitting the model n times (for n data points), which brings us back to the computational challenge. For complex Bayesian models, true leave-one-out cross-validation (LOO-CV) is often computationally infeasible. Pareto-smoothed importance sampling leave-one-out cross-validation (PSIS-LOO) provides an elegant solution by approximating LOO-CV using just a single model fit. When we fit a Bayesian model, we obtain samples from the posterior distribution\\(p(\\theta|y_1,...,y_n)\\)- the distribution of model parameters given all observations. For LOO-CV, we need\\(p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)\\)- the distribution without the i-th observation. Importance sampling bridges this gap by reweighting the full posterior samples to approximate the LOO posterior. The importance weights for the i-th observation are: \\[w_i(\\theta) = \\frac{p(\\theta|y_1,...,y_{i-1},y_{i+1},...,y_n)}{p(\\theta|y_1,...,y_n)} \\propto \\frac{1}{p(y_i|\\theta)}\\] By Bayes’ theorem, this simplifies to: w_i(θ) ∝ 1 / p(yᵢ|θ) These weights effectively “undo” the influence of the i-th observation on the posterior. However, standard importance sampling can be unstable when:\n* The full posterior and LOO posterior differ substantially\n* Some importance weights become extremely large\n* The variance of the weights is high Pareto smoothing improves the reliability of importance sampling: The diagnostic parameter k from the Pareto fit helps assess reliability:\n* k < 0.5: Reliable estimation\n* 0.5 < k < 0.7: Somewhat reliable, proceed with caution\n* k > 0.7: Unreliable, consider using other methods for this observation These diagnostics help identify problematic observations that might require more attention or alternative methods. The full PSIS-LOO method follows these steps: Fit the Bayesian model once to all available data For each observation i: Calculate raw importance weights using the log-likelihood: w_i(θ) ∝ 1/p(yᵢ|θ) Apply Pareto smoothing to stabilize the largest weights Normalize the smoothed weights Use the weights to compute the expected log predictive density (ELPD) Sum the individual ELPD contributions to get the overall PSIS-LOO estimate [I NEED TO FIND A BETTER WAY TO EXPLAIN AND VISUALIZE!] The red circle represents our “left-out” data point, while the blue line shows the model fit using all data points (including that red circle). The red diamond shows the prediction we get when we actually refit the model without the red circle. When we fit a model (the blue line), each data point “pulls” the model fit toward itself to some degree. The red circle data point influenced the original model to bend slightly closer to it. This is why the red circle appears relatively close to the blue line—it helped shape that line! When we perform true leave-one-out cross-validation, we remove that red circle point completely and refit the model using only the remaining data. Without the “pull” from that point, the model (which we don’t directly show) follows a slightly different path determined solely by the other points. The prediction from this new model (the red diamond) naturally lands in a different position. This difference between the original model prediction and the leave-one-out prediction is exactly what makes cross-validation valuable: It reveals how much individual data points influence your model It gives a more honest assessment of how your model will perform on truly unseen data Large differences can help identify influential or outlier points The purple diamond (PSIS-LOO prediction) attempts to approximate where that red diamond would be without actually refitting the model, by mathematically down-weighting the influence of the left-out point—which is why it’s positioned close to the red diamond if the approximation is working well. Now that we understand the principles, let’s apply these techniques to compare cognitive models using a simulation-based approach. This approach has two key advantages: We’ll simulate data from two different model types: By fitting both models to data generated from each type of agent, we can evaluate whether our model comparison techniques correctly identify the true generating model. Now we need to implement our two competing models in Stan. Both will be multilevel (hierarchical) to account for individual differences among agents. The key feature for model comparison is that we’ll include alog_likcalculation in thegenerated quantitiesblock of each model. When comparing models, we need a way to quantify how well each model explains the observed data. The log-likelihood represents the logarithm of the probability that a model would generate the observed data given specific parameter values. Given certain values for our parameters (let’s say a bias of 0 and beta for memory of 1) and for our variables (let’s say the vector of memory values estimated by the agent on a trial by trial basis), the model will predict a certain distribution of outcomes, that is, a certain distribution of choices (n times right, m times left hand). Comparing this to the actual data, we can identify how likely the model is to produce it. In other words, the probability that the model will actually generate the data we observed out of all its possible outcomes. Remember that we are doing Bayesian statistics, so this probability needs to be combined with the probability of the parameter values given the priors on those parameters. This would give us aposterior likelihoodof the model’s parameter values given the data. The last step is that we need to work on a log scale. Working on a log scale is very useful because it avoids low probabilities (close to 0) being rounded down to exactly 0. By log-transforming the posterior likelihood, we now have thelog-posterior likelihood. Now, remember that our agent’s memory varies on a trial by trial level. In other words, for each data point, for each agent we can calculate separate values of log-posterior likelihood for each of the possible values of the parameters. That is, we can have a distribution of log-posterior likelihood for each data point. Telling Stan to calculate these distributions is straightforward: we add to thegenerated quantitiesblock the same log probability statements used in the model block, but save them to variables instead of adding them to the target. N.B. Some of you might be wandering: if Stan is already using the log-posterior probability in the sampling process, why do we need to tell it to calculate and save it? Fair enough point. But Stan does not save by default (to avoid clogging your computer with endless data) and we need the log posterior likelihood saved as “log_lik” in order to be able to use more automated functions later on. Here’s the Stan model for the biased agent (remember that we will add the log_lik part in the generated quantities block!). Let’s break down this Stan model: Data Block: Defines the input data - number of trials, number of agents, and the choice data matrix.\nParameters Block: Specifies the parameters we want to estimate: thetaM: The population mean bias thetaSD: The population standard deviation of bias theta: Individual bias parameters for each agent Model Block: Defines the prior distributions and likelihood function: Priors for population parameters Individual parameters drawn from the population distribution Likelihood of observing the choice data given the parameters Generated Quantities Block: Calculates additional quantities of interest: Prior and posterior predictive samples Log-likelihood for each observation - this is crucial for model comparison The most important part for model comparison is the log_lik calculation in the generated quantities block. This computes the log probability of each observation given the model and its parameters, which we’ll use for comparing models. Now let’s implement our second model - the memory agent model: The memory agent model is more complex, but follows a similar structure: Data Block: Includes the same data as the biased model, plus the opponent’s choices. Parameters Block: Includes parameters for the memory model: biasM: Population mean baseline bias betaM: Population mean memory sensitivity tau: Population standard deviations z_IDs: Standardized individual parameters L_u: Cholesky factor of correlation matrix Transformed Parameters Block: Calculates derived quantities: memory: The memory state for each agent and trial IDs: Individual parameters for each agent Model Block: Defines priors and likelihood: Priors for population parameters Memory-based choice likelihood Generated Quantities Block: Calculates additional quantities: Prior and posterior predictive samples Log-likelihood for each observation The use of a non-centered parameterization for the individual parameters (through z_IDs and transformation) is a technique to improve sampling efficiency in hierarchical models. This is important when estimating multilevel models with potentially correlated parameters. Now that we’ve defined our models, we’ll fit them to our simulated data and perform model comparison. We’ll fit both models to both types of data: Biased agent model fitted to biased agent data Biased agent model fitted to memory agent data Memory agent model fitted to biased agent data Memory agent model fitted to memory agent data Now that we’ve fit our models, we can use cross-validation techniques to compare them. We’ll start with PSIS-LOO since it’s computationally efficient, and then validate with true k-fold cross-validation. To better understand how our models perform, let’s visualize the pointwise differences in ELPD: It’s quite clear that the bulk of the data a equally well explained by the models. And the differences for the biased data are tricky to see in the plot. Yet, we can see that in memory data there are a non trivial amount of data points better explained by the memory model (the true underlying data-generating mechanism). Now let’s perform formal model comparison using the loo_compare function, which computes the difference in ELPD between models and the standard error of this difference: Here it is clear that formal model comparison can clearly pick the right model. Hurrah! While PSIS-LOO is efficient, we need to check how it relates with true cross-validation. First we create new stan models, which separates data into training and test data and include the ability to calculate log-likelihood for test data. This is crucial for cross-validation, as we need to evaluate the model’s performance on unseen data. These CV-ready models extend our original models with additional structures to handle test data and compute separate log-likelihoods for training and test observations. Now, let’s demonstrate how to implement k-fold cross-validation using these models (N.b. we only fit both models to the memory data, I still need to implement the full comparison against biased data as well) Funnily enough cross-validation indicates the wrong model. While cross-validation and ELPD provide powerful tools for model comparison, it’s important to understand their limitations: Training vs. Transfer Generalization\nCross-validation only assesses a model’s ability to generalize to new data from the same distribution (training generalization). It doesn’t evaluate how well models transfer to different contexts or populations (transfer generalization). Model Misspecification\nAll models are wrong, but some are useful. Cross-validation helps identify which wrong model is most useful for prediction, but doesn’t guarantee we’ve captured the true generating process. Limited Data\nWith limited data, cross-validation estimates can have high variance, especially for complex models. K-fold CV with small k can help mitigate this issue. Computational Cost\nTrue cross-validation requires refitting models multiple times, which can be prohibitively expensive for complex Bayesian models. PSIS-LOO offers an efficient approximation but may not always be reliable. Parameter vs. Predictive Focus\nModel comparison based on predictive performance might select different models than if we were focused on accurate parameter estimation. The “best” model depends on your goals. Compare the models on different subsets of the data (e.g., early vs. late trials). Does the preferred model change depending on which portion of the data you use? Experiment with different priors for the models. How sensitive are the model comparison results to prior choices? Implement a different model (e.g., win-stay-lose-shift) and compare it to the biased and memory models. Which performs best? Explore how the amount of data affects model comparison. How many trials do you need to reliably identify the true model? Investigate the relationship between model complexity and predictive performance in this context. Are there systematic patterns in when simpler models are preferred? However, we need to think carefully about what we mean by “out of sample.” There are actually two distinct types of test sets we might consider: internal and external.\nInternal test sets come from the same data collection effort as our training data - for example, we might randomly set aside 20% of our matching pennies games to test on. While this approach helps us detect overfitting to specific participants or trials, it cannot tell us how well our model generalizes to truly new contexts. Our test set participants were recruited from the same population, played the game under the same conditions, and were influenced by the same experimental setup as our training participants.\nExternal test sets, in contrast, come from genuinely different contexts. For our matching pennies model, this might mean testing on games played: The distinction matters because cognitive models often capture not just universal mental processes, but also specific strategies that people adopt in particular contexts. A model that perfectly predicts behavior in laboratory matching pennies games might fail entirely when applied to high-stakes poker games, even though both involve similar strategic thinking.\nThis raises deeper questions about what kind of generalization we want our models to achieve. Are we trying to build models that capture universal cognitive processes, or are we content with models that work well within specific contexts? The answer affects not just how we evaluate our models, but how we design them in the first place.\nIn practice, truly external test sets are rare in cognitive science - they require additional data collection under different conditions, which is often impractical. This means we must be humble about our claims of generalization. When we talk about a model’s predictive accuracy, we should be clear that we’re usually measuring its ability to generalize within a specific experimental context, not its ability to capture human cognition in all its diversity.\nThis limitation of internal test sets is one reason why cognitive scientists often complement predictive accuracy metrics with other forms of model evaluation, such as testing theoretical predictions on new tasks or examining whether model parameters correlate sensibly with individual differences. These approaches help us build confidence that our models capture meaningful cognitive processes rather than just statistical patterns specific to our experimental setup.\n***"
}