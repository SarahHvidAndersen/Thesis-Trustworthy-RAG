{
    "document_type": "online_article",
    "title": "Chapter 10 Win-Stay-Lose-Shift: A Heuristic Decision Strategy",
    "author": "Riccardo Fusaroli",
    "source": "https://fusaroli.github.io/AdvancedCognitiveModeling2023/win-stay-lose-shift-a-heuristic-decision-strategy.html",
    "date_published": "Unknown",
    "flag": "",
    "text": "Win-Stay-Lose-Shift (WSLS) represents one of the simplest yet most fascinating decision-making strategies observed in both human and animal behavior. The core principle is intuitive: if an action leads to success, repeat it; if it leads to failure, try something else. Despite its simplicity, this strategy can produce sophisticated behavioral patterns and proves surprisingly effective in many scenarios.\nIn this chapter, we’ll explore WSLS through computational modeling, building on our previous work with random agents. We’ll see how this apparently simple strategy can capture important aspects of learning and adaptation. Through careful implementation and testing, we’ll develop insights into both the strengths and limitations of WSLS as a model of decision-making. Our exploration will follow several key steps: Implementing the basic WSLS strategy in code Testing it against different opponents Scaling up to multiple agents Analyzing patterns in the resulting data The WSLS strategy differs significantly from our previous random agent models. Rather than making choices based on fixed probabilities, a WSLS agent: Remembers its previous choice Tracks whether that choice was successful Uses this information to determine its next move This creates an interesting form of path dependence - the agent’s choices are shaped by its history of interactions (and we need to ensure that memory is calculated for the correct trial and applied to the following trial). Let’s begin by loading the packages we’ll need for our analysis: Now we’ll set up a simulation environment where WSLS agents interact with random agents. The parameters we define here will shape our simulation. We’ll create 100 agents who each play 120 trials, allowing us to observe both individual behavior and broader patterns across many interactions. Our WSLS agent implementation uses a parameterized approach where: alpha represents a baseline bias toward choosing one option over another betaWin represents the strength of the “stay” response after a win betaLose represents the strength of the “shift” response after a loss noise allows for occasional random deviations from the strategy This parameterization lets us explore variants of the WSLS strategy with different sensitivities to wins and losses. Next, we’ll define functions to implement our agent strategies: Now let’s generate the simulation data. We’ll simulate each agent playing against a random opponent, and track their choices, wins, and losses: Let’s process our data to create useful variables for analysis and visualization: Now let’s visualize the choice patterns of our agents to get a sense of their behavior: The visualization above shows how the choice patterns evolve over time for both the Random and WSLS agents. The random agents show an approximately stable choice pattern (though with individual biases), while the WSLS agents show more varied patterns as they adapt to their opponents. Let’s also examine how each strategy performs against its opponent: The performance plots reveal an interesting pattern. The WSLS agents generally maintain a winning percentage above 0.5 (the dashed line), indicating they can effectively exploit the biases in the random agents. This demonstrates a key strength of the WSLS strategy - it can adapt to and take advantage of predictable patterns in opponent behavior. Let’s check some key properties of our simulation to ensure it’s working as expected: The plot confirms that our win and lose indicators are mutually exclusive - when the win signal is active (±1), the lose signal is 0, and vice versa. This is important for the proper functioning of our WSLS model. Now let’s check that the WSLS agents are indeed responding to win and lose signals as expected: This plot confirms that our WSLS agents are behaving as expected: The variation in probabilities reflects the individual differences in our agents’ parameters. Let’s first build a model to infer the parameters of a single WSLS agent. This will help us understand the basic mechanics before scaling up to the multilevel model: Now let’s define the Stan model for a single WSLS agent: Now let’s fit the model to our single agent’s data: Let’s examine the parameter estimates and convergence for the single agent model: The parameter recovery for our single agent looks good. The posterior distributions (blue) are centered around the true parameter values (dashed lines), showing that our model can accurately recover the underlying parameters that generated the agent’s behavior. The posteriors are also substantially narrower than the priors (red), indicating that the data is informative. [MISSING A FULL PARAMETER RECOVERY] Now let’s scale up to model all agents simultaneously with a multilevel (hierarchical) model. This allows us to estimate both population-level parameters and individual differences: Now let’s define the multilevel Stan model: Now let’s fit the multilevel model to all agents: Let’s examine convergence diagnostics to ensure our model has estimated the parameters reliably: The trace plots show the parameter values across iterations. Good mixing (chains overlapping without patterns) indicates convergence. Rank histograms near uniform also suggest good convergence, while U-shaped or inverted-U histograms would indicate poor mixing. Now let’s visualize how our knowledge about the parameters has been updated by the data: This visualization shows how our knowledge about the parameters has been updated by the data. The prior distributions (pink) represent our knowledge before seeing the data, while the posterior distributions (blue) show what we learned after fitting the model to the data. Narrower posteriors centered near the true values indicate that our model effectively learned from the data. One of the key advantages of multilevel modeling is the ability to estimate parameters for individual agents. Let’s extract individual parameters and assess recovery: These scatter plots show how well our model recovers individual-level parameters. Points near the diagonal line (dashed) indicate accurate parameter recovery, while the red regression line shows the overall relationship between true and estimated values. High correlation coefficients suggest good recovery of individual differences. Posterior predictive checks help us assess whether our model can generate data that resembles the observed data: I’ll modify the posterior predictive checks chunk to use the regeneration_simulations flag for better efficiency. This will save the computed results and reload them if they already exist:\nrCopy# Posterior predictive checks with regeneration flag This posterior predictive check compares the observed choice patterns (solid lines) with those predicted by our model (dashed lines) for a few selected agents. Close alignment indicates that our model captures the key patterns in the data well. Finally, let’s compute Leave-One-Out Cross-Validation (LOO-CV) to assess our model’s predictive performance. We’ll also demonstrate how this could be used to compare our WSLS model with a simpler alternative: The LOO-CV computation provides an estimate of the model’s expected predictive accuracy. In a full analysis, we would compare this with alternative models to determine which provides the best balance of fit and generalizability. This visualization shows the distribution of pointwise expected log predictive density (ELPD) values, with the mean indicated by the dashed line. Observations with higher ELPD values are better predicted by our model. A long left tail would suggest some observations are particularly difficult for the model to predict and in a real project we should explore what the issues are."
}