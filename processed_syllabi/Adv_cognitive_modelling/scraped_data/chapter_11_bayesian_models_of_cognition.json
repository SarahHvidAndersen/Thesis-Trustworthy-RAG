{
    "document_type": "online_article",
    "title": "Chapter 11 Bayesian Models of Cognition",
    "author": "Riccardo Fusaroli",
    "source": "https://fusaroli.github.io/AdvancedCognitiveModeling2023/bayesian-models-of-cognition.html",
    "date_published": "Unknown",
    "flag": "",
    "text": "The impressive power of Bayes theorem and Bayesian approaches to modeling has tempted cognitive scientists into exploring how far they could get in thinking the mind and brain as Bayesian machines. The idea is that the mind is a probabilistic machine that updates its beliefs based on the evidence it receives. The human mind constantly receives input from various sources – direct personal experience, social information from others, prior knowledge, and sensory input. A fundamental question in cognitive science is how these disparate pieces of information are combined to produce coherent beliefs about the world.\nThe Bayesian framework offers a powerful approach to modeling this process. Under this framework, the mind is conceptualized as a probabilistic machine that continuously updates its beliefs based on new evidence. This contrasts with rule-based or purely associative models by emphasizing: Representations of uncertainty: Beliefs are represented as probability distributions, not single values Optimal integration: Information is combined according to its reliability Prior knowledge: New evidence is interpreted in light of existing beliefs In this chapter, we will explore how Bayesian integration can be formalized and used to model cognitive processes. We’ll start with simple models that give equal weight to different information sources, then develop more sophisticated models that allow for differential weighting based on source reliability, and finally consider how beliefs might update over time. This chapter is not a comprehensive review of Bayesian cognitive modeling, but rather a practical introduction to the topic. We’ll focus on simple models that illustrate key concepts and provide a foundation for more advanced applications. To go further in your learning check on Bayesian models of cognition check: Ma, W. J., Kording, K. P., & Goldreich, D. (2023). Bayesian models of perception and action: An introduction. MIT press. Griffiths, T. L., Chater, N., & Tenenbaum, J. B. (Eds.). (2024). Bayesian models of cognition: reverse engineering the mind. MIT Press. N. D. Goodman, J. B. Tenenbaum, and The ProbMods Contributors (2016). Probabilistic Models of Cognition (2nd ed.). Retrieved 2025-3-10 fromhttps://probmods.org/ After completing this chapter, you will be able to: Understand the basic principles of Bayesian information integration Implement models that combine multiple sources of information in a principled Bayesian way Fit and evaluate these models using Stan Differentiate between alternative Bayesian updating schemes Apply Bayesian cognitive models to decision-making data In this chapter, we will: Introduce the Bayesian framework for cognitive modeling Implement a simple Bayesian integration model Develop and test a weighted Bayesian model that allows for different source reliability Explore temporal Bayesian updating Extend our models to multilevel structures that capture individual differences Compare alternative Bayesian models and evaluate their cognitive implications Bayesian models of cognition explore the idea that the mind operates according to principles similar to Bayes’ theorem, combining different sources of evidence to form updated beliefs. Most commonly this is framed in terms of prior beliefs being updated with new evidence to form updated posterior beliefs. Formally: P(belief | evidence) ∝ P(evidence | belief) × P(belief) Where: P(belief | evidence) is the posterior belief after observing evidence P(evidence | belief) is the likelihood of observing the evidence given a belief P(belief) is the prior belief before observing evidence In cognitive terms, this means people integrate new information with existing knowledge, giving more weight to reliable information sources and less weight to unreliable ones. Yet, there is nothing mathematically special about the prior and the likelihood. They are just two sources of information that are combined in a way that is consistent with the rules of probability. Any other combination of information sources can be modeled with the same theorem. Note that a more traditional formula for Bayes Theorem would be P(belief | evidence) = [P(evidence | belief) × P(belief)] / P(evidence) where the product of prior and likelihood is normalized by P(evidence) (bringing it back to a probability scale).\nThat’s why we used a ∝ symbol in the formula above, to indicate that we are not considering the normalization constant, and that the posterior is only proportional (and not exactly equal) to the multiplication of the two sources of information. Nevertheless, this is a first useful approximation of the theorem, which we can build on in the rest of the chapter.\n*** To better understand Bayesian updating, let’s create a conceptual diagram: This diagram illustrates the key elements of Bayesian updating: Prior belief (blue dashed line): Our initial uncertainty about a phenomenon, before seeing evidence Likelihood (red dotted line): The pattern of evidence we observe Posterior belief (purple solid line): Our updated belief after combining prior and evidence Notice how the posterior distribution: Is narrower than either the prior or likelihood alone (indicating increased certainty) Sits between the prior and likelihood, but closer to the likelihood (as the evidence was fairly strong) Has its peak shifted from the prior toward the likelihood (reflecting belief updating) The bottom diagram shows the algebraic process: we multiply the prior by the likelihood, then normalize to get the posterior belief. Bayesian cognitive models have been successfully applied to a wide range of phenomena: Perception: How we combine multiple sensory cues (visual, auditory, tactile) to form a unified percept Learning: How we update our knowledge from observation and instruction Decision-making: How we weigh different sources of evidence when making choices Social cognition: How we integrate others’ opinions with our own knowledge Language: How we disambiguate words and sentences based on context Psychopathology: How crucial aspects of conditions like schizophrenia and autism can be understood in terms of atypical Bayesian inference (e.g. atypical weights given to different sources of information, or hyper-precise priors or hyper-precise likelihood). To ground our discussion in a cognitive science context, let’s consider a simplified version of a recent study examining how people with and without schizophrenia integrate information from different sources (Simonsen et al., 2021). In this task, participants needed to guess the color of the next marble drawn from a jar. They received information from two sources: Direct evidence: A small sample of 8 marbles drawn from the jar (e.g., 6 blue and 2 red marbles) Social evidence: The choices and confidence ratings of four other people who had seen their own independent samples from the jar This paradigm allows researchers to examine how individuals integrate their own direct perceptual evidence with socially transmitted information — a fundamental process in human cognition that may be altered in certain clinical conditions and be involved in generating some aspects of their psychopathology. For simplicity, we’ll focus on a binary version where participants must guess whether the next marble will be blue or red, and we’ll examine how they integrate their direct sample with social information from just what one other person has chosen. Further, at every trial the participants are given a new jar with a potentially different proportion of blue and red marbles, so there is no learning involved. In a fully Bayesian approach, participants would: Use direct evidence to form a belief about the proportion of blue marbles in the jar Use social evidence to form another belief about the same proportion Combine these beliefs in a principled way to make their final judgment The beta distribution provides an elegant way to represent beliefs about proportions (like the proportion of blue marbles in a jar): The beta distribution is defined by two parameters, traditionally called α (alpha) and β (beta). These parameters have an intuitive interpretation: you can think of α as the number of “successes” you’ve observed (e.g., blue marbles) plus 1, and β as the number of “failures” (e.g., red marbles) plus 1. So a Beta(1,1) distribution represents a uniform belief - no prior knowledge about the proportion. After observing evidence, you simply add the counts to these parameters: To combine multiple sources of evidence, you simply add all the counts together: If direct evidence gives Beta(7, 3) and social evidence suggests Beta(2, 4) Your combined belief is Beta(7+2, 3+4) = Beta(9, 7) This has its peak at 9/(9+7) = 0.56, reflecting a compromise between the two sources The beauty of this approach is that it automatically weights evidence by its strength (amount of data) and properly represents uncertainty through the width of the distribution. For our marble task, the Bayesian inference process involves: Direct evidence: Observing blue1 blue marbles and red1 red marbles out of total1 trials Social evidence: Inferring blue2 blue marbles and red2 red marbles from social information. If we consider only their choice: red corresponds to the sampling of one red marble; blue corresponds to the sampling of one blue marble. If we consider their confidence, we might try to make this correspond to the marbles the sampled: “Clear blue” might imply 8 blue marbles; maybe blue might imply 6 blue and 2 red marbles; “maybe red” might imply 6 red and 2 blue marbles; “clear red” might imply 8 red marbles. Alternatively we can keep it more uncertain and reduce the assumed sample to 0 blue out of 3, 1 blue out of 3, 2 blue marbles out of 3, or 3 blue marbles out of 3. This intrinsically models the added uncertainty in observing the other’s choice and not their samples. The integrated belief is represented by a posterior beta distribution: Beta(α + blue1 + blue2, β + red1 + red2) Where α and β are prior parameters (typically 1 each for a uniform prior) Final choice (blue or red) depends on whether the expected value of this distribution is above 0.5 Confidence depends on the concentration of the distribution We’ll create a comprehensive set of scenarios by varying both direct evidence (number of blue marbles observed directly) and social evidence (number of blue marbles inferred from social information). Let’s examine how expected proportion and uncertainty vary across different evidence combinations: A few notes about the plot: Evidence integration: The expected proportion of blue marbles (top plot) varies with both direct and social evidence. I would normally expect a non-linear interaction: when direct evidence is ambiguous (e.g., 4 blue out of 8), social evidence should have a stronger effect on the final belief. However, the effect is subtle if any. Evidence Interaction: It may be hard to see, but the influence of social evidence is strongest when direct evidence is ambiguous (around 4 blue marbles) and weakest at the extremes (0 or 8 blue marbles). This reflects the Bayesian property that stronger evidence dominates weaker evidence. Credible intervals: The 95% credible intervals (shaded regions) show our uncertainty about the true proportion. These intervals narrow with more evidence, indicating increased confidence in our estimates. This is better seen in the lower plot than in the upper one. Notice how the variance is highest when direct evidence is ambiguous (around 4 blue marbles) and lowest at the extremes (as they combine congruent evidence from both sources). While the summary statistics give us a high-level view, examining the full posterior distributions provides deeper insight into how evidence is combined. Let’s visualize the complete probability distributions for a selected subset of scenarios: This comprehensive visualization shows how the different probability distributions interact: Prior distribution (gray line): Our initial uniform belief about the proportion of blue marbles. Direct evidence distribution (blue dashed line): Belief based solely on our direct observation of marbles. Notice how it becomes more concentrated with more extreme evidence (e.g., 1 or 7 blue marbles). Social evidence distribution (red dashed line): Belief based solely on social information. This is generally less concentrated than the direct evidence distribution since it’s based on lower evidence (0-3 vs. 0-8). Posterior distribution (purple area): The final belief that results from combining all information sources. Notice how it tends to lie between the direct and social evidence distributions, but is typically narrower than either, reflecting increased certainty from combining information, unless the evidence is in conflict. In real cognitive systems, people often weight information sources differently based on their reliability or relevance. Let’s implement a weighted Bayesian model that allows for differential weighting of evidence sources. Our weighted Bayesian integration model extends the simple model by introducing weight parameters for each information source: Start with prior: Beta(α₀, β₀) Observe direct evidence: k₁ blue marbles out of n₁ total Observe social evidence: k₂ blue marbles out of n₂ total Apply weights: w₁ for direct evidence, w₂ for social evidence Posterior: Beta(α₀ + w₁·k₁ + w₂·k₂, β₀ + w₁·(n₁-k₁) + w₂·(n₂-k₂)) The weights represent the degree to which each information source influences the final belief. A weight of 2.0 means you treat that evidence as if you had observed twice as many marbles as you actually did (as more reliable than what the current evidence would warrant), while a weight of 0.5 means you treat it as half as informative. From a cognitive perspective, they might reflect judgments about reliability, relevance, or attentional focus. Let’s create a comprehensive visualization showing how different weights affect belief formation: The visualization showcases weighted Bayesian integration: First, when both weights (w1 and w2) are low (top left panels), beliefs remain moderate regardless of the evidence values, representing high uncertainty. As weights increase (moving right and down), beliefs become more extreme, showing increased confidence in the integrated evidence. Second, the slope of the lines indicates the relative influence of each source. Steeper slopes (bottom right panels) demonstrate that Source1 has stronger influence on belief when both weights are high, while the spacing between lines shows the impact of Source2. Third, when weights are asymmetric (e.g., high w1 and low w2), the belief is dominated by the source with the higher weight, essentially ignoring evidence from the other source. This illustrates how selective attention to certain evidence sources can be modeled as differential weighting in a Bayesian framework. To further understand how weighted Bayesian integration resolves conflicts between evidence sources, let’s examine two specific conflict scenarios: These visualizations illustrate how different weight combinations resolve conflicts between evidence sources: Decision boundary: The black line represents combinations of weights that lead to equal evidence for red and blue (expected rate = 0.5). Weight combinations above this line lead to a “blue” decision, while those below lead to a “red” decision. Relative evidence strength: The slope of the decision boundary reflects the relative strength of the evidence sources. A steeper slope indicates that direct evidence is stronger relative to social evidence. Individual differences: Different individuals might give different weights to evidence sources, leading to different decisions even when faced with identical evidence. This provides a mechanistic explanation for individual variation in decision-making. Weights effectively scale the relative importance of each source of evidence. A weight of 0 means ignoring that evidence source entirely. A weight of 1 means treating the evidence as observed, at face value. Weights above 1 amplify the evidence, while weights below 1 dampen it. A negative weight would make the agent invert the direction of the evidence (if more evidence for red, they’d tend to pick blue).\nRemember that weights moderate the evidence, so a strong weight doesn’t guarantee a strong influence if the evidence itself is weak. Bayesian integration is not simply the averaging of evidence across sources, because it naturally includes how precise the evidence is (how narrow the distribution). Normally, this would happen when we multiply the distributions involved. The Beta-Binomial model handles this automatically by incorporating sample sizes (the n of marbles). There is something tricky in this model when it comes to confidence. We can say that a belief that the next sample is going to be blue with a 0.8 (average) probability more confident than one with a 0.6 (average) probability. We can also say that a belief that the next sample is going to be blue with a 0.8 (95% CIs 0.5-1) probability is less confident than a belief with a 0.6 (95% CIs 0.55-0.65) probability.\nWe need to keep these two aspects separate. The first one is about the average probability, the second one is about the uncertainty around that average probability.\nIn the code above we only call the second confidence and use entropy of the posterior distribution to quantify it. To prepare for our model fitting, we’ll simulate three distinct agents: Balanced Agent: This agent treats both direct and social evidence at face value, applying equal weights (w_direct = 1.0, w_social = 1.0). This represents an unbiased integration of information. Self-Focused Agent: This agent overweights their own direct evidence (w_direct = 1.5) while underweighting social evidence (w_social = 0.5). This represents someone who trusts their own observations more than information from others. Socially-Influenced Agent: This agent does the opposite, overweighting social evidence (w_social = 2.0) while underweighting their own direct evidence (w_direct = 0.7). This might represent someone who is highly responsive to social information. Let’s generate decisions for these three agents in an experiment exposing them to all possible evidence combinations and visualize how their different weighting strategies affect their beliefs and choices. Now let’s create visualizations to compare how these different agents make decisions based on the same evidence: Our simulation highlights several important aspects of Bayesian evidence integration with different weighting strategies: Evidence Thresholds: The decision boundaries (Visualization 2) clearly show how much evidence each agent requires to switch from choosing red to blue. The Self-Focused agent needs less direct evidence when social evidence supports blue, compared to the Socially-Influenced agent. Influence of Social Evidence: In the first visualization, we can observe how the lines for different social evidence levels are spaced. For the Socially-Influenced agent, these lines are widely spaced, indicating that social evidence strongly affects their beliefs. For the Self-Focused agent, the lines are closer together, showing less impact from social evidence. Confidence Patterns: The third visualization reveals how confidence varies across evidence combinations and agent types. All agents are most confident when evidence is strong and consistent across sources, but they differ in how they handle conflicting evidence. Decision Regions: The Self-Focused agent has a larger region where they choose blue based primarily on direct evidence, while the Socially-Influenced agent has more regions where social evidence can override moderate direct evidence. These patterns highlight the profound impact that evidence weighting can have on decision-making, even when agents are all using the same underlying Bayesian integration mechanism. In the next section, we’ll implement these agents in Stan to perform more sophisticated parameter estimation. Now, let’s define our Stan models to implement: a simple bayesian agent (equivalent to assuming both weights to be 1); and a weighted bayesian agent (explicitly inferring weights for direct and social evidence). Model quality checks are crucial for understanding how well our Bayesian models capture the underlying data-generating process. We’ll use three primary techniques: Prior predictive checks help us understand what our model assumes about the world before seeing any data. They answer the question: “What kind of data would we expect to see if we only used our prior beliefs?” Posterior predictive checks are the same, but after having seen the data. This helps us assess whether the model can generate data that looks similar to our observed data. This visualization shows how our beliefs change after observing data, comparing the prior and posterior distributions for key parameters. In this section, we’ll explore how to compare the simple Bayesian agent (where weights are equal) and the weighted Bayesian agent (where weights can differ) using Leave-One-Out Cross-Validation (LOO-CV). We’ll leverage the models we’ve already fitted to our three simulated agent types: Balanced, Self-Focused, and Socially-Influenced. LOO-CV is a powerful method for model comparison that estimates how well a model will predict new, unseen data. At its core, LOO-CV works by: In a Bayesian context, exact LOO-CV would require refitting our model N times (where N is the number of observations), which is computationally expensive. Instead, we use Pareto-Smoothed Importance Sampling (PSIS-LOO), which approximates LOO-CV from a single model fit. The key insight of PSIS-LOO is that we can use importance sampling to approximate how the posterior would change if an observation were removed: \\[p(\\theta | y_{-i}) \\approx \\frac{p(\\theta | y)}{p(y_i | \\theta)} \\propto \\frac{p(\\theta | y)}{p(y_i | \\theta)}\\] where\\(p(\\theta | y_{-i})\\)is the posterior without observation\\(i\\), and\\(p(\\theta | y)\\)is the full posterior. Let’s apply LOO-CV to compare our models across the three scenarios. Before we compare models, it’s important to check the reliability of our LOO estimates. PSIS-LOO provides diagnostics through the Pareto k values: Now we can compare the models within each scenario: Let’s create informative visualizations to better understand the comparison results: Now let’s take a deeper look at what these LOO comparisons tell us: In the Balanced Agent scenario (where both direct and social evidence are weighted equally), we expect the simple Bayesian model to perform well, since it assumes equal weights by design. If our LOO comparison shows the weighted model doesn’t provide much advantage, this confirms our expectations - the additional complexity of differential weighting isn’t justified when the true process gives equal weight to evidence sources. For the Self-Focused Agent (who overweights direct evidence and underweights social evidence), we expect the weighted Bayesian model to outperform the simple model. If the LOO comparison shows a substantial advantage for the weighted model, it suggests that capturing the differential weighting of evidence is important for predicting this agent’s behavior. Similarly, for the Socially-Influenced Agent (who overweights social evidence), we expect the weighted model to have an advantage. The size of this advantage indicates how crucial it is to account for the specific weighting pattern to understand this agent’s decision-making process. Let’s look at the mathematical foundations of LOO-CV to better understand what’s happening: Log Predictive Density: For each observation\\(i\\), the log predictive density is: \\[\\log p(y_i | y_{-i}) = \\log \\int p(y_i | \\theta) p(\\theta | y_{-i}) d\\theta\\] This represents how well we can predict observation\\(i\\)using a model trained on all other observations. PSIS-LOO Approximation: Since we don’t want to refit our model for each observation, we use importance sampling: \\[\\log p(y_i | y_{-i}) \\approx \\log \\frac{\\sum_{j=1}^S w_i^j p(y_i | \\theta^j)}{\\sum_{j=1}^S w_i^j}\\] where\\(w_i^j \\propto \\frac{1}{p(y_i | \\theta^j)}\\)are importance weights and\\(\\theta^j\\)are samples from the full posterior. Expected Log Predictive Density (ELPD): The overall measure of model predictive accuracy is: \\[\\text{ELPD} = \\sum_{i=1}^N \\log p(y_i | y_{-i})\\] Higher ELPD values indicate better predictive performance. To understand where model differences arise, we can look at the pointwise contributions to LOO: In the previous sections, we explored how individuals integrate direct and social evidence using Bayesian principles. However, our models assumed that all individuals use the same weighting strategy. In reality, people vary in how they weigh different sources of information - some may trust their own observations more, while others may be more influenced by social information. Multilevel (hierarchical) models allow us to capture this individual variation while still leveraging the commonalities across individuals. They offer several advantages: In this section, we’ll develop multilevel versions of both our simple beta-binomial and weighted beta-binomial models. First, let’s simulate a population of agents with varying evidence-weighting parameters: The simulation generates data from two types of agents: Simple Integration Agents: These agents weight direct and social evidence equally, but with varying overall scaling factors. This creates individual differences in how strongly evidence affects beliefs, but without preferential weighting of sources. Weighted Integration Agents: These agents can weight direct and social evidence differently. Some might trust their direct evidence more, others might be more influenced by social information. The key visual difference in their decision patterns is: Simple integration agentsshow parallel curves for different social evidence levels. The spacing between curves is consistent across all levels of direct evidence, indicating equal influence. Weighted integration agentsshow varying spacing between curves. When an agent weights social evidence more heavily, the curves are more separated; when direct evidence is weighted more, the curves converge. By generating data from both models, we can: In the next section, we’ll fit both our multilevel models to this data and compare their performance.. Looking at these visualizations, we can see clear individual differences in how agents integrate evidence: In this section, we implement two multilevel Bayesian models that capture different hypotheses about how individuals integrate evidence from multiple sources. Both models allow for individual differences, but they differ in what aspects of evidence integration can vary across individuals. Our first model implements a cognitively simple integration strategy where all evidence sources are weighted equally (taken at “face value”), but the overall impact of evidence can vary across individuals: Mathematically, this means that for individualj:\n- Direct evidence weight = scaling_factor[j] × 0.5\n- Social evidence weight = scaling_factor[j] × 0.5 This model captures the hypothesis that individuals differ in their overall sensitivity to evidence, but not in how they relatively weight different sources. Some individuals might be more conservative (low scaling factor), requiring more evidence to shift their beliefs, while others might be more responsive to evidence overall (high scaling factor). Our second model implements a more complex integration strategy where both the overall impact of evidence and the relative weighting of different evidence sources can vary across individuals: We parameterize this model using two key parameters for each individualj:\n-scaling_factor[j]: The total weight given to all evidence\n-weight_ratio[j]: The ratio of direct evidence weight to social evidence weight From these, we derive the actual weights:\n- Direct evidence weight = scaling_factor[j] × weight_ratio[j] / (1 + weight_ratio[j])\n- Social evidence weight = scaling_factor[j] / (1 + weight_ratio[j]) This parameterization ensures that the sum of weights equals the scaling factor, while the ratio between weights is determined by the weight ratio. Including individual variation in the scaling factor for the simple model serves several important purposes: Fair Comparison: It ensures that the comparison between models focuses specifically on differential weighting rather than just the presence of individual differences. The key question becomes “Do individuals weight evidence sources differently?” rather than “Do individuals vary in how they use evidence?” Statistical Control: The scaling parameter serves as a statistical control, ensuring that any evidence for differential weighting isn’t just capturing overall differences in evidence sensitivity. Nested Model Structure: It creates a proper nested model relationship - the simple model is a special case of the weighted model where the weight ratio is constrained to be 1.0 (equal weights) for everyone. This approach allows us to conduct a more precise test of our cognitive hypothesis about differential weighting of evidence sources, while accounting for individual differences in overall evidence use that likely exist regardless of weighting strategy. When moving from single-agent to multilevel modeling, we need to extend our Stan code to capture both population-level patterns and individual differences. This transformation requires careful consideration of parameter structure, prior specification, and computational efficiency. Let’s explore how we adapted our single-agent models into multilevel versions. In our single-agent models, we had straightforward parameters like total_weight and weight_prop (for the weighted model) or just a scaling factor (for the simple model). For multilevel modeling, we need to create parameters that vary across individuals while maintaining population coherence. For the simple integration model: Note several key changes: We now have population-level parameters (mu_scaling, sigma_scaling) that describe the distribution from which individual parameters are drawn We use non-centered parameterization with standardized z-scores to improve sampling efficiency We work in log space to ensure positive scaling factors Priors also need to be restructured in a hierarchical fashion: The prior structure now has: This creates a proper hierarchical structure where individual parameters are partially pooled toward the population mean, with the degree of pooling determined by the population variance. The data structure must be modified to associate observations with specific individuals: The key addition is agent_id, which maps each observation to its corresponding agent. This allows us to apply the correct individual-level parameters to each observation. The likelihood must be adapted to use the appropriate individual-level parameters: We now index individual parameters by agent_id[i] to ensure each observation uses the correct agent’s parameters. The non-centered parameterization (using z-scores) is critical for efficient sampling in hierarchical models. When individual parameters are close to the population mean or when population variance is small, direct parameterization can cause the sampler to get stuck in a difficult geometry called the “funnel” problem. By separating the individual effects into standardized z-scores, we create better sampling geometry and improve convergence. This is why we use: instead of directly sampling individual parameters. For parameters that must be positive (like scaling factors), working in log space ensures we maintain proper bounds while allowing the parameter to vary freely on the unconstrained scale: Similarly, for parameters constrained between 0 and 1 (like weight_prop), we use the logit transformation. Now we are ready for the full implementation of our multilevel Bayesian models for evidence integration. Now let’s implement the multilevel weighted beta-binomial model, which allows both population-level estimates of evidence weights and individual variations around these population means. Now that we’ve generated data from both simple and weighted integration strategies, we can fit our two multilevel models to this data. This will allow us to: We’ll fit both models to the full dataset, which contains a mixture of simple and weighted integration agents. This represents a realistic scenario where we don’t know in advance which strategy each individual is using. Now that we’ve fitted both models, let’s examine how well we can recover the true individual parameters. This is a crucial step in validating our models - if we can’t recover the parameters that generated our data, we might need to refine our models or collect more data. Now that we’ve fitted both models, we can formally compare them to see which better explains the observed data. We’ll use Leave-One-Out Cross-Validation (LOO-CV) to estimate each model’s predictive accuracy. In a real application, we wouldn’t know in advance whether individuals use simple or weighted integration strategies. Model comparison helps us determine which cognitive model is more consistent with observed behavior. In real-world learning scenarios, people continuously update their beliefs as they gather new evidence. While our previous models considered decision-making based on static evidence, a more realistic approach is to incorporate sequential updating where beliefs evolve over time. Let’s develop an extension of our Bayesian evidence integration models that captures how agents dynamically update their beliefs across trials. In sequential Bayesian updating, an agent’s posterior belief from one trial becomes the prior for the next trial. This creates a continuous learning process where the agent’s beliefs evolve over time based on observed evidence.\nThe key components of a sequential updating model are: Initial prior belief - The agent’s belief before encountering any evidence Trial-by-trial updating - How beliefs are updated after each new piece of evidence Response mechanism - How updated beliefs translate into observable choices Let’s implement this framework in Stan, starting with the single-agent version and then extending to a multilevel model. Now let’s extend this to a multilevel model that captures individual differences in learning rates and evidence weighting: To test our sequential updating model, we need to generate data that involves a sequence of decisions where beliefs are updated over time. Here’s how we can simulate such data: Now we can fit the models to our simulated data: Real-world learning rarely happens all at once - it’s a dynamic process where our beliefs evolve as we gather new evidence over time. The sequential Bayesian models we’ve developed capture this dynamic learning process by tracking how beliefs are updated from trial to trial. Our sequential updating models build on the static evidence integration models from earlier, but with a crucial difference: beliefs are continuously updated based on new evidence. This creates a recursive structure where: The agent starts with some initial belief (prior) After observing evidence, they update their belief (posterior) This posterior becomes the prior for the next trial The process repeats for each new piece of evidence The key parameters that govern this updating process are: Evidence weights (weight_direct and weight_social): How much influence each type of evidence has Learning rate (alpha): How quickly beliefs change in response to new evidence The learning rate parameter is particularly important - it determines whether an agent is conservative (low learning rate) or responsive (high learning rate) to new information. A learning rate near 1.0 means the agent fully incorporates new evidence, while a rate closer to 0 means the agent makes only small adjustments to beliefs. For each trial t, the agent’s belief is updated according to: α_t = α_t − 1 + λ × (w_d × E_d, t − 1 + w_s × E_s, t − 1) β_t = β_t − 1 + λ × (w_d × (T_d, t−1−E_d, t−1) + w_s × (T_s,t−1−E_s,t−1)) Belief_t = α_t α_t + β_t Where: α_t and β_t are the parameters of the Beta distribution representing the belief at trial t λ is the learning rate w_d and w_s are the weights for direct and social evidence E_{d,t−1} and E_{s,t-1} are the counts of blue marbles/signals in the previous trial T_{d,t-1} and T_{s,t-1} are the total counts of marbles/signals in the previous trial The multilevel extension allows us to model individual differences in learning while still leveraging the commonalities across individuals. This approach: Captures individual learning styles: Some people may learn faster, others may weight certain evidence types more heavily Models population distributions: Helps understand the typical learning patterns and the range of variation\nImproves parameter estimation: Especially for individuals with limited or noisy data The multilevel structure adds substantial complexity to the model implementation, requiring careful handling of: Trial sequences: Each agent has their own sequence of trials and updating process\nParameter correlations: Learning rate might correlate with evidence weighting\nComputational efficiency: Sequential updating creates dependencies that make parallelization challenging Interpreting Model Results\nOur simulation and model fitting reveal several important insights:\nParameter Recovery\nThe model successfully recovers the key cognitive parameters: Evidence weights: How much individuals trust different information sources\nLearning rate: How quickly they update their beliefs This validates that our model can meaningfully measure these cognitive processes from observed choices.\nLearning Style Differences\nThe scatterplot of learning styles shows a two-dimensional space of cognitive strategies: The x-axis represents relative weighting of direct vs. social evidence\nThe y-axis represents learning speed (how quickly beliefs change) This creates a typology of learners: Fast direct learners: Rapidly update based primarily on their own observations\nCautious social learners: Slowly incorporate information, with emphasis on social cues\nBalanced adapters: Moderate learning rate with equal weighting of evidence sources Belief Trajectories\nThe plots of belief trajectories over time reveal how individuals track changing environmental statistics: The shaded regions show the model’s uncertainty about beliefs\nThe comparison with true simulated beliefs validates the model’s ability to recover learning dynamics\nThe background coloring shows how beliefs align with true environmental states (jar probabilities) Parameter Correlations\nThe correlation matrix reveals relationships between cognitive parameters: A negative correlation between learning rate and total evidence weight would suggest compensatory strategies (fast updating with conservative evidence weighting, or slow updating with strong evidence weighting)\nCorrelations between direct and social weights might indicate general trust or skepticism toward evidence"
}