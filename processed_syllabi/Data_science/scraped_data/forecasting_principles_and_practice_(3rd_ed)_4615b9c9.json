{
    "document_type": "online_article",
    "title": "Forecasting: Principles and Practice (3rd ed)",
    "author": "Rob J Hyndman, George Athanasopoulos",
    "source": "https://otexts.com/fpp3/regression-evaluation.html",
    "date_published": "23 March 2025",
    "flag": "",
    "text": "The differences between the observed\\(y\\)values and the corresponding fitted\\(\\hat{y}\\)values are the training-set errors or âresidualsâ defined as,\\[\\begin{align*}\n  e_t &= y_t - \\hat{y}_t \\\\\n      &= y_t - \\hat\\beta_{0} - \\hat\\beta_{1} x_{1,t} - \\hat\\beta_{2} x_{2,t} - \\cdots - \\hat\\beta_{k} x_{k,t}\n\\end{align*}\\]for\\(t=1,\\dots,T\\). Each residual is the unpredictable component of the associated observation. The residuals have some useful properties including the following two:\\[\n  \\sum_{t=1}^{T}{e_t}=0 \\quad\\text{and}\\quad \\sum_{t=1}^{T}{x_{k,t}e_t}=0\\qquad\\text{for all $k$}.\n\\]As a result of these properties, it is clear that the average of the residuals is zero, and that the correlation between the residuals and the observations for the predictor variable is also zero. (This is not necessarily true when the intercept is omitted from the model.) After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied. There are a series of plots that should be produced in order to check different aspects of the fitted model and the underlying assumptions. We will now discuss each of them in turn. With time series data, it is highly likely that the value of a variable observed in the current time period will be similar to its value in the previous period, or even the period before that, and so on. Therefore when fitting a regression model to time series data, it is common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient â there is some information left over which should be accounted for in the model in order to obtain better forecasts. The forecasts from a model with autocorrelated errors are still unbiased, and so they are not âwrongâ, but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals. It is always a good idea to check whether the residuals are normally distributed. As we explained earlier, this is not essential for forecasting, but it does make the calculation of prediction intervals much easier. Using thegg_tsresiduals()function introduced in Section5.3, we can obtain all the useful residual diagnostics mentioned above. Figure 7.8: Analysing the residuals from a regression model for US quarterly consumption. The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate. The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals. The autocorrelation plot shows a significant spike at lag 7, and a significant Ljung-Box test at the 5% level. However, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have any noticeable impact on the forecasts or the prediction intervals. In Chapter10we discuss dynamic regression models used for better capturing information left in the residuals. We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. See Section7.7for a discussion of nonlinear regression. It is also necessary to plot the residuals against any predictors that arenotin the model. If any of these show a pattern, then the corresponding predictor may need to be added to the model (possibly in a nonlinear form). The residuals from the multiple regression model for forecasting US consumption plotted against each predictor in Figure7.9seem to be randomly scattered. Therefore we are satisfied with these in this case. Figure 7.9: Scatterplots of residuals versus each predictor. A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be âheteroscedasticityâ in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required (see Section3.1). Continuing the previous example, Figure7.10shows the residuals plotted against the fitted values. The random scatter suggests the errors are homoscedastic. Figure 7.10: Scatterplots of residuals versus fitted values. Observations that take extreme values compared to the majority of the data are calledoutliers. Observations that have a large influence on the estimated coefficients of a regression model are calledinfluential observations. Usually, influential observations are also outliers that are extreme in the\\(x\\)direction. There are formal methods for detecting outliers and influential observations that are beyond the scope of this textbook. As we suggested at the beginning of Chapter2, becoming familiar with your data prior to performing any analysis is of vital importance. A scatter plot of\\(y\\)against each\\(x\\)is always a useful starting point in regression analysis, and often helps to identify unusual observations. One source of outliers is incorrect data entry. Simple descriptive statistics of your data can identify minima and maxima that are not sensible. If such an observation is identified, and it has been recorded incorrectly, it should be corrected or removed from the sample immediately. Outliers also occur when some observations are simply different. In this case it may not be wise for these observations to be removed. If an observation has been identified as a likely outlier, it is important to study it and analyse the possible reasons behind it. The decision to remove or retain an observation can be a challenging one (especially when outliers are influential observations). It is wise to report results both with and without the removal of such observations. Figure7.11highlights the effect of a single outlier when regressing US consumption on income (the example introduced in Section7.1). In the left panel the outlier is only extreme in the direction of\\(y\\), as the percentage change in consumption has been incorrectly recorded as -4%. The orange line is the regression line fitted to the data which includes the outlier, compared to the black line which is the line fitted to the data without the outlier. In the right panel the outlier now is also extreme in the direction of\\(x\\)with the 4% decrease in consumption corresponding to a 6% increase in income. In this case the outlier is extremely influential as the orange line now deviates substantially from the black line. Figure 7.11: The effect of outliers and influential observations on regression More often than not, time series data are ânon-stationaryâ; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. We will deal with time series stationarity in more detail in Chapter9, but here we need to address the effect that non-stationary data can have on regression models. For example, consider the two variables plotted in Figure7.12. These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea. Figure 7.12: Trending time series data can appear to be related, as shown in this example where air passengers in Australia are regressed against rice production in Guinea. Regressing non-stationary time series can lead to spurious regressions. The output of regressing Australian air passengers on rice production in Guinea is shown in Figure7.13. High\\(R^2\\)and high residual autocorrelation can be signs of spurious regression. Notice these features in the output below. We discuss the issues surrounding non-stationary data and spurious regressions in more detail in Chapter10. Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future. Figure 7.13: Residuals from a spurious regression."
}