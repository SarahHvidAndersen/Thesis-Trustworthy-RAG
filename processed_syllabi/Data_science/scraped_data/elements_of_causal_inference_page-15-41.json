{
    "document_type": "book",
    "title": "Elements of Causal Inference",
    "author": "Peters, Jonas, Dominik Janzing, and Bernhard Schölkopf",
    "source": "raw_syllabi\\master_courses\\Data_science\\pdf_material\\elements_of_causal_inference.pdf",
    "date_published": "2017-09-06",
    "keywords": "Unavailable",
    "flag": "",
    "text": "2 Assumptions for Causal Inference Now that we have encountered the basic components of SCMs, it is a good time to pause and consider some of the assumptions we have seen, as well as what these assumptions imply for the purpose of causal reasoning and learning. A crucial notion in our discussion will be a form of independence , and we can informally introduce it using an optical illusion known as the Beuchet chair. When we see an object such as the one on the left of Figure 2.1, our brain makes the assumption that the object and the mechanism by which the information contained in its light reaches our brain are independent. We can violate this assumption by looking at the object from a very speciﬁc viewpoint. If we do that, perception goes wrong: We perceive the three-dimensional structure of a chair, which in reality is not there. Most of the time, however, the independence assumption does hold. If we look at an object, our brain assumes that the object is independent from our vantage point and the illumination. So there should be no unlikely coincidences, no separate 3D structures lining up in two dimensions, or shadow boundaries coin- ciding with texture boundaries. This is called the generic viewpoint assumption in vision [Freeman, 1994]. The independence assumption is more general than this, though. We will see in Section 2.1 below that the causal generative process is composed of autonomous modules that do not inform or inﬂuence each other. As we shall describe below, this means that while one module’s output may inﬂuence another module’s input, the modules themselves are independent of each other. In the preceding example, while the overall percept is a function of object, light- ing, and viewpoint, the object and the lighting are not affected by us moving about — in other words, some components of the overall causal generative model remain invariant , and we can infer three-dimensional information from this invariance. \nFigure 2.1: The left panel shows a generic view of the (separate) parts comprising a Beuchet chair . The right panel shows the illusory percept of a chair if the parts are viewed from a single, very special vantage point. From this accidental viewpoint , we perceive a chair. (Image courtesy of Markus Elsholz.) This is the basic idea of structure from motion [Ullman, 1979], which plays a cen- tral role in both biological vision and computer vision. 2.1 The Principle of Independent Mechanisms We now describe a simple cause-effect problem and point out several observations. Subsequently, we shall try to provide a uniﬁed view of how these observation relate to each other, arguing that they derive from a common independence principle. Suppose we have estimated the joint density p ( a , t ) of the altitude A and the average annual temperature T of a sample of cities in some country (see Figure 4.6 on page 65). Consider the following ways of expressing p ( a , t ) : p ( a , t ) = p ( a | t ) p ( t ) = p ( t | a ) p ( a ) (2.1) The ﬁrst decomposition describes T and the conditional A | T . It corresponds to a factorization of p ( a , t ) according to the graph T → A . 1 The second decomposition corresponds to a factorization according to A → T (cf. Deﬁnition 6.21). Can we 1 Note that the conditional density p ( a | t ) allows us to compute p ( a , t ) (and thus also p ( a ) ) from \ndecide which of the two structures is the causal one (i.e., in which case would we be able to think of the arrow as causal)? A ﬁrst idea (see Figure 2.2, left) is to consider the effect of interventions . Imag- ine we could change the altitude A of a city by some hypothetical mechanism that raises the grounds on which the city is built. Suppose that we ﬁnd that the average temperature decreases. Let us next imagine that we devise another intervention ex- periment. This time, we do not change the altitude, but instead we build a massive heating system around the city that raises the average temperature by a few de- grees. Suppose we ﬁnd that the altitude of the city is unaffected. Intervening on A has changed T , but intervening on T has not changed A . We would thus reasonably prefer A → T as a description of the causal structure. Why do we ﬁnd this description of the effect of interventions plausible, even though the hypothetical intervention is hard or impossible to carry out in practice? If we change the altitude A , then we assume that the physical mechanism p ( t | a ) responsible for producing an average temperature (e.g., the chemical composition of the atmosphere, the physics of how pressure decreases with altitude, the mete- orological mechanisms of winds) is still in place and leads to a changed T . This would hold true independent of the distribution from which we have sampled the cities, and thus independent of p ( a ) . Austrians may have founded their cities in locations subtly different from those of the Swiss, but the mechanism p ( t | a ) would apply in both cases. 2 If, on the other hand, we change T , then we have a hard time thinking of p ( a | t ) as a mechanism that is still in place — we probably do not believe that such a mechanism exists in the ﬁrst place. Given a set of different city distributions p ( a , t ) , while we could write them all as p ( a | t ) p ( t ) , we would ﬁnd that it is impossible to explain them all using an invariant p ( a | t ) . Our intuition can be rephrased and postulated in two ways: If A → T is the correct causal structure, then (i) it is in principle possible to perform a localized intervention on A , in other words, to change p ( a ) without changing p ( t | a ) , and (ii) p ( a ) and p ( t | a ) are autonomous , modular , or invariant mechanisms or objects in the world. p ( t ) , which may serve to motivate the direction of the arrow in T → A for the time being. This will be made precise in Deﬁnition 6.21. 2 This is an idealized setting — no doubt counterexamples to these general remarks can be con- structed. \nInterestingly, while we started off with a hypothetical intervention experiment to arrive at the causal structure, our reasoning ends up suggesting that actual interven- tions may not be the only way to arrive at causal structures. We may also be able to identify the causal structure by checking, for data sources p ( a , t ) , which of the two decompositions (2.1) leads to autonomous or invariant terms. Sticking with the preceding example, let us denote the joint distributions of altitude and temper- ature in Austria and Switzerland by p ¨o ( a , t ) and p s ( a , t ) , respectively. These may be distinct since Austrians and Swiss founded their cities in different places (i.e., p ¨o ( a ) and p s ( a ) are distinct). The causal factorizations, however, may still use the same conditional, i.e. p ¨o ( a , t ) = p ( t | a ) p ¨o ( a ) and p s ( a , t ) = p ( t | a ) p s ( a ) . We next describe an idea (see Figure 2.2, middle), closely related to the previous example, but different in that it also applies for individual distributions. In the causal factorization p ( a , t ) = p ( t | a ) p ( a ) , we would expect that the conditional density p ( t | a ) (viewed as a function of t and a ) provides no information about the marginal density function p ( a ) . This holds true if p ( t | a ) is a model of a physical mechanism that does not care about what distribution p ( a ) we feed into it. In other words, the mechanism is not inﬂuenced by the ensemble of cities to which we apply it. If, on the other hand, we write p ( a , t ) = p ( a | t ) p ( t ) , then the preceding indepen- dence of cause and mechanism does not apply. Instead, we will notice that to connect the observed p ( t ) and p ( a , t ) , the mechanism p ( a | t ) would need to take a rather peculiar shape constrained by the equation p ( a , t ) = p ( a | t ) p ( t ) . This could be empirically checked, given an ensemble of cities and temperatures. 3 We have already seen several ideas connected to independence, autonomy, and invariance, all of which can inform causal inference. We now turn to a ﬁnal one (see Figure 2.2, right), related to the independence of noise terms and thus best explained when rewriting (2.1) as a distribution entailed by an SCM with graph A → T , realizing the effect T as a noisy function of the cause A , A : = N A , T : = f T ( A , N T ) , where N T and N A are statistically independent noises N T ⊥⊥ N A . Making suitable restrictions on the functional form of f T (see Sections 4.1.3–4.1.6 and 7.1.2) al- lows us to identify which of two causal structures ( A → T or T → A ) has entailed the observed p ( a , t ) (without such restrictions though, we can always realize both 3 We shall formalize this idea in Section 4.1.7. \n(physical) independence of mechanisms Principle 2.1 intervenability, autonomy, modularity, invariance, transfer independence of information contained in mechanisms independence of noises, conditional independence of structures Figure 2.2: The principle of independent mechanisms and its implications for causal infer- ence (Principle 2.1). decompositions (2.1)). Furthermore, in the multivariate setting and under suitable conditions, the assumption of jointly independent noises allows the identiﬁcation of causal structures by conditional independence testing (see Section 7.1.1). We like to view all these observations as closely connected instantiations of a general principle of (physically) independent mechanisms . Principle 2.1 (Independent mechanisms) The causal generative process of a system’s variables is composed of autonomous modules that do not inform or in- ﬂuence each other. In the probabilistic case, this means that the conditional distribution of each variable given its causes (i.e., its mechanism) does not inform or inﬂuence the other conditional distributions. In case we have only two variables, this reduces to an independence between the cause distribution and the mechanism producing the effect distribution. The principle is plausible if we conceive our system as being composed of mod- ules comprising (sets of) variables such that the modules represent physically in- dependent mechanisms of the world. The special case of two variables has been referred to as independence of cause and mechanism (ICM) [Daniuˇsis et al., 2010, Shajarisales et al., 2015]. It is obtained by thinking of the input as the result of a preparation that is done by a mechanism that is independent of the mechanism that turns the input into the output. Before we discuss the principle in depth, we should state that not all systems will satisfy it. For instance, if the mechanisms that an overall system is composed of have been tuned to each other by design or evolution, this independence may be violated. \nWe will presently argue that the principle is sufﬁciently broad to cover the main aspects of causal reasoning and causal learning (see Figure 2.2). Let us address three aspects, corresponding, from left to right, to the three branches of the tree in Figure 2.2. 1. One way to think of these modules is as physical machines that incorporate an input-output behavior. This assumption implies that we can change one mechanism without affecting the others — or, in causal terminology, we can intervene on one mechanism without affecting the others. Changing a mechanism will change its input-output behavior, and thus the inputs other mechanisms downstream might receive, but we are assuming that the phys- ical mechanisms themselves are unaffected by this change. An assumption such as this one is often implicit to justify the possibility of interventions in the ﬁrst place, but one can also view it as a more general basis for causal rea- soning and causal learning. If a system allows such localized interventions, there is no physical pathway that would connect the mechanisms to each other in a directed way by “meta-mechanisms.” The latter makes it plausi- ble that we can also expect a tendency for mechanisms to remain invariant with respect to changes within the system under consideration and possibly also to some changes stemming from outside the system (see Section 7.1.6). This kind of autonomy of mechanisms can be expected to help with trans- fer of knowledge learned in one domain to a related one where some of the modules coincide with the source domain (see Sections 5.2 and 8.3). 2. While the discussion of the ﬁrst aspect focused on the physical aspect of independence and its ramiﬁcations, there is also an information theoretic as- pect that is implied by the above. A time evolution involving several coupled objects and mechanisms can generate statistical dependence. This is related to our discussion from page 10, where we considered the dependence be- tween the class label and the image of a handwritten digit. Similarly, mech- anisms that are physically coupled will tend to generate information that can be quantiﬁed in terms of statistical or algorithmic information measures (see Sections 4.1.9 and 6.10 below). Here, it is important to distinguish between two levels of information: ob- viously, an effect contains information about its cause, but — according to the independence principle — the mechanism that generates the effect from its cause contains no information about the mechanism generating the cause. For a causal structure with more than two nodes, the independence princi- \nple states that the mechanism generating every node from its direct causes contain no information about each other. 4 3. Finally, we should discuss how the assumption of independent noise terms, commonly made in structural equation modeling, is connected to the princi- ple of independent mechanism. This connection is less obvious. To this end, consider a variable E : = f ( C , N ) where the noise N is discrete. For each value s taken by N , the assignment E : = f ( C , N ) reduces to a deterministic mechanism E : = f s ( C ) that turns an input C into an output E . Effectively, this means that the noise randomly chooses between a number of mecha- nisms f s (where the number equals the cardinality of the range of the noise variable N ). Now suppose the noise variables for two mechanisms at the vertices X j and X k were statistically dependent. 5 Such a dependence could ensure, for instance, that whenever one mechanism f s j is active at node j , we know which mechanism f t k is active at node k . This would violate our principle of independent mechanisms. The preceding paragraph uses the somewhat extreme view of noise vari- ables as selectors between mechanisms (see also Section 3.4). In practice, the role of the noise might be less pronounced. For instance, if the noise is additive (i.e., E : = f ( C ) + N ), then its inﬂuence on the mechanism is re- stricted. In this case, it can only shift the output of the mechanism up or down, so it selects between a set of mechanisms that are very similar to each other. This is consistent with a view of the noise variables as variables out- side the system that we are trying to describe, representing the fact that a system can never be totally isolated from its environment. In such a view, one would think that a weak dependence of noises may be possible without invalidating the principle of independent mechanisms. All of the above-mentioned aspects of Principle 2.1 may help for the problem of causal learning, in other words, they may provide information about causal struc- tures. It is conceivable, however, that this information may in cases be conﬂicting, depending on which assumptions hold true in any given situation. 4 There is an intuitive relation between this aspect of independence and the one described under 1.: whenever the mechanisms change independently, the change of one mechanism does not provide information on how the others have changed. Despite this overlap, the second independence contains an aspect that is not strictly contained in the ﬁrst one because it is also applicable to a scenario in which none of the mechanisms has changed; for example, it refers also to homogeneous data sets. 5 Although we have so far focused on the two-variable case, we phrase this argument such that it also applies for causal structures with more than two variables. \nFigure 2.3: Early path diagram; dam and sire are the female and male parents of a guinea pig, respectively. The path coefﬁcients capture the importance of a given path, deﬁned as the ratio of the variability of the effect to be found when all causes are constant except the one in question, the variability of which is kept unchanged, to the total variability. (Reproduced from Wright [1920].) 2.2 Historical Notes The idea of autonomy and invariance is deeply engrained in the concept of struc- tural equation models (SEMs) or SCMs. We prefer the latter term, since the term SEM has been used in a number of contexts where the structural assignments are used as algebraic equations rather than assignments. The literature is wide ranging, with overviews provided by Aldrich [1989], Hoover [2008], and Pearl [2009]. An intellectual antecedent to SEMs is the concept of a path model pioneered by Wright [1918, 1920, 1921] (see Figure 2.3). Although Wright was a biolo- gist, SEMs are nowadays most strongly associated with econometrics. Following Hoover [2008], pioneering work on structural econometric models was done in the \n1930s by Jan Tinbergen, and the conceptual foundations of probabilistic econo- metrics were laid in Trgyve Haavelmo’s work [Haavelmo, 1944]. Early economists were trying to conceptualize the fact that unlike correlation, regression has a nat- ural direction. The regression of Y on X leads to a solution that usually is not the inverse of the regression of X on Y . 6 But how would the data then tell us in which direction we should perform the regression? This is a problem of observational equivalence , and it is closely related to a problem econometricians call identiﬁca- tion . A number of early works saw a connection between what made a set of equations or relations structural [Frisch and Waugh, 1933], and properties of invariance and autonomy — according to Aldrich [1989], indeed the central notion in the pioneer- ing work of Frisch et al. [1948]. Here, a structural relation was aiming for more than merely modeling an observed distribution of data — it was trying to capture an underlying structure connecting the variables of the model. At the time, the Cowles Commission was a major economic research institute, instrumental in creating the ﬁeld of econometrics. Its work related causality to the invariance properties of the structural econometric model [Hoover, 2008]. Pearl [2009] credits Marschak’s opening chapter of a 1950 Cowles monograph with the idea that structural equations remain invariant to certain changes in the system [Marschak, 1950]. A crucial distinction emphasized by the Cowles work was the one between endogenous and exogenous variables . Endogeneous variables are those that the modeler tries to understand, while exogenous ones are determined by factors outside the model, and are taken as given. Koopmans [1950] assayed two principles for determining what should be treated as exogeneous. The de- partmental principle considers variables outside of the scope of the discipline as exogeneous (e.g., weather is exogeneous to economics). The (preferred) causal principle calls those variables exogenous that inﬂuence the remaining (endoge- neous) variables, but are (almost) not inﬂuenced thereby. Haavelmo [1943] interpreted structural equations as statements about hypothet- ical controlled experiments. He considered cyclic stochastic equation models and discussed the role of invariance as well as policy interventions. Pearl [2015] gives an appraisal of Haavelmo’s role in the study of policy intervention questions and the development of the ﬁeld of causal inference. In an account of causality in 6 As an aside, while most of the early works were using linear equations only, there have also been attempts to generalize to nonlinear SEMs [Hoover, 2008]. \neconomics and econometrics, Hoover [2008] discusses a system of the form X i : = N i X Y i : = θ X i + N i Y , where the errors N i X , N i Y are i.i.d., and θ is a parameter. He attributes to Simon [1953] the view (which does not require any temporal order) that X i may be re- ferred to as causing Y i since one knows all about X i without knowing about Y i , but not vice versa. The equations also allow us to predict the effect of interventions. Hoover goes on to argue that one can rewrite the system reversing the roles of X i and Y i while retaining the property that the error terms are uncorrelated. 7 He thus points out that we cannot infer the correct causal direction on the basis of a single set of data (“observational equivalence”). Experiments, either controlled or natu- ral, could help us decide. If, for example, an experiment can change the conditional distribution of Y i given X i , without altering the marginal distribution of X i , then it must be that X i causes Y i . Hoover refers to this as Simon’s invariance criterion : the true causal order is the one that is invariant under the right sort of intervention. 8 Hurwicz [1962] argues that an equation system becomes structural by virtue of in- variance to a domain of modiﬁcations. Such a system then bears resemblance to a natural law. Hurwicz recognized that one can use such modiﬁcations to determine structure, and that while structure is necessary for causality, it is not for prediction. Aldrich [1989] provides an account of the role of autonomy in structural equation modeling. He argues that autonomous relations are likely to be more stable than others. He equates Haavelmo’s autonomous variables with what subsequently be- came known as exogeneous variables. Autonomous variables are parameters ﬁxed by external forces, or treated as stochastically independent. 9 Following Aldrich [1989, page 30], “the use of the qualiﬁer autonomous and the phrase forces exter- nal to the sector under consideration suggest that ... the parameters of that model would be invariant to changes in the sectoral parameters.” He also relates invari- ance to a notion termed super-exogeneity [Engle et al., 1983]. While the early proponents of structural equation modeling already had some profound insights in their causal underpinnings, the developments in computer sci- 7 We shall revisit this topic in more detail in Section 4.1.3. 8 We would argue that this may not hold true if interventions are coupled to each other, for exam- ple, to keep the anticausal conditional (which describes the cause, given its effect) invariant. This could be seen as a violation of Principle 2.1 on the level of interventions . We return to this point in Section 2.3.4. 9 This is akin to the independence of noise terms we use in SCMs. \nence initially happened separately. Pearl [2009, p. 104] relates how he and his coworkers started connecting Bayesian networks and structural equation modeling: “It suddenly hit me that the century-old tension between economists and statisti- cians stems from simple semantic confusion: statisticians read structural equa- tions as statements about E [ Y | x ] while economists read them as E [ Y | do ( x )] . This would explain why statisticians claim that structural equations have no meaning and economists retort that statistics has no substance.” Pearl [2009, p. 22] formu- lates the independence principle as follows: “that each parent-child relationship in the network represents a stable and autonomous physical mechanism — in other words, that it is conceivable to change one such relationship without changing the others.” It is noteworthy, and indeed a motivation for writing the present book, that among the different implications of Principle 2.1, shown in Figure 2.2, most of the work using causal Bayesian networks only exploits the independence of noise terms. 10 It leads to a rich structure of conditional independences [Pearl, 2009, Spirtes et al., 2000, Dawid, 1979, Spohn, 1980], ultimately deriving from Reichenbach’s Prin- ciple 1.1. The other aspects of independence received signiﬁcantly less attention [Hausman and Woodward, 1999, Lemeire and Dirkx, 2006], but there is a recent thread of work aiming at formalizing and using them. A major motivation for this has been the cause-effect problem where conditional independence is useless since we have only two variables (see Sections 4.1.2 and 6.10). Janzing and Sch¨olkopf [2010] formalize independence of mechanism in terms of algorithmic information theory (Section 4.1.9). They view the functions in an SCM as representing in- dependent causal mechanisms that persist after manipulating the distribution of inputs or other mechanisms. More speciﬁcally, in the context of causal Bayesian networks, they postulate that the conditional distributions of all nodes given their parents are algorithmically independent. In particular, for the causal Bayesian net- work X → Y , P X and P Y | X contain no algorithmic information about each other — meaning that knowledge of one does not admit a shorter description of the other. The idea that unrelated mechanisms are algorithmically independent follows from the generalization of SCMs from random variables to individual objects where sta- tistical dependences are replaced with algorithmic dependences. Sch¨olkopf et al. [2012, e.g., Section 2.1.1.] discuss the question of robustness with respect to changes in the distribution of the cause (in the two-variable set- 10 Certain Bayesian structure learning methods [Heckerman et al., 1999] can be viewed as imple- menting the independence principle by assigning independent priors to the conditional probabilities of each variable given its causes. \nting), and connect it to problems of machine learning ; see also Chapter 5. Within an SCM, they analyze invariance of either the function or of the noises, for differ- ent learning scenarios (e.g., transfer learning, concept drift). They employ a notion of independence of mechanism and input that subsumes both independence un- der changes and information-theoretic independence (we called this the “overlap” between the ﬁrst and second independence in Figure 2.2 in the discussion of the boxes): “ P E | C contains no information about P C and vice versa; in particular, if P E | C changes at some point in time, there is no reason to believe that P C changes at the same time.” Further links to transfer and related machine learning problems are discussed by Bareinboim and Pearl [2016], Rojas-Carulla et al. [2016], Zhang et al. [2013] and Zhang et al. [2015]. Peters et al. [2016] exploited invariance across envi- ronments for learning parts of the graph structure underlying a multivariate SCM (Section 7.1.6). 2.3 Physical Structure Underlying Causal Models We conclude this chapter with some notes on connections to physics. Readers whose interests are limited to mathematical and statistical structures may prefer to skip this part. 2.3.1 The Role of Time An aspect that is conspicuously missing in Section 2.1 is the role of time. Indeed, physics incorporates causality into its basic laws by excluding causation from fu- ture to past. 11 This does not do away with all problems of causal inference, though. Already Simon [1953] recognized that while time ordering can provide a useful asymmetry, it is asymmetry that is important, not the temporal sequence. Microscopically, the time evolution of both classical systems and quantum me- chanical systems is widely believed to be invertible. This seems to contradict our intuition that the world evolves in a directed way — we believe we would be able to tell if time were to ﬂow backward. The contradiction can be resolved in two ways. In one of them, suppose we have a complexity measure for states [Bennett, 1982, Zurek, 1989], and we start with a state whose complexity is very low. In that 11 More precisely, an event can only inﬂuence events lying in its light cone since no signal can travel faster than the speed of light in a vacuum, according to the theory of relativity. \ncase, time evolution (assuming it is sufﬁciently ergodic) will tend to increase com- plexity. In the other way, we assume that we are considering open systems. Even if the time evolution for a closed system is invertible (e.g., in quantum mechanics, a unitary time evolution), the time evolution of an open subsystem (which interacts with its environment) in the generic case need not be invertible. 2.3.2 Physical Laws An often discussed causal question can be addressed with the following example. The ideal gas law stipulates that pressure p , volume V , amount of substance n , and absolute temperature T satisfy the equation p · V = n · R · T , (2.2) where R is the ideal gas constant. If we, for instance, change the volume V allo- cated to a given amount of gas, then pressure p and/or temperature T will change, and the speciﬁcs will depend on the exact setup of the intervention. If, on the other hand, we change T , then V and/or p will change. If we keep p constant, then we can, at least approximately, construct a cycle involving T and V . So what causes what? It is sometimes argued that such laws show that it does not make sense to talk about causality unless the system is temporal. In the next paragraph, we ar- gue that this is misleading. The gas law (2.2) refers to an equilibrium state of an underlying dynamical system, and writing it as a simple equation does not provide enough information about what interventions are in principle possible and what is their effect. SCMs and their corresponding directed acyclic graphs do provide us with this information, but in the general case of non-equilibrium systems, it is a hard problem whether and how a given dynamical systems leads to an SCM. 2.3.3 Cyclic Assignments We think of SCMs as abstractions of underlying processes that take place in time. For these underlying processes, there is no problem with feedback loops, since at a sufﬁciently fast time scale, those loops will be unfolded in time, assuming there are no instantaneous interactions, which are arguably excluded by the ﬁniteness of the speed of light. Even though the time-dependent processes do not have cycles, it is possible that an SCM derived from such processes (for instance, by methods mentioned below in Remarks 6.5 and 6.7), involving only quantities that no longer depend on time, does have cycles. It becomes a little harder to deﬁne general interventions in such \nsystems, but certain types of interventions should still be doable. For instance, a hard intervention where we set the value of one variable to a ﬁxed value may be possible (and realizable physically by a forcing term in an underlying set of differential equations; see Remark 6.7). This cuts the cycle, and we can then derive the entailed intervention distribution. However, it may be impossible to derive an entailed observational distribution from a cyclic set of structural assignments. Let us consider the two assignments X : = f X ( Y , N X ) Y : = f Y ( X , N Y ) and noise variables N X ⊥⊥ N Y . Just like in the case of acyclic models, we consider the noises and functions as given and seek to compute the entailed joint distribution of X and Y . To this end, let us start with the ﬁrst assignment X : = f X ( Y , N X ) , and substitute some initial Y into it. This yields an X , which we can then substitute into the other assignment. Suppose we iterate the two assignments and converge to some ﬁxed point. This point would then correspond to a joint distribution of X , Y simultaneously satisfying both structural assignments as equalities of random variables. 12 Note that we have here assumed that the same N X , N Y are used at every step, rather than independent copies thereof. However, such an equilibrium for X , Y need not always exist, and even if it does, it need not be the case that it can be found using the iteration. In the linear case, this has been analyzed by Lacerda et al. [2008] and Hyttinen et al. [2012]; see also Lauritzen and Richardson [2002]. For further details see Remark 6.5. This observation that one may not always be able to get an entailed distribution satisfying two cyclic structural assignments is consistent with the view of SCMs as abstractions of underlying physical processes — abstractions whose domain of va- lidity as causal models is limited. If we want to understand general cyclic systems, it may be unavoidable to study systems of differential equations rather than SCMs. For certain restricted settings, on the other hand, it can still make sense to stay on the phenomenologically more superﬁcial level of SCMs; see, for example, Mooij et al. [2013]. One may speculate that this difﬁculty inherent to SCMs (or SEMs) is part of the reason why the econometrics community started off viewing SEMs as 12 The fact that the assignments are satisﬁed as equalities of random variables means that we are considering an ensemble of systems that differ in the realizations of the noise variables. Each realiza- tion leads to a (possibly different) realization for X , Y , and thus the distribution of the noises implies a distribution over X , Y . \ncausal models, but later on parts of the community decided to forgo this interpre- tation in favor of a view of structural equations as purely algebraic equations. 2.3.4 Feasibility of Interventions We have used the principle of independent mechanisms to motivate interventions that only affect one mechanism (or structural assignment) at a time. While real systems may admit such kind of interventions, there will also be interventions that replace several assignments at the same time. The former type of interventions may be considered more elementary in an intuitive physical sense. If multiple elementary interventions are combined, then this may in principle happen in a way such that they tuned to each other, and we would view this as violating a form of our independence Principle 2.1; see footnote 8 on page 24. One may hope that combined interventions that are “natural” will not violate independence. However, to tell whether an intervention is “natural” in this sense requires knowledge of the causal structure, which we do not have when trying to use such principles to perform causal learning in the ﬁrst place. Ultimately, one can try to resort to physics to assay what is elementary or natural. The questions of which operations on a physical system are elementary plays a crucial role in modern quantum information theory. There, the question is closely related to analyzing the structure of physical interactions. 13 Likewise, we believe that understanding physical mechanisms underlying causal relations may some- times explain why some interventions are natural and others are complex, which essentially deﬁnes the “modules” given by the different structural equations. 2.3.5 Independence of Cause and Mechanism and the Thermodynamic Arrow of Time We provide a discussion as well as a toy model illustrating how the principle of independent mechanisms can be viewed as a principle of physics. To this end, we 13 For the interested reader: A system consisting of n two-level quantum systems is described by the 2 n -dimensional Hilbert space C 2 ⊗···⊗ C 2 . Unitary operators acting on this Hilbert space cor- respond to physical processes. For several such systems, researchers have shown how to implement “basic” unitaries that act on at most two of the n tensor components [Nielsen and Chuang, 2000] and act trivially on the remaining n − 2 ones. Then one can generate any other unitary [DiVincenzo, 1995] approximately by concatenation. Although this is by no means the only possible choice for the set of “basic” unitary operations, the choice seems natural given the structure of physical interactions. \nFigure 2.4: Simple example of the independence of initial state and dynamical law: beam of particles that are scattered at an object. The outgoing particles contain information about the object while the incoming do not. consider the special case of two variables and postulate the following as a special- ization of Principle 2.1: Principle 2.2 (Initial state and dynamical law) If s is the initial state of a phys- ical system and M a map describing the effect of applying the system dynamics for some ﬁxed time, then s and M are independent. Here, we assume that the initial state, by deﬁnition, is a state that has not interacted with the dynamics before. Here, the “initial” state s and “ﬁnal” state M ( s ) are considered as “cause” and “effect.” Accordingly, M is the mechanism relating cause and effect. The last sen- tence of Principle 2.2 requires some explanation to avoid erroneous conclusions. We now discuss its meaning for an intuitive example. Figure 2.4 shows a scenario where the independence of initial state and dynamics is so natural that we take it for granted: a beam of n particles propagating in exactly the same direction are approaching some object, where they are scattered in various directions. The directions of the outgoing particles contain information about the object, while the beam of incoming particles does not contain information about it. The assumption that the particles initially propagate exactly in the same direction can certainly be weakened. Even if there is some disorder in the incoming beam, the outgoing beam can still contain information about the object. Indeed, vision and photography are only possible because photons contain information about the objects at which they were scattered. We can easily time-reverse the scenario by “hand-designing” an incoming beam for which all particles propagate in the same direction after the scattering process. We now argue how to make sense of Principle 2.2 in this case. Certainly, such a \nbeam can only be prepared by a machine or a subject that is aware of the object’s shape and then directs the particles accordingly. As a matter of fact, particles that have never been in contact with the object cannot a priori contain information about it. Then, Principle 2.2 can be maintained if we consider the process of directing the particles as part of the mechanism and reject the idea of calling the state of the hand-designed beam an initial state. Instead, the initial state then refers to the time instant before the particles have been given the ﬁne-tuned momenta. The fact that photographic images show what has happened in the past and not what will happen in the future is among the most evident asymmetries between past and future. The preceding discussion shows that this asymmetry can be seen as an implication of Principle 2.2. The principle thus links asymmetries between cause and effect with asymmetries between past and future that we take for granted. After having explained the relation between Principle 2.1 and the asymmetry between past and future in physics on an informal level, we brieﬂy mention that this link has been made more formally by Janzing et al. [2016] using algorithmic information theory. In the same way as Principle 4.13 formalizes independence of P C and P E | C as algorithmic independence, Principle 2.2 can also be interpreted as algorithmic independence of s and M . Janzing et al. [2016, Theorem 1] show that for any bijective M , Principle 2.2 then implies that the physical entropy of M ( s ) cannot be smaller than the entropy of s (up to an additive constant) provided that one is willing to accept Kolmogorov complexity (see Section 4.1.9) as the right formalization of physical entropy, as proposed by Bennett [1982] and Zurek [1989]. Principle 2.2 thus implies non-decrease of entropy in the sense of the standard arrow of time in physics. \n\n3 Cause-Effect Models The present chapter formalizes some basic concepts of causality for the case where the causal models contain only two variables. Assuming, these two variables are non-trivially related and their dependence is not solely due to a common cause, this constitutes a cause-effect model. We brieﬂy introduce SCMs, interventions, and counterfactuals. All of these concepts are deﬁned again in the context of mul- tivariate causal models (Chapter 6) and we hope that encountering them for two variables ﬁrst makes the ideas more easily accessible. 3.1 Structural Causal Models SCMs constitute an important tool to relate causal and probabilistic statements. Deﬁnition 3.1 (Structural causal models) An SCM C with graph C → E consists of two assignments C : = N C , (3.1) E : = f E ( C , N E ) , (3.2) where N E ⊥⊥ N C , that is, N E is independent of N C . In this model, we call the random variables C the cause and E the effect . Fur- thermore, we call C a direct cause of E , and we refer to C → E as a causal graph . This notation hopefully clariﬁes and coincides with the reader’s intuition when we talk about interventions, for example, in Example 3.2. If we are given both the function f E and the noise distributions P N C and P N E , we can sample data from such a model in the following way: We sample noise values \nN E , N C and then evaluate (3.1) followed by (3.2). The SCM thus entails a joint distribution P C , E over C and E (for a formal proof see Proposition 6.3). 3.2 Interventions As discussed in Section 1.4.2, we are often interested in the system’s behavior under an intervention. The intervened system induces another distribution, which usually differs from the observational distribution. If any type of intervention can lead to an arbitrary change of the system, these two distributions become unrelated and instead of studying the two systems jointly we may consider them as two sep- arate systems. This motivates the idea that after an intervention only parts of the data-generating process change. For example, we may be interested in a situation in which variable E is set to the value 4 (irrespective of the value of C ) without chang- ing the mechanism (3.1) that generates C . That is, we replace the assignment (3.2) by E : = 4. This is called a (hard) intervention and is denoted by do ( E : = 4 ) . The modiﬁed SCM, where (3.2) is replaced, entails a distribution over C that we denote by P do ( E : = 4 ) C or P C ; do ( E : = 4 ) C , where the latter makes explicit that the SCM C was our starting point. The corresponding density is denoted by c 7→ p do ( E : = 4 ) ( c ) or, in slight abuse of notation, p do ( E : = 4 ) ( c ) . 1 However, manipulations can be much more general. For example, the intervention do \u0000 E : = g E ( C )+ ˜ N E \u0001 keeps a functional dependence on C but changes the noise distribution. This is an example of a soft intervention . We can replace either of the two equations. The following example motivates the namings “cause” and “effect”: Example 3.2 (Cause-effect interventions) Suppose that the distribution P C , E is entailed by an SCM C C : = N C E : = 4 · C + N E , (3.3) with N C , N E iid ∼N ( 0 , 1 ) , and graph C → E . Then, P C E = N ( 0 , 17 ) ̸ = N ( 8 , 1 ) = P C ; do ( C : = 2 ) E = P C E | C = 2 ̸ = N ( 12 , 1 ) = P C ; do ( C : = 3 ) E = P C E | C = 3 . 1 In the literature, the notation p ( c | do ( E : = 4 )) is also commonly used. We prefer p do ( E : = 4 ) since interventions are conceptually different from conditioning, and p ( c | do ( E : = 4 )) resembles the usual notation for the latter, p ( c | E = 4 ) . \nIntervening on C changes the distribution of E . But on the other hand, P C ; do ( E : = 2 ) C = N ( 0 , 1 ) = P C C = P C ; do ( E : = 314159265 ) C \u0010 ̸ = P C C | E = 2 \u0011 . (3.4) No matter how strongly we intervene on E , the distribution of C remains what it was before. This model behavior corresponds well to our intuition of C “caus- ing” E : for example, no matter how much we whiten someone’s teeth, this will not have any effect on this person’s smoking habits. (Importantly, the conditional dis- tribution of C given E = 2 is different from the distribution of C after intervening and setting E to 2.) The asymmetry between cause and effect can also be formulated as an indepen- dence statement. When we replace the assignment (3.3) with E : = ˜ N E (think about randomizing E ), we break the dependence between C and E . In P C ; do ( E : = ˜ N E ) C , E we ﬁnd C ⊥⊥ E . This independence does not hold when randomizing C . As long as var [ ˜ N C ] ̸ = 0, we ﬁnd C ̸⊥⊥ E in P C ; do ( C : = ˜ N C ) C , E ; the correlation between C and E remains non-zero. Code Snippet 3.3 The code samples from the SCM described in Example 3.2. 1 set.seed(1) 2 # generates a sample from the distribution entailed by the SCM 3 C <- rnorm(300) 4 E <- 4*C + rnorm(300) 5 c(mean(E), var(E)) 6 # [1] 0.1236532 16.1386767 7 # 8 # generates a sample from the intervention distribution do(C:=2); 9 # this changes the distribution of E 10 C <- rep(2,300) 11 E <- 4*C + rnorm(300) 12 c(mean(E), var(E)) 13 # [1] 7.936917 1.187035 14 # 15 # generates a sample from the intervention distribution do(E:=N ~ ); 16 # this breaks the dependence between C and E 17 C <- rnorm(300) 18 E <- rnorm(300) 19 cor.test(C,E)$p.value 20 # [1] 0.2114492 \n3.3 Counterfactuals Another possible modiﬁcation of an SCM changes all of its noise distributions. Such a change can be induced by observations and allows us to answer counter- factual questions. To illustrate this, imagine the following hypothetical scenario: Example 3.4 (Eye disease) There exists a rather effective treatment for an eye disease. For 99% of all patients, the treatment works and the patient gets cured ( B = 0); if untreated, these patients turn blind within a day ( B = 1). For the remaining 1%, the treatment has the opposite effect and they turn blind ( B = 1) within a day. If untreated, they regain normal vision ( B = 0). Which category a patient belongs to is controlled by a rare condition ( N B = 1) that is unknown to the doctor, whose decision whether to administer the treatment ( T = 1) is thus independent of N B . We write it as a noise variable N T . Assume the underlying SCM C : T : = N T B : = T · N B +( 1 − T ) · ( 1 − N B ) (3.5) with Bernoulli distributed N B ∼ Ber ( 0 . 01 ) ; note that the corresponding causal graph is T → B . Now imagine a speciﬁc patient with poor eyesight comes to the hospital and goes blind ( B = 1) after the doctor administers the treatment ( T = 1). We can now ask the counterfactual question “What would have happened had the doctor admin- istered treatment T = 0 ?” Surprisingly, this can be answered. The observation B = T = 1 implies with (3.5) that for the given patient, we had N B = 1. This, in turn, lets us calculate the effect of do ( T : = 0 ) . To this end, we ﬁrst condition on our observation to update the distribution over the noise variables. As we have seen, conditioned on B = T = 1, the distribution for N B and the one for N T collapses to a point mass on 1, that is, δ 1 . This leads to a modiﬁed SCM: C | B = 1 , T = 1 : T : = 1 B : = T · 1 +( 1 − T ) · ( 1 − 1 ) = T (3.6) Note that we only update the noise distributions; conditioning does not change the structure of the assignments themselves. The idea is that the physical mechanisms are unchanged (in our case, what leads to a cure and what leads to blindness), but we have gleaned knowledge about the previously unknown noise variables for the given patient . \nNext, we calculate the effect of do ( T = 0 ) for this patient: C | B = 1 , T = 1; do ( T : = 0 ) : T : = 0 B : = T (3.7) Clearly, the entailed distribution puts all mass on ( 0 , 0 ) , and hence P C | B = 1 , T = 1; do ( T : = 0 ) ( B = 0 ) = 1 . This means that the patient would thus have been cured ( B = 0) if the doctor had not given him treatment, in other words, do ( T : = 0 ) . Because of P C ; do ( T : = 1 ) ( B = 0 ) = 0 . 99 and P C ; do ( T : = 0 ) ( B = 0 ) = 0 . 01 , however, we can still argue that the doctor acted optimally (according to the avail- able knowledge). Interestingly, Example 3.4 shows that we can use counterfactual statements to falsify the underlying causal model (see Section 6.8). Imagine that the rare con- dition N B can be tested, but the test results take longer than a day. In this case, it is possible that we observe a counterfactual statement that contradicts the mea- surement result for N B . The same argument is given by Pearl [2009, p.220, point (2)]. Since the scientiﬁc content of counterfactuals has been debated extensively, it should be emphasized that the counterfactual statement here is falsiﬁable because the noise variable is not unobservable in principle but only at the moment when the decision of the doctor has to be made. 3.4 Canonical Representation of Structural Causal Models We have discussed two types of causal statements both entailed by SCMs: ﬁrst, the behavior of the system under potential interventions, and second, counterfac- tual statements. To further understand the difference between them, we introduce the following “canonical representation” of an SCM. 2 According to the structural assignment E = f E ( C , N E ) , 2 This representation has been used in the literature in various places, for example, [Pearl, 2009] although we have not found the term “canonical representation.” \nfor each ﬁxed value n E of the noise N E , E is a deterministic function of C : E = f E ( C , n E ) . (3.8) In order words, if C and E attain values in C and E , respectively, then the noise N E switches between different functions from C to E . Without loss of generality, we may therefore assume that N E attains values in the set of functions from C to E , denoted by E C . Using this convention, we can also rewrite (3.8) as E = n E ( C ) , (3.9) and call this the canonical representation of the structural equation relating C and E . Let us now explain why two SCMs with different canonical representations may induce the same interventional probabilities, although they differ in their counter- factual statements. To this end, we restrict the attention to the case where C attains values in the ﬁnite set C = { 1 ,..., k } . Then the set of functions from C to E is given by the k -fold Cartesian product E k : = E ×· · ·×E | {z } k times , where the j th component describes which value E attains for C = j . Accordingly, the distribution P N E is given by a joint distribution on E k whose marginal distri- bution of the j th component determines the conditional P E | C = j . Since C is the cause and E the effect, we have P do ( C : = j ) E = P E | C = j ; in other words, here interven- tional probabilities and observational conditional probabilities coincide. Thus, the interventional causal implications of the SCM are completely determined by the marginal distributions of each component of the vector-valued noise variable N E even though the SCM includes a precise speciﬁcation of P N E , that is, the joint dis- tribution of all components. While the statistical dependences between the compo- nents of the noise variable N E referring to the effect are irrelevant for interventional causal statements, they do matter for counterfactual statements. To see this, let C and E be binary, that is, C = E = { 0 , 1 } . The set of functions from { 0 , 1 } to { 0 , 1 } reads E C = { 0 , 1 , ID , NOT } where 0 , 1 denote the constant functions attaining 0 and 1, respectively, and ID and NOT denote identity and negation, respectively. To construct two different distributions P 1 N E and P 2 N E inducing the same conditional P E | C = 0 , P E | C = 1 , ﬁrst choose the uniform mixture of 0 and 1 and second the uniform mixture of ID and NOT. In both cases, C and E are statistically independent and the distribution of E is unaffected by interventions on C because E remains an un- biased coin toss regardless of C . In the Cartesian product representation, the four \nfunctions read E C = { ( 0 , 0 ) , ( 1 , 1 ) , ( 0 , 1 ) , ( 1 , 0 ) } , the ﬁrst and the second compo- nent denote the images of C = 0 and C = 1, respectively. Obviously, the uniform mixture of ( 0 , 0 ) and ( 1 , 1 ) and the uniform mixture of ( 0 , 1 ) and ( 1 , 0 ) both in- duce the same marginal distributions on the ﬁrst and the second component of the Cartesian product — in agreement with our remark that they induce the same in- tervention distributions. The counterfactual statement “ E would have attained a different value if C had been set to a different one,” however, is true only for the mixture of ID and NOT, but not for the mixture of 0 and 1 . Hence, counterfactual statements depend not only on the marginal distributions of the components of the noise variable N E , but also on the statistical dependences between the Cartesian product components. Note that two formally different SCMs may induce not only the same interven- tional distribution but even imply the same counterfactual statements: Given the assignment E : = f E ( C , N E ) , reparameterizations of N E are obviously irrelevant. More explicitly, we may set E : = ˜ f E ( C , ˜ N E ) = f E ( C , g − 1 ( ˜ N E )) , for some bijection g on the range of N E and redeﬁne the noise variable by ˜ N E : = g ( N E ) . Using the canonical representation (3.9), we got rid of this additional degree of freedom that would have confused this discussion of counterfactuals. 3.5 Problems Problem 3.5 (Sampling from an SCM) Consider the SCM X : = Y 2 + N X (3.10) Y : = N Y (3.11) with N X , N Y iid ∼N ( 0 , 1 ) . Generate an i.i.d. sample of size 200 from the joint distri- bution ( X , Y ) . Problem 3.6 (Conditional distributions) Show that P C C | E = 2 in Equation (3.4) is a Gaussian distribution: C | E = 2 ∼N \u0012 8 17 , σ 2 = 1 17 \u0013 . \n40 Chapter 3. Cause-Effect Models Problem 3.7 (Interventions) Assume that we know that a process either follows the SCM X : = Y + N X Y : = N Y , where N X ∼N ( µ X , σ 2 X ) and N Y ∼N ( µ Y , σ 2 Y ) with unknown µ X , µ Y and σ X , σ Y > 0 , or it follows the SCM X : = M X Y : = X + M Y , where M X ∼N ( ν X , τ 2 X ) and M Y ∼N ( ν Y , τ 2 Y ) with unknown ν X , ν Y and τ X , τ Y > 0 . Is there a single intervention distribution that lets you distinguish between the two SCMs? Problem 3.8 (Cyclic SCMs) We have mentioned that if the assignments inherit a cyclic structure, the SCM does not necessarily induce a unique distribution over the observed variables. Sometimes there is no solution and sometimes it is not unique. a) We ﬁrst look at an example that induces a unique solution. Consider the SCM X : = 2 · Y + N X (3.12) Y : = 2 · X + N Y (3.13) with ( N X , N Y ) ∼ P for an arbitrary distribution P. Compute α , β , γ , δ such that X : = α N X + β N Y Y : = γ N X + δ N Y yields a solution ( X , Y , N X , N Y ) of the SCM; that is, the vector satisﬁes Equa- tions (3.12) and (3.13) . The solution can be seen as a special case of Equa- tion (6.2) . b) Consider the SCM X : = Y + N X Y : = X + N Y \nwith ( N X , N Y ) ∼ P. Show that if P allows for a density with respect to Lebesgue measure and factorizes, that is, N X ⊥⊥ N Y , then there is no solu- tion ( X , Y , N X , N Y ) of the SCM. Furthermore, construct a distribution P, and a vector ( X , Y , N X , N Y ) that solves the SCM."
}