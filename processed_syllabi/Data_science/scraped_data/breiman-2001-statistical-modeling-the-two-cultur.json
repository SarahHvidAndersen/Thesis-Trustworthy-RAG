{
    "document_type": "research_paper",
    "title": "Statistical Modeling: The Two Cultures",
    "author": "Leo Breiman",
    "source": "raw_syllabi\\master_courses\\Data_science\\pdf_material\\Breiman-2001-Statistical-modeling-the-two-cultur.pdf",
    "date_published": "2001-12-19",
    "keywords": "Unavailable",
    "flag": "",
    "text": "y x nature y x linear regression logistic regression Cox model y x unknown decision trees neural nets • Led to irrelevant theory and questionable sci- \n• Kept statisticians from using more suitable • Prevented statisticians from working on excit- \n\n• The conclusions are about the model’s mecha- • If the model is a poor emulation of nature, the y = b 0 + b m x m + ε where the coefﬁcients \u0006 b m \u0007 are to be estimated, ε is N \u0002 0 σ 2 \u0003 and σ 2 is to be estimated. Given that R 2 which was often closer to zero than one and parameters. Besides computing R 2 , nothing else \n\nput y ′ . The closeness of y and y ′ is a measure of \n\ny = 2 \u0004 1 + 3 \u0004 8 x 3 − 0 \u0004 6 x 8 + 83 \u0004 2 x 12 − 2 \u0004 1 x 17 + 3 \u0004 2 x 27 y = − 8 \u0004 9 + 4 \u0004 6 x 5 + 0 \u0004 01 x 6 + 12 \u0004 0 x 15 + 17 \u0004 5 x 21 + 0 \u0004 2 x 22 y = − 76 \u0004 7 + 9 \u0004 3 x 2 + 22 \u0004 0 x 7 − 13 \u0004 2 x 8 + 3 \u0004 4 x 11 + 7 \u0004 2 x 28 \u0004 \nShuttle × 10 3 \n• Accuracy generally requires more complex pre- • Instead of reducing dimensionality, increase it probability estimates p 0 \u0004 \u0004 \u0004 \n p 9 that it represents \noptimal hyperplane support vector x m 1 x m 2 . A hyperplane in the original variables plus y x nature y x neural nets forests support vectors \n• The goal is not interpretability, but accurate – . 5 .5 1.5 2.5 3.5 standardized coefficients 0 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 variables \n– 1 0 0 1 0 2 0 3 0 4 0 5 0 percent increse in error 0 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 variables – 4 – 3 – 2 – 1 0 1 variable 12 0 .2 .4 .6 .8 1 class 1 probability VARIABLE 12 vs PROBABILITY #1 – 3 – 2 – 1 0 1 variable 17 0 .2 .4 .6 .8 1 class 1 probability VARIABLE 17 vs PROBABILITY #1 \n\n\n• Higher predictive accuracy is associated with • Algorithmic models can give better predictive \u0002 y −ˆ y \u0003 2 . Given a set of data (training set) consist- ing of \u0006\u0002 y n x n \u0003 n = 1 2 \u0004 \u0004 \u0004 \n N \u0007 , use it to construct the predictor φ 1 \u0002 x \u0003 . Take another bootstrap sam- predictor φ 2 \u0002 x \u0003 . Continue this way for K steps. In regression, average all of the \u0006 φ k \u0002 x \u0003\u0007 to get the \nwhich has the plurality vote of the \u0006 φ k \u0002 x \u0003\u0007 is the \n\n\n\n• The “prediction culture,” at least around Stan- • Estimation and testing are a form of prediction: • Prediction by itself is only occasionally sufﬁ- \n\ngence is 2 \u0002 E \u000f S \u000e y = 1 \u0010 − E \u000f S \u000e y = 0 \u0010\u0003 2 / \u0002 V \u000f S \u000e y = \n\nrithm that yields a black box, for which x 1 is impor- set of input variables, excluding x 1 and an algorithm this black box, x 1 is unimportant. \n\ninverse F − 1 \u0002 u \u0003 of a distribution function F \u0002 x \u0003 , sity f , F \u0002 Q \u0002 u \u0003\u0003 = u , q \u0002 u \u0003 = Q ′ \u0002 u \u0003 = 1 /f \u0002 Q \u0002 u \u0003\u0003 . function, Q ˜ for a raw estimator from a sample, and Q ˆ for a smooth estimator of the true Q . sity d \u0002 u \u0015 F\n G \u0003 = D ′ \u0002 u \u0015 F\n G \u0003 \u0004 For F\n G continu- ous, deﬁne D \u0002 u \u0015 F\n G \u0003 = G \u0002 F − 1 \u0002 u \u0003\u0003 d \u0002 u \u0015 F\n G \u0003 = g \u0002 F − 1 \u0002 u \u0003\u0003 /f \u0002 F − 1 \u0002 u \u0003\u0003 assuming f \u0002 x \u0003 = 0 implies probability mass functions p F and p G deﬁne (assum- ing G ≪ F ) d \u0002 u \u0015 F\n G \u0003 = p G \u0002 F − 1 \u0002 u \u0003\u0003 /p F \u0002 F − 1 \u0002 u \u0003\u0003 . density d \u0002 t\n u \u0003 = d \u0002 u \u0015 F Y F Y \u000e X = Q X \u0002 t \u0003 \u0003 \u0004 When X\n Y = f X\nY \u0002 Q X \u0002 t \u0003 Q Y \u0002 u \u0003\u0003 /f X \u0002 Q X \u0002 t \u0003\u0003 f Y \u0002 Q Y \u0002 u \u0003\u0003 \u0004 \nThe statistical independence hypothesis F X\nY = F X F Y is equivalent to d \u0002 t\n u \u0003 = 1, all t\n u . A funda- Q Y \u000e X = x \u0002 u \u0003 = Q Y \u0002 D − 1 \u0002 u \u0015 F Y F Y \u000e X = x \u0003\u0003 = Q Y \u0002 s \u0003 u = D \u0002 s \u0015 F Y F Y \u000e X = x \u0003 \u0004 is deﬁned (note F Y is the pooled distribution func- d 1 \u0002 u \u0003 = d \u0002 u \u0015 F Y F Y \u000e X = 1 \u0003 = P \u000f X = 1 \u000e Y = Q Y \u0002 u \u0003\u0010 /P \u000f X = 1 \u0010 \u0004 \u0002 Y 1 \u0004 \u0004 \u0004 \n Y m \u0003 . dimensional comparison densities d \u0002 u 1 \u0004 \u0004 \u0004 \n u m \u0003 to when we have two samples, d 1 \u0002 u 1 \u0004 \u0004 \u0004 \n u m \u0003 to test \u0003 1 0 du 1 \u0004 \u0004 \u0004 \u0003 1 0 du m d \u0002 u 1 \u0004 \u0004 \u0004 \n u m \u0003 d 1 \u0002 u 1 \u0004 \u0004 \u0004 \n u m \u0003 = 1 \u0004 A decile quantile bin B \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 is deﬁned to be the set of observations \u0002 Y 1 \u0004 \u0004 \u0004 \n Y m \u0003 satisfy- ing, for j = 1 \u0004 \u0004 \u0004 \n m\n Q Y j \u0002\u0002 k j − 1 \u0003 / 10 \u0003 < Y j ≤ Q Y j \u0002 k j / 10 \u0003 \u0004 Instead of deciles k/ 10 we could use To test the hypothesis that Y 1 \u0004 \u0004 \u0004 \n Y m are statis- tically independent we form for all k j = 1 \u0004 \u0004 \u0004 10, d \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 = P \u000f Bin \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003\u0010 / P \u000f Bin \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003\u000e independence \u0010 \u0004 d 1 \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 = P \u000f Bin \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003\u000e population I \u0010 / P \u000f Bin \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003\u000e pooled sample \u0010 for all \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 such that the denominator is d 1 \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 = P \u000f X = I \u000e observation from Bin \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003\u0010 /P \u000f X = I \u0010 \u0004 the values d \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 and d 1 \u0002 k 1 \u0004 \u0004 \u0004 \n k m \u0003 . \n“ R 2 = 0 \u0004 34, reasonably large by the standards usual"
}