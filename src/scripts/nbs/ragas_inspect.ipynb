{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c64e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from internal.database_setup.preprocessing import clean_text\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# if we want to see the cache in action, set the logging level to debug\n",
    "#import logging\n",
    "#from ragas.utils import set_logging_level\n",
    "#set_logging_level(\"ragas.cache\", logging.DEBUG)\n",
    "\n",
    "CACHE_DIR = \".cache/ragas\" # or data/.cache/ragas\n",
    "cacher = DiskCacheBackend(cache_dir=CACHE_DIR) # \".cache/ragas\"\n",
    "print(\"Cache entries:\", len(cacher.cache))\n",
    "\n",
    "llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", \n",
    "                                         model_kwargs={\"response_format\": {\"type\": \"json_object\"}}),\n",
    "                                           cache=cacher)\n",
    "    \n",
    "embedder = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3426\n",
      "3426\n"
     ]
    }
   ],
   "source": [
    "# after loading the old KG:\n",
    "kg = KnowledgeGraph.load(\"../output/archive/knowledge_graph_first_run.json\")\n",
    "\n",
    "# retain only the *unfinished* nodes\n",
    "unfinished = [\n",
    "    node for node in kg.nodes\n",
    "    if not all(k in node.properties for k in (\"headlines\", \"summary\", \"summary_embedding\"))\n",
    "]\n",
    "kg.nodes = unfinished\n",
    "\n",
    "print(len(unfinished))\n",
    "print(len(kg.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run the transforms again *only* on those\n",
    "apply_transforms(kg, default_transforms(...))\n",
    "# finally, merge back the newly-finished nodes into your old KG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 3616\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(\"../output/archive/full_knowledge_graph_second_test copy.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91154c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes without a score: 3616\n"
     ]
    }
   ],
   "source": [
    "# pick the property that always gets added by your filter, e.g. \"score\"\n",
    "missing = [\n",
    "    node for node in kg.nodes\n",
    "    if node.get_property(\"score\") is None\n",
    "]\n",
    "print(f\"Nodes without a score: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55bd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding'])\n"
     ]
    }
   ],
   "source": [
    "#print(kg.nodes[0].properties)  \n",
    "# or\n",
    "print(kg.nodes[0].properties.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9af626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Themes on 2948 chunks, Entities on 2919 chunks\n",
      "1f8e1a02-5618-46c2-a8d3-47031a256905 ['Heating patterns', 'Temperature change', 'Measurement patterns', 'Missing data', 'Linear mixed-effects model', 'Lognormal mixed-effects model', \"Newton's Law of Cooling/Heating\", 'Heat diffusion equation', 'Lumped capacitance model', 'Bayesian implementation'] ['Newton’s Law of Cooling/Heating', 'Stan', 'pizza stone', 'gas-fired oven', 'heat diffusion equation', 'heat transfer coefficient', 'temperature', 'time', 'specific heat capacity', 'thermal conductivity']\n"
     ]
    }
   ],
   "source": [
    "# Count how many chunks got themes/entities\n",
    "themes_count = sum(1 for n in kg.nodes if \"themes\" in n.properties)\n",
    "entities_count = sum(1 for n in kg.nodes if \"entities\" in n.properties)\n",
    "print(f\"Themes on {themes_count} chunks, Entities on {entities_count} chunks\")\n",
    "\n",
    "# Inspect a sample chunk\n",
    "sample = next(n for n in kg.nodes if \"themes\" in n.properties)\n",
    "print(sample.id, sample.properties[\"themes\"], sample.properties[\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 272\n",
      "Surviving chunks     : 3161\n",
      "Embedding tasks   : 272\n",
      "Themes tasks      : 2948\n",
      "NER tasks         : 2919\n",
      "Total reported    : 6139\n"
     ]
    }
   ],
   "source": [
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 5240\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 396\n",
      "Surviving chunks     : 4785\n",
      "Embedding tasks   : 396\n",
      "Themes tasks      : 2975\n",
      "NER tasks         : 2764\n",
      "Total reported    : 6135\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"../output/archive/full_knowledge_graph_third_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 4844\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 437\n",
      "Surviving chunks     : 4389\n",
      "Embedding tasks   : 437\n",
      "Themes tasks      : 4205\n",
      "NER tasks         : 4186\n",
      "Total reported    : 8828\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(\"../output/archive/full_knowledge_graph_fourth_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16eba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 4844, relationships: 8052)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kg = KnowledgeGraph.load(\"../output/archive/full_knowledge_graph_fourth_test.json\")\n",
    "kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1466526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#clusters found: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No multi-hop clusters found; generation will be single-hop only",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m clusters = spec.get_node_clusters(kg)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#clusters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clusters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > \u001b[32m0\u001b[39m, \\\n\u001b[32m     11\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mNo multi-hop clusters found; generation will be single-hop only\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: No multi-hop clusters found; generation will be single-hop only"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    ")\n",
    "spec = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "clusters = spec.get_node_clusters(kg)\n",
    "print(f\"#clusters found: {len(clusters)}\")\n",
    "\n",
    "assert len(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > 0, \\\n",
    "       \"No multi-hop clusters found; generation will be single-hop only\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23390f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 4844, relationships: 8052)\n",
      "#valid clusters: 0\n",
      "#nodes without embedding: 4407\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import MultiHopSpecificQuerySynthesizer\n",
    "kg = KnowledgeGraph.load(\"../output/archive/full_knowledge_graph_fourth_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "#print(\"edges by type:\", {r.relation_type for r in kg.relationships})\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "missing_embed = sum(1 for n in kg.nodes if \"summary_embedding\" not in n.properties)\n",
    "print(\"#nodes without embedding:\", missing_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67323031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 255, relationships: 944)\n",
      "#valid clusters: 538\n",
      "❗ Nodes with missing or invalid summaries: 223\n",
      "❗ Nodes missing embeddings: 223\n",
      "✅ Total nodes: 255\n",
      "❗ Content nodes with missing/invalid summary: 223\n",
      "❗ Content nodes missing embeddings: 223\n",
      "✅ Total content nodes: 255\n"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(\"../output/archive/knowledge_graph_pilot_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "\n",
    "broken = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Nodes with missing or invalid summaries: {len(broken)}\")\n",
    "\n",
    "print(f\"❗ Nodes missing embeddings: {len(no_emb)}\")\n",
    "\n",
    "\n",
    "print(f\"✅ Total nodes: {len(kg.nodes)}\")\n",
    "\n",
    "content_nodes = [n for n in kg.nodes if \"page_content\" in n.properties]\n",
    "\n",
    "broken = [n for n in content_nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in content_nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Content nodes with missing/invalid summary: {len(broken)}\")\n",
    "print(f\"❗ Content nodes missing embeddings: {len(no_emb)}\")\n",
    "print(f\"✅ Total content nodes: {len(content_nodes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae34779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 1606, relationships: 20974)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 1500 cluster(s)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 12700 cluster(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m synth \u001b[38;5;129;01min\u001b[39;00m [SingleHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      5\u001b[39m               MultiHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      6\u001b[39m               MultiHopAbstractQuerySynthesizer(generator_llm)]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         count = \u001b[38;5;28mlen\u001b[39m(\u001b[43msynth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_node_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynth.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cluster(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\multi_hop\\abstract.py:47\u001b[39m, in \u001b[36mMultiHopAbstractQuerySynthesizer.get_node_clusters\u001b[39m\u001b[34m(self, knowledge_graph)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_node_clusters\u001b[39m(\u001b[38;5;28mself\u001b[39m, knowledge_graph: KnowledgeGraph) -> t.List[t.Set[Node]]:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     node_clusters = \u001b[43mknowledge_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_indirect_clusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelationship_condition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_property\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummary_similarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdepth_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mfound \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m clusters\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(node_clusters))\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node_clusters\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:306\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters\u001b[39m\u001b[34m(self, relationship_condition, depth_limit)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes:\n\u001b[32m    305\u001b[39m     initial_cluster = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Remove duplicates by converting clusters to frozensets\u001b[39;00m\n\u001b[32m    309\u001b[39m unique_clusters = [\n\u001b[32m    310\u001b[39m     \u001b[38;5;28mset\u001b[39m(cluster) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mfrozenset\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m clusters)\n\u001b[32m    311\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:288\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rel \u001b[38;5;129;01min\u001b[39;00m relationships:\n\u001b[32m    287\u001b[39m     neighbor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m rel.target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[32m    289\u001b[39m         neighbor = rel.target\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    291\u001b[39m         rel.bidirectional\n\u001b[32m    292\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.target == node\n\u001b[32m    293\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster\n\u001b[32m    294\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:82\u001b[39m, in \u001b[36mNode.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mobject\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Node):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id == other.id\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(\"../output/archive/knowledge_graph_split_01_100.json\")\n",
    "print(kg)\n",
    "\n",
    "for synth in [SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopAbstractQuerySynthesizer(generator_llm)]: # could this be failing because i run out of ram? size is 166600 for the succeeding graph\n",
    "    try:\n",
    "        count = len(synth.get_node_clusters(kg))\n",
    "        print(f\"✅ {synth.name} → {count} cluster(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {synth.name} → error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1725c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No clusters found for LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)); skipping.\n"
     ]
    }
   ],
   "source": [
    "query_distribution = []\n",
    "synths = [\n",
    "    SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopAbstractQuerySynthesizer(generator_llm)\n",
    "]\n",
    "\n",
    "for s in synths:\n",
    "    try:\n",
    "        clusters = s.get_node_clusters(kg)\n",
    "        if clusters:\n",
    "            query_distribution.append((s, 1.0))  # weight can be adjusted\n",
    "        else:\n",
    "            print(f\"⚠️ No clusters found for {s.name}; skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Synthesizer {s.name} failed: {e}\")\n",
    "\n",
    "# re-normalise weights\n",
    "total = sum(w for _, w in query_distribution)\n",
    "query_distribution = [(s, w/total) for s, w in query_distribution]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
