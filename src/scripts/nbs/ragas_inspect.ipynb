{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c64e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# if we want to see the cache in action, set the logging level to debug\n",
    "#import logging\n",
    "#from ragas.utils import set_logging_level\n",
    "#set_logging_level(\"ragas.cache\", logging.DEBUG)\n",
    "\n",
    "\n",
    "# Default paths and parameters\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "KG_PATH = Path(\"knowledge_graph_first_run.json\")\n",
    "JSON_OUT = Path(\"testset_fr.json\")\n",
    "CSV_OUT = Path(\"testset_fr.csv\")\n",
    "TESTSET_SIZE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291c4a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>samples</th>\n",
       "      <th>model</th>\n",
       "      <th>settings</th>\n",
       "      <th>uncertainty_method</th>\n",
       "      <th>raw_uncertainty</th>\n",
       "      <th>calibrated_confidence</th>\n",
       "      <th>retrieved_documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have to write an essay for my neuroscience c...</td>\n",
       "      <td>Hi! I'd be happy to help you with some ideas f...</td>\n",
       "      <td>[\"Hi! I'd be happy to help you with some ideas...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.286630</td>\n",
       "      <td>0.278019</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the exam format for the NLP course?</td>\n",
       "      <td>I'm not sure about that, sorry! The context pr...</td>\n",
       "      <td>[\"Since there's no context provided for the NL...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.223334</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But what about the Natural Language Processing...</td>\n",
       "      <td>It seems you're interested in knowing more abo...</td>\n",
       "      <td>[\"I see you're asking about the specific cours...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.266403</td>\n",
       "      <td>0.170762</td>\n",
       "      <td>[{'id': 'Natural_language_processing_manning_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you help me create a python snippet that t...</td>\n",
       "      <td>Based on the context provided from the [Course...</td>\n",
       "      <td>[\"Based on the context provided from the [Cour...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.597852</td>\n",
       "      <td>0.938207</td>\n",
       "      <td>[{'id': 'Methods_3_python_machine_learning_seb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the colours of the Danish flag?</td>\n",
       "      <td>A nice question! As a friendly cognitive scien...</td>\n",
       "      <td>[\"A nice question! As a friendly cognitive sci...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.352888</td>\n",
       "      <td>0.590158</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Generate 5 flashcards covering machine learnin...</td>\n",
       "      <td>I can help with that!\\r\\n\\r\\nHere are five fla...</td>\n",
       "      <td>[\"I can help with that!\\n\\nHere are five flash...</td>\n",
       "      <td>chatui</td>\n",
       "      <td>{\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...</td>\n",
       "      <td>lexical_similarity</td>\n",
       "      <td>-0.284148</td>\n",
       "      <td>0.262163</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  I have to write an essay for my neuroscience c...   \n",
       "1        What is the exam format for the NLP course?   \n",
       "2  But what about the Natural Language Processing...   \n",
       "3  Can you help me create a python snippet that t...   \n",
       "4           What are the colours of the Danish flag?   \n",
       "5  Generate 5 flashcards covering machine learnin...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Hi! I'd be happy to help you with some ideas f...   \n",
       "1  I'm not sure about that, sorry! The context pr...   \n",
       "2  It seems you're interested in knowing more abo...   \n",
       "3  Based on the context provided from the [Course...   \n",
       "4  A nice question! As a friendly cognitive scien...   \n",
       "5  I can help with that!\\r\\n\\r\\nHere are five fla...   \n",
       "\n",
       "                                             samples   model  \\\n",
       "0  [\"Hi! I'd be happy to help you with some ideas...  chatui   \n",
       "1  [\"Since there's no context provided for the NL...  chatui   \n",
       "2  [\"I see you're asking about the specific cours...  chatui   \n",
       "3  [\"Based on the context provided from the [Cour...  chatui   \n",
       "4  [\"A nice question! As a friendly cognitive sci...  chatui   \n",
       "5  [\"I can help with that!\\n\\nHere are five flash...  chatui   \n",
       "\n",
       "                                            settings  uncertainty_method  \\\n",
       "0  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "1  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "2  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "3  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "4  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "5  {\"temperature\": 0.9, \"top_p\": 0.95, \"max_new_t...  lexical_similarity   \n",
       "\n",
       "   raw_uncertainty  calibrated_confidence  \\\n",
       "0        -0.286630               0.278019   \n",
       "1        -0.223334               0.030126   \n",
       "2        -0.266403               0.170762   \n",
       "3        -0.597852               0.938207   \n",
       "4        -0.352888               0.590158   \n",
       "5        -0.284148               0.262163   \n",
       "\n",
       "                                 retrieved_documents  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{'id': 'Natural_language_processing_manning_2...  \n",
       "3  [{'id': 'Methods_3_python_machine_learning_seb...  \n",
       "4                                                 []  \n",
       "5                                                 []  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../output/streamlit_run/mina_experiment.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67c79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3426\n",
      "3426\n"
     ]
    }
   ],
   "source": [
    "# after loading the old KG:\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\knowledge_graph_first_run.json\")\n",
    "\n",
    "# retain only the *unfinished* nodes\n",
    "unfinished = [\n",
    "    node for node in kg.nodes\n",
    "    if not all(k in node.properties for k in (\"headlines\", \"summary\", \"summary_embedding\"))\n",
    "]\n",
    "kg.nodes = unfinished\n",
    "\n",
    "print(len(unfinished))\n",
    "print(len(kg.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run the transforms again *only* on those\n",
    "apply_transforms(kg, default_transforms(...))\n",
    "# finally, merge back the newly-finished nodes into your old KG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ragas imports\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Text cleaning\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "# Ensure your OpenAI key is set\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "CLEANED_DOCS_PATH = Path(\"cleaned_docs.pkl\")\n",
    "KG_PATH = Path(\"output/full_knowledge_graph.json\")\n",
    "JSON_OUT = Path(\"output/full_testset.json\")\n",
    "CSV_OUT = Path(\"output/full_testset.csv\")\n",
    "TESTSET_SIZE = 100\n",
    "CACHE_DIR = \".cache/ragas\"\n",
    "\n",
    "# test if the cached cleaned docs load correctly later\n",
    "def load_and_clean_documents(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    glob_pattern: str = GLOB\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Reads JSON syllabus files, extracts text, and returns cleaned Document objects.\n",
    "    \"\"\"\n",
    "    # If we have a cached cleaned docs file, load it\n",
    "    if CLEANED_DOCS_PATH.exists():\n",
    "        print(f\"Loading cleaned documents from cache: {CLEANED_DOCS_PATH}\")\n",
    "        return pd.read_pickle(CLEANED_DOCS_PATH)\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=glob_pattern,\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"is_content_key_jq_parsable\": False,\n",
    "            \"json_lines\": False,\n",
    "            \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(len(docs))\n",
    "\n",
    "    cleaned_documents = []\n",
    "    for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "        raw = doc.page_content               \n",
    "        cleaned = clean_text(raw)            \n",
    "        # rewrap into a Document, preserving metadata:\n",
    "        cleaned_doc = Document(\n",
    "            page_content=cleaned,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "    pd.to_pickle(cleaned_documents, CLEANED_DOCS_PATH)\n",
    "    print(f\"Cached cleaned docs to {CLEANED_DOCS_PATH}\")\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "def build_or_load_kg(docs, generator_llm, generator_embeddings, cache):\n",
    "    if KG_PATH.exists():\n",
    "        print(f\"Loading existing KG from {KG_PATH}\")\n",
    "        kg = KnowledgeGraph.load(str(KG_PATH))\n",
    "    else:\n",
    "        print(\"Creating new KG and applying transforms...\")\n",
    "        kg = KnowledgeGraph()\n",
    "        for doc in docs:\n",
    "            kg.nodes.append(Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            ))\n",
    "\n",
    "        transforms = default_transforms(documents=docs, \n",
    "                                    llm=generator_llm, \n",
    "                                    embedding_model=generator_embeddings)\n",
    "\n",
    "        apply_transforms(kg, transforms)\n",
    "        kg.save(str(KG_PATH))\n",
    "    return kg\n",
    "\n",
    "\n",
    "def generate_test_data(kg, generator_llm, generator_embeddings, test_size=TESTSET_SIZE):\n",
    "\n",
    "    # instantiate testsetgenerator with the finished kg\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings,\n",
    "        knowledge_graph=kg\n",
    "    )\n",
    "\n",
    "    # we are not using the generate_with_langchain_docs function from documentation\n",
    "    # because it will create the kg all over. we create it separately, so that we can save it\n",
    "    print(\"Generating testset from existing KG...\")\n",
    "    dataset = generator.generate(\n",
    "        testset_size=test_size,\n",
    "        query_distribution=None,\n",
    "        run_config=None,\n",
    "        callbacks=None,\n",
    "        with_debugging_logs=True\n",
    "    )\n",
    "\n",
    "    # Persist any new KG nodes \n",
    "    # (should be none if KG was complete, but in case of re-runs, save the updated version)\n",
    "    generator.knowledge_graph.save(str(KG_PATH))\n",
    "\n",
    "    # save the samples in json and csv format\n",
    "    df = dataset.to_pandas()\n",
    "    df.to_json(JSON_OUT, orient=\"records\", indent=2)\n",
    "    df.to_csv(CSV_OUT, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize persistent disk cache\n",
    "    cacher = DiskCacheBackend(cache_dir=CACHE_DIR) # \".cache/ragas\"\n",
    "    print(\"Cache entries:\", len(cacher.cache))\n",
    "\n",
    "    # Prepare LLM + embedding wrappers with the shared cache\n",
    "    llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"), cache=cacher)\n",
    "    embedder = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)\n",
    "\n",
    "    # Load & clean docs\n",
    "    docs = load_and_clean_documents()\n",
    "\n",
    "    # Build or load KG\n",
    "    kg = build_or_load_kg(docs, llm, embedder, cache)\n",
    "\n",
    "    # Generate testset\n",
    "    df = generate_test_data(kg, llm, embedder)\n",
    "\n",
    "    print(f\"Pipeline complete. KG saved to {KG_PATH}; testset saved to {JSON_OUT} and {CSV_OUT}.\")\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive, running version right now\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# if we want to see the cache in action, set the logging level to debug\n",
    "import logging\n",
    "from ragas.utils import set_logging_level\n",
    "set_logging_level(\"ragas.cache\", logging.DEBUG)\n",
    "\n",
    "\n",
    "# Default paths and parameters\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "KG_PATH = Path(\"full_knowledge_graph.json\")\n",
    "JSON_OUT = Path(\"full_testset.json\")\n",
    "CSV_OUT = Path(\"full_testset.csv\")\n",
    "TESTSET_SIZE = 100\n",
    "\n",
    "\n",
    "def load_and_clean_documents(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    glob_pattern: str = GLOB\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Reads JSON syllabus files, extracts text, and returns cleaned Document objects.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=glob_pattern,\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"is_content_key_jq_parsable\": False,\n",
    "            \"json_lines\": False,\n",
    "            \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(len(docs))\n",
    "\n",
    "    cleaned_documents = []\n",
    "    for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "        raw = doc.page_content               \n",
    "        cleaned = clean_text(raw)            \n",
    "        # rewrap into a Document, preserving metadata:\n",
    "        cleaned_doc = Document(\n",
    "            page_content=cleaned,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "\n",
    "def create_kg(\n",
    "    docs: list[Document],\n",
    "    generator_llm,\n",
    "    generator_embeddings,\n",
    "    kg_path: Path = KG_PATH\n",
    ") -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    Builds a knowledge graph from cleaned docs, applies transforms, and saves to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    kg = KnowledgeGraph()\n",
    "    for doc in docs:\n",
    "        kg.nodes.append(\n",
    "            Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            )\n",
    "        )\n",
    "    transforms = default_transforms(documents=docs, \n",
    "                                    llm=generator_llm, \n",
    "                                    embedding_model=generator_embeddings)\n",
    "    apply_transforms(kg, transforms)\n",
    "    kg.save(str(kg_path))\n",
    "\n",
    "    return kg\n",
    "\n",
    "\n",
    "def generate_test_data(\n",
    "    docs: list[Document],\n",
    "    kg: KnowledgeGraph,\n",
    "    generator_llm,\n",
    "    generator_embeddings,\n",
    "    testset_size: int = TESTSET_SIZE,\n",
    "    json_out: Path = JSON_OUT,\n",
    "    csv_out: Path = CSV_OUT\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses Ragas TestsetGenerator to produce a pandas DataFrame and saves JSON/CSV.\n",
    "    \"\"\"\n",
    "    print('starting generate_test_data function.')\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings,\n",
    "        knowledge_graph=kg\n",
    "    )\n",
    "    print('finished TestsetGenerator function')\n",
    "\n",
    "    # change this code to use \n",
    "\n",
    "    #dataset = generator.generate_with_langchain_docs(docs, \n",
    "     #                                                testset_size=testset_size,\n",
    "     #                                               with_debugging_logs=True)\n",
    "    dataset = generator.generate(\n",
    "        testset_size=TESTSET_SIZE,\n",
    "        query_distribution=None,\n",
    "        run_config=None,\n",
    "        callbacks=None,\n",
    "        with_debugging_logs=True\n",
    "        )\n",
    "    #print('finished generator.generate_with_langchain function')\n",
    "    \n",
    "    print('finished generator.generate')\n",
    "    generator.knowledge_graph.save(\"updated_full_knowledge_graph.json\")\n",
    "    \n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    df.to_json(json_out, orient=\"records\", indent=2)\n",
    "    df.to_csv(csv_out, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    cacher = DiskCacheBackend(cache_dir=\".cache/ragas\")\n",
    "    print('cache length:')\n",
    "    print(len(cacher.cache))\n",
    "\n",
    "    # Load and clean\n",
    "    docs = load_and_clean_documents()\n",
    "\n",
    "    # Generate test data\n",
    "    #docs = docs[0:10] # TEST\n",
    "\n",
    "    # Initialize raw models\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"), cache=cacher)\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)\n",
    "\n",
    "    # Build or load KnowledgeGraph\n",
    "    if KG_PATH.exists():\n",
    "        print(f\"Loading existing KnowledgeGraph from {KG_PATH}\")\n",
    "        kg = KnowledgeGraph.load(str(KG_PATH))\n",
    "\n",
    "    else:\n",
    "        print(f\"Creating new KnowledgeGraph and saving to {KG_PATH}\")\n",
    "        kg = create_kg(docs, generator_llm, generator_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "    TESTSET_SIZE = 100\n",
    "\n",
    "    df = generate_test_data(docs, kg, generator_llm, \n",
    "                            generator_embeddings, testset_size=TESTSET_SIZE)\n",
    "\n",
    "    print(f\"Pipeline complete. KG saved to {KG_PATH}; testset saved to {JSON_OUT} and {CSV_OUT}.\")\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 3616\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_second_test copy.json\")\n",
    "# 2298 in first\n",
    "#kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph.json\")\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91154c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes without a score: 3616\n"
     ]
    }
   ],
   "source": [
    "# pick the property that always gets added by your filter, e.g. \"score\"\n",
    "missing = [\n",
    "    node for node in kg.nodes\n",
    "    if node.get_property(\"score\") is None\n",
    "]\n",
    "print(f\"Nodes without a score: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55bd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding'])\n"
     ]
    }
   ],
   "source": [
    "#print(kg.nodes[0].properties)  \n",
    "# or\n",
    "print(kg.nodes[0].properties.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9af626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Themes on 2948 chunks, Entities on 2919 chunks\n",
      "1f8e1a02-5618-46c2-a8d3-47031a256905 ['Heating patterns', 'Temperature change', 'Measurement patterns', 'Missing data', 'Linear mixed-effects model', 'Lognormal mixed-effects model', \"Newton's Law of Cooling/Heating\", 'Heat diffusion equation', 'Lumped capacitance model', 'Bayesian implementation'] ['Newton’s Law of Cooling/Heating', 'Stan', 'pizza stone', 'gas-fired oven', 'heat diffusion equation', 'heat transfer coefficient', 'temperature', 'time', 'specific heat capacity', 'thermal conductivity']\n"
     ]
    }
   ],
   "source": [
    "# Count how many chunks got themes/entities\n",
    "themes_count = sum(1 for n in kg.nodes if \"themes\" in n.properties)\n",
    "entities_count = sum(1 for n in kg.nodes if \"entities\" in n.properties)\n",
    "print(f\"Themes on {themes_count} chunks, Entities on {entities_count} chunks\")\n",
    "\n",
    "# Inspect a sample chunk\n",
    "sample = next(n for n in kg.nodes if \"themes\" in n.properties)\n",
    "print(sample.id, sample.properties[\"themes\"], sample.properties[\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 272\n",
      "Surviving chunks     : 3161\n",
      "Embedding tasks   : 272\n",
      "Themes tasks      : 2948\n",
      "NER tasks         : 2919\n",
      "Total reported    : 6139\n"
     ]
    }
   ],
   "source": [
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 5240\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 396\n",
      "Surviving chunks     : 4785\n",
      "Embedding tasks   : 396\n",
      "Themes tasks      : 2975\n",
      "NER tasks         : 2764\n",
      "Total reported    : 6135\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_third_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 4844\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 437\n",
      "Surviving chunks     : 4389\n",
      "Embedding tasks   : 437\n",
      "Themes tasks      : 4205\n",
      "NER tasks         : 4186\n",
      "Total reported    : 8828\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16eba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 4844, relationships: 8052)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "422\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ragas imports\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Text cleaning\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"C:/Users/au644610/OneDrive - Aarhus universitet/Desktop/Thesis-Trustworthy-RAG/processed_syllabi/\"\n",
    "glob_pattern = \"**/scraped_data/*.json\"\n",
    "CLEANED_DOCS_PATH = Path(\"data/doc_splits/all_data.pkl\")\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    data_dir,\n",
    "    glob=glob_pattern,\n",
    "    loader_cls=JSONLoader,\n",
    "    loader_kwargs={\n",
    "        \"jq_schema\": \".\",\n",
    "        \"content_key\": \"text\",\n",
    "        \"is_content_key_jq_parsable\": False,\n",
    "        \"json_lines\": False,\n",
    "        \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "cleaned_documents = []\n",
    "for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "    raw = doc.page_content               \n",
    "    cleaned = clean_text(raw)            \n",
    "    # rewrap into a Document, preserving metadata:\n",
    "    cleaned_doc = Document(\n",
    "        page_content=cleaned,\n",
    "        metadata=doc.metadata\n",
    "    )\n",
    "    cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "\n",
    "print(len(cleaned_documents))\n",
    "\n",
    "\n",
    "docs = pd.read_pickle(\"../data/doc_splits/all_data.pkl\")\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1466526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#clusters found: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No multi-hop clusters found; generation will be single-hop only",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m clusters = spec.get_node_clusters(kg)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#clusters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clusters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > \u001b[32m0\u001b[39m, \\\n\u001b[32m     11\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mNo multi-hop clusters found; generation will be single-hop only\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: No multi-hop clusters found; generation will be single-hop only"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    ")\n",
    "spec = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "clusters = spec.get_node_clusters(kg)\n",
    "print(f\"#clusters found: {len(clusters)}\")\n",
    "\n",
    "assert len(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > 0, \\\n",
    "       \"No multi-hop clusters found; generation will be single-hop only\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23390f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 4844, relationships: 8052)\n",
      "#valid clusters: 0\n",
      "#nodes without embedding: 4407\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import MultiHopSpecificQuerySynthesizer\n",
    "kg = KnowledgeGraph.load(\"../output/archive/full_knowledge_graph_fourth_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "#print(\"edges by type:\", {r.relation_type for r in kg.relationships})\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "missing_embed = sum(1 for n in kg.nodes if \"summary_embedding\" not in n.properties)\n",
    "print(\"#nodes without embedding:\", missing_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67323031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 255, relationships: 944)\n",
      "#valid clusters: 538\n",
      "❗ Nodes with missing or invalid summaries: 223\n",
      "❗ Nodes missing embeddings: 223\n",
      "✅ Total nodes: 255\n",
      "❗ Content nodes with missing/invalid summary: 223\n",
      "❗ Content nodes missing embeddings: 223\n",
      "✅ Total content nodes: 255\n"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\knowledge_graph_pilot_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "\n",
    "broken = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Nodes with missing or invalid summaries: {len(broken)}\")\n",
    "\n",
    "print(f\"❗ Nodes missing embeddings: {len(no_emb)}\")\n",
    "\n",
    "\n",
    "print(f\"✅ Total nodes: {len(kg.nodes)}\")\n",
    "\n",
    "content_nodes = [n for n in kg.nodes if \"page_content\" in n.properties]\n",
    "\n",
    "broken = [n for n in content_nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in content_nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Content nodes with missing/invalid summary: {len(broken)}\")\n",
    "print(f\"❗ Content nodes missing embeddings: {len(no_emb)}\")\n",
    "print(f\"✅ Total content nodes: {len(content_nodes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae34779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 1606, relationships: 20974)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 1500 cluster(s)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 12700 cluster(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m synth \u001b[38;5;129;01min\u001b[39;00m [SingleHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      5\u001b[39m               MultiHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      6\u001b[39m               MultiHopAbstractQuerySynthesizer(generator_llm)]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         count = \u001b[38;5;28mlen\u001b[39m(\u001b[43msynth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_node_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynth.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cluster(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\multi_hop\\abstract.py:47\u001b[39m, in \u001b[36mMultiHopAbstractQuerySynthesizer.get_node_clusters\u001b[39m\u001b[34m(self, knowledge_graph)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_node_clusters\u001b[39m(\u001b[38;5;28mself\u001b[39m, knowledge_graph: KnowledgeGraph) -> t.List[t.Set[Node]]:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     node_clusters = \u001b[43mknowledge_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_indirect_clusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelationship_condition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_property\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummary_similarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdepth_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mfound \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m clusters\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(node_clusters))\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node_clusters\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:306\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters\u001b[39m\u001b[34m(self, relationship_condition, depth_limit)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes:\n\u001b[32m    305\u001b[39m     initial_cluster = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Remove duplicates by converting clusters to frozensets\u001b[39;00m\n\u001b[32m    309\u001b[39m unique_clusters = [\n\u001b[32m    310\u001b[39m     \u001b[38;5;28mset\u001b[39m(cluster) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mfrozenset\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m clusters)\n\u001b[32m    311\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:288\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rel \u001b[38;5;129;01min\u001b[39;00m relationships:\n\u001b[32m    287\u001b[39m     neighbor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m rel.target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[32m    289\u001b[39m         neighbor = rel.target\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    291\u001b[39m         rel.bidirectional\n\u001b[32m    292\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.target == node\n\u001b[32m    293\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster\n\u001b[32m    294\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:82\u001b[39m, in \u001b[36mNode.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mobject\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Node):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id == other.id\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(r\"..\\output\\knowledge_graph_split_01_100.json\")\n",
    "print(kg)\n",
    "\n",
    "for synth in [SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopAbstractQuerySynthesizer(generator_llm)]: # could this be failing because i run out of ram? size is 166600 for the succeeding graph\n",
    "    try:\n",
    "        count = len(synth.get_node_clusters(kg))\n",
    "        print(f\"✅ {synth.name} → {count} cluster(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {synth.name} → error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1725c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No clusters found for LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)); skipping.\n"
     ]
    }
   ],
   "source": [
    "query_distribution = []\n",
    "synths = [\n",
    "    SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopAbstractQuerySynthesizer(generator_llm)\n",
    "]\n",
    "\n",
    "for s in synths:\n",
    "    try:\n",
    "        clusters = s.get_node_clusters(kg)\n",
    "        if clusters:\n",
    "            query_distribution.append((s, 1.0))  # weight can be adjusted\n",
    "        else:\n",
    "            print(f\"⚠️ No clusters found for {s.name}; skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Synthesizer {s.name} failed: {e}\")\n",
    "\n",
    "# re-normalise weights\n",
    "total = sum(w for _, w in query_distribution)\n",
    "query_distribution = [(s, w/total) for s, w in query_distribution]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    query_distribution = [\n",
    "        (\n",
    "            SingleHopSpecificQuerySynthesizer(llm=generator_llm, property_name=\"headlines\"),\n",
    "            0.5,\n",
    "        ),\n",
    "        (\n",
    "            SingleHopSpecificQuerySynthesizer(\n",
    "                llm=generator_llm, property_name=\"keyphrases\"\n",
    "            ),\n",
    "            0.5,\n",
    "        ),\n",
    "    ]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
