{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ba1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, scipy.stats as ss\n",
    "from sacrebleu.metrics import BLEU\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/quantitative_metrics/all_scalers_testset.csv\")\n",
    "\n",
    "# DISTRIBUTION PLOTS (one plot per metric)\n",
    "metrics = [\"alignscore\"]\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    plt.hist(df[m].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution of {m}\")\n",
    "    plt.xlabel(m)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../output/quantitative_metrics/all_scalers_testset.csv\")\n",
    "\n",
    "# Answer length in *tokens* (white‑space split)\n",
    "df[\"answer_len\"] = df[\"final_answer\"].str.split().str.len()\n",
    "\n",
    "# DISTRIBUTION PLOTS (one plot per metric)\n",
    "metrics = [\"lex_score\", \"ecc_score\", \"deg_score\"]\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    plt.hist(df[m].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution of {m}\")\n",
    "    plt.xlabel(m)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# LENGTH vs UNCERTAINTY SCATTERS \n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"answer_len\"], df[m], alpha=0.6)\n",
    "    plt.title(f\"{m} vs Answer Length\")\n",
    "    plt.xlabel(\"Answer Length (tokens)\")\n",
    "    plt.ylabel(m)\n",
    "    plt.show()\n",
    "\n",
    "    # print Spearman rank correlation\n",
    "    rho, p = ss.spearmanr(df[\"answer_len\"], df[m])\n",
    "    print(f\"Spearman ρ(length, {m}) = {rho:.3f} (p = {p:.3e})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same plots with scaled values\n",
    "\n",
    "# Answer length in *tokens* (white‑space split)\n",
    "df[\"answer_len\"] = df[\"final_answer\"].str.split().str.len()\n",
    "\n",
    "# ---------- 1. DISTRIBUTION PLOTS (one plot per metric) ----------\n",
    "metrics = [\"lex_score_conf_q\", \"lex_score_conf_iso\", \"lex_score_conf_sig\", \"ecc_score_conf_q\",  \"ecc_score_conf_iso\",\"ecc_score_conf_sig\",\"deg_score_conf_q\", \"deg_score_conf_iso\",\"deg_score_conf_sig\"]\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    plt.hist(df[m].dropna(), bins=30)\n",
    "    plt.title(f\"Distribution of {m}\")\n",
    "    plt.xlabel(m)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------- 2. LENGTH vs UNCERTAINTY SCATTERS ----------\n",
    "for m in metrics:\n",
    "    plt.figure()\n",
    "    plt.scatter(df[\"answer_len\"], df[m], alpha=0.6)\n",
    "    plt.title(f\"{m} vs Answer Length\")\n",
    "    plt.xlabel(\"Answer Length (tokens)\")\n",
    "    plt.ylabel(m)\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: print Spearman rank correlation in the notebook output.\n",
    "    rho, p = ss.spearmanr(df[\"answer_len\"], df[m])\n",
    "    print(f\"Spearman ρ(length, {m}) = {rho:.3f} (p = {p:.3e})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b2d15",
   "metadata": {},
   "source": [
    "### alignscore results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1a8ef",
   "metadata": {},
   "source": [
    "=== Spearman ρ (UE ↓  vs  AlignScore ↑) ===\n",
    "lex_score        ρ = +0.018   p = 7.82e-01\n",
    "ecc_score        ρ = +0.047   p = 4.83e-01\n",
    "deg_score        ρ = +0.093   p = 1.62e-01\n",
    "\n",
    "=== PRR ===\n",
    "lex_score        PRR = +0.156\n",
    "ecc_score        PRR = +0.161\n",
    "deg_score        PRR = +0.217\n",
    "\n",
    "=== Binary AUROC (good = AlignScore >= 0.7) ===\n",
    "lex_score        AUROC = 0.883\n",
    "ecc_score        AUROC = 0.909\n",
    "deg_score        AUROC = 0.897\n",
    "\n",
    "=== ECE on held‑out test fold (lower is better) ===\n",
    "lex_score        ECE = 0.212\n",
    "ecc_score        ECE = 0.187\n",
    "deg_score        ECE = 0.238\n",
    " ---\n",
    "\n",
    "Spearman ρ ≈ 0.3 → UE is mildly correlated with factual quality; ρ ≤ 0 shows the ranking is broken.\n",
    "\n",
    "PRR > 0.2 on only 220 answers is a respectable signal; < 0 means you’d do worse than random by trusting that UE score.\n",
    "\n",
    "AUROC ≥ 0.7 indicates the score can separate good vs bad answers fairly well at some threshold.\n",
    "\n",
    "ECE ≤ 0.1 on the 20 % hold‑out set suggests reasonable calibration; large ECE flags that the raw UE numbers aren’t usable as probabilities without extra scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e39b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/quantitative_metrics/all_scalers_testset.csv\")\n",
    "\n",
    "# Add BLEU (sentence‑level) & ROUGE‑L‑F1 columns\n",
    "bleu_metric  = BLEU(tokenize='13a', effective_order=True)  \n",
    "rl_scorer    = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rouge_l_f1(pred, ref):\n",
    "    return rl_scorer.score(ref, pred)['rougeL'].fmeasure       # 0‑1\n",
    "\n",
    "df[\"bleu\"]     = [bleu_metric.sentence_score(sys, [ref]).score / 100\n",
    "                  for sys, ref in zip(df[\"final_answer\"], df[\"reference\"])]\n",
    "df[\"rouge_l\"]  = [rouge_l_f1(sys, ref)\n",
    "                  for sys, ref in zip(df[\"final_answer\"], df[\"reference\"])]\n",
    "\n",
    "\n",
    "# Configure UE columns and quality metrics (+ thresholds)\n",
    "ue_cols   = [\"lex_score\", \"ecc_score\", \"deg_score\"]\n",
    "qual_cfgs = {\n",
    "    \"alignscore\": 0.70,   # factual‑consistency “good” cut‑off\n",
    "    \"bleu\":       0.20,   # surface‑overlap “good” cut‑off\n",
    "    \"rouge_l\":    0.25    # sentence‑LCS “good” cut‑off\n",
    "}\n",
    "\n",
    "#  Prediction–Rejection Ratio\n",
    "def prr(quality, uncertainty, higher_quality_better=True):\n",
    "    q = quality if higher_quality_better else -quality\n",
    "    # coverage‑sorted areas\n",
    "    def area(vals):                      # vals sorted *descending* quality\n",
    "        csum = np.cumsum(vals)\n",
    "        cov  = np.arange(1, len(vals)+1) / len(vals)\n",
    "        return np.trapz(csum / np.arange(1, len(vals)+1), cov)\n",
    "\n",
    "    oracle_area  = area(np.sort(q)[::-1])\n",
    "    random_area  = q.mean()                   # flat line over coverage\n",
    "    ue_area      = area(q[np.argsort(uncertainty)])\n",
    "    return (ue_area - random_area) / (oracle_area - random_area + 1e-12)\n",
    "\n",
    "\n",
    "rows = []          # gather result rows here\n",
    "\n",
    "for qual, thr in qual_cfgs.items():\n",
    "    quality = df[qual]\n",
    "    good    = (quality >= thr).astype(int)\n",
    "\n",
    "    for ue in ue_cols:\n",
    "        # Spearman (UE ↓ vs quality ↑) – negate UE so higher rank = better\n",
    "        rho, p = ss.spearmanr(-df[ue], quality)\n",
    "\n",
    "        # PRR   (quality first, UE second) – correct order!\n",
    "        prr_val = prr(quality.values, df[ue].values)\n",
    "\n",
    "        # AUROC (UE ↓ so pass -UE)\n",
    "        auroc   = roc_auc_score(good, -df[ue])\n",
    "\n",
    "        rows.append({\n",
    "            \"quality_metric\": qual,\n",
    "            \"ue_metric\":      ue,\n",
    "            \"spearman_rho\":   round(rho, 3),\n",
    "            \"prr\":            round(prr_val, 3),\n",
    "            \"auroc\":          round(auroc, 3)\n",
    "        })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(rows)\n",
    "display(results_df)               \n",
    "results_df.to_csv(\"../output/quantitative_metrics/ue_metrics_overview.csv\", index=False)\n",
    "print(\"\\nSaved to ue_metrics_overview.csv \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07000f7b",
   "metadata": {},
   "source": [
    "### calibration results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce87751",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/quantitative_metrics/all_scalers_testset.csv\")\n",
    "\n",
    "\n",
    "# only using the 20 % test fold that the quantile scaler never saw\n",
    "test_mask = df[\"split\"] == \"test\"\n",
    "n_bins = 10\n",
    "\n",
    "def ece(prob, label, n_bins=10):\n",
    "    bins = pd.qcut(prob, q=n_bins, duplicates=\"drop\")\n",
    "    ece_val = 0.0\n",
    "    for b in bins.unique():\n",
    "        mask = bins == b\n",
    "        if mask.any():\n",
    "            conf = prob[mask].mean()\n",
    "            acc  = label[mask].mean()\n",
    "            ece_val += np.abs(acc - conf) * mask.mean()\n",
    "    return ece_val\n",
    "\n",
    "print(\"\\n=== ECE on held‑out test fold (lower is better) ===\")\n",
    "for m in ue_cols:\n",
    "    # rescale UE to (0,1] confidence by min‑max inversion\n",
    "    conf = 1.0 - (df.loc[test_mask, m]  - df[m].min()) / (df[m].max() - df[m].min())\n",
    "    lab  = df.loc[test_mask, \"is_good\"]\n",
    "    print(f\"{m:15s}  ECE = {ece(conf.values, lab.values, n_bins):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 0. config ----------\n",
    "ue_cols   = [\"lex_score\", \"ecc_score\", \"deg_score\"]\n",
    "n_bins    = 10\n",
    "test_mask = df[\"split\"] == \"test\"\n",
    "labels    = (df[\"alignscore\"] >= 0.70).astype(int)       # good‑answer flag\n",
    "\n",
    "def ece(prob, label, n_bins=10):\n",
    "    \"\"\"Vectorised Expected Calibration Error.\"\"\"\n",
    "    bins = pd.qcut(prob, q=n_bins, duplicates=\"drop\")\n",
    "    ece_val = 0.0\n",
    "    for b in bins.unique():\n",
    "        mask = bins == b\n",
    "        if mask.any():\n",
    "            conf = prob[mask].mean()\n",
    "            acc  = label[mask].mean()\n",
    "            ece_val += np.abs(acc - conf) * mask.mean()\n",
    "    return ece_val\n",
    "\n",
    "# ---------- 1. compute & print ----------\n",
    "print(\"\\n=== ECE on held‑out 20 % test split ===\")\n",
    "header = f\"{'UE':12s} | {'raw_minmax':>10} | {'quantile':>8} | {'isotonic':>8}\"\n",
    "print(header)\n",
    "print(\"-\"*len(header))\n",
    "\n",
    "for m in ue_cols:\n",
    "    # 1) raw min‑max rescale (0‑1)\n",
    "    lo, hi = df[m].min(), df[m].max()\n",
    "    raw_conf = 1.0 - (df.loc[test_mask, m] - lo) / (hi - lo + 1e-12)\n",
    "\n",
    "    # 2) quantile confidence (f'{col}_conf_q')\n",
    "    q_conf = df.loc[test_mask, f\"{m}_conf_q\"]\n",
    "\n",
    "    # 3) isotonic confidence (f'{col}_conf_iso')\n",
    "    iso_conf = df.loc[test_mask, f\"{m}_conf_iso\"]\n",
    "\n",
    "    # sig conf\n",
    "    sig_conf = df.loc[test_mask, f\"{m}_conf_sig\"]\n",
    "\n",
    "    ece_raw = ece(raw_conf.values, labels.loc[test_mask].values, n_bins)\n",
    "    ece_q   = ece(q_conf.values, labels.loc[test_mask].values, n_bins)\n",
    "    ece_iso = ece(iso_conf.values, labels.loc[test_mask].values, n_bins)\n",
    "    ece_sig  = ece(sig_conf.values, labels.loc[test_mask].values, n_bins)\n",
    "\n",
    "\n",
    "    print(f\"{m:12s} | {ece_raw:10.3f} | {ece_q:8.3f} | {ece_iso:8.3f} | {ece_sig:8.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
