{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c64e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: 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\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# if we want to see the cache in action, set the logging level to debug\n",
    "#import logging\n",
    "#from ragas.utils import set_logging_level\n",
    "#set_logging_level(\"ragas.cache\", logging.DEBUG)\n",
    "\n",
    "\n",
    "# Default paths and parameters\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "KG_PATH = Path(\"knowledge_graph_first_run.json\")\n",
    "JSON_OUT = Path(\"testset_fr.json\")\n",
    "CSV_OUT = Path(\"testset_fr.csv\")\n",
    "TESTSET_SIZE = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67c79cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3426\n",
      "3426\n"
     ]
    }
   ],
   "source": [
    "# after loading the old KG:\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\knowledge_graph_first_run.json\")\n",
    "\n",
    "# retain only the *unfinished* nodes\n",
    "unfinished = [\n",
    "    node for node in kg.nodes\n",
    "    if not all(k in node.properties for k in (\"headlines\", \"summary\", \"summary_embedding\"))\n",
    "]\n",
    "kg.nodes = unfinished\n",
    "\n",
    "print(len(unfinished))\n",
    "print(len(kg.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28079810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run the transforms again *only* on those\n",
    "apply_transforms(kg, default_transforms(...))\n",
    "# finally, merge back the newly-finished nodes into your old KG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ragas imports\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Text cleaning\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "# Ensure your OpenAI key is set\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "CLEANED_DOCS_PATH = Path(\"cleaned_docs.pkl\")\n",
    "KG_PATH = Path(\"output/full_knowledge_graph.json\")\n",
    "JSON_OUT = Path(\"output/full_testset.json\")\n",
    "CSV_OUT = Path(\"output/full_testset.csv\")\n",
    "TESTSET_SIZE = 100\n",
    "CACHE_DIR = \".cache/ragas\"\n",
    "\n",
    "# test if the cached cleaned docs load correctly later\n",
    "def load_and_clean_documents(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    glob_pattern: str = GLOB\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Reads JSON syllabus files, extracts text, and returns cleaned Document objects.\n",
    "    \"\"\"\n",
    "    # If we have a cached cleaned docs file, load it\n",
    "    if CLEANED_DOCS_PATH.exists():\n",
    "        print(f\"Loading cleaned documents from cache: {CLEANED_DOCS_PATH}\")\n",
    "        return pd.read_pickle(CLEANED_DOCS_PATH)\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=glob_pattern,\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"is_content_key_jq_parsable\": False,\n",
    "            \"json_lines\": False,\n",
    "            \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(len(docs))\n",
    "\n",
    "    cleaned_documents = []\n",
    "    for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "        raw = doc.page_content               \n",
    "        cleaned = clean_text(raw)            \n",
    "        # rewrap into a Document, preserving metadata:\n",
    "        cleaned_doc = Document(\n",
    "            page_content=cleaned,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "    pd.to_pickle(cleaned_documents, CLEANED_DOCS_PATH)\n",
    "    print(f\"Cached cleaned docs to {CLEANED_DOCS_PATH}\")\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "def build_or_load_kg(docs, generator_llm, generator_embeddings, cache):\n",
    "    if KG_PATH.exists():\n",
    "        print(f\"Loading existing KG from {KG_PATH}\")\n",
    "        kg = KnowledgeGraph.load(str(KG_PATH))\n",
    "    else:\n",
    "        print(\"Creating new KG and applying transforms...\")\n",
    "        kg = KnowledgeGraph()\n",
    "        for doc in docs:\n",
    "            kg.nodes.append(Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            ))\n",
    "\n",
    "        transforms = default_transforms(documents=docs, \n",
    "                                    llm=generator_llm, \n",
    "                                    embedding_model=generator_embeddings)\n",
    "\n",
    "        apply_transforms(kg, transforms)\n",
    "        kg.save(str(KG_PATH))\n",
    "    return kg\n",
    "\n",
    "\n",
    "def generate_test_data(kg, generator_llm, generator_embeddings, test_size=TESTSET_SIZE):\n",
    "\n",
    "    # instantiate testsetgenerator with the finished kg\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings,\n",
    "        knowledge_graph=kg\n",
    "    )\n",
    "\n",
    "    # we are not using the generate_with_langchain_docs function from documentation\n",
    "    # because it will create the kg all over. we create it separately, so that we can save it\n",
    "    print(\"Generating testset from existing KG...\")\n",
    "    dataset = generator.generate(\n",
    "        testset_size=test_size,\n",
    "        query_distribution=None,\n",
    "        run_config=None,\n",
    "        callbacks=None,\n",
    "        with_debugging_logs=True\n",
    "    )\n",
    "\n",
    "    # Persist any new KG nodes \n",
    "    # (should be none if KG was complete, but in case of re-runs, save the updated version)\n",
    "    generator.knowledge_graph.save(str(KG_PATH))\n",
    "\n",
    "    # save the samples in json and csv format\n",
    "    df = dataset.to_pandas()\n",
    "    df.to_json(JSON_OUT, orient=\"records\", indent=2)\n",
    "    df.to_csv(CSV_OUT, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize persistent disk cache\n",
    "    cacher = DiskCacheBackend(cache_dir=CACHE_DIR) # \".cache/ragas\"\n",
    "    print(\"Cache entries:\", len(cacher.cache))\n",
    "\n",
    "    # Prepare LLM + embedding wrappers with the shared cache\n",
    "    llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"), cache=cacher)\n",
    "    embedder = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)\n",
    "\n",
    "    # Load & clean docs\n",
    "    docs = load_and_clean_documents()\n",
    "\n",
    "    # Build or load KG\n",
    "    kg = build_or_load_kg(docs, llm, embedder, cache)\n",
    "\n",
    "    # Generate testset\n",
    "    df = generate_test_data(kg, llm, embedder)\n",
    "\n",
    "    print(f\"Pipeline complete. KG saved to {KG_PATH}; testset saved to {JSON_OUT} and {CSV_OUT}.\")\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive, running version right now\n",
    "\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "from ragas.testset.graph import Node, NodeType\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# if we want to see the cache in action, set the logging level to debug\n",
    "import logging\n",
    "from ragas.utils import set_logging_level\n",
    "set_logging_level(\"ragas.cache\", logging.DEBUG)\n",
    "\n",
    "\n",
    "# Default paths and parameters\n",
    "DATA_DIR = \"processed_syllabi/\"\n",
    "GLOB = \"**/scraped_data/*.json\"\n",
    "KG_PATH = Path(\"full_knowledge_graph.json\")\n",
    "JSON_OUT = Path(\"full_testset.json\")\n",
    "CSV_OUT = Path(\"full_testset.csv\")\n",
    "TESTSET_SIZE = 100\n",
    "\n",
    "\n",
    "def load_and_clean_documents(\n",
    "    data_dir: str = DATA_DIR,\n",
    "    glob_pattern: str = GLOB\n",
    ") -> list[Document]:\n",
    "    \"\"\"\n",
    "    Reads JSON syllabus files, extracts text, and returns cleaned Document objects.\n",
    "    \"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=glob_pattern,\n",
    "        loader_cls=JSONLoader,\n",
    "        loader_kwargs={\n",
    "            \"jq_schema\": \".\",\n",
    "            \"content_key\": \"text\",\n",
    "            \"is_content_key_jq_parsable\": False,\n",
    "            \"json_lines\": False,\n",
    "            \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    print(len(docs))\n",
    "\n",
    "    cleaned_documents = []\n",
    "    for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "        raw = doc.page_content               \n",
    "        cleaned = clean_text(raw)            \n",
    "        # rewrap into a Document, preserving metadata:\n",
    "        cleaned_doc = Document(\n",
    "            page_content=cleaned,\n",
    "            metadata=doc.metadata\n",
    "        )\n",
    "        cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "    return cleaned_documents\n",
    "\n",
    "\n",
    "\n",
    "def create_kg(\n",
    "    docs: list[Document],\n",
    "    generator_llm,\n",
    "    generator_embeddings,\n",
    "    kg_path: Path = KG_PATH\n",
    ") -> KnowledgeGraph:\n",
    "    \"\"\"\n",
    "    Builds a knowledge graph from cleaned docs, applies transforms, and saves to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    kg = KnowledgeGraph()\n",
    "    for doc in docs:\n",
    "        kg.nodes.append(\n",
    "            Node(\n",
    "                type=NodeType.DOCUMENT,\n",
    "                properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "            )\n",
    "        )\n",
    "    transforms = default_transforms(documents=docs, \n",
    "                                    llm=generator_llm, \n",
    "                                    embedding_model=generator_embeddings)\n",
    "    apply_transforms(kg, transforms)\n",
    "    kg.save(str(kg_path))\n",
    "\n",
    "    return kg\n",
    "\n",
    "\n",
    "def generate_test_data(\n",
    "    docs: list[Document],\n",
    "    kg: KnowledgeGraph,\n",
    "    generator_llm,\n",
    "    generator_embeddings,\n",
    "    testset_size: int = TESTSET_SIZE,\n",
    "    json_out: Path = JSON_OUT,\n",
    "    csv_out: Path = CSV_OUT\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uses Ragas TestsetGenerator to produce a pandas DataFrame and saves JSON/CSV.\n",
    "    \"\"\"\n",
    "    print('starting generate_test_data function.')\n",
    "    generator = TestsetGenerator(\n",
    "        llm=generator_llm,\n",
    "        embedding_model=generator_embeddings,\n",
    "        knowledge_graph=kg\n",
    "    )\n",
    "    print('finished TestsetGenerator function')\n",
    "\n",
    "    # change this code to use \n",
    "\n",
    "    #dataset = generator.generate_with_langchain_docs(docs, \n",
    "     #                                                testset_size=testset_size,\n",
    "     #                                               with_debugging_logs=True)\n",
    "    dataset = generator.generate(\n",
    "        testset_size=TESTSET_SIZE,\n",
    "        query_distribution=None,\n",
    "        run_config=None,\n",
    "        callbacks=None,\n",
    "        with_debugging_logs=True\n",
    "        )\n",
    "    #print('finished generator.generate_with_langchain function')\n",
    "    \n",
    "    print('finished generator.generate')\n",
    "    generator.knowledge_graph.save(\"updated_full_knowledge_graph.json\")\n",
    "    \n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    df.to_json(json_out, orient=\"records\", indent=2)\n",
    "    df.to_csv(csv_out, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    cacher = DiskCacheBackend(cache_dir=\".cache/ragas\")\n",
    "    print('cache length:')\n",
    "    print(len(cacher.cache))\n",
    "\n",
    "    # Load and clean\n",
    "    docs = load_and_clean_documents()\n",
    "\n",
    "    # Generate test data\n",
    "    #docs = docs[0:10] # TEST\n",
    "\n",
    "    # Initialize raw models\n",
    "    generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"), cache=cacher)\n",
    "    generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)\n",
    "\n",
    "    # Build or load KnowledgeGraph\n",
    "    if KG_PATH.exists():\n",
    "        print(f\"Loading existing KnowledgeGraph from {KG_PATH}\")\n",
    "        kg = KnowledgeGraph.load(str(KG_PATH))\n",
    "\n",
    "    else:\n",
    "        print(f\"Creating new KnowledgeGraph and saving to {KG_PATH}\")\n",
    "        kg = create_kg(docs, generator_llm, generator_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "    TESTSET_SIZE = 100\n",
    "\n",
    "    df = generate_test_data(docs, kg, generator_llm, \n",
    "                            generator_embeddings, testset_size=TESTSET_SIZE)\n",
    "\n",
    "    print(f\"Pipeline complete. KG saved to {KG_PATH}; testset saved to {JSON_OUT} and {CSV_OUT}.\")\n",
    "    return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "693c9d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 3616\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_second_test copy.json\")\n",
    "# 2298 in first\n",
    "#kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph.json\")\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91154c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes without a score: 3616\n"
     ]
    }
   ],
   "source": [
    "# pick the property that always gets added by your filter, e.g. \"score\"\n",
    "missing = [\n",
    "    node for node in kg.nodes\n",
    "    if node.get_property(\"score\") is None\n",
    "]\n",
    "print(f\"Nodes without a score: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55bd4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding'])\n"
     ]
    }
   ],
   "source": [
    "#print(kg.nodes[0].properties)  \n",
    "# or\n",
    "print(kg.nodes[0].properties.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9af626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Themes on 2948 chunks, Entities on 2919 chunks\n",
      "1f8e1a02-5618-46c2-a8d3-47031a256905 ['Heating patterns', 'Temperature change', 'Measurement patterns', 'Missing data', 'Linear mixed-effects model', 'Lognormal mixed-effects model', \"Newton's Law of Cooling/Heating\", 'Heat diffusion equation', 'Lumped capacitance model', 'Bayesian implementation'] ['Newton’s Law of Cooling/Heating', 'Stan', 'pizza stone', 'gas-fired oven', 'heat diffusion equation', 'heat transfer coefficient', 'temperature', 'time', 'specific heat capacity', 'thermal conductivity']\n"
     ]
    }
   ],
   "source": [
    "# Count how many chunks got themes/entities\n",
    "themes_count = sum(1 for n in kg.nodes if \"themes\" in n.properties)\n",
    "entities_count = sum(1 for n in kg.nodes if \"entities\" in n.properties)\n",
    "print(f\"Themes on {themes_count} chunks, Entities on {entities_count} chunks\")\n",
    "\n",
    "# Inspect a sample chunk\n",
    "sample = next(n for n in kg.nodes if \"themes\" in n.properties)\n",
    "print(sample.id, sample.properties[\"themes\"], sample.properties[\"entities\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b9b3c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 272\n",
      "Surviving chunks     : 3161\n",
      "Embedding tasks   : 272\n",
      "Themes tasks      : 2948\n",
      "NER tasks         : 2919\n",
      "Total reported    : 6139\n"
     ]
    }
   ],
   "source": [
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 5240\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 396\n",
      "Surviving chunks     : 4785\n",
      "Embedding tasks   : 396\n",
      "Themes tasks      : 2975\n",
      "NER tasks         : 2764\n",
      "Total reported    : 6135\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_third_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes in graph: 4844\n",
      "Node types in this graph: {<NodeType.DOCUMENT: 'document'>, <NodeType.CHUNK: 'chunk'>}\n",
      "Documents with embeddings: 437\n",
      "Surviving chunks     : 4389\n",
      "Embedding tasks   : 437\n",
      "Themes tasks      : 4205\n",
      "NER tasks         : 4186\n",
      "Total reported    : 8828\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "# 2298 in first\n",
    "\n",
    "total = len(kg.nodes)\n",
    "print(f\"Total nodes in graph: {total}\")\n",
    "\n",
    "# 1) Inspect node types present\n",
    "print(\"Node types in this graph:\", {n.type for n in kg.nodes})\n",
    "\n",
    "# 2) Count embeddings correctly\n",
    "docs_for_embed = sum(\n",
    "    1\n",
    "    for n in kg.nodes\n",
    "    if n.type == NodeType.DOCUMENT\n",
    "       and n.properties.get(\"summary_embedding\") is not None\n",
    ")\n",
    "print(f\"Documents with embeddings: {docs_for_embed}\")\n",
    "\n",
    "# 3) Count chunks for comparison\n",
    "chunk_count = sum(1 for n in kg.nodes if n.type == NodeType.CHUNK)\n",
    "\n",
    "# 2) How many chunks got themes & entities?\n",
    "chunks_with_themes   = sum(1 for n in kg.nodes if \"themes\"    in n.properties)\n",
    "chunks_with_entities = sum(1 for n in kg.nodes if \"entities\"  in n.properties)\n",
    "\n",
    "print(f\"Surviving chunks     : {chunk_count}\")\n",
    "print(f\"Embedding tasks   : {docs_for_embed}\")\n",
    "print(f\"Themes tasks      : {chunks_with_themes}\")\n",
    "print(f\"NER tasks         : {chunks_with_entities}\")\n",
    "print(f\"Total reported    : {docs_for_embed + chunks_with_themes + chunks_with_entities}\")\n",
    "# fourth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16eba7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 4844, relationships: 8052)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e99b8f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422\n",
      "422\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Ragas imports\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Text cleaning\n",
    "from embedding_process.preprocessing import clean_text\n",
    "\n",
    "# Configuration\n",
    "data_dir = \"C:/Users/au644610/OneDrive - Aarhus universitet/Desktop/Thesis-Trustworthy-RAG/processed_syllabi/\"\n",
    "glob_pattern = \"**/scraped_data/*.json\"\n",
    "CLEANED_DOCS_PATH = Path(\"cleaned_docs.pkl\")\n",
    "KG_PATH = Path(\"output/full_knowledge_graph_fourth_test.json\")\n",
    "JSON_OUT = Path(\"output/full_testset_fourth_test.json\")\n",
    "CSV_OUT = Path(\"output/full_testset_fourth_test.csv\")\n",
    "TESTSET_SIZE = 200\n",
    "CACHE_DIR = \".cache/ragas\"\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    data_dir,\n",
    "    glob=glob_pattern,\n",
    "    loader_cls=JSONLoader,\n",
    "    loader_kwargs={\n",
    "        \"jq_schema\": \".\",\n",
    "        \"content_key\": \"text\",\n",
    "        \"is_content_key_jq_parsable\": False,\n",
    "        \"json_lines\": False,\n",
    "        \"metadata_func\": lambda obj, meta: {**meta, **{k: v for k, v in obj.items() if k != \"text\"}}\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "cleaned_documents = []\n",
    "for doc in docs:                    # `documents` is a list of langchain.schema.Document\n",
    "    raw = doc.page_content               \n",
    "    cleaned = clean_text(raw)            \n",
    "    # rewrap into a Document, preserving metadata:\n",
    "    cleaned_doc = Document(\n",
    "        page_content=cleaned,\n",
    "        metadata=doc.metadata\n",
    "    )\n",
    "    cleaned_documents.append(cleaned_doc)\n",
    "\n",
    "\n",
    "print(len(cleaned_documents))\n",
    "\n",
    "\n",
    "docs = pd.read_pickle(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\cleaned_docs.pkl\")\n",
    "print(len(docs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdaa9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 4844, relationships: 8052)\n",
      "Cache entries: 14910\n",
      "[(SingleHopSpecificQuerySynthesizer(name='single_hop_specifc_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a single-hop query and answer based on the specified conditions (persona, term, style, length) and the provided context. Ensure the answer is entirely faithful to the context, using only the information directly from the provided context.### Instructions:\n",
      "1. **Generate a Query**: Based on the context, persona, term, style, and length, create a question that aligns with the persona's perspective and incorporates the term.\n",
      "2. **Generate an Answer**: Using only the content from the provided context, construct a detailed answer to the query. Do not add any information not included in or inferable from the context.\n",
      ", examples=[(QueryCondition(persona=Persona(name='Software Engineer', role_description='Focuses on coding best practices and system design.'), term='microservices', query_style='Formal', query_length='Medium', context='Microservices are an architectural style where applications are structured as a collection of loosely coupled services. Each service is fine-grained and focuses on a single functionality.'), GeneratedQueryAnswer(query='What is the purpose of microservices in software architecture?', answer='Microservices are designed to structure applications as a collection of loosely coupled services, each focusing on a single functionality.'))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english), property_name='entities'), 0.3333333333333333), (MultiHopAbstractQuerySynthesizer(name='multi_hop_abstract_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
      "1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
      "2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
      "3. **Multi-Hop Context Tags**:\n",
      "   - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
      "   - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), concept_combination_prompt=ConceptCombinationPrompt(instruction=Form combinations by pairing concepts from at least two different lists.\n",
      "**Instructions:**\n",
      "- Review the concepts from each node.\n",
      "- Identify concepts that can logically be connected or contrasted.\n",
      "- Form combinations that involve concepts from different nodes.\n",
      "- Each combination should include at least one concept from two or more nodes.\n",
      "- List the combinations clearly and concisely.\n",
      "- Do not repeat the same combination more than once., examples=[(ConceptsList(lists_of_concepts=[['Artificial intelligence', 'Automation'], ['Healthcare', 'Data privacy']], max_combinations=2), ConceptCombinations(combinations=[['Artificial intelligence', 'Healthcare'], ['Automation', 'Data privacy']]))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)), 0.3333333333333333), (MultiHopSpecificQuerySynthesizer(name='multi_hop_specific_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
      "1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
      "2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
      "3. **Multi-Hop Context Tags**:\n",
      "   - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
      "   - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), relation_type='entities_overlap', property_name='entities', theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)), 0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 3/3 [00:00<00:00, 235.50it/s]\n",
      "Generating Scenarios:  33%|███▎      | 1/3 [00:00<00:00, 60.97it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No clusters found in the knowledge graph. Try changing the relationship condition.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m query_distribution = default_query_distribution(generator_llm)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(query_distribution)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m testset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_distribution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m testset.to_pandas()\n\u001b[32m     23\u001b[39m testset.to_csv(\u001b[33m'\u001b[39m\u001b[33mmultihoptest.csv\u001b[39m\u001b[33m'\u001b[39m, index = \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\generate.py:413\u001b[39m, in \u001b[36mTestsetGenerator.generate\u001b[39m\u001b[34m(self, testset_size, query_distribution, num_personas, run_config, batch_size, callbacks, token_usage_parser, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    412\u001b[39m     scenario_generation_rm.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    415\u001b[39m     scenario_generation_rm.on_chain_end(\n\u001b[32m    416\u001b[39m         outputs={\u001b[33m\"\u001b[39m\u001b[33mscenario_sample_list\u001b[39m\u001b[33m\"\u001b[39m: scenario_sample_list}\n\u001b[32m    417\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\generate.py:410\u001b[39m, in \u001b[36mTestsetGenerator.generate\u001b[39m\u001b[34m(self, testset_size, query_distribution, num_personas, run_config, batch_size, callbacks, token_usage_parser, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    401\u001b[39m     exec.submit(\n\u001b[32m    402\u001b[39m         scenario.generate_scenarios,\n\u001b[32m    403\u001b[39m         n=splits[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m         callbacks=scenario_generation_grp,\n\u001b[32m    407\u001b[39m     )\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     scenario_sample_list: t.List[t.List[BaseScenario]] = \u001b[43mexec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    412\u001b[39m     scenario_generation_rm.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:213\u001b[39m, in \u001b[36mExecutor.results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m             nest_asyncio.apply()\n\u001b[32m    211\u001b[39m             \u001b[38;5;28mself\u001b[39m._nest_asyncio_applied = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m results = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m sorted_results = \u001b[38;5;28msorted\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:141\u001b[39m, in \u001b[36mExecutor._process_jobs\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pbar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    137\u001b[39m         total=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.jobs),\n\u001b[32m    138\u001b[39m         desc=\u001b[38;5;28mself\u001b[39m.desc,\n\u001b[32m    139\u001b[39m         disable=\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.show_progress,\n\u001b[32m    140\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m internal_pbar:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_coroutines(\n\u001b[32m    142\u001b[39m             \u001b[38;5;28mself\u001b[39m.jobs, internal_pbar, results, max_workers\n\u001b[32m    143\u001b[39m         )\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_coroutines(\n\u001b[32m    146\u001b[39m         \u001b[38;5;28mself\u001b[39m.jobs, \u001b[38;5;28mself\u001b[39m.pbar, results, max_workers\n\u001b[32m    147\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:191\u001b[39m, in \u001b[36mExecutor._process_coroutines\u001b[39m\u001b[34m(self, jobs, pbar, results, max_workers)\u001b[39m\n\u001b[32m    189\u001b[39m coroutines = [afunc(*args, **kwargs) \u001b[38;5;28;01mfor\u001b[39;00m afunc, args, kwargs, _ \u001b[38;5;129;01min\u001b[39;00m jobs]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m as_completed(coroutines, max_workers):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    192\u001b[39m     results.append(result)\n\u001b[32m    193\u001b[39m     pbar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:631\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:48\u001b[39m, in \u001b[36mas_completed.<locals>.sema_coro\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msema_coro\u001b[39m(coro):\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:100\u001b[39m, in \u001b[36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_exceptions:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    102\u001b[39m         exec_name = \u001b[38;5;28mtype\u001b[39m(e).\u001b[34m__name__\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\executor.py:96\u001b[39m, in \u001b[36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_callable_async\u001b[39m(\n\u001b[32m     93\u001b[39m     *args, **kwargs\n\u001b[32m     94\u001b[39m ) -> t.Tuple[\u001b[38;5;28mint\u001b[39m, t.Callable | \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(*args, **kwargs)\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m counter, result\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\base.py:94\u001b[39m, in \u001b[36mBaseSynthesizer.generate_scenarios\u001b[39m\u001b[34m(self, n, knowledge_graph, persona_list, callbacks)\u001b[39m\n\u001b[32m     88\u001b[39m callbacks = callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m     89\u001b[39m scenario_generation_rm, scenario_generation_group = new_group(\n\u001b[32m     90\u001b[39m     name=\u001b[38;5;28mself\u001b[39m.name,\n\u001b[32m     91\u001b[39m     inputs={\u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n, \u001b[33m\"\u001b[39m\u001b[33mknowledge_graph\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(knowledge_graph)},\n\u001b[32m     92\u001b[39m     callbacks=callbacks,\n\u001b[32m     93\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m scenarios = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_scenarios(\n\u001b[32m     95\u001b[39m     n, knowledge_graph, persona_list, scenario_generation_group\n\u001b[32m     96\u001b[39m )\n\u001b[32m     97\u001b[39m scenario_generation_rm.on_chain_end(outputs={\u001b[33m\"\u001b[39m\u001b[33mscenarios\u001b[39m\u001b[33m\"\u001b[39m: scenarios})\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scenarios\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\multi_hop\\abstract.py:79\u001b[39m, in \u001b[36mMultiHopAbstractQuerySynthesizer._generate_scenarios\u001b[39m\u001b[34m(self, n, knowledge_graph, persona_list, callbacks)\u001b[39m\n\u001b[32m     76\u001b[39m scenarios = []\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node_clusters) == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo clusters found in the knowledge graph. Try changing the relationship condition.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m     )\n\u001b[32m     82\u001b[39m num_sample_per_cluster = \u001b[38;5;28mint\u001b[39m(np.ceil(n / \u001b[38;5;28mlen\u001b[39m(node_clusters)))\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m node_clusters:\n",
      "\u001b[31mValueError\u001b[39m: No clusters found in the knowledge graph. Try changing the relationship condition."
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "from ragas.testset import TestsetGenerator\n",
    "loaded_kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "print(loaded_kg)\n",
    "\n",
    "# Initialize persistent disk cache\n",
    "cacher = DiskCacheBackend(cache_dir=r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.cache\\ragas\") # \".cache/ragas\"\n",
    "print(\"Cache entries:\", len(cacher.cache))\n",
    "\n",
    "# Prepare LLM + embedding wrappers with the shared cache\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"), cache=cacher)\n",
    "embedding_model = LangchainEmbeddingsWrapper(OpenAIEmbeddings(), cache=cacher)\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=loaded_kg)\n",
    "\n",
    "query_distribution = default_query_distribution(generator_llm)\n",
    "print(query_distribution)\n",
    "\n",
    "testset = generator.generate(testset_size=8, query_distribution=query_distribution)\n",
    "testset.to_pandas()\n",
    "\n",
    "testset.to_csv('multihoptest.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1466526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#clusters found: 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No multi-hop clusters found; generation will be single-hop only",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m clusters = spec.get_node_clusters(kg)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m#clusters found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(clusters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > \u001b[32m0\u001b[39m, \\\n\u001b[32m     11\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mNo multi-hop clusters found; generation will be single-hop only\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: No multi-hop clusters found; generation will be single-hop only"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    "    MultiHopSpecificQuerySynthesizer,\n",
    "    MultiHopAbstractQuerySynthesizer,\n",
    ")\n",
    "spec = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "clusters = spec.get_node_clusters(kg)\n",
    "print(f\"#clusters found: {len(clusters)}\")\n",
    "\n",
    "assert len(MultiHopSpecificQuerySynthesizer(generator_llm).get_node_clusters(loaded_kg)) > 0, \\\n",
    "       \"No multi-hop clusters found; generation will be single-hop only\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a23390f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 4844, relationships: 8052)\n",
      "#valid clusters: 0\n",
      "#nodes without embedding: 4407\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset.synthesizers import MultiHopSpecificQuerySynthesizer\n",
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\full_knowledge_graph_fourth_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "#print(\"edges by type:\", {r.relation_type for r in kg.relationships})\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "missing_embed = sum(1 for n in kg.nodes if \"summary_embedding\" not in n.properties)\n",
    "print(\"#nodes without embedding:\", missing_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67323031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 255, relationships: 944)\n",
      "#valid clusters: 538\n",
      "❗ Nodes with missing or invalid summaries: 223\n",
      "❗ Nodes missing embeddings: 223\n",
      "✅ Total nodes: 255\n",
      "❗ Content nodes with missing/invalid summary: 223\n",
      "❗ Content nodes missing embeddings: 223\n",
      "✅ Total content nodes: 255\n"
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(r\"C:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\output\\knowledge_graph_pilot_test.json\")\n",
    "print(kg)\n",
    "\n",
    "synth = MultiHopSpecificQuerySynthesizer(generator_llm)\n",
    "\n",
    "clusters = synth.get_node_clusters(kg)\n",
    "print(\"#valid clusters:\", len(clusters))\n",
    "\n",
    "\n",
    "broken = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in kg.nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Nodes with missing or invalid summaries: {len(broken)}\")\n",
    "\n",
    "print(f\"❗ Nodes missing embeddings: {len(no_emb)}\")\n",
    "\n",
    "\n",
    "print(f\"✅ Total nodes: {len(kg.nodes)}\")\n",
    "\n",
    "content_nodes = [n for n in kg.nodes if \"page_content\" in n.properties]\n",
    "\n",
    "broken = [n for n in content_nodes if not isinstance(n.properties.get(\"summary\"), str)]\n",
    "no_emb = [n for n in content_nodes if not isinstance(n.properties.get(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Content nodes with missing/invalid summary: {len(broken)}\")\n",
    "print(f\"❗ Content nodes missing embeddings: {len(no_emb)}\")\n",
    "print(f\"✅ Total content nodes: {len(content_nodes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae34779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraph(nodes: 1606, relationships: 20974)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 1500 cluster(s)\n",
      "✅ LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)) → 12700 cluster(s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m synth \u001b[38;5;129;01min\u001b[39;00m [SingleHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      5\u001b[39m               MultiHopSpecificQuerySynthesizer(generator_llm),\n\u001b[32m      6\u001b[39m               MultiHopAbstractQuerySynthesizer(generator_llm)]:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         count = \u001b[38;5;28mlen\u001b[39m(\u001b[43msynth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_node_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msynth.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cluster(s)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\multi_hop\\abstract.py:47\u001b[39m, in \u001b[36mMultiHopAbstractQuerySynthesizer.get_node_clusters\u001b[39m\u001b[34m(self, knowledge_graph)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_node_clusters\u001b[39m(\u001b[38;5;28mself\u001b[39m, knowledge_graph: KnowledgeGraph) -> t.List[t.Set[Node]]:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     node_clusters = \u001b[43mknowledge_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_indirect_clusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelationship_condition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_property\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummary_similarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdepth_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mfound \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m clusters\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(node_clusters))\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m node_clusters\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:306\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters\u001b[39m\u001b[34m(self, relationship_condition, depth_limit)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nodes:\n\u001b[32m    305\u001b[39m     initial_cluster = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;66;03m# Remove duplicates by converting clusters to frozensets\u001b[39;00m\n\u001b[32m    309\u001b[39m unique_clusters = [\n\u001b[32m    310\u001b[39m     \u001b[38;5;28mset\u001b[39m(cluster) \u001b[38;5;28;01mfor\u001b[39;00m cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mfrozenset\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m clusters)\n\u001b[32m    311\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:298\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    295\u001b[39m         neighbor = rel.source\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m neighbor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m         \u001b[43mdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Add completed path-based cluster\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cluster) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:288\u001b[39m, in \u001b[36mKnowledgeGraph.find_indirect_clusters.<locals>.dfs\u001b[39m\u001b[34m(node, cluster, depth, path)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rel \u001b[38;5;129;01min\u001b[39;00m relationships:\n\u001b[32m    287\u001b[39m     neighbor = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m rel.target \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[32m    289\u001b[39m         neighbor = rel.target\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    291\u001b[39m         rel.bidirectional\n\u001b[32m    292\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.target == node\n\u001b[32m    293\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m rel.source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m cluster\n\u001b[32m    294\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\ragas\\testset\\graph.py:82\u001b[39m, in \u001b[36mNode.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m.id)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;28mobject\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Node):\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.id == other.id\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "kg = KnowledgeGraph.load(r\"..\\output\\knowledge_graph_split_01_100.json\")\n",
    "print(kg)\n",
    "\n",
    "for synth in [SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "              MultiHopAbstractQuerySynthesizer(generator_llm)]: # could this be failing because i run out of ram? size is 166600 for the succeeding graph\n",
    "    try:\n",
    "        count = len(synth.get_node_clusters(kg))\n",
    "        print(f\"✅ {synth.name} → {count} cluster(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {synth.name} → error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca1a39c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Node' object has no attribute 'property'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m broken = [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m kg.nodes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproperty\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mstr\u001b[39m)]\n\u001b[32m      2\u001b[39m missing_embed = [n \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m kg.nodes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n.property(\u001b[33m\"\u001b[39m\u001b[33msummary_embedding\u001b[39m\u001b[33m\"\u001b[39m), \u001b[38;5;28mlist\u001b[39m)]\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m❗ Nodes with invalid/missing summaries: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(broken)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\au644610\\OneDrive - Aarhus universitet\\Desktop\\Thesis-Trustworthy-RAG\\.venv\\Lib\\site-packages\\pydantic\\main.py:994\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Node' object has no attribute 'property'"
     ]
    }
   ],
   "source": [
    "broken = [n for n in kg.nodes if not isinstance(n.property(\"summary\"), str)]\n",
    "missing_embed = [n for n in kg.nodes if not isinstance(n.property(\"summary_embedding\"), list)]\n",
    "\n",
    "print(f\"❗ Nodes with invalid/missing summaries: {len(broken)}\")\n",
    "print(f\"❗ Nodes with missing embeddings: {len(missing_embed)}\")\n",
    "print(f\"✅ Total nodes: {len(kg.nodes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1725c0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No clusters found for LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)); skipping.\n"
     ]
    }
   ],
   "source": [
    "query_distribution = []\n",
    "synths = [\n",
    "    SingleHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopSpecificQuerySynthesizer(generator_llm),\n",
    "    MultiHopAbstractQuerySynthesizer(generator_llm)\n",
    "]\n",
    "\n",
    "for s in synths:\n",
    "    try:\n",
    "        clusters = s.get_node_clusters(kg)\n",
    "        if clusters:\n",
    "            query_distribution.append((s, 1.0))  # weight can be adjusted\n",
    "        else:\n",
    "            print(f\"⚠️ No clusters found for {s.name}; skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Synthesizer {s.name} failed: {e}\")\n",
    "\n",
    "# re-normalise weights\n",
    "total = sum(w for _, w in query_distribution)\n",
    "query_distribution = [(s, w/total) for s, w in query_distribution]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    query_distribution = [\n",
    "        (\n",
    "            SingleHopSpecificQuerySynthesizer(llm=generator_llm, property_name=\"headlines\"),\n",
    "            0.5,\n",
    "        ),\n",
    "        (\n",
    "            SingleHopSpecificQuerySynthesizer(\n",
    "                llm=generator_llm, property_name=\"keyphrases\"\n",
    "            ),\n",
    "            0.5,\n",
    "        ),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc902d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the split version, contains some abstract\n",
    "df = pd.read_csv(\"../output/raw_test_data/full_testset_fourth_test.csv\") # the 200 single hop\n",
    "df1 = pd.read_csv(\"../output/testset_multi_all.csv\")  # the 50 multihop from all data\n",
    "df2 = pd.read_csv(\"../output/testset_all_150.csv\")  # the 150 multihop from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41818002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does an electroencephalogram play in...</td>\n",
       "      <td>['What is computational modeling of behavioral...</td>\n",
       "      <td>An electroencephalogram (EEG) is used in laten...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some key considerations for designing...</td>\n",
       "      <td>['Design a good experiment! Computational mode...</td>\n",
       "      <td>When designing a good experiment in computatio...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do Courville and Daw contribute to the und...</td>\n",
       "      <td>['good models Just as bad experiments can limi...</td>\n",
       "      <td>Courville and Daw are referenced in the contex...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does Daw et al. contribute to the underst...</td>\n",
       "      <td>['Simulate, simulate, simulate! Once you have ...</td>\n",
       "      <td>Daw et al. (2011) emphasize the importance of ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can Matlab's fmincon function assist in th...</td>\n",
       "      <td>['Fit the parameters A key component of comput...</td>\n",
       "      <td>Matlab's fmincon function can assist in the ma...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What does the refinement paradox reveal about ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nRESEARCH ARTICLE The refinement p...</td>\n",
       "      <td>The refinement paradox reveals that while refi...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>How do the dynamics of scientific discovery an...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nRESEARCH ARTICLE Scientific disco...</td>\n",
       "      <td>The dynamics of scientific discovery and cultu...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>How do social roles and leadership effectivene...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nSOCIAL ROLES AND THE EVOLUTION OF...</td>\n",
       "      <td>The evolution of networks in isolated environm...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>How does the deluge of papers published annual...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nSlowed canonical progress in larg...</td>\n",
       "      <td>The deluge of papers published annually can im...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>How does psychometric network analysis utilize...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nIn many scientific fields, resear...</td>\n",
       "      <td>Psychometric network analysis employs network ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What role does an electroencephalogram play in...   \n",
       "1   What are some key considerations for designing...   \n",
       "2   How do Courville and Daw contribute to the und...   \n",
       "3   What does Daw et al. contribute to the underst...   \n",
       "4   How can Matlab's fmincon function assist in th...   \n",
       "..                                                ...   \n",
       "45  What does the refinement paradox reveal about ...   \n",
       "46  How do the dynamics of scientific discovery an...   \n",
       "47  How do social roles and leadership effectivene...   \n",
       "48  How does the deluge of papers published annual...   \n",
       "49  How does psychometric network analysis utilize...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   ['What is computational modeling of behavioral...   \n",
       "1   ['Design a good experiment! Computational mode...   \n",
       "2   ['good models Just as bad experiments can limi...   \n",
       "3   ['Simulate, simulate, simulate! Once you have ...   \n",
       "4   ['Fit the parameters A key component of comput...   \n",
       "..                                                ...   \n",
       "45  ['<1-hop>\\n\\nRESEARCH ARTICLE The refinement p...   \n",
       "46  ['<1-hop>\\n\\nRESEARCH ARTICLE Scientific disco...   \n",
       "47  ['<1-hop>\\n\\nSOCIAL ROLES AND THE EVOLUTION OF...   \n",
       "48  ['<1-hop>\\n\\nSlowed canonical progress in larg...   \n",
       "49  ['<1-hop>\\n\\nIn many scientific fields, resear...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   An electroencephalogram (EEG) is used in laten...   \n",
       "1   When designing a good experiment in computatio...   \n",
       "2   Courville and Daw are referenced in the contex...   \n",
       "3   Daw et al. (2011) emphasize the importance of ...   \n",
       "4   Matlab's fmincon function can assist in the ma...   \n",
       "..                                                ...   \n",
       "45  The refinement paradox reveals that while refi...   \n",
       "46  The dynamics of scientific discovery and cultu...   \n",
       "47  The evolution of networks in isolated environm...   \n",
       "48  The deluge of papers published annually can im...   \n",
       "49  Psychometric network analysis employs network ...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "..                                   ...  \n",
       "45  multi_hop_abstract_query_synthesizer  \n",
       "46  multi_hop_abstract_query_synthesizer  \n",
       "47  multi_hop_abstract_query_synthesizer  \n",
       "48  multi_hop_abstract_query_synthesizer  \n",
       "49  multi_hop_abstract_query_synthesizer  \n",
       "\n",
       "[251 rows x 4 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge the single and multi hop files for the gpu run:\n",
    "df = pd.read_csv(\"../output/raw_test_data/testset_pilot_100.csv\") # 50 without abstract'\n",
    "df1 = pd.read_csv(\"../output/raw_test_data/testset_split_00_100.csv\")  # 50 with abstract\n",
    "df2 = pd.read_csv(\"../output/raw_test_data/testset_split_01_100.csv\")  # 50 with abstract\n",
    "df3 = pd.read_csv(\"../output/raw_test_data/testset_split_02_100.csv\")  # 50 with abstract\n",
    "df4 = pd.read_csv(\"../output/raw_test_data/testset_split_03_22.csv\")  # 22 with abstract\n",
    "\n",
    "testset_merged = pd.concat([df,df1,df2,df3,df4])\n",
    "testset_merged.to_csv('split_testset_merged.csv', index = False)\n",
    "testset_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d8155f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How does the temperature of a pizza stone affe...</td>\n",
       "      <td>['This chapter introduces core modeling concep...</td>\n",
       "      <td>Understanding how pizza stones heat up is cruc...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What are the key factors that influence the ti...</td>\n",
       "      <td>['needed to reach a target temperature: To und...</td>\n",
       "      <td>The time needed to reach the target temperatur...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>How does computational modeling contribute to ...</td>\n",
       "      <td>['What is computational modeling of behavioral...</td>\n",
       "      <td>Computational modeling in behavioral science a...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does Anne Collins play in the contex...</td>\n",
       "      <td>['Design a good experiment! Computational mode...</td>\n",
       "      <td>Anne Collins is associated with providing illu...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>How do Courville and Daw contribute to the und...</td>\n",
       "      <td>['good models Just as bad experiments can limi...</td>\n",
       "      <td>Courville and Daw are referenced in the contex...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the significance of Levene's test in a...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n5.7. Testing for homogeneity of v...</td>\n",
       "      <td>Levene's test is significant in assessing homo...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does Bremner's research play in unde...</td>\n",
       "      <td>[\"&lt;1-hop&gt;\\n\\nA direction. This is a level of c...</td>\n",
       "      <td>Bremner's research is pivotal in understanding...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does the hippocampus play in memory ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe nature of child-directed spee...</td>\n",
       "      <td>The hippocampus plays a crucial role in the st...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>What does Figure 1.1 show about the relationsh...</td>\n",
       "      <td>[\"&lt;1-hop&gt;\\n\\n= 'lm') + ggtitle('Log RT ~ raw f...</td>\n",
       "      <td>Figure 1.1 illustrates that wage increases wit...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1</td>\n",
       "      <td>How does centering a predictor variable affect...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n6.3. Assessing Assumptions The cu...</td>\n",
       "      <td>Centering a predictor variable involves subtra...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     annotation                                         user_input  \\\n",
       "0             1  How does the temperature of a pizza stone affe...   \n",
       "1             1  What are the key factors that influence the ti...   \n",
       "2             1  How does computational modeling contribute to ...   \n",
       "3             1  What role does Anne Collins play in the contex...   \n",
       "4             1  How do Courville and Daw contribute to the und...   \n",
       "..          ...                                                ...   \n",
       "395           1  What is the significance of Levene's test in a...   \n",
       "396           1  What role does Bremner's research play in unde...   \n",
       "397           1  What role does the hippocampus play in memory ...   \n",
       "398           0  What does Figure 1.1 show about the relationsh...   \n",
       "399           1  How does centering a predictor variable affect...   \n",
       "\n",
       "                                    reference_contexts  \\\n",
       "0    ['This chapter introduces core modeling concep...   \n",
       "1    ['needed to reach a target temperature: To und...   \n",
       "2    ['What is computational modeling of behavioral...   \n",
       "3    ['Design a good experiment! Computational mode...   \n",
       "4    ['good models Just as bad experiments can limi...   \n",
       "..                                                 ...   \n",
       "395  ['<1-hop>\\n\\n5.7. Testing for homogeneity of v...   \n",
       "396  [\"<1-hop>\\n\\nA direction. This is a level of c...   \n",
       "397  ['<1-hop>\\n\\nThe nature of child-directed spee...   \n",
       "398  [\"<1-hop>\\n\\n= 'lm') + ggtitle('Log RT ~ raw f...   \n",
       "399  ['<1-hop>\\n\\n6.3. Assessing Assumptions The cu...   \n",
       "\n",
       "                                             reference  \\\n",
       "0    Understanding how pizza stones heat up is cruc...   \n",
       "1    The time needed to reach the target temperatur...   \n",
       "2    Computational modeling in behavioral science a...   \n",
       "3    Anne Collins is associated with providing illu...   \n",
       "4    Courville and Daw are referenced in the contex...   \n",
       "..                                                 ...   \n",
       "395  Levene's test is significant in assessing homo...   \n",
       "396  Bremner's research is pivotal in understanding...   \n",
       "397  The hippocampus plays a crucial role in the st...   \n",
       "398  Figure 1.1 illustrates that wage increases wit...   \n",
       "399  Centering a predictor variable involves subtra...   \n",
       "\n",
       "                         synthesizer_name  \n",
       "0    single_hop_specifc_query_synthesizer  \n",
       "1    single_hop_specifc_query_synthesizer  \n",
       "2    single_hop_specifc_query_synthesizer  \n",
       "3    single_hop_specifc_query_synthesizer  \n",
       "4    single_hop_specifc_query_synthesizer  \n",
       "..                                    ...  \n",
       "395  multi_hop_specific_query_synthesizer  \n",
       "396  multi_hop_specific_query_synthesizer  \n",
       "397  multi_hop_specific_query_synthesizer  \n",
       "398  multi_hop_specific_query_synthesizer  \n",
       "399  multi_hop_specific_query_synthesizer  \n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../output/annotated_merged_testset_3.csv\", sep = ';') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37f6a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../output/annotated_merged_testset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "813473e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../output/raw_test_data/anno_split_testset_merged.csv\", sep = ';') \n",
    "df_filtered = df.loc[df['annotation'] == 1]\n",
    "df_filtered.to_csv(\"../output/filt_anno_split_testset_merged.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f447ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>How does the temperature of a pizza stone affe...</td>\n",
       "      <td>['This chapter introduces core modeling concep...</td>\n",
       "      <td>Understanding how pizza stones heat up is cruc...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What are the key factors that influence the ti...</td>\n",
       "      <td>['needed to reach a target temperature: To und...</td>\n",
       "      <td>The time needed to reach the target temperatur...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>How does computational modeling contribute to ...</td>\n",
       "      <td>['What is computational modeling of behavioral...</td>\n",
       "      <td>Computational modeling in behavioral science a...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does Anne Collins play in the contex...</td>\n",
       "      <td>['Design a good experiment! Computational mode...</td>\n",
       "      <td>Anne Collins is associated with providing illu...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>How do Courville and Daw contribute to the und...</td>\n",
       "      <td>['good models Just as bad experiments can limi...</td>\n",
       "      <td>Courville and Daw are referenced in the contex...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1</td>\n",
       "      <td>What insights do Berk et al. and Bell et al. p...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nthe proposed iterative workflow i...</td>\n",
       "      <td>Berk et al. discuss the importance of understa...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the significance of Levene's test in a...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n5.7. Testing for homogeneity of v...</td>\n",
       "      <td>Levene's test is significant in assessing homo...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does Bremner's research play in unde...</td>\n",
       "      <td>[\"&lt;1-hop&gt;\\n\\nA direction. This is a level of c...</td>\n",
       "      <td>Bremner's research is pivotal in understanding...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>What role does the hippocampus play in memory ...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\nThe nature of child-directed spee...</td>\n",
       "      <td>The hippocampus plays a crucial role in the st...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>How does centering a predictor variable affect...</td>\n",
       "      <td>['&lt;1-hop&gt;\\n\\n6.3. Assessing Assumptions The cu...</td>\n",
       "      <td>Centering a predictor variable involves subtra...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     annotation                                         user_input  \\\n",
       "0             1  How does the temperature of a pizza stone affe...   \n",
       "1             1  What are the key factors that influence the ti...   \n",
       "2             1  How does computational modeling contribute to ...   \n",
       "3             1  What role does Anne Collins play in the contex...   \n",
       "4             1  How do Courville and Daw contribute to the und...   \n",
       "..          ...                                                ...   \n",
       "281           1  What insights do Berk et al. and Bell et al. p...   \n",
       "282           1  What is the significance of Levene's test in a...   \n",
       "283           1  What role does Bremner's research play in unde...   \n",
       "284           1  What role does the hippocampus play in memory ...   \n",
       "285           1  How does centering a predictor variable affect...   \n",
       "\n",
       "                                    reference_contexts  \\\n",
       "0    ['This chapter introduces core modeling concep...   \n",
       "1    ['needed to reach a target temperature: To und...   \n",
       "2    ['What is computational modeling of behavioral...   \n",
       "3    ['Design a good experiment! Computational mode...   \n",
       "4    ['good models Just as bad experiments can limi...   \n",
       "..                                                 ...   \n",
       "281  ['<1-hop>\\n\\nthe proposed iterative workflow i...   \n",
       "282  ['<1-hop>\\n\\n5.7. Testing for homogeneity of v...   \n",
       "283  [\"<1-hop>\\n\\nA direction. This is a level of c...   \n",
       "284  ['<1-hop>\\n\\nThe nature of child-directed spee...   \n",
       "285  ['<1-hop>\\n\\n6.3. Assessing Assumptions The cu...   \n",
       "\n",
       "                                             reference  \\\n",
       "0    Understanding how pizza stones heat up is cruc...   \n",
       "1    The time needed to reach the target temperatur...   \n",
       "2    Computational modeling in behavioral science a...   \n",
       "3    Anne Collins is associated with providing illu...   \n",
       "4    Courville and Daw are referenced in the contex...   \n",
       "..                                                 ...   \n",
       "281  Berk et al. discuss the importance of understa...   \n",
       "282  Levene's test is significant in assessing homo...   \n",
       "283  Bremner's research is pivotal in understanding...   \n",
       "284  The hippocampus plays a crucial role in the st...   \n",
       "285  Centering a predictor variable involves subtra...   \n",
       "\n",
       "                         synthesizer_name  \n",
       "0    single_hop_specifc_query_synthesizer  \n",
       "1    single_hop_specifc_query_synthesizer  \n",
       "2    single_hop_specifc_query_synthesizer  \n",
       "3    single_hop_specifc_query_synthesizer  \n",
       "4    single_hop_specifc_query_synthesizer  \n",
       "..                                    ...  \n",
       "281  multi_hop_specific_query_synthesizer  \n",
       "282  multi_hop_specific_query_synthesizer  \n",
       "283  multi_hop_specific_query_synthesizer  \n",
       "284  multi_hop_specific_query_synthesizer  \n",
       "285  multi_hop_specific_query_synthesizer  \n",
       "\n",
       "[286 rows x 5 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../output/filt_anno_merged_testset.csv\") \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
