# config.yaml
retrieval:
  hybrid:
    Î±: 0.5                   # weight for dense vs. sparse
    sparse_k: 20            # how many BM25 hits to keep
    dense_k: 20             # how many vector hits to keep
    rerank_k: 10            # how many to send to reranker
    reranker_model:         # HuggingFace cross-encoder
      id: "cross-encoder/ms-marco-MiniLM-L-6-v2"
      batch_size: 16


model:
  type: "chatui"    # or "hf"
  hf_model: "https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct"
  chatui_model: "llama3.2:1b"

generation:
  n_samples: 5       # number of samples to draw; 0 triggers dummy/demo
  top_k: 5           # number of docs to retrieve
  temperature: 0.9   # sampling temperature
  top_p: 0.9         # nucleus sampling
  max_new_tokens: 150

uncertainty:
  method: "lexical_similarity"
  lexical_similarity:
    metric: "rougeL"
  deg_mat:
    batch_size: 10
    device: "cpu"
    affinity: "entail"
    verbose: True
  eccentricity:
    similarity_score: "NLI_score"
    affinity: "entail"
    verbose: True
    thres: 0.9